Issue,Storypoint
"HDFS ItemWriter
Base integration of core HDFS writer functionality with Spring Batch.",1
"HDFS Core writing helper classes
Simple file writer that has existed in the spring hadoop samples.",1
"Channel Registry
nan",3
"Tuple data structure
The tuple data structure should be backward compatible in functionality for use in spring batch.  Porting over FieldSet tests in spring batch to use the tuple data structure is one way help ensure that compatibility.",1
"Syslog Ingestion
Have a syslog.xml config file that can be added to a module and registered with a module registry.",1
"Basic implementation of a reactor based tcp server
nan",13
"Reactor based http ingestion
When there is support for boostrapping a http server in the reactor project, and inbound SI adapter and associated XD source module should be created.",5
"Initial work that uses the module architecture from DIRT 
nan",5
"Jolokia based aggregator for cluster monitoring
nan",8
"Tail file channel adapters
nan",8
"Design for deploying XD on EC2
create enough of a design to develop additional stories.",3
"Websocket based taps
nan",5
"DIRT Runtime that deploys an application context across multiple nodes using redis.
nan",3
"add file source and sink modules
nan",1
"Create pipes and filters DSL for ingestion
Initial simple handcoded implementation for straight through pipe and filter model, e.g. a | b | c",2
"Basic Performance Test For syslog injestion
nan",20
"Gemfire CQ module for ingestion
nan",1
"Create simple gague service
A gauge just stores a number.  Implementations for in-memory and redis.",1
"Create rich gauge service
A rich gauge stores a number and also rmd, min, max. Implementations for in-memory and redis.",5
"Create a simple counter service
A simple counters can increment/decrement a number.  Implementations for in-memory and redis.",1
"Create field-value counters
A field-value counter is useful for bar chart graphs, Strings on x-axis and count on y-axis.  Maps well to zset in redis. Implementations for in-memory and redis.",5
"Create CI process for XD build
bamboo based",1
"SI Outbound HDFS Channel Adapter
nan",3
"Gradle based multi-project build
multi project build. - look to Spring Framework for source of starting point.",2
"Build script that creates an executable server as an artifact
Gradle application plugin is a good starting point.  this should be the main server that would host SI based modules to do syslog->file ingestion (as an example)",2
"Add JUnit @Rule so Tests Fail Fast with Clear Messaging if Redis Not Available
nan",1
"Metric repositories should support Spring Data CrudRepository interface
This provides common CRUD behavior and a shared interface that can be useful in testing scenarios.  ",5
"Redis based repositories should use a NamingStrategy class to calculate the name of the key to use for persistence
RedisCounterRepository and RedisGaugeRepository have duplicated code that needs to be factored out into a one place.  One such duplication is the determination of the key name to use for persistence.  This should be abstracted out into a strategy helper class.",1
"Remove the expiry of keys in Redis based repositories
There is duplicated code in Redis based repositories that related to expiry behavior, move into a common shared helper class and/or base class.",1
"HDFS sink module
nan",1
"Parameterizable streams
nan",2
"Move Redis Queue Channel Adapters into spring-integration-redis
Currently these implementations are in the spring-xd-dirt module, but they should be moved into spring-integration-redis. We are already depending upon Spring Integration 3.0 snapshots since the ChannelRegistry implementation is not yet at a milestone, so this should be okay for the Redis Channel Adapters also - until Spring Integration M2 is released.",3
"Add tap support to DIRT
syntax:    {code}  tap @ somechannel --key=value | somecounter  {code}  ",2
"Add xd.stream.name property in StreamPlugin
nan",1
"add twitter search source module
nan",1
"Design and document desired high level DSL for configuring data processing in XD
Start to explore how the DSL can cover both advanced (non-linear) spring integration flows as well as spring batch jobs.",13
"XD Metrics backed Message Counter
A Spring Integration based @ServiceActivator that counts the number of messages using the Spring XD metrics support",1
"SI ServiceActivator for an XD Metrics backed Field Value Counter
A Spring Integration based @ServiceActivator that counts the occurrence of field names, from either a tuple data structure or a POJO, using the Spring XD metrics support.",3
"Switch to use Lettuce driver for Redis
Replace the use of Jedis with Lettuce as it has higher performance",2
"add counter module
nan",1
"build.gradle doesn't handle a small handful of libraries
Trying to build spring-xd for the first resulted in lots of errors inside STS (I had an empty .m2 repo).",1
"Tuple should support storing nested tuples
Nested tuple structures shoudl be supported,  getTuple(int index), getTuple(String name)",1
"Saving a metric (Counter, Gauge..) with an existing name should throw an exception
The difference between saving a new metric and updating an existing one needs to be defined.  Suggest that if we try to save when an existing counter is already in the database to throw exception, such as DataIntegrityViolationException.",1
"Create distributable artifact that contains server application and start/stop scripts
The gradle application task should get us most of the way to create a distributable artifact akin to what you see when downloading tomcat/jetty etc.    Now there is a launch task    task(launch, dependsOn: 'classes', type: JavaExec) {  		main = 'org.springframework.xd.dirt.stream.StreamServer'  		classpath = sourceSets.test.runtimeClasspath  	}      The same main should be referenced in the application plugin, a task to create a .zip distributable is needed.    Ideally would be nice to   1. download .zip  2. unzip  3. cd spring-xd/bin  4. xdserver start      and gracefully shutdown later with     5. xdserver stop    I don't know if we can/should bundle redis, I think we should bundle it.    The scripts can be for unix/linux and for windows.      Discuss a brew based install as well.  ",8
"Use the tuple data structure to process data in a spring batch step 
Do not require a POJO in order to do end-to-end processing in a batch step.",5
"Document tuple data structure on XD wiki
nan",2
"Gemfire Sink to update a gemfire cache.
Update a gemfire region.",2
"Gauge and Counter hash and equals should not depend on values
The value is mutable so this causes problems if storing metrics in a HashSet, for example.",1
"Submit a brew-based install for Spring XD
- Host the Spring XD distributable zip somewhere that is accessible by external http request.  - Create brew formula for Spring XD install while specifying redis as dependency.   - starting up stream server upon successful brew install    couple of questions:  - should we name the brew task springxd? (name not taken yet)  - should we start the stream server as part of the brew install process?  - should we specify redis as a recommended dependency? user can pass in 'brew install springxd --without-redis' to skip redis installation. by default, 'brew install springxd' will install redis as well.",8
"Export of data from HDFS to a relational database
Based on a single process running a Spring Batch job, support the ETL of data from HDFS to a RDBMS",8
"Export of data from HDFS to MongoDB
Based on a single process Spring Batch job, ETL of data from HDFS to MongoDB.",5
"Create general structure for AsciiDoc based wiki and Spring XD guide.
Adopt Asciidoc as the markdown syntax, useful for generating pdf and more feature rich than standard github flavored markdown.  Loosely following the conventions of https://community.jboss.org/wiki/TheHolyGrailAsciiDocOnGitHubToDocBookTrain that have generate docbook/pdf docs from the Asciidoc wiki.    The asciidoctor project is a key element in the adoption of AsciiDoc for use as the format in github, it is the rendering engine used by github for AsciiDoc.  See http://asciidoctor.org/docs/asciidoc-writers-guide/ for guidance.    ",2
"Remove UUID from Tuple class or replace with more efficient implementation
The Java UUID class is known not to be the fasted implementation available.     See https://github.com/stephenc/eaio-uuid and http://mvnrepository.com/artifact/com.eaio.uuid/uuid for high perf impls.  ",1
"Provide a http source
stream should be able to ingest data from http ",5
"add test to start/stop stream server
in additional to existing tests that check for redis connection, we need to add tests that start/stop stream server. ",2
"Create XD module for tail file adapter
nan",1
"Add redis bundle to distribution zip file
for linux and mac",5
"Add gemfire-server application to the distribution zip of the project spring-xd-gemfire-server
nan",1
"Documentation for ""http | file"" processing
Put on the guide as a section in an 'input sources' wiki page.    https://github.com/springsource/spring-xd/wiki/GuideGettingStarted    ",1
"End user guide for data streams
Put on the guide as a section in an 'streams' wiki page.    End user focused, no need to mention spring underpinning, impl details.    ",5
"Documentation for ""http | hdfs"" processing
Put on the guide as a section in an 'input-stream' wiki page.      ",1
"Documentation for ""syslog | file"" processing
Put on the guide as a section in an 'input-stream' wiki page.      ",3
"Documentation for ""tail | file"" processing
Put on the guide as a section in an 'input-stream' wiki page.      ",3
"Documentation for ""tail | hdfs"" processing
Put on the guide as a section in an 'input-stream' wiki page.      ",1
"Documentation for ""http | gemfire"" processing
Put on the guide as a section in an 'input-stream' wiki page.      ",3
"Documentation for ""gemfirecq | file"" processing
Put on the guide as a section in an 'input-stream' wiki page.      ",3
"Documentation for ""twittersearch | file"" processing
Put on the guide as a section in an 'input-stream' wiki page.      ",3
"Documentation that introduces taps
Put on the guide as a section in an 'input-stream' wiki page.      ",3
"Documentation for counter taps
Put on the guide as a section in an 'input-stream' wiki page.      ",3
"Documentation for gauge taps
Put on the guide as a section in an 'input-stream' wiki page.      ",3
"Documentation for rich gauge taps
Put on the guide as a section in an 'input-stream' wiki page.      ",3
"Documentation for field value taps
Put on the guide as a section in an 'input-stream' wiki page.      ",3
"Normalize and refactor component packaging decomposition
Normalize and refactor as needed functionality currently included in   - spring-integration-core (LocalChannelRegistry)  - spring-integration-module (Module types upon which Flow are built)  - xd-module (Depend Module types common to DIRT and Spring Integration)  - spring-integration-flow (Flow specific types, namespace support, etc)",5
"Document The ability to use flows in streams
Test and document e.g. ""http | flow1 | flow2 | file""",3
"The ability to override default module path for the plugin or an individual flow
Currently modules are assumed to be in classpath:/META-INF/spring/integration/module/${flow}.xml. To reuse modules defined with DIRT  may require more flexibility.  e.g.,   <int-flow:flow root-path=""file:///dirt/module""/>",3
"The ability to import beans referenced in the main context into a module
This should be a core feature of any Spring based module. The plugin should be able to import explicitly referenced beans. This minimizes potential side effects of making the module a child context and is simpler than declaring a shared context (parent) of the application and the module. Provide namespace support for flow:",3
"The ability to route within streams
A Flow or a processor component may require routing semantics. Currently the stream assumes a single input and output for each module. A Flow may support multiple outputs - Switch routing that is - Recipient list is not currently supported (another subtask?). We need to support semantics like:  a |[output.foo:c,output.bar:d,default:e]",8
"Documentation on the module system and how to contribute new modules
For people who are familiar with Spring/Spring Integration provide documents that show how to add additional input sources/sinks.",3
"Sonar build is failing
https://build.springsource.org/browse/XD-SONAR-34    Caused by: java.lang.ClassNotFoundException: org.sonar.api.Plugin          at org.codehaus.plexus.classworlds.strategy.SelfFirstStrategy.loadClass(SelfFirstStrategy.java:50)          at org.codehaus.plexus.classworlds.realm.ClassRealm.loadClass(ClassRealm.java:244)          at org.codehaus.plexus.classworlds.realm.ClassRealm.loadClass(ClassRealm.java:230)          ... 94 more",1
"Rename Tuple class in spring-xd-tuple
The Tuple classes in Reactor follow the more traditional data structure concept of Tuples, an immutable fixed length sequence of values where each value can have different types.  They are ordered and can often be access by index.    An example in a static language is the Tuple class found in .NET http://msdn.microsoft.com/en-us/library/system.tuple.aspx or in Scala http://www.tutorialspoint.com/scala/scala_tuples.htm    Using this standard definition of a Tuple, they do not support named values.  There is also a different tuple class instance for each length, e.g. Tuple<T1,T2>, Tuple<T1,T2,T3>.    The Tuple class in XD is more like a record or named tuple.     Python has a named tuple concept - http://docs.python.org/2/library/collections.html#collections.namedtuple    and http://stackoverflow.com/questions/1490413/languages-that-allow-named-tuples shows that other languages use the term 'Record' for a 'named tuple' - Haskell, Standard ML, OCaml, and F#.    http://en.wikibooks.org/wiki/F_Sharp_Programming/Tuples_and_Records#Defining_Records    So boiling it all down, to avoid conflicts of names, and also to open up the possibility of using Reactor tuples as keys (instead of strings for names), we should change the name to either NamedTuple or Record.  ATM, there is no direct relationship between Reactor's Tuple and NamedTuple (such as inheritance) and so probably Record is the way to go.      ",2
"Add gradle tasks that build and bundle the redis server
nan",2
"Create XDContainer class to start stream server
Provide optional command line arg to embed the container launcher, aka - xd-admin server.    XDContainer.sh --embeddAdmin",1
"Create XDAdmin server to start container launcher
This will launch the RedisContainerLauncher, in future will be able to select from a variety of middleware options.",1
"Add README to be included in root directory of distribution
should explain basic layout of the distribution",1
"Add LICENSE to be included in root directory of distribution
should contain apache licence",1
"Container server does not log a message that it has started or stopped successfully
$ ./xd-container  processing module 'Module [name=file, type=sink]' from group 'tailtest' with index: 1 processing module 'Module [name=tail, type=source]' from group 'tailtest' with index: 0   Logging of 'processing module' should have log level, time..",1
"Clean shutdown of redis in xd-admin
A ctrl-c of xd-admin results in exception messages about disconnecting from redis.  14:16:07,327 ERROR task-scheduler-1 handler.LoggingHandler:136 - org.springframework.data.redis.RedisSystemException: Redis command interrupted; nested exception is com.lambdaworks.redis.RedisCommandInterruptedException: Command interrupted ",2
"Build script should not package 'spring-xd-dirt' scripts 
We are packaging separate scripts to start XDAdmin and XDContainer.  The Gradle application plugin will generate an unwanted 'spring-xd-dirt' scripts, this should be removed from the bin directory when creating a distribution zip.",1
"Documentation for starting Spring XD servers
nan",1
"Remove use of system property xd.home to define location for install location, rely on environment variable XD_HOME
This is in the AdminMain and ContainerMain.      Can get the environment property directly in java code unless provided explicitly on the command line using --xdHomeDir.",1
"Create final distribution zip across multiple projects
The final directory structure should look like    <install-dir>/xd  <install-dir>/redis  <install-dir>/gemfire    inside the XD directory     /xd/bin - which has xd-container and xd-admin scripts  /xd/lib    inside the gemfire directory  /gemfire/bin - has the gemfire-server script  /gemfire/lib     inside the redis directory is     /redis/redis-latest-v.x.y.z.tar  /redis/README  /readis/install-redis  - script that does the basic 4 commands to install redis.      There should be a gradle task that runs after the distZip task, that will take the contents of different project directories, script diretories and 'redis-binary' directories and creates the final layout for the distribution.",5
"Update XD to Use SI 3.0.0.M2
nan",1
"Add install script for Redis
This assumes the redis source tar is available under $rootDir/redis/redis-2.6.13.tar.gz    The install script does the following:    - Check the platform OS & arch  - unzip the tar, compile the sources",2
"add SpEL 'transform' processor
It should provide an 'expression' param for SpEL and have a default pass-thru of the payload.",1
"add SpEL 'filter' processor
It should provide an 'expression' param for SpEL and have a default value of true (accept everything).",1
"add spring-integration-groovy to container dependencies
This will enable the use of groovy scripts within modules.",1
"replace testsource with time and testsink with log
This should facilitate testing while avoiding any class dependencies. Also, log is a generally useful sink by itself and time is a more interesting source for testing (should accept --interval for the seconds between time messages).",1
"HDFS sink should default to hdfs://localhost:8020
The current default is hdfs://localhost:9000 but most new distributions/installs use 8020",1
"Find and eliminate package-level cycles across XD projects
nan",3
"Add pre-compiled redis distributions for the selective OS platforms
nan",10
"XD scripts need to have spring-integration milestone versions updated
Spring-integration version is changed to 3.0.0.M2 and since we manually create the XD scripts, they still point to the 3.0.0.BUILD-SNAPSHOT version.  As discussed, we also need to have a better strategy on updating the lib directory inside the XD scripts.",2
"XD scripts lib path needs to be dynamic
We currently have the manually created XD scripts. This makes it difficult to maintain as the lib path is error prone with the changes. We need to make sure that the properties such as lib path etc., are dynamically updated.",3
"Clean shutdown of redis in xd-container
Need to shutdown cleanly, no exception messages are shown.  Order of components in the stream should be shut down from 'first to last'  (opposite of creation)",2
"Stream documentation review
nan",2
"Documentation for sources, sinks, modules should define which attributes are required and which optional
This will eventually be supplied by the admin server, but for now write it up by hand in the documentation",2
"Create TCP source module
Based off SI tcp inbound adapter.  This will allow for event forwarding that can select among the existing SI serialized/deserializer options. ",3
"Create TCP sink module
Based off SI tcp inbound adapter.  This will allow for event fowarding.",3
"Documenation for building/starting redis servers
nan",2
"Remove container entry in Redis when the application context event to shutdown the container is fired
nan",3
"Upgrade Lettuce to 2.3.2
nan",2
"Profile support for modules
To allow for groups of beans to be defined or not in the container that runs a module.    When deploying a stream (e.g. via the REST API), it should be possible to also provide profile names. Then those would apply to any modules within that particular stream deployment.",8
"Fail Sonar CI build if there are any package tangles violated.
Similar to what would show up on structure101 reports.",1
"Investigate link checking tool for user guide
Asciidoc/doctor might have one as part of it toolchain",2
"Documentation on XD Architecture
Show overall flow of data in a stream, the server components 'admin' and 'container'.  How modules are deployed.",4
"Documentation that points on how to install hadoop
Pointers to other documentation on how to install hadoop. ",3
"Release Spring XD 1.0 M1
nan",5
"Prepare blog post for M1
nan",5
"Update README.txt to include instructions on how to build
Building XD should not be part of the out first out of the box experience, but we should include some instructions on what targets are available, such as distXD.",2
"Parameterize syslog Source; Add Support for TCP
The syslog source currently is hard-coded to use udp on port 11111.    Need to parameterize the port and provide an option to use TCP.",2
"install-redis script should not use relative path to determine redis source dist
Currently, the install-redis script uses relative path to determine redis source  dist file. Since this is error prone, we need to fix it.",1
"StreamServer Context Lifecycle Issues
The {{ModuleDeployer}} calls {{getBeansOfType}} before the context has had its {{PropertySourcesPlaceholderConfigurer}} attached. This can cause issues with {{FactoryBean}} s with placeholders in constructor args because the unresolved placeholder is used when the {{FactoryBean}} is pre-instantiated to determine the type of object it will serve up.",8
"Create externalized property file to support connectivity to redis
We need to have an externalized property file(under xd/conf/) for the xd-container & admin scripts to use as options. ",3
"Remove use of application plugin for redis project
Currently redis project uses application plugin to bundle distribution. This also includes 'java plugin' which causes java specific build behavior on this project. We should try removing the use of application plugin and use something similar or custom tasks that does the bundling. ",2
"Improve User Experience when Redis is not running
Redis is not running we get a nasty stacktrace:    {code}  ~/dev/git/spring-xd/dist/spring-xd/xd/bin (master)] âž” ./xd-container   13/05/29 16:17:15 INFO support.ClassPathXmlApplicationContext: Refreshing org.springframework.context.support.ClassPathXmlApplicationContext@851052d: startup date [Wed May 29 16:17:15 EDT 2013]; root of context hierarchy  13/05/29 16:17:16 INFO xml.XmlBeanDefinitionReader: Loading XML bean definitions from class path resource [META-INF/spring/launcher.xml]  13/05/29 16:17:16 INFO xml.XmlBeanDefinitionReader: Loading XML bean definitions from class path resource [META-INF/spring/redis.xml]  13/05/29 16:17:17 INFO support.DefaultListableBeanFactory: Pre-instantiating singletons in org.springframework.beans.factory.support.DefaultListableBeanFactory@4934ce4a: defining beans [org.springframework.context.support.PropertySourcesPlaceholderConfigurer#0,redisConnectionFactory,org.springframework.xd.dirt.launcher.RedisContainerLauncher#0]; root of factory hierarchy  13/05/29 16:17:17 INFO support.DefaultListableBeanFactory: Destroying singletons in org.springframework.beans.factory.support.DefaultListableBeanFactory@4934ce4a: defining beans [org.springframework.context.support.PropertySourcesPlaceholderConfigurer#0,redisConnectionFactory,org.springframework.xd.dirt.launcher.RedisContainerLauncher#0]; root of factory hierarchy  Exception in thread ""main"" org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'redisConnectionFactory' defined in class path resource [META-INF/spring/redis.xml]: Invocation of init method failed; nested exception is com.lambdaworks.redis.RedisException: Unable to connect    org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1488)    org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:524)    org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:461)    org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:295)    org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:223)    org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:292)    org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:194)    org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:626)    org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:932)    org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:479)    org.springframework.context.support.ClassPathXmlApplicationContext.<init>(ClassPathXmlApplicationContext.java:139)    org.springframework.context.support.ClassPathXmlApplicationContext.<init>(ClassPathXmlApplicationContext.java:83)    org.springframework.xd.dirt.launcher.RedisContainerLauncher.main(RedisContainerLauncher.java:68)    org.springframework.xd.ContainerMain.main(ContainerMain.java:68)  Caused by: com.lambdaworks.redis.RedisException: Unable to connect    com.lambdaworks.redis.RedisClient.connect(RedisClient.java:176)    com.lambdaworks.redis.RedisClient.connectAsync(RedisClient.java:139)    org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.initConnection(LettuceConnectionFactory.java:108)    org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.afterPropertiesSet(LettuceConnectionFactory.java:86)    org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1547)    org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1485)  	... 13 more  Caused by: java.net.ConnectException: Connection refused: localhost/127.0.0.1:6379    sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)    sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:599)    org.jboss.netty.channel.socket.nio.NioClientBoss.connect(NioClientBoss.java:150)    org.jboss.netty.channel.socket.nio.NioClientBoss.processSelectedKeys(NioClientBoss.java:105)    org.jboss.netty.channel.socket.nio.NioClientBoss.process(NioClientBoss.java:79)    org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312)    org.jboss.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:42)    java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)    java.lang.Thread.run(Thread.java:680)  {code}    I think it would be helpful to provide users with some helpful advice e.g.:    Redis does not seem to be running at 'localhost/127.0.0.1:6379'. Did you start or install Redis? Please see for further help http://foo/bar      ",2
"Create asciidoc toolchain script to create a 'toc2' style html output
Publishing an html version of the guide that uses the 'toc2' style format, table of contents on the left.    Looks like a 'stylesheet factory' http://asciidoctor.org/docs/produce-custom-themes-using-asciidoctor-stylesheet-factory/  needs to be installed.    From the theme showcase, http://themes.asciidoctor.org/preview/, the 'golo' theme has a toc2 style.    In the root of the git repo for the wiki is a build.gradle file that uses the asciidoctor gradle plugin, but it doesn't support using a single file with import as input. (See See https://github.com/asciidoctor/asciidoctor-gradle-plugin/issues/15 )    The mvn plugin does support this, so maybe using mvn is an option or just a bash script.",4
"Publish Spring XD final distribution zip as part of Bamboo artifactory plugin
Currently, Bamboo's gradle artifactory plugin has the artifacts configured to projects target(build) directory 'archives'. We need to have a way to set the final distribution archive as one of the gradle 'configurations' in our build.gradle and refer it inside bamboo artifacts.",2
"Add Redis binaries for Windows
Presently, Spring XD does not ship Windows binaries for Redis. However, Microsoft is actively working [1] on supporting Redis on Windows. You can download Windows Redis binaries from:    https://github.com/MSOpenTech/redis/tree/2.6/bin/release     [1] http://blogs.msdn.com/b/interoperability/archive/2013/04/22/redis-on-windows-stable-and-reliable.aspx",2
"Create rich gauge module
Spring config for rich gauge plus message handler to coerce a numeric or string payload to a double.",5
"create a gauge module
Spring config for simple gauge plus message handler to process message. There is some code common to RichGaugeHandler to coerce the payload to a double that should be refactored for reuse.",5
"Provided console output of started server
Shouldn't we have something like a ContextRefreshedEvent Listener and output some informational messages to the console, so the user knows the Container is up (Which contexts. Maybe even print a link to the docs))? Maybe even some simple ascii art (for demos)? Right now it looks somewhat barren.    Redis provides something similar.    This may even go hand in hand to provided a better configuration model (storing common config parameters centrally)      {code}     _____            _              __   _______      / ____|          (_)             \ \ / /  __ \    | (___  _ __  _ __ _ _ __   __ _   \ V /| |  | |    \___ \| '_ \| '__| | '_ \ / _` |   > < | |  | |    ____) | |_) | |  | | | | | (_| |  / . \| |__| |   |_____/| .__/|_|  |_|_| |_|\__, | /_/ \_\_____/           | |                  __/ |                         |_|  v1.0.0.M1      |___/   eXtreme Data    Using Redis at localhost:6379      The Server (PID: 12345) is now ready on http://myserver:123/streams    Documentation: https://github.com/SpringSource/spring-xd/wiki           {code}",2
"Add a groovy script processor module
A processor module that accepts either the location of a groovy script resource or an inline script (string). Also some discussion about a default classpath location for scripts. ",5
"Create config support for Redis
We would like to have Redis driven from a config property file under XD_HOME.",2
"Verify use of JMX managed bean to shutdown cleanly the xd-admin and xd-container servers
In both cases there are multiple application contexts that can be running in the process.  The JMX Managed bean should call 'close' on those application contexts.  The more detailed lifecycle of cleanly shutting down components within those application contexts is another story.",4
"Expose shutdown operation over http
This will allow for a simple way to shutdown the server via an HTTP call.  Support for security is a separate story. The end goal is to have some shell scripts distributed that can issue HTTP requests to shutdown the xd-admin and xd-container servers.    The newest version of Jolokia has the ability to boostrap itself inside an application context vs. requiring a java agent.  I suspect using the application context approach will provide us with more flexibility (e.g. property replacement etc) but not sure.",4
"Parameter parsing does not work if an argument contains '--'.
Parameter parsing does not work if an argument contains '--'.    For example:    {code}  ... | transform --expression=42 | transform --expression=--payload |...  {code}    Also, I was surprised that this worked..    {code}  | transform --expression=new StringBuilder(payload).reverse() |  {code}    ... but this didn't...    {code}  | transform --expression='new StringBuilder(payload).reverse()' |  {code}    I think we need to tokenize the argument (with ' if contains spaces) and remove any surrounding '...' from the result. This means if someone wants a SpEL literal they would have to use something like     {code}--expression=''Hello, world!''{code}    resulting in a SpEL literal 'Hello, world!'",1
"Publish golo themed docs documentation to static.springsource.org as part of nightly build
The wiki repo contains a script, gen-docs.sh, that we are planning to use to generate a pretty HTML version of the Getting Started guide.     We should consider using maven (or gradle, but there is currently an issue documented in build.gradle) to generate this and other reference docs and publish them automatically as part of a nightly build.",3
"Add HTTP Delete Stream Operation
nan",5
"Create design document for implementation strategy to support message conversion in ChannelRegistry
The conversion should be based on content-type headers, similar to the way Spring's HttpMessageConverters work (with mime types).    Also, the map of available converters should be extensible while including the most common defaults (for JSON, XML, etc). We most likely want to add a few of our own content types also (e.g. for Tuples).    Most likely, this logic and the configuration methods for extending the converter map, belong in AbstractChannelRegistry since it should be common across all implementations (i.e. the logic should be the same regardless of the transport used after-serialization/before-deserialization).",5
"Enable grouping of modules for co-located deployment
example:    {code}  a | (b | c) | d  {code}    ...where b and c modules are deployed together as a composite module.    There are 2 options (maybe more) for how we could handle that. One would be defining a CompositeModule type that simply bridges the channels (b's output to c's input in this example). The second option would be to deploy those together on the same node as modules but using the LocalChannelRegistry between them.  ",12
"Validate processing modules declare the required channels
Validate that modules have required channels declared according to their type.  Currently the stream deployer accepts processors with no input, but the stream doesn't complete. We should fail earlier and more loudly.",2
"xd-container and xd-admin should log to a file out of the box
We should have an externally editable log4j config file in a conf dir and the default should log to a file (presumably in a logs dir)",2
"Create config support based on channel registry type
We need to have the XD container & admin reading the registry specific property based on the registry type selected.   From Mark F, on one of the code review comments:  Maybe rather than having redis, rabbit, etc. properties all within a container.properties we should rely upon naming conventions instead. Specifically, we could have a single configurable property for the type of channel registry (""redis"", ""rabbit"", or ""local"" being possible values), and then we could use something like:  <import resource=""config/${registry.type}.xml""/>  <context:property-placeholder location=""config/${registry.type}.properties""/>",3
"Script to generate reference documentation from wiki and include in .zip distribution
nan",5
"Decide on location to host http reference documentation and automate upload in build scripts
nan",4
"XD should run offline
Trying to run XD offline results in an error in redis.xml because the cloudfoundry schema file is missing.  We need to add the cf-runtime jar to the classpath to resolve this.",2
"Home wiki page improvements
Add more structure, more easily find the reference guide.  The style that is here  https://github.com/snowplow/snowplow/wiki is nice. ",2
"Create project home page for SpringXD on springsource.org/spring-xd
A minimal project page of a 'top level project' page that has basic information of docs and links to the github wiki page.   No need to list maven coordinates.",4
"Create links to SpringXD on other pages of springsource.org site
bottom home page - list of projects  data/integration category landing pages - related projects.  ",4
"Document how to create a custom input/output module for existing SI channel adapters
Document how to take an existing input/output channel adapters in spring integration and add them as a XD source/sink module.    Should be as end-user focused, step by step guide as possible.    Consider including a getting started gradle/pom.xml ",6
"Document how to create a custom processor module.
The use case is to write custom code that does processing on a specific domain class (perhaps from twitter adapter) or a tuple.  Need to package up this code so that it can be used inside XD.  ",4
"Create and document a syslog aggregation example
2-3 containers (separate processes) that the stream: syslog | tcp   1 container (separate process) that aggregates the data sent from those conainers, tcp | severityFilter | hdfs              ",8
"Support exponential moving average in RichGauge 
This could easily be supported in the existing gauge by adding a setAlpha method to RichGaugeService and adding the extra parameter ""alpha"" to the gauge data (https://en.wikipedia.org/wiki/Exponential_moving_average). If not set it would default to the current behaviour (simple mean), otherwise it would calculate the exponential moving average in place of the mean.",2
"Easily switch between a single process that performs all admin and processing tasks to one that has a dedicated admin processes and distributed processing containers.
nan",13
"DefaultContainer should have a default constructor that generates a UUID
The current incrementAndGet approach based off redis will not easily be applicable in local model deployment",1
"Have three startup scripts, xd-singlenode, xd-admin, and xd-container
The xd-singlenode script will launch a main application that creates both the admin node (to process http admin requests) and the container node (to execute modules for data processing) within in the same process     the xd-admin script will launch a main application that creates only the admin node (remove current embeddedContainer options)    the xd-container script will launch a main application that creates only the container node (as it is now)",1
"The command line for xd-admin and xd-container to support an additional option, pipeProtocol, that is used to determine the middleware for sending admin requests and data between processing steps
The name 'pipeProtocol' is tentative.      1. The command line scripts for xd-admin and xd-container would support a --pipeProtocol option, with the default being to use Redis.  (Otherwise use xd-singlenode).  2. The xd-admin and xd-container scripts will use the value of pipeProtocol to set the java system property xd.pipeProtocol when launching the app.  ",1
"Update launcher.xml to have protocol independent beans defined and an import statement to load protocol specific defintiions from a system property defined location.
launcher.xml can make use of the system property xd.pipeProtocol inside an import statement.  This determines which version of the XD infrastructure to load, for example what ChannelRegistry implementation, Local or Redis based, or specific message listener containers.   File name conventions should be used, so if the option passed in from the command line is --pipeProtocol localChannel  then the XML filename looked for has the 'Protocol' suffix applied, e.g. localChannelProtocol, and is loaded via the classpath.  Redis and Local will not be the only options, other implementations will be provided in the future, e.g. Rabbit, and the user may be able to provide their own implementations of these infrastructure classes (an advanced task).  ",3
"Create redisProtocol.xml that will load all the Redis specific implementations to suppor the XD container runtime and administration
The redis specific beans that are defined in the current launcher.xml should move into this configuration file.   ",3
"Create localChannelProtocol.xml that will load all the SI specific implementations to suppor the XD container runtime and administration
nan",3
"Add unregistration support to the channel registry
nan",4
"Refactor StreamServer to an interface and create Redis and Local implementations
The current StreamServer depends on RedisStreamDeployer. Call this RedisStreamServer and extract interface to allow alternate implementations",2
"Create a pipe protocol independent StreamDeployer
Create StreamDeployer that does not depend on an adapter implementation",2
"Create XD script for xd-single node
This script will launch XD admin along with the module container.    As part of this implementation, we will also remove the embedded options for XD admin & container scripts.",2
"Add  directory to classpath in server startup scripts so groovy based processors can be easily referenced by name without a resource uri prefix
a stream such as time | filter --script=oddMinuteFilter.groovy | file     would load the groovy script 'oddMinuteFilter.groovy' that is located in the directory modules/processor or perhaps in modules/processor/scripts.    Not sure the benefit of having a subdirectory below processor just for scripts.",1
"Investigate running XD on Cloud Foundry
nan",5
"Cleanup embedded container story
The --embeddedX options are a bit confusing in code right now, as the Admin can embed the Container and vice-versa. I guess we should only keep the Admin>Container side of things.",1
"Move Redis connection metadata logging into the code closest to establishing that connection
XD-106 included detailed logging about the Redis metadata within the RedisContainerListener, but it seems as though that info could be logged somewhere closer to the establishment of a Redis connection for the XD runtime (and could be logged even if this listener, whose main role is to capture Container-related events, is not enabled).",2
"Update getting started documentation to use xd-singlenode start script.
With the new option of starting without requiring redis, the getting started documentation should reflect this easier way to start processing data.",2
"Need more unique resource locations for XD internal configuration
Currently internal config files are in META-INF/spring with fairly generic names. To avoid potential collisions if users add their own configuration in the classpath, we should have a more unique location, e.g. META-INF/spring/xd",3
"Users should be able to package custom modules into a single jar
If a user needs to deploy a module containing custom code, they have to build a jar for the lib dir and somehow deploy the app context file separately to a modules dir.  This is a bit inconvenient to build from a project, since context files in src/main/resources typically get built into a jar.    Might be better to accept both jar and xml files in the modules dir, though that brings up the issue of classpath isolation.",5
"The {{time}} Source Should Emit String by Default
When running in local mode (no Redis) {{time | tcp}} no longer works.    Change the {{time}} source to emit the date as a String, while allowing an option to emit a {{Date}} object.",1
"replace the hacky parser with a good one
Replace the existing DSL parser that uses string indexing with a more robust one based on a derivative of SpEL.  This will provide a stable base on which to quickly iterate on syntax.  ",4
"Create a scriptProcessor module that allows the execution of a groovy (potentially jruby,jython) based SI Service Activator
This will enable arbitrary processing logic to be used in a processing step.    See http://blog.springsource.org/2011/12/08/spring-integration-scripting-support-part-1/    <int:service-activator ...>    <script:script lang=""groovy"" location=""file:scripts/groovy/myscript.groovy"">  </int:service-activator>    would be the essence of the module.  Probably 'lang' gets detected from the file extension.",4
"Documentation for developing streams in the IDE needs to mention including scripts dir to project classpath
{{curl -X POST -d ""time --interval=3 | transform | log"" http://localhost:8080/streams/test}}  results in the following stack trace in the DEBUG log. It's apparently benign, but ugly...  {code} 2013-06-06 10:43:36,875 [task-scheduler-1] DEBUG: org.springframework.scripting.support.ResourceScriptSource - class path resource [transform.groovy] could not be resolved in the file system - current timestamp not available for script modification check java.io.FileNotFoundException: class path resource [transform.groovy] cannot be resolved to URL because it does not exist   org.springframework.core.io.ClassPathResource.getURL(ClassPathResource.java:177)   org.springframework.core.io.AbstractFileResolvingResource.lastModified(AbstractFileResolvingResource.java:170)   org.springframework.scripting.support.ResourceScriptSource.retrieveLastModifiedTime(ResourceScriptSource.java:101)   org.springframework.scripting.support.ResourceScriptSource.getScriptAsString(ResourceScriptSource.java:79)   org.springframework.integration.scripting.RefreshableResourceScriptSource.<init>(RefreshableResourceScriptSource.java:46)   sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)   sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)   sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)   java.lang.reflect.Constructor.newInstance(Constructor.java:513)   org.springframework.beans.BeanUtils.instantiateClass(BeanUtils.java:147)   org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:121)   org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:280)   org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1035)   org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:939)   org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:485)   org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:456)   org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveInnerBean(BeanDefinitionValueResolver.java:271)   org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:126)   org.springframework.beans.factory.support.ConstructorResolver.resolveConstructorArguments(ConstructorResolver.java:616)   org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:148)   org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1035)   org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:939)   org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:485)   org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:456)   org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveInnerBean(BeanDefinitionValueResolver.java:271)   org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:126)   org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyPropertyValues(AbstractAutowireCapableBeanFactory.java:1360)   org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1118)   org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:517)   org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:456)   org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:294)   org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:225)   org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:291)   org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:193)   org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:589)   org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:925)   org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:472)   org.springframework.xd.module.SimpleModule.start(SimpleModule.java:97)   org.springframework.xd.dirt.module.ModuleDeployer.deployModule(ModuleDeployer.java:120)   org.springframework.xd.dirt.module.ModuleDeployer.handleMessageInternal(ModuleDeployer.java:108)   org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:73)   sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)   sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)   sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)   java.lang.reflect.Method.invoke(Method.java:597)   org.springframework.expression.spel.support.ReflectiveMethodExecutor.execute(ReflectiveMethodExecutor.java:69)   org.springframework.expression.spel.ast.MethodReference.getValueInternal(MethodReference.java:84)   org.springframework.expression.spel.ast.CompoundExpression.getValueInternal(CompoundExpression.java:57)   org.springframework.expression.spel.ast.SpelNodeImpl.getTypedValue(SpelNodeImpl.java:102)   org.springframework.expression.spel.standard.SpelExpression.getValue(SpelExpression.java:102)   org.springframework.integration.util.AbstractExpressionEvaluator.evaluateExpression(AbstractExpressionEvaluator.java:126)   org.springframework.integration.util.MessagingMethodInvokerHelper.processInternal(MessagingMethodInvokerHelper.java:230)   org.springframework.integration.util.MessagingMethodInvokerHelper.process(MessagingMethodInvokerHelper.java:129)   org.springframework.integration.handler.MethodInvokingMessageProcessor.processMessage(MethodInvokingMessageProcessor.java:73)   org.springframework.integration.handler.ServiceActivatingHandler.handleRequestMessage(ServiceActivatingHandler.java:67)   org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:137)   org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:73)   org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:115)   org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:102)   org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)   org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:178)   org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:149)   org.springframework.integration.core.MessagingTemplate.doSend(MessagingTemplate.java:304)   org.springframework.integration.core.MessagingTemplate.send(MessagingTemplate.java:165)   org.springframework.integration.endpoint.MessageProducerSupport.sendMessage(MessageProducerSupport.java:92)   org.springframework.integration.x.redis.RedisQueueInboundChannelAdapter.access$4(RedisQueueInboundChannelAdapter.java:1)   org.springframework.integration.x.redis.RedisQueueInboundChannelAdapter$ListenerTask.run(RedisQueueInboundChannelAdapter.java:110)   org.springframework.scheduling.support.DelegatingErrorHandlingRunnable.run(DelegatingErrorHandlingRunnable.java:53)   java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)   java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)   java.util.concurrent.FutureTask.run(FutureTask.java:138)   java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)   java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:206)   java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)   java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)   java.lang.Thread.run(Thread.java:662) {code}  ",1
"Ensure the DELETE Operation can Delete a Tap
While deleting a stream doesn't remove any taps right now, we should be able to explicitly delete a tap.    Determine whether the current DELETE works and, if not, make it so.",3
"Creating a tap throws an exception
Creating a tap throws an exception. In local mode: Cannot resolve reference to bean 'redisConnectionFactory' while setting constructor argument; nested exception is org.springframework.beans.factory.NoSuchBeanDefinitionException: No bean named 'redisConnectionFactory' is defined  But also fails when using redis.  ",4
"Fix XD scripts on windows
Currently the XD scripts are broken in windows. ",2
"Update twittersearch module for Twitter 1.0 API retirement
nan",3
"Provided modules should be integration tested
I don't see that we have automated tests for the modules we provide out-of-the-box.  We could make the modules folder an Eclipse project (which would also help solve XD-198) and add some integration tests similar to those documented here:    https://github.com/SpringSource/spring-xd/wiki/Creating-Custom-Modules",5
"Document processor modules
Fill in https://github.com/SpringSource/spring-xd/wiki/Processors",2
"Processor modules should support scripts in languages other than groovy
Filter, Transform, and Script modules all assume the provided script is written in Groovy.     This is partly due to the fact that the ""lang"" attribute of <int-script:script> can't be set to a property value (i.e. lang=""${lang:groovy}""), which would allow users to pass in the expected language. Or perhaps we could use a SPEL expression or script to pick the language based on the file extension?",3
"XD AdminMain & ContainerMain should check xd.home property from scripts
Currently, the system property xd.home is set as JVM_OPTS (via SPRING_XD_ADMIN_OPTS) into xd-admin & xd-container scripts.    Inside the ContainerMain & AdminMain, we need to check if this system property is set and use it. It seems like, this check is missing now.",1
"Provide configurable properties for hdfs sink.
The config file modules/sink/hdfs.xml has a hardcoded value to locate the namenode.    	<hdp:configuration register-url-handler=""false"">  		fs.default.name=hdfs://localhost:9000  	</hdp:configuration>    the fs.default.name proprety should be configurable and we should also support loading an external configuration file using        <hdp:configuration properties-location=""${xd.home}/config/hadoop.properties"">       </hdp:configuration>    ",2
"Document the file sink
nan",1
"Create or document existing project template for custom module creation
The wiki docs on creating custom modules have entire build.gradle files in them. Would be good to explore existing STS templates, maven archetypes, etc or create new ones for authoring custom modules.",3
"If output directory does not exist for a file sink, by default allow it to be created
There shouldn't be a need to do a mkdir -p before sending data to a file sink.",1
"Document the log sink
nan",1
"Add http port command line option to AdminMain
Currently StreamServer has setPort, but no way for end user to set it. ",2
"Temporarily add toString() Logic in Local Mode Inter-Module Comms
nan",2
"Create documentation on the general DSL syntax
The asciidoc wiki should have a section (included in the _Sidebar.asciidoc as well) that describes the general usage of the DSL syntax.",3
"Add authentication information to twittersearch source doc
Since the changes for XD-202, twittersearch requires authentication. Need to update the docs to reflect this.",1
"Add support for tap foo.bar syntax in the DSL
nan",2
"Add config dir to classpath to support custom properties-locations
The transform, filter, and script processor modules support passing in a properties-location for script variables. We need a default location on the classpath for users to provide custom properties files.",2
"Add support to load a twitter.properties file in the source
nan",1
"Surpress tap WARNING message in local mode
nan",1
"Add twitter oauth properties file to config dir
Those property keys should then be provided as defaults for the placeholders in source/twittersearch.xml",2
"Links in asciidoctor generated HTML+docbook documentation are broken
The issue arises because the link:document[Label] asciidoc macro is meant for ""external documents"" and creates {{<ulink>}} in docbook / {{<a href=""document"">}} in html, whereas we want {{<link linkend=""anchor"">}} / {{<a href=""doc#anchor>}} resp. We also want it to continue working in github live view.    I guess what could work is to have the macro (either override the link macro or create our own if github supports that) that looks like :  {{link:document#anchor[Label]}}  (the #anchor works out of the box in asciidoc and should work in github) but override it for the html and docbook backends to render to the correct form.    The thing is, there are several ways to create/override macros (and templates they render to), some of which make sense to our setup:  - having asciidoc.conf in the directory of the processed document (http://asciidoc.org/userguide.html#X27)  - having docbook.conf/html.conf in the directory of the processed document (http://asciidoc.org/userguide.html#X27)  - defining macros using attributes (http://asciidoc.org/userguide.html#_setting_configuration_entries)    I tried all of those, but to no avail. These DO WORK with plain asciidoc, but not with our toolchain. Don't know if the problem is with asciidocTOR or with the gradle wrapper though.        ",2
"Add docs for Deleting a simple stream.
curl -X DELETE http://localhost:8080/streams/ticktock",1
"Update Creating a Custom Source Module doc with a different SI adapter due to Twitter issues
https://github.com/SpringSource/spring-xd/wiki/Creating-a-Source-Module uses the SI twittersearch inbound channel adapter, which is no longer going to work once Twitter disallows anonymous searches.     Ideally we update the example to use a new version of SI-twitter that adds support for this (as opposed to the XD workaround.)",2
"Reduce necessity for quoting in parameter values in DSL expressions
parameter values that include spaces need to be quoted, but this becomes overly complex in this kind of case.  Here is what you want to say:    {code}  http --port=9995 | filter --expression=payload.matches('hello world')  {code}    With the rule 'parameter values that contain spaces must be quoted' it would be this:    {code}  http --port=9995 | filter --expression='payload.matches('hello world')'  {code}    But then to include single quotes within a single quoted string you need to use two of them: '' - so it becomes    {code}  http --port=9995 | filter --expression='payload.matches(''hello world'')'  {code}    Less than ideal.        ",2
"field-value-counter should support nested fieldNames
for example, when using the 'twittersearch' source module, the ""hashTags"" are nested within ""entities"", and the value for hashTags is itself an object with a ""text"" field, so the following would be needed to count the actual value of interest:    {code}  tap @ tweets | field-value-counter --fieldName=entities.hashTags.text  {code}  ",2
"Cleanup and Optimize gradle tasks to bundle spring-xd distribution
We need to cleanup some of the duplicate gradle tasks that bundle spring-xd distributions.     Currently, distXD does the copy of distributions from ""spring-xd-dirt"", ""redis"" and ""spring-xd-gemfire-server"" projects into ""$rootDir/dist/spring-xd"".    And, the task ""zipXD"" makes the zip archive.    These tasks should be combined with the ""distZip"" & ""docZip"" tasks.    We also need to remove the duplicate artifacts configuration from these tasks.",2
"Add jetty-util-6.1.26.jar and jsr311-api-1.1.1.jar as required jars so they will be on the XD classpath
This is needed for the use of the webhdfs:// scheme to talk to HDFS over http.",1
"Missing '=' in example of http stream
In documentation attached to M1, in Streams/Introduction section, there's {noformat} http --port 8091 | file --dir=/tmp/httpdata/ {noformat} while it should be: {noformat} http --port=8091 | file --dir=/tmp/httpdata/ {noformat} missing ""{{=}}"" in {{http}}",1
"Add RabbitMQ-based implementation of ChannelRegistry
nan",8
"Add RabbitMQ source module
configurable parameters should include the queue-name(s) and optional binding key pattern    connection info, such as host and port, should also be configurable but with defaults (localhost and default port), and that should likely fallback to a rabbit.properties file in the $XD_HOME/config directory",3
"Add Twitter gardenhose source module
we have a prototype gardenhose adapter that was built directly upon RestTemplate (streaming on a background thread), but Spring Social Twitter has an issue on its 1.1 roadmap that is relevant:  https://jira.springsource.org/browse/SOCIALTW-2  ",3
"A HATEOAS designed REST API using Spring HATEOAS library.
nan",21
"Create an shell to control all aspect of stream/job management
nan",21
"Support various output format, e.g. Avro, SequenceFile, more advanced rollover options.
nan",21
"Modules (sinks, processors, sources) should be able to be easily tested inside the IDE using JUnit 
nan",21
"Create an Aggregate Counter
An aggregate counter rolls up counts into discrete time buckets.  There is an existing POC implementation in Java based off the library  https://github.com/thheller/timed-counter     The README there has a good description of the desired feature set.",8
"Spring Batch jobs should be able to be deployed to the DIRT runtime and managed.
nan",21
"Deploying Custom Code
When a module is deployed, it should run in its own isolated classpath.  The current code has all dependencies in a single classpath, taken from the lib directory at startup.  This has a number of drawbacks, one of the most important is the batch jobs can not be contributed to the system at runtime.  The work for this epic is decoupled from any module deployment story.  The assumption is that there will be a directory layout as shown below.  Current layout  ./modules/. |-- common |-- job |-- processor |-- sink |-- source |-- trigger  And inside source |-- source |   |-- file.xml |   |-- gemfire-cq.xml |   |-- gemfire.xml |   |-- http.xml |   |-- jms.xml |   |-- mqtt.xml |   |-- rabbit.xml |   |-- syslog-tcp.xml |   |-- syslog-udp.xml |   |-- tail.xml |   |-- tap.xml |   |-- tcp.xml |   |-- time.xml |   |-- twittersearch.xml |   |-- twitterstream.xml   Using an example of the source directory from the current layout.e.g ./modules/source/file, the new layout would be  ./modules/source/file/lib/spring-integration-file.jar ./modules/source/file/config/file.xml   We should support both the new and old layout styles simultaneously.  There what is under 'file' directory is the 'package'  No .zip, war, is required.",21
"Throughput optimized TCP based adapters based on Reactor TCP project
nan",21
"Add JMS source module
nan",2
"Create a Trigger
h2. Narrative  As the XD system, I need to be able to execute a job (or potentially a stream) based on a given condition (time, data existence, etc).  This story is intended is for a local trigger implementation but remote triggers will also need to exist.    h2.  Acceptance Criteria  # Implement the ability to register a time based trigger {{trigger <CRON_STRING>}} for example  # Implement the ability to register a file existence based trigger {{trigger <PATH>}} for example  # Implement the ability to execute a job via an anonymous trigger: {{job_name @ <CRON_STRING OR PATH>}}  # Implement the ability to execute a job via a job via the previously registered trigger: {{job_name @ trigger_name}}  ",8
"Deploy Batch Jobs on XD
h2. Narrative  As a developer, I need a way to deploy job configurations as well as the related custom code to XD.    h2.  Acceptance Criteria  # Provide the ability to register jobs that have been deployed as modules via something like {{curl -d ""job"" http://localhost:8080/streams/myJob}} where job is the name of the job definition located in /modules/job and myJob is the name of the resulting registered job  # Confirm that both ""regular"" jobs and Spring Hadoop based jobs can be packaged/run.",8
"MessageChannelItemWriter
h2. Narrative  As a user of XD, I want to be able to use a job as a source.  To do so, we need the output of a job to be written to a message channel    h2.  Acceptance Criteria  # Create a new ItemWriter in the Spring Batch project to write to a Spring Integration message channel.",4
"Need to be able to specify password for Redis
Running on Cloud Foundry (and other managed environments) we need to be able to specify a Redis password in addition to host and port.",2
"Set up a project for XD REST client library
nan",5
"Add support for stream creation
For stream creation we need to be able to specify:  source sink processor  - filter  - transformer  - script  etc. ",10
"Set up a project for XD Shell
Set up a basic Spring Shell project for XD Shell",3
"Create a banner page for XD Shell
nan",2
"Create the base implementation for XDCommands for the shell
This is the basic setup of the commands file - no specific command implementations",3
"Add command for stream creation
nan",3
"Add command for tap creation
nan",1
"Add command for listing streams
nan",3
"Add command for deleting a stream
nan",1
"Convert current REST servlet to Spring MVC
nan",3
"Support GET /streams
Pagination support, maybe querying by name as well",5
"Spring MVC infrastructure tests
nan",8
"Retrieve description of all registered modules 
GET /streams/{streamname}/modules     and  GET /streams/{streamname}/modules/{modulename}    The former returning links to the latter  ",5
"Create java client lib over REST API
So that clients (e.g. Shell or custom user program) are insulated from REST details (ala Cloud Foundry).  May go even further if we want a Java DSL for stream definitions (that may reuse Batch command POJOs btw): Difference between: xdClient.createStream(""mystream"", ""http --port=9000 | file"") and  import static stuff.*;  StreamDef stream = http().port(9000).pipe(file()); xdClient.createStream(""mystream"", stream);",8
"Job as a Source
h2. Narrative  As an XD developer, I need to be able to use a batch job to stream data as a source.    h2.  Acceptance Criteria  # Implement the ability for a job to be defined as a source in the DSL  # Add the configurations for the batch infrastructure transparently to the user  # Add the ability to specify if the job is stateful (picks up where it left off if it stops or restarts at the beginning).",10
"Streamline command-line arg management
Command line arguments (and especially their default values) are currently scattered around different places.    The aim is to regroup those in a common place (*Options classes make sense).    Also, not very happy with how System properties are used as a vehicle for options.transport / options.home",2
"Create JobDefinition repository
h2. Narrative As XD, I need a persistent way to register job definitions (beyond the map registry implementation provided by Spring Batch).    h2.  Acceptance Criteria # XD should be able to register, unregister, and find job definitions via the registry. # The registry should be backed by Redis so that it is persistent.",4
"The HDFS Sink should support a file naming strategy to distinguish between file currently being written and completed files
A file that is in the process of being written to should have a customized suffix added to the name, e.g. 'temp'.    Once the file is closed, the suffix is removed and replaced with another value - default value can be dependent on the serialization format used, but can be customized",8
"The HDFS Sink should support a number of rollover options
A strategy to roll over files that allows the user to choose between   1) the size of the file  2) the number of events/items in the file  3) an idle timeout value that if exceeded, will close the file",8
"The HDFS Sink should support writing POJOs to HDFS using Avro Serialization
Writing POJOs using CDK Data (Avro)  We should support both partitioned and un-partitioned.  This story addresses only un-partitioned.  Document limitations in terms of which Java types are supported and not supported by the Avro serialization ",10
"Redis backed container's RedisQueueInboundChannelAdapter is not performant
Currently, the RedisQueueInboundChannelAdapter has blocking operation when pulling the messages out of redis queue and this is not performant.     There are few ideas from the discussion to make it better:    1) Get more items from the redis queue per connection  2) We will also have compression of messages(at the channel registry) before being sent to the redis queue     We also need to investigate what redis connection strategy makes the RedisQueueInboundAdapter better. ",2
"HTTP Source still listens on port 9000 after removal.
Steps to reproduce:    1.  curl -d ""http | log"" http://localhost:8080/streams/testHttp    2.  curl -X DELETE http://localhost:8080/streams/testHttp    3.  curl -d ""http | log"" http://localhost:8080/streams/testHttp    org.jboss.netty.channel.ChannelException: Failed to bind to: 0.0.0.0/0.0.0.0:9000",1
"Redis 'install-redis' script fails on Ubuntu64
The installation script for redis fails on Ubuntu64 when trying to untar the redis distribution.  The script uses REDIS_ZIPNAME instead of REDIS_ZIP_PATH.      This bug will be seen on any Linux 64 bits platform and looking at the code, even Linux 32 bits platform.",1
"Ensure package-info.java is present for each package
We should ensure that each Package of Spring XD is documented. Right now the created JavaDoc looks barren:    http://static.springsource.org/spring-xd/docs/1.0.0.M1/api/     ",4
"Apply JavaDoc HotFix
nan",1
"redis.properties values ignored
The container application loads {{redis.properties}}, but for some reason the values are ignored, and defaults are used instead.  Repro steps: # Unpack Spring XD 1.0.0.M1 to a machine with no running Redis instance # Change /xd/config/redis.properties to specify a different hostname # Run /xd/bin/xd-container # Observe error about inability to connect to Redis on localhost  Workaround * Pass -Dredis.hostname={desired IP} as a JVM parameter",1
"Add log config file to gemfire in final distro
The changes for XD-144 mean that log4j files are no longer in the library jars. The admin server already has a logging configuration which should be activated by the startup scripts, but the separate gemfire app doesn't.",1
"File sink should support rollover
I wanted to have a rollover feature when I was streaming tweets to a file overnight, just to avoid dealing with a single enormous file (in case I collected more data than my demo could handle and needed to split it up).",2
"Refactor gardenhose into more generic twitterstream source
Twitter's streaming APIs have more capabilities than just the plain statuses/sample.json. In particular we should support the filter.json option and the use of ""track"" (https://dev.twitter.com/docs/streaming-apis/parameters#track) as well as other request parameters (delimited, language etc).",2
"Creating a tap with same name as existing streams results in infinite loop
See http://stackoverflow.com/questions/17157068/counter-analytics-in-springxd    The underlying issue is stream creation with a name already taken though",5
"Request to create a repo for Spring XD performance testing
It would be nice if we have a git repo for Spring XD performance testing.    This would enable us to have a common repository (rather than inside spring-xd as a subproject) for all performance related code specific to any module, message middleware etc.,   ",4
"User wants ability to create a mock source
To send a pre-set message to process(es)",8
"User wants ability to create a in-process sink or tap
So that we can validate the message content in the stream",8
"User wants ability to test processors
Be able to point to the processor xml file, e.g. modules/processors/transformer.xml, and have access to a source channel that drives messages into the processor and a output channel where output messages are send.  The outbound channel is queue backed.  Test sending JSON to a processor module that uses Tuples. ",8
"User wants ability to test sinks
Handled by 1245",8
"User wants ability to test sources
Examples: 1. Be able to start the rabbitmq source just by pointing to modules/source/rabbit.xml, pass in some property file for parameters to be replaced, and outgoing message is placed in a in-memory queue backed channel for use with assertions to verify functionality.   2. Test for as many source types as is 'reasonable', e.g. MQTT/TCP testing might be harder than say rabbitmq. 3. Test that sending json, results in media-type header is set to json 4. Test that sending POJO,   ""  POJO 5. Test that sending Tuple, ""   Tuple 6. Test that sending raw bytes, "" raw bytes ",8
"User to send a message directly to module and receive a message from a module
nan",8
"User wants ability to test multiple processors in a chain
nan",8
"Users should be able to package custom modules as a single zip file
See XD-194 for additional considerations. Zip support should be similar to uber-jar, or possibly replace uber-jar support.",8
"Add Jolokia Agent Depending on Run Mode
WAR Vs. JVM Jolokia Agent    Jolokia Vs. JVM MBeanServer    Probably needs support for Spring Profiles.",5
"Add Spring/Integration MBean Exporters to Module ApplicationContexts
Global option?    Override for individual modules? module types?",8
"Package Shell ""binary"" next to xd-admin and xd-container
The shell should be an 'executable' delivered out of the box in much the same way that xd-container and xd-admin are right now. If we follow how redis/mongo distribut the shell, it sits side by side with the other binaries",3
"Create a common exception framework for XD
Need to capture exceptions from the various projects that make up XD and wrap them in XD Specific exceptions.  An example of this is when leaving out the channels in the module definitions, we see NoSuchBeanExceptions and IllegalArgumentExceptions thrown based on which module and what channel is missing. ",5
"Add Documentation Chapter on Executing Batch Jobs
nan",4
"Create Splunk sink module
This would be based off the spring-integration-extenstions splunk project.  The use of this adapter for storing tweet data is in  https://github.com/markpollack/springone    We should be able to reproduce the use case as done in that demo",10
"Add command for deleting a tap
nan",1
"Support for GET of /taps
nan",1
"Add create() and deploy() to TapsController
POST?",1
"Support for DELETE of taps
nan",1
"Add command for listing of taps
nan",1
"Retrieve information for a Counter
nan",5
"Retrieve information for a Field Value Counter
TODO as part of this (see XD-537):   * Get rid of so-called Service layer in analytics project (doesn't do much right now, and logic would better live in the 'Handler' IMO) * Have REST controllers depend on XRepository in all cases",3
"Retrieve information for a Gauge
TODO as part of this (see XD-537):   * Get rid of so-called Service layer in analytics project (doesn't do much right now, and logic would better live in the 'Handler' IMO) * Have REST controllers depend on XRepository in all cases",3
"Retrieve information for a Rich Gauge
TODO as part of this (see XD-537):   * Get rid of so-called Service layer in analytics project (doesn't do much right now, and logic would better live in the 'Handler' IMO) * Have REST controllers depend on XRepository in all cases",3
"Retrieve information for an aggregate counter
TODO as part of this (see XD-537):   * Get rid of so-called Service layer in analytics project (doesn't do much right now, and logic would better live in the 'Handler' IMO) * Have REST controllers depend on XRepository in all cases",3
"Document the structure of the REST API
nan",5
"Review DSL
updated story points to 14 since 5 of us just participated in a 2 hour call, and we still need to discuss ""topology"" support after some dev spikes later this week",14
"Document Splunk source sink
nan",2
"Test connection pooling on Redis blocking/nonblocking operations
nan",2
"Create tcp/udp load generator script for XD performance testing
Create a load generator script which can generate messages at specific    1) Rate  2) Payload  3) Concurrency    to a specific tcp/udp port where a syslog adapter is listening.",4
"Investigate using profiler when doing the performance testing
Investigate how efficiently we can integrate profiler into the performance test.",1
"Create script to extract table data from JSON based on a given HAWQ table structure
We should be able to write a script that can examine the table structure for a given HAWQ table and then extract the data from JSON without the custom script we are using now.",8
"Document JMX features
Document jmx command line options and refer to jolokia",2
"Fix classpath error caused by multiple conflicting servlet-api jars
There is some conflicting Servlet API jars on the claspath that needs cleanup. Building and running with xd-singlenode script gave this error:    Jun 27, 2013 3:18:16 PM org.apache.coyote.http11.AbstractHttp11Processor process  SEVERE: Error processing request  java.lang.NoSuchMethodError: javax.servlet.ServletContext.getEffectiveSessionTrackingModes()Ljava/util/Set;    org.apache.catalina.connector.CoyoteAdapter.postParseRequest(CoyoteAdapter.java:674)    org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:402)    org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1004)    org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:589)    org.apache.tomcat.util.net.JIoEndpoint$SocketProcessor.run(JIoEndpoint.java:310)    java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)    java.lang.Thread.run(Thread.java:680)  ",3
"Investigate JMX object naming of deployed modules and inbound/outbound channel adapters.
The object naming is still not ideal for XD since SI conventions add some noise. Likely  need to design and implement a custom naming strategy",5
"Add BatchMbeanExporter for batch modules
nan",5
"Replace ""gardenhose"" doc with new ""twitterstream""
nan",1
"Esper based Complex Event Processing module
I would like to see a module created that supports complex event processing.  I have reviewed GemFire Continuous Query but was not able to find a feature for time windows.    I have used Esper in the past for this type of processing.",1
"Investigate Redis connection timeout issues when running performance test
With the performance test run, the numbers (messages sent/received per second) keep varying as there are   ""redis client connection timeout exceptions"" (Caused by: org.jboss.netty.channel.ConnectTimeoutException: connection timed out) at both redis inbound/outbound channel adapters as I increase the total number of messages being processed (max. 10K/second).  Some of the exception messages for the review:  1) With connection pool (at Redis outbound):  Caused by: org.springframework.data.redis.connection.PoolException: Could not get a resource from the pool; nested exception is com.lambdaworks.redis.RedisException: Unable to connect  at org.springframework.data.redis.connection.lettuce.DefaultLettucePool.getResource(DefaultLettucePool.java:95)  at org.springframework.data.redis.connection.lettuce.DefaultLettucePool.getResource(DefaultLettucePool.java:36)  at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.createLettuceConnector(LettuceConnectionFactory.java:318)  at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.getConnection(LettuceConnectionFactory.java:109)  at org.springframework.data.redis.core.RedisConnectionUtils.doGetConnection(RedisConnectionUtils.java:81)  at org.springframework.data.redis.core.RedisConnectionUtils.getConnection(RedisConnectionUtils.java:53)  at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:157)  at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:137)  at org.springframework.data.redis.core.AbstractOperations.execute(AbstractOperations.java:84)  at org.springframework.data.redis.core.DefaultListOperations.leftPush(DefaultListOperations.java:71)  at org.springframework.data.redis.core.DefaultBoundListOperations.leftPush(DefaultBoundListOperations.java:67)  at org.springframework.xd.perftest.redis.outbound.RedisQOutboundChannelAdapter.handleMessageInternal(RedisQOutboundChannelAdapter.java:71)  at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:73)  ... 17 more  Caused by: com.lambdaworks.redis.RedisException: Unable to connect  at com.lambdaworks.redis.RedisClient.connect(RedisClient.java:176)  at com.lambdaworks.redis.RedisClient.connectAsync(RedisClient.java:139)  at org.springframework.data.redis.connection.lettuce.DefaultLettucePool$LettuceFactory.makeObject(DefaultLettucePool.java:252)  at org.apache.commons.pool.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:1181)  at org.springframework.data.redis.connection.lettuce.DefaultLettucePool.getResource(DefaultLettucePool.java:93)  ... 29 more  Caused by: org.jboss.netty.channel.ConnectTimeoutException: connection timed out: localhost/127.0.0.1:6379  at org.jboss.netty.channel.socket.nio.NioClientBoss.processConnectTimeout(NioClientBoss.java:137)  at org.jboss.netty.channel.socket.nio.NioClientBoss.process(NioClientBoss.java:83)  at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312)  at org.jboss.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:42)  2) Without connection pool (at Redis inbound):  Caused by: com.lambdaworks.redis.RedisException: Unable to connect  at com.lambdaworks.redis.RedisClient.connect(RedisClient.java:176)  at com.lambdaworks.redis.RedisClient.connectAsync(RedisClient.java:139)  at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.createLettuceConnector(LettuceConnectionFactory.java:321)  ... 12 more  Caused by: org.jboss.netty.channel.ConnectTimeoutException: connection timed out: localhost/127.0.0.1:6379  at org.jboss.netty.channel.socket.nio.NioClientBoss.processConnectTimeout(NioClientBoss.java:137)  at org.jboss.netty.channel.socket.nio.NioClientBoss.process(NioClientBoss.java:83)  at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312)  at org.jboss.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:42)  ... 3 more",2
"Trigger - Add support for fixed-delay interval
Trigger - Add support for fixed-delay interval",1
"Trigger - Add support for date-based one-time execution
Currently Jobs can be either executed using cron expression or immediately at once. We should also support the one-time scheduling of jobs in the future.    Would this possibly require us to implement schedule-persistence? That could severely impact story-points.",4
"Support hourly resolution in redis aggregate counter
nan",4
"Support daily query resolution in redis aggregate counter
nan",4
"In-memory implementation of aggregate counter
nan",3
"Clean up Spring Configuration
- Container context should be separate from Admin Context in local mode (consistency across transports). Provide a LocalChannelRegistry to bridge deploy and undeploy channels to the ModuleDeployer    - Verify Plugins are not in common module context. They are only needed by the ModuleDeployer not the Modules.    - Add global-beans Config for beans to be shared among admin and container (and available to modules). This would be set as the parent context where needed. (Currently â€˜analytics-context.xmlâ€™ and JobRepository shared by Admin and modules)    - Fix ModuleDeployer sets parentContext twice  - Rename common.xml to module-common.xml    NOTE: Analytics parent only required in local mode    - Write unit tests to verify configurations are as expected. Correct bean implementations and no stray beans or redundant instances where not needed    - Decouple Command options from System properties (in general XD property names), this gives us some flexibility in mapping to properties or profiles where appropriate  ",5
"Investigate Reactor-based Dispatchers in the common ApplicationContext that can be used by Modules
nan",8
"Enable configuration of Executors in source and sink modules, by default using Dispatchers in parent context
Need to understand how individual modules may or may not share Dispatchers that are part of the parent context.  If modules have their own dispatchers, those also need to be configurable.",8
"Test startup scripts on windows
startup scripts on windows should be tested, xd-admin, xd-container, xd-shell.",1
"Creating a base class for Plugins 
It might be worth creating a base class for Plugins that combines common concerns across plugins. E.g. That would allow us to hide the commonApplicationContext and BeanDefinitionAddingPostProcessor for common cases, instead exposing a simple addBeanDefinition method to sub-classes.""",4
"Add support for creating named cron triggers
Simple cron based triggers",2
"Add support for creating a spring batch job that has an embedded trigger expression
nan",1
"Add support for creating a spring batch job that references a named trigger
nan",2
"Create a trigger from Shell
nan",1
"Add support for creating fixed delay/ fixed rate triggers
nan",1
"Create TriggerDefinition Repository 
Redis based.",1
"Support having multiple property placeholders defined in different modules
Right now the PPC for jolokia mgmt is conflicting with the PPC used to resolve redis properties.  Need to determine a strategy such that multiple PPCs can be used.",4
"Create a XD job definition
when posting the DSL to create a spring batch job  e.g. ""trigger job.xml --option1=foo""    it should be stored (in redis) so that a listing of XD job definitions can be retrieved.",3
"Final review of REST API structure document for streams, taps and jobs
Get closure on open discussion points for REST API wrt to streams, taps and jobs.  ",3
"Improve connection handling in RedisAggregateCounterService.
This is currently too chatty. It should be possible to use a single connection for each ""increment"" operation.",2
"Further DSL extensions
Extend the DSL in the following ways:    - stream naming, use <name>'='  {code}  mystream = http | file  {code}    - module aliasing (for later referencing) use <label>':'  {code}  mystream = http | t1: transform --expression=payload.toUpperCase() | t2: transform --expression=payload.toLowerCase()  {code}    - sequence of job steps with '&'  {code}  mybigjobby = step1 --option=value & step2 --option=value  {code}    - Channels for sources and sinks, use '>', channels references prefixed ':'  {code}  // sink channel called foo  http | transform --expression=payload.toUppercase() > :foo  // source channel called foo  :foo > count | file  {code}    - Qualify channels with a stream ':'<stream>'.'<channel>  {code}  mystream = http | transform --expression=payload.toUppercase() > :foo  :mystream.foo > count | log  {code}      - Reusable substreams, define then reuse  {code}  myIntricateFlow = transform --expression=payload.toUppercase() | transform --expression=payload.toLowercase()    http | myIntricateFlow | file  {code}    - Parameterized substreams, use $XX or ${XX} to indicate parameters  {code}  obfuscateName = transform --expression=payload.replaceAll(""${name}"",XXX)    twitter --query=Bieber | obfuscateName --name=Justin | file  {code}    - Tapping (still not 100% happy about the format here)  {code}  mystream = http | filter | t1: transform XXX | t2: transform YYY | file    // These are then equivalent (tapping on a module is effectively tapping a channel, hence the '>')  tap filter > count | file  tap mystream.filter > count | file    mystream = http | filter > :foo    tap :foo > count | file  tap :mystream.foo > count | file  {code}  I'd still like to thing about removing 'tap' and using a symbol (perhaps '@', but @:mystream.foo is a little odd)    still not covered, topological constraints on the stream components.",3
"Add command for deploying a Stream
Deploy a named stream. The stream must exist in the StreamRepository",3
"Command for creating a job
optional --autostart switch to also deploy the job",1
"Command to deploy a job
Deploy an existing job. Must exist in the JobsRepository",1
"Command to create a tap
To store it's definition and optionally deploy with --autostart flag",1
"Command to list taps
nan",1
"Command to delete tap
nan",1
"add create() and deploy() methods to JobsController
see https://docs.google.com/a/gopivotal.com/drawings/d/1kCNbVspRBjGc10itF9cSkwST8WgCjgg3wGzFKqLCAzU/edit    Create the controller if it doesn't exist. Test with MvcTest",1
"add create() and deploy() methods to JobDeployer
see https://docs.google.com/a/gopivotal.com/drawings/d/1kCNbVspRBjGc10itF9cSkwST8WgCjgg3wGzFKqLCAzU/edit    Create the deployer if it doesn't exist.",1
"add create() and deploy() methods to StreamsController
see https://docs.google.com/a/gopivotal.com/drawings/d/1kCNbVspRBjGc10itF9cSkwST8WgCjgg3wGzFKqLCAzU/edit    create optionally deploys",1
"add create() and deploy() methods to StreamDeployer
see https://docs.google.com/a/gopivotal.com/drawings/d/1kCNbVspRBjGc10itF9cSkwST8WgCjgg3wGzFKqLCAzU/edit    Refactor current DefaultStreamDeployer",1
"add create() and deploy() methods to TapsController
see https://docs.google.com/a/gopivotal.com/drawings/d/1kCNbVspRBjGc10itF9cSkwST8WgCjgg3wGzFKqLCAzU/edit    create TapsController if necessary",1
"add create() and deploy() methods to TapDeployer
see https://docs.google.com/a/gopivotal.com/drawings/d/1kCNbVspRBjGc10itF9cSkwST8WgCjgg3wGzFKqLCAzU/edit",1
"Create TapRepository
see StreamsRepository as an example. This includes in memory and Redis implementations",1
"Implement list() method on TapController
nan",1
"Implement list() method on TapDeployer()
nan",1
"Error handling on Streams
Have proper exceptions for common error cases on Stream creation/deployment and propagate those to clients correctly.",5
"Automate copyright header management
Some (java) files are currently missing headers.    The plugin at https://github.com/hierynomus/license-gradle-plugin can help, but initial trial revealed that:    - skipExistingHeaders does not seem to be honored. We may then need to use a year construction like 2001-${current} or force all files to have ${current} year. Don't know the legal implications of this  - Default source sets encompass all files ""in the classpath"" basically, so that means .xml as well as .properties files for example. It would seem logical to add header to those as well, but I don't think this is what we do on other projetcs.",3
"Create design document for implementation strategy for ingesting data from twitter into HDFS that can be analyzed by HAWQ
As part of the Hadoop World demonstration work, the flow of data using XD from twitter to be analyzed by HAWQ as done.  Part of this work had the data going into HDFS that HAWQ was able to query using external tables.    The work for this story is to identify the concrete technical tasks/stories to be created do deliver and document this functionality in XD.    ",3
"Create a stubbed out job controller 
nan",1
"Allow Streamserver/tomcat to chooce free port
Currently you need to define a port which tomcat will use to bind to. If XD is embedded or run in a Hadoop it is not possible to know which port should be used. Current embedded tomcat version is able to use the 0-port trick for it to choose free port.  Real binded port can be asked from a connector using below example.  org/springframework/xd/dirt/stream/StreamServer.java: public int getLocalPort() {   return this.tomcat.getConnector().getLocalPort(); }   Also I believe Streamserver should provide a method to ask what is the real url to connect to or atleast what streamserver thinks what it is. This will make life much easier for components empedding spring-xd. ",1
"The shell introduced a fundamental new way to interact with the system, all use of CURL in examples needs to change to use the shell.
nan",20
"Add section to documentation that shows command line options available for each server
This should likely be in the ""start the runtime"" section of Getting Started section.",1
"Document Monitoring & Management Features
This section should discuss what is exposed via JMX, how you can view it in JConsole, and how you can view it over http via Jolikia.    in particular showing how some existing metrics for inbound message channel adapters or the 'inbound' channel of the stream, that indicate the number of messages processed per section.      ",2
"Update Getting Started chapter to use Shell commands instead of curl
See http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#getting-started    ",1
"Update Getting Started chapter to include a section on starting the shell.
The chapter on how to start up the shell should ocme right after ""start the runtime"" and before ""create the stream""",1
"Update Streams Chapter to use shell commands instead of curl
the current streams chapter    http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#streams    shows creation and deleting streams using CURL - switch to use shell.  Also add listing of a stream.    there is also an example of creating a stream, this should be replaced as well.  ",1
"Create a shell command to post data to an http port for use with the http source module
the current streams chapter    http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#streams    shows using curl to post some data to a http source module,     curl -d ""hello"" http://localhost:9000    create a shell command so curl doesn't have to be used.    https://github.com/SpringSource/rest-shell    has a command already developed for this.",1
"Create a command to browse the HDFS file system
There are existing commands that can be taken from     https://github.com/SpringSource/spring-hadoop-samples    or     https://github.com/SpringSource/impala    that can be used for this",4
"Update Sources section to use Shell commands instead of curl
See http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#http        ",1
"Update documentation section ""Running in Distributed Mode"" to show use of RabbitMQ in addition to Redis
The documentation in the Running in Distributed Mode chapter should discuss that the distributed runtime can use essentially any middleware to communicate between nodes.  This functionality is provided by the core ChannelRegistry abstraction.  A new intro paragraph shoul convey that it isn't a 'redis' only or 'rabbitmq' only system.  There should be ""Installing RabbitMQ"" and ""Starting RabbitMQ"" sections to match those for Redis.  ""Starting Spring XD in Distributed Mode"" should cover how to configure the system to select to use Redis or Rabbit.",3
"Update Sources tail section to use Shell commands instead of curl
http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#tail",1
"Update Sources twitter search section to use Shell commands instead of curl
See   http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#twittersearch",1
"Update Sources Gemfire CQ section to use Shell commands instead of curl 
See   http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#gemfire-cq",1
"Update Source Syslog section to use Shell commands instead of curl 
See   http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#syslog",1
"Update Sources TCP section to use Shell commands instead of curl 
See   http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#tcp",1
"Update Processors Filter & JSon Filed Value Filter section to use Shell commands instead of curl 
See   http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#filter http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#json-value-filter",1
"Update Processors Transform section to use Shell commands instead of curl 
See   http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#transform",1
"Update Processors JSON Field Extractor section to use Shell commands instead of curl 
See   http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#json-field-extractor",1
"Update Processors Script section to use Shell commands instead of curl 
See   http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#script",1
"Update Sink's Log section to use Shell commands instead of curl 
See   http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#log_sinks",1
" Update Sink's File section to use Shell commands instead of curl 
See   http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#file_sinks",1
"Update Sink's HDFS section to use Shell commands instead of curl 
See   http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#hdfs",1
"Update Sink's TCP section to use Shell commands instead of curl 
See   http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#tcp_sinks",1
"Update Sink's GemFire section to use Shell commands instead of curl 
See   http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#gemfire ",1
"Taps introduction section should show use of shell to create a real stream and a real tap using the shell
See   http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#taps  The existing docs should be made to show a real stream being created with filter and/or transformer and then a tap that goes to logging.    The shell syntax to also stop/undeploy a tap should be shown here as well since the lifecycle is discussed.",2
"Documentation for AggregateCounter
Add section to Analytics chapter on use of AggregateCounter.  The example should show the use of the shell to create the tap that uses the AggregateCounter.",3
"Update Analytics Counter section to use Shell commands instead of curl 
nan",1
"Update Analytics Field Value Counter section to use Shell commands instead of curl 
nan",1
"Update Analytics Gauge section to use Shell commands instead of curl 
nan",1
"Update Analytics Rich Gauge section to use Shell commands instead of curl 
nan",1
"Update Samples syslog ingestion section to use Shell commands instead of curl 
nan",1
"Update Creating a Processor Module section to use Shell commands instead of curl 
""test the deployed module"" sub-section uses curl.",1
"Update Creating a Source Module section to use Shell commands instead of curl 
See http://static.springsource.org/spring-xd/docs/1.0.x-SNAPSHOT/reference/html/#_test_the_deployed_module",1
"Update Creating a Sink Module section to use Shell commands instead of curl 
See http://static.springsource.org/spring-xd/docs/1.0.x-SNAPSHOT/reference/html/#_test_the_deployed_module_3 ",1
"Document time source
time source is used in some examples, but it isn't documented explicitly, eg. --interval option in seconds.",1
"Use a Different Default Jolokia Port for Admin Vs. Container
Avoid the need for {{--jmxPort=xxxx}} when running both a {{Container}} and {{Admin}} on the same server",1
"Make String conversion optional with local transport
nan",5
"Homogenize Container Initialization Failures 
If Redis is not running, the container fails to initialize in {{ContainerMain.launch()}} because the connection factory attempts to eagerly connect.  If RabbitMQ is not running, the container fails to initialize in {{AbstractContainerLauncher.launch()}}.  Make the failure behavior consistent from a user perspective and add a spring-retry {{RetryTemplate}} to retry container startup.",5
"Consider removing the Topic/Queues when deleting the Stream
As a user, I'd like to have the option to delete the queues/topics so that we can include an _optional_ attribute as part of the stream destroy command to also clean-up the associated queues/topics.    *Notes:*  * Spring-AMQP {{RabbitAdmin}} now has a {{getQueueProperties()}} method which returns the number of consumers so it may be possible to use it for this purpose.  * Consider the possibility of _producers_ and/or _queues_ still containing data  * Consider the scenario even after the topics/queues are cleaned-up, what to do with fanout exchange?    *Some Further Thoughts*  * Consider using the upcoming Spring AMQP REST API {{RabbitManagementTemplate}} if the timing is not right, we could temporarily invoke the rabbit REST API directly.  * Should be optional; perhaps via {{stream destroy foo --clean}}  * Should this be done by the admin? Or, via a new plugin handling module undeployments - in the rabbit case, undeploying a consumer would check for us being the last consumer and remove the queue/binding/exchange, since we undeploy left->right, everything can be cleaned up on the consumer side.  * Third option would be new methods on the bus {{cleanConsumer}} etc invoked by the {{StreamPlugin}}  * Down side of doing it on the admin is that he wouldn't necessarily know which rabbit cluster a stream was deployed to - so it probably has to happen on the container - even so, we'd need the admin url(s) for the cluster.",5
"Create tests to load the standard runtime app context configurations
The basic launch configurations should be tested automatically to ensure that startup scripts and launch aren't broken by changes.",2
"Fix Package Tangles
Looking at the latest Sonar run we have 3 package tangles in Spring XD:  https://sonar.springsource.org/drilldown/measures/7717?metric=87",4
"More DSL work: exploiting source/sink channels
The DSL changes under XD-369 now build stream Ast objects that can include a source and sink channel:    {code}  // Source Channel  :mystream.foo > count | log    // Sink Channel  http | count > :foo  {code}    These new fields in the Ast object need to be copied into the module deployment request objects and then used at the destination as the channels for wiring things together.  Currently the only channels used are the .NNN numeric channels where NNN is the index of the module in the stream definition. The source/sink channels are 'extra' channels that need creating - the source channel acting as a real source for the next module in the chain whilst the sink channel acts as a sink output for the last channel in the chain.      I can think of two ways to handle the implementation:  - In order to police the stream structure as ""source | processor* | sink"" maybe special SourceChannel and SinkChannel modules are created to represent these channels and when deployment happens the deployer understands that they don't represent a real request to deploy a module but simply the channels to wire up to the adjacent module.    - Carry source/sink channel info in the existing module definitions. But then the verification of source/processor*/sink structure will need modification to say a source isn't necessary if the first processor has a source channel attached and a sink isn't necessary if the last processor has a sink channel attached.  ",4
"More DSL work: hooking up stream directory
Following stream parsing there is now a stream resolution stage that chases down substream references and fills in parameterization. The 'lookup' of streams is done through implementors of the StreamLookupEnvironment interface. Currently the parser implements this itself but it is really a job for the stream directory.  The parser implementation doesn't know about stream deletions, for example, so may still resolve streams that no longer exist.",2
"More DSL work: using and policing & for job step lists
The new parser supports | for connecting regular modules and & for connecting job steps. The modules in the ast that were connected with & are tagged but nothing is currently using that information (it doesnt get into the module deployment request). We need to think about using this data: policing the modules that are being deployed to ensure they are job steps, for example.",3
"More DSL work: checking behaviour for non-deployable streams
With support for substreams/parameterized streams now in the parser it will be possible to create a stream that cannot be deployed: it may not fit the source/processor*/sink structure or it is a parameterized stream with no default values for parameters. Need to check how XD is going to handle these - after creating them, attempting to 'deploy' them should return appropriate errors. (They should exist in the stream directory).",3
"More DSL work: Documentation updates for new format
Once issues like XD-438 have been completed the wiki doc will need updates to reflect the current behaviour and syntax options.",2
"Add MQTT Source
nan",3
"Make Redis/Rabbit @Rules Conditional
Dependent servers should be required on the CI server, but optional on developer systems.",1
"Add support to set the read timeout for http request
We need to have the ability to set read timeout for http request.  This is already implemented here: https://github.com/SpringSource/rest-shell/",1
"Investigate stream lifecycle issues with redis store
When running with ""./gradlew launch"" I get a 500 error when I try to re-deploy an undeployed stream. A subsequent create or destroy also fail  http://localhost:8080:>stream create --definition ""time | log"" --name ticktock Created new stream 'ticktock' http://localhost:8080:>stream undeploy --name ticktock Un-deployed stream 'ticktock' http://localhost:8080:>stream deploy --name ticktock 13:02:09,936  WARN Spring Shell client.RestTemplate:524 - PUT request for ""http://localhost:8080/streams/ticktock"" resulted in 500 (Internal Server Error); invoking error handler Command failed org.springframework.web.client.HttpServerErrorException: 500 Internal Server Error http://localhost:8080:>stream deploy --name ticktock 13:03:11,453  WARN Spring Shell client.RestTemplate:524 - PUT request for ""http://localhost:8080/streams/ticktock"" resulted in 500 (Internal Server Error); invoking error handler Command failed org.springframework.web.client.HttpServerErrorException: 500 Internal Server Error http://localhost:8080:>stream undeploy --name ticktock 13:03:54,576  WARN Spring Shell client.RestTemplate:524 - PUT request for ""http://localhost:8080/streams/ticktock"" resulted in 500 (Internal Server Error); invoking error handler Command failed org.springframework.web.client.HttpServerErrorException: 500 Internal Server Error http://localhost:8080:>stream undeploy --name ticktock 13:04:48,872  WARN Spring Shell client.RestTemplate:524 - PUT request for ""http://localhost:8080/streams/ticktock"" resulted in 500 (Internal Server Error); invoking error handler Command failed org.springframework.web.client.HttpServerErrorException: 500 Internal Server Error http://localhost:8080:>stream create --definition ""time | log"" --name ticktock 13:04:52,066  WARN Spring Shell client.RestTemplate:524 - POST request for ""http://localhost:8080/streams"" resulted in 500 (Internal Server Error); invoking error handler Command failed org.springframework.web.client.HttpServerErrorException: 500 Internal Server Error http://localhost:8080:>stream destroy --name ticktock 13:05:14,207  WARN Spring Shell client.RestTemplate:524 - DELETE request for ""http://localhost:8080/streams/ticktock"" resulted in 500 (Internal Server Error); invoking error handler Command failed org.springframework.web.client.HttpServerErrorException: 500 Internal Server Error  ",1
"Add an MQTT Sink
nan",1
"The user needs the ability to set up a end-time where the trigger should no longer be in effect.
nan",1
"The user needs the ability to set up a misfire policy for a Trigger
2 options are: 1) Fire the trigger immediate - Launch the job when trigger can gather the resources necessary start the job 2) Do nothing - Ignore this job fire time and catch   this scenario can occur if XD is down or resources (threads) are not available at the time a job is to be launched. ",1
"Retrieve description of all registered modules by type
The following will retrieve the names of all module types (eg- sources, sinks, jobs, processors, triggers).   {code} GET /module-types/ {code}  I'm expecting that the plural would be used, but singular would work as well.  The following gets modules of a given type: {code} GET /module-types/{type} {code}  This would be similar to the {{/modules/}} call in XD-265, but it would only return modules of the specified type.  ",5
"Add a Access-Control-Allow-Origin header to responses in order to support cross-origin requests
If we want spring-xd to support interactions through a UI, then it would make sense that we should support the UI coming from a separate origin.  This way, the UI could remain a separate project and be served from wherever we want.  So, we would need to add the header to *all* outgoing REST responses. We may also need to add a {{Access-Control-Allow-Methods}} header as well.  In the short term, the {{Access-Control-Allow-Origin}} header could be hard coded to a specific url (I'm using http://localhost:9889 for now), but in the long term we would need this configurable.",5
"User wants the ability to persist the final state of a job (success or failure)
nan",1
"Use wants the ability to persist Trigger Context
nan",1
"User wants the ability to persist the state of a Trigger Instance
nan",1
"User wants the ability to limit the total number of jobs a trigger can have running simultaneously
nan",1
"User wants the ability to limit the total number of jobs running simultaneously
nan",1
"User wants to setup a priority for triggers. 
 In the case that there are not enough resources to fire a trigger, the highest priority will be fired first.",1
"User wants the ability to exclude certain days (like holidays) for a trigger to fire.
  Commonly called Calendar support ",1
"The user needs the ability to pause and resume triggers ad-hoc.
A pause means that a trigger will wait to fire its job until after the pause is removed.  It does not apply the misfire behavior. ",1
"The user needs the ability to pause and resume triggers based on a calendar.
nan",1
"User wants to be able to know what triggers are associated with a job
nan",1
"User wants a list of currently executing jobs
nan",1
"Deploy Spring XD on Hadoop YARN
This is a master ticker tracking work enabling XD to run on Hadoop as Yarn application.",1
"Shell should display error messages returned from the server
For example, using tcpdump I can see both an exception and message information:  'HTTP/1.1 500 Internal Server Error Server: Apache-Coyote/1.1 Content-Type: application/json;charset=UTF-8 Transfer-Encoding: chunked Date: Fri, 12 Jul 2013 13:38:26 GMT Connection: close  275 [{""links"":[],""logref"":""MessageHandlingException"",""message"":""org.springframework.context.ApplicationContextException: Failed to start bean 'org.springframework.integration.monitor.IntegrationMBeanExporter#0#92e27d20-4c48-4131-866c-91b6cb642ca2'; nested exception is org.springframework.jmx.export.UnableToRegisterMBeanException: Unable to register MBean [MessageChannelMonitor: [name=nullChannel, sends=0, receives=0]] with key 'xd.tap1:type=MessageChannel,name=nullChannel,index=1,module=log'; nested exception is javax.management.InstanceAlreadyExistsException: xd.tap1:type=MessageChannel,name=nullChannel,index=1,module=log""}]  However the client only shows:  http://localhost:8080:>tap create --name ""tap1"" --definition ""tap@test1.file | log"" --deploy true 14:38:26,113  WARN Spring Shell client.RestTemplate:524 - POST request for ""http://localhost:8080/taps"" resulted in 500 (Internal Server Error); invoking error handler Error creating tap 'tap1'  The error doesn't seem to be logged in the XD Admin server either, so the information is effectively lost.  ",2
"Add JSON conversion to tuple
Support toString() to emit JSON by default. Should be backed by a simple strategy to allow the possibility of other representations.  Also provide toTuple(String json). This supports seamless mapping JSON<->Tuple in XD",3
"JMX shouldn't register taps or streams if the creation fails
There's a lifecycle problem when a tap creation fails (e.g. because the DSL syntax is wrong). Subsequent attempts to create the tap will fail with an error:   [{""links"":[],""logref"":""MessageHandlingException"",""message"":""org.springframework.context.ApplicationContextException: Failed to start bean 'org.springframework.integration.monitor.IntegrationMBeanExporter#0#92e27d20-4c48-4131-866c-91b6cb642ca2'; nested exception is org.springframework.jmx.export.UnableToRegisterMBeanException: Unable to register MBean [MessageChannelMonitor: [name=nullChannel, sends=0, receives=0]] with key 'xd.tap1:type=MessageChannel,name=nullChannel,index=1,module=log'; nested exception is javax.management.InstanceAlreadyExistsException: xd.tap1:type=MessageChannel,name=nullChannel,index=1,module=log""}]  Disabling JMX solves the issue.   reproduce Create a bad stream definition name 'bad' Try to recreate with the same name, but correct stream definitions.  The system will report that the stream already exists.",3
"Upgrade to spring-data-hadoop 1.0.1.RC1
spring-data-hadoop 1.0.1.RC1 provides flavors for commonly used Hadoop distros/versions and we should make use of that.",1
"Create JDBC sink
we need a JDBC sink for writing to HAWQ (using int-jdbc:outbound-channel-adapter and postgresql JDBC driver)  ",3
"Batching JDBC channel adapter
we need a batching JDBC channel adapter (int-jdbc:outbound-channel-adapter is not batching statements AFAICT) ",8
"Add spring-xd-hadoop distro specific sub-projects
we need to modify build adding two sub-projects for spring-xd-hadoop: one for hadoop 1.1.2 and one for phd1 (Pivotal HD) to pull in transitive dependencies for correct Hadoop distro",8
"Modify startup script of xdadmin/xdcontainer to allow specifying hadoop distro to use
we need to modify startup script to use hadoop 1.1.2 as default or phd1 when specified with --hadoopDistro=phd1",5
"Create JSON to tab-delimited text transformer script
We need a generic script that can do JSON to tab-delimited text transformation for data written to HDFS/HAWQ external tables. Users should be able to specify columns/fields to be included.",8
"Avro sink for HDFS
We need a sink that can write data in Avro serialized format. This story is for investigating what we would need to do to support that. The Spring Integration Kafka adapter provides Avro support for Kafka.",8
"The stream definition is not deleted in redis container when the stream is destroyed
This only happened with distributed mode that uses redis as store. The single mode which uses in memory store works fine.  Step to reproduce:  Create stream: curl -X POST -d ""name=httptest"" -d ""definition=http|log"" http://localhost:8080/streams  redis 127.0.0.1:6379> keys *httptest* 1) ""modules:httptest"" 2) ""streams.httptest"" 3) ""stream.definitions.httptest""  Delete Stream: curl -X DELETE http://localhost:8080/streams/httptest  redis 127.0.0.1:6379> keys *httptest* 1) ""streams.httptest"" 2) ""stream.definitions.httptest""  stream still there not deleted  Recreate the stream curl -X POST -d ""name=httptest"" -d ""definition=http|log"" http://localhost:8080/streams  Got: <?xml version=""1.0"" encoding=""UTF-8"" standalone=""yes""?><errors xmlns:ns2=""http://www.w3.org/2005/Atom""><error logref=""StreamAlreadyExistsException""><message>There is already a stream with name 'httptest'</message></error>",2
"Support pagination in list() command for streams
Spring HATEOAS is here to help. Nonetheless, there are currently a number of outstanding issues, namely:  https://github.com/SpringSource/spring-hateoas/pull/98 https://jira.springsource.org/browse/SPR-10262#comment-91685 https://github.com/SpringSource/spring-hateoas/pull/94  Creating a issue here for future reference",5
"Add accepted type logic to module
A module can declare one ore more payload types it will accept. This will inform the runtime re. automatic payload conversion.  This can be done in the module XML configuration and processed by StreamPlugin",3
"Add conversion support to ChannelRegistrar and ChannelRegistry 
Implements automatic conversion. Provide APIs to channel registry to register Payload conversion. Includes Redis and Rabbit transport.",8
"In certain scenarios a job can be redeployed more than once
In a scenario where we are using the same job definition i.e. Job.xml and we create Job Instance Foo.  If I create and deploy Foo2 using Job.xml  I will see only 2 job definitions(correct), but I will see the job run 3 times.  If I create Foo3 & deploy, I will see 3 job definitions(correct), but the jobs will run 5 times.  ",2
"Running Job with time delay (non cron) launches 2 instances before job is supposed to fire
nan",2
"When creating a job with the fixed-delay parameter in the shell command fails
Need to change the name from fixed-delay to fixed_delay or fixedDelay.  System rejects the '-'.",1
"Job Delete/Destroy Command for shell
nan",2
"Create required infrastructure to easily perform integration testing of shell commands
nan",10
"Create stories to enable the use of Spring Shell's 2.0 branch testing facilities 
We need a few steps  1. Investigate if we need to move off Spring Shell 1.0 dependency, e.g. need to use code in Spring Shell 2.0 branch  2. If we need to use code in Spring Shell 2.0 branch, we need to release a Spring Shell 1.1 M1 release with appropriate code changes.  Create stories related to Shell release.  3. Determine and document the basic recipe for doing integration tests.  4. Create stories to provide integration tests for each existing command",5
"All controllers to return XYZResource objects not the raw domain objects.
Resource objects should be returned from all controller methods.  MVC Tests should be added to check returned values.",2
"StreamsController to return paged results for list() 
See implementation used for Steams and apply to jobs, taps, triggers.",4
"Rename controllers to have pluralized named (e.g. JobsController)
See implementation used for Steams and apply to jobs, taps, triggers.",1
"Introduce distinction between TapDefinition and Tap (the instance)
Rename existing Tap class to something else.",3
"StreamDeployer to implement ResourceDeployer
StreamController to not access the repository instance directly, all access to go through StreamDeployer  ",4
"Exception Consistency
Favor using custom  exceptions instead of using Assert.notNull, review usage and make changes.  Eg. if a stream can't be found (or another definition) a 'XYZNotFoundException' instead of Assert.Null on the return value of a findOne method ",3
"Rename create to 'save' in ResourceDeployer
To be consistent with Spring Data Repository method names.",1
"Remove duplicate code in ResourceDeployer implementations, create abstract base class.
Only â€˜createâ€™ in TapDeployer has some additional code to check if the stream exists, could take place in another location.",4
"Implement common set of controller methods
Save : Save a XYZDefinition - method used to be 'create'  Delete : Delete a XYZDefinition - method used to be called 'destroy'  Deploy : Deploy a XYZDefinition  Undeploy : Undeploy a XYZDefinition  List : List a XYZDefinition         Returns PagedResources<XYZDefinitionResource>  Display : Get specific information about a XYZDefinition    Create other stories for each Controller and include in this weeks sprint  ",2
"Document MQTT Source and Sink
nan",1
"Disable Collection to Object conversion in DefaultTuple
DefaultFormattingConversionService provides Collection -> Object conversion which will produce the first item if the target type matches. Here, this results in an unfortunate side effect, getTuple(List<Tuple> list) would return a Tuple which is misleading. In this case it is preferable to treat it as an error if the argument is not a Tuple.",3
"Fix Sonar build!
Caused by that weird annotation dependency problem that I worked around for compile. But Sonar complains.    One solution would be to add Jackson 2 to the Sonar ""classpath"", but I did not manage to do that.    ",3
"Better UX when admin is not running
Current behavior is to just have a prompt of ""unknown:>""  I think any return value of a @CliCommand method is not shown b/c the whole infrastructure is not up at that time",1
"Cron Jobs stop firing when a named trigger is created and deployed
nan",3
"If a job is created that uses a trigger that has not been created and deployed it throws a 500 error instead of a 400
nan",2
"Stream Plugin cleanup
	public StreamPlugin(){ 		postProcessContextPath = CHANNEL_REGISTRY; 	}  Subclasses should not directly update superclass fields.",1
"Delete a trigger from Shell
Shell command to delete a trigger.     Note: this command will only remove the trigger definition not modifying the jobs that use the trigger.",1
"Jobs are created even though they have an invalid definition
Also, if you deploy the Job it will fail, but then you can't delete the job.",2
"Add ""How to Build Spring-XD"" instructions to the documentation
We need to determine where this information could fit in.  It can be either in ""README"" at the project home page or ""Getting started"" wiki page.",1
"Fix XD modules parameters with ""-"" to use camel case
XD-482 addresses the use of camel case in 'fixed-delay' job module parameter name. and, we need to fix the same for other module parameters wherever '-' is being used. ",2
"Support pagination in list() command for jobs
See XD-477",1
"Support pagination in list() command for taps
See XD-477",1
"Support pagination in list() command for triggers
See XD-477",1
"Investigate intermittent failure of RedisStreamDefinitionRepositoryTests
the test for findAll often fails for me when running inside gradle. (Could not reproduce inside eclipse)    I already tried fixing it by using a different redis key space, but to no avail.  One explanation would be if gradle runs tests concurrently, but my understanding is that it does not.",3
"Ensure that each controller's list() returns PagedResources
nan",4
"Create XD integration test framework
Add top level utility methods to manage XD runtime (deploy, start and stop). These methods will be used by underlying integration tests to control runtime test environment.  ",2
"Create XD shell integration test parser
Since most of the xd.shell.itests will do more then one thing (deploy a stream, start it, add a tap, add a job, stop, etc) we decided to decouple writing testcases with running it. Test cases will be written in spring-shell scriptlets. Scriptlets are json files with command and expectedResult as tokens.  Here's an example:  {code} { ""testscript"":   [ {""command"" : ""stream create --definition ""http | file"" --name http2file"",     ""result"" : ""Created new stream 'http2file'""},    {""command"" : ""stream list"",     ""result"" : ""...""}  ] } {code}  A parser will parse scriptlets, executes it and asserts on expected results.   ",3
"Add CI job in bamboo to run XD integration tests
CI job will run integration tests that are tagged for CI build.",1
"Create proper test coverage for Controllers
Create proper test coverage for Controllers",8
"Address already in use for tomcat/hsqldb should fail completly
Currently, process is left running and if logs / sysout are not monitored, you have no clue",3
"Admin should fail immediately if Rabbit is not running
Container currently does that.",3
"Add Server Runtime Info to Banner
Standalone Admin currently has no shiny banner as container has. More importantly, it does not say which port it's listening on, the transport used, etc.",3
"Upgrade to Jackson 2.2.2
nan",2
"Modules need to validate their parameters at create time.  
We need to fail fast.",3
"All parameters for modules need to use ""hump case"" formerly camel hump
nan",1
"The parser should be able to handle a parameter name with a '-' hyphen embedded.
Right now it treats it as a new parameter start and fails.",1
"Cannot create tap if you have already tried to create an invalid one of same name
From the shell:  {code} > stream create --name aaa --definition ""time|log"" Created new stream 'aaa'  > tap create --name aa --definition ""tap aaa | log"" Command failed org.springframework.xd.rest.client.impl.SpringXDException: XD111E:(pos 8): Unexpected token.  Expected 'dot(.)' but was 'pipe(|)' tap aaa | log  >tap create --name aa --definition ""tap aaa . log"" Command failed org.springframework.xd.rest.client.impl.SpringXDException: There is already a tap named 'aa' {code}  Looks like the first tap was created even though there was a parse error.  And so the second attempt to create the tap failed due to an existing tap.",1
"Parser blows on modules names with '-'
Tried to create a module named ""tcp-poll"" and got this:  XD108E:(pos 3): missing expected character '-'  tcp-poll --host=54.208.22.193 --port=8081 | log      I believe this should be supported, and indeed we have several module names of this form already",5
"Update jobs section to use shell
nan",1
"Provide .settings formatting rules so that they're shared
Thinking about using the official SpringSource rules as a template",3
"Make module files classpath aware
Currently, living at the root of the project, those files don't benefit from IDE SI awareness.  Make it so that they belong to a java project which sees the correct version of the SI jars used.  Has impact on the build.gradle file",5
"Create new implementations of existing infrastructure (syslog adapters and TaskExecutors)
nan",10
"Create SI components that wrap Reactor's TCP server
nan",4
"Review existing Reactor syslog codec implementation
nan",2
"Create XD module for syslog-tcp-reactor 
Still keep existing one.",3
"Check for high CPU usage with syslog-tcp-reactor module
nan",2
"Adapt SpringOne 2012 UI code from keynote demo of election results to use XD
Existing code: https://github.com/ghillert/springone2012",6
"Create shell integration tests for stream lifeycle
create, delete, deploy streams...",2
"Create shell integration tests for tap lifeycle
creating taps, deleting",2
"Create shell integration tests for trigger lifeycle
creating triggers, deleting triggers",4
"Create shell integration tests for job lifeycle
creating job defs, deploying jobs, undeploying jobs, deleting job defs",4
"Develop infrastructure to enable testability of commands
This requires to boostrap the singlenode admin server in process, submit commands to the shell programmatically, and assert on the results of executing the command.",5
"Investigate using Redis txs and pipeline for Inbound/Outbound Q Adapter perf improvements
Investigate using transactions and pipelining to improve performance of both the inbound and outbound RedisQueue channel adapters. Involves testing against a pre-release of SDR 1.1 M2.",5
"Broadcast Undeploy Requests
Use an 'undeploy' topic to broadcast undeploy requests to all containers.    Applies to Redis and Rabbit transports, not local.    Also, rename {{ModuleDeploymentRequest}} to {{ModuleOperationRequest}} with an enum {{DEPLOY}}, {{UNDEPLOY}}.",5
"Add Message Source for error messages returned to users
nan",3
"Refactor Module to Encapsulate Group and Index
Currently many methods take module, group, index - defining a module instance; group and index can be encapsulated in {{Module}} so one arg can be passed around.",2
"Handle Pagination in Spring XD Shell
nan",5
"Fix In-Memory Analytics
Most of the infrastructure and code cleanup has been done for In-Memory Analytics. The only remaining issue is that, by including memory-analytics.xml from common.xml, we're actually creating e.g. a new InMemoryCounterRepository that is different from the one present in the Admin process space.    This story involves fixing that. It may actually be done as part of XD-353, handling the ""local"" transport as a special case (context inheritance) rather than import based on xd.transport",3
"Display a counter
nan",2
"Display a Field Value Counter
nan",2
"Display an Aggregate Counter
nan",3
"Display a Gauge
nan",1
"Display a Rich Gauge
nan",2
"Leverage fieldNameToCounterNameMap in FieldValueCounterHandler
FieldValueCounterHandler was first written to support setting several fields at once, but the current constructor / field-value-counter.xml does not use it.  Either leverage that feature or simplify code",3
"Add ""trigger list"" support to Spring XD Shell
nan",1
"Add status column for 'stream list'  shell command result
{{stream list}} shell command should display status of the stream (deployed, undeployed)",1
"Add additional options to File source
Seems like the current file source results from an initial POC. Very few things can be parameterized, including the polled directory that needs to be in /tmp/xxx  To be useful in production, we might want to revisit",3
"Separate Module Context Refresh from Context Start
Split plugin module processing into pre and post processing where preprocessing is done before the context is refreshed and post processing is done after the refresh, but before the start.    In the Stream plugin, wire the module into the {{ChannelRegistry}} during post processing, instead of using the {{ChannelRegistrar}}.",5
"make application/json the default output type for the REST API?
the top-level URL works via simple curl (without an Accept header) or the browser:    {code}  > curl http://localhost:8080    {""links"":[{""rel"":""streams"",""href"":""http://localhost:8080/streams""},{""rel"":""triggers"",""href"":""http://localhost:8080/triggers""},{""rel"":""jobs"",""href"":""http://localhost:8080/jobs""},{""rel"":""taps"",""href"":""http://localhost:8080/taps""},{""rel"":""counters"",""href"":""http://localhost:8080/metrics/counters""}]}  {code}    However, trying any of those links then fails, e.g.:    {code}  > curl http://localhost:8080/streams    <?xml version=""1.0"" encoding=""UTF-8"" standalone=""yes""?><errors xmlns:ns2=""http://www.w3.org/2005/Atom""><error logref=""HttpMessageNotWritableException""><message>Could not marshal [PagedResource { content: [links: [&lt;http://localhost:8080/streams/mqttdemo&gt;;rel=&quot;self&quot;]], metadata: Metadata { number: 0, total pages: 1, total elements: 1, size: 20 }, links: [] }]: null; nested exception is javax.xml.bind.MarshalException   - with linked exception:  [com.sun.istack.internal.SAXException2: unable to marshal type &quot;org.springframework.xd.rest.client.domain.StreamDefinitionResource&quot; as an element because it is not known to this context.]</message></error></errors>  {code}  ",2
"CORS support
XD instances will not accept xhr requests from browsers whose page origin does not match the XD instance.      As an example, the Kodiak UI is served from a different process (and url) than the XD instance.  When users open the Kodiak UI in a browser, requests from the browser to the XD instance, but these requests will fail due to cross-site scripting limitations.    CORS (Cross-Origin Resource Sharing) is a way to get around this.  We can configure the server to accept requests from browsers whose origins are not the XD instance.    I have this working in a local branch and will submit a pull request.      More information:  CORS Spec: http://www.w3.org/TR/cors/  SPR-9278 CORS support for SpringFramework  ",5
"HTML Doco has font issues for [source]
It seems the html rendering of documentation is using a variable width font for some of the code (esp. [source,sh] apparently) rendering.  Weird thing though is that when mouse hovering over some of them, they went back to fixed width. See screenshot.  http://static.springsource.org/spring-xd/docs/current-SNAPSHOT/reference/html/#_start_the_runtime_and_the_xd_shell ",3
"Create IntegralMetric and IntegralResource types
There is some overlap between the gauge and counter repository types and also between the domain resources used by the REST controllers (CounterResource, GaugeResource). The aggregate counter may wish to return a simple count value for the counter too, which would also be a simple integral value.",3
"Send failing sonar build message to spring-xd mailing list.
nan",1
"Ad-hoc Jobs do not start
After XD-554 the ModuleJobExecutor.start does not get called.  Thus jobs that are ""run one time"" do not fire.",3
"Failure when creating/deploying stream leaves invalid stream registry/definitions in the Repository implementations.
reproduce  1) Create a bad stream definition name 'bad'  Try to recreate with the same name, but correct stream definitions.  The system will report that the stream already exists.    ",4
"Documentation for use of conversion service and creating custom processing modules that use the Tuple data structure.
nan",2
"Regression test existing functionality of stream/taps based on introduction of new conversion functionality
make sure nothing is broken - spot check using.    1) ticktock  2) twitter  3) gemfire      ",2
"Upgrade sink and processor modules to use new conversion service
nan",2
"Documentation for using a specific Hadoop distribution
Show how to select a specific hadoop distribution when starting embedded/standalone XDContainer.",1
"Document JDBC module
nan",1
"Documentation for fixed rate triggers
nan",1
"Documentation for deleting triggers
nan",1
"Investigate failures to start a stream when using named channels.
Create a reproducible series of steps or shell integration test.  ",2
"Create shell integration test for named chanels
Expected usage (ATM) would be    // sink channel called foo  http | transform --expression=payload.toUppercase() > :foo  // source channel called foo  :foo > count | file",3
"Prepare Blog post for XD M2
nan",5
"Replace usage of 'raw' curl with shell command to post http data in documentation
e.g. http://localhost:8080:>post httpsource --target http://localhost:9090 --data 10    I believe this will also help to avoid ugly syntax to escaping quotes for json as in the gemfire example.",1
"Change http command to post data by putting 'http' as the main command option
The current http command is of the form    http://localhost:8080:>post httpsource --target http://localhost:9090 --data 10      It isn't intuitive to think 'post', rather the command can be     http post --target http://localhost:9090 --data 10    which will allow us to have support for other http verbs and cleanly separate the namespace from 'hadoop' etc.    The RestShell from which this came was only concerned with http actions, so the leading command classification probably seemed superfluous.  ",1
"Change banner of shell to say only 'xd'
nan",1
"In documentation, replace usage of 'raw' hadoop command with shell 'hadoop' commands
nan",1
"Can't access HDFS using webhdfs protocol
http://localhost:8080:>hadoop config fs --namenode webhdfs://localhost:50070  http://localhost:8080:>hadoop fs ls /  Hadoop configuration changed, re-initializing shell...  run HDFS shell failed. Message is: org/mortbay/util/ajax/JSON    This was on a hadoop 1.0.1 install    The hdfs http interface was available    $ curl -i ""http://localhost:50070/webhdfs/v1/tmp?op=GETFILESTATUS""  HTTP/1.1 200 OK  Content-Type: application/json  Transfer-Encoding: chunked  Server: Jetty(6.1.26)    {""FileStatus"":{""accessTime"":0,""blockSize"":0,""group"":""supergroup"",""length"":0,""modificationTime"":1365015846724,""owner"":""mpollack"",""pathSuffix"":"""",""permission"":""777"",""replication"":0,""type"":""DIRECTORY""}}  ",3
"Modify startup script of xd shell to allow specifying hadoop distro to use
nan",1
"XD Shell needs to support multiple Hadoop distros
From https://github.com/SpringSource/spring-xd/pull/161:  ""The command shell needs to also support different hadoop distribution options. Perhaps the shell just uses a relative path to the location of xd/lib/""",3
"configuration conflict when using ""--transport"", ""local"", ""--store"", ""redis"", ""--disableJmx"", ""true"", ""--analytics"", ""redis""
results in both in-memory and redis based definitions of RichGaugeService - can't satisfy autowiring because there are two candidates.  Had to change --analytics=memory to get the application context to load.",2
"Support named channels when using local transport
Sending data to an incomplete stream which is created using a named sink channel only works when using Redis (or Rabbit?, not tested). Since the in-memory version doesn't use a queue, it will fail if you are using xd-singlenode.    We should use a queue channel with unlimited capacity to allow messages to be sent before the full stream is created.",4
"Dispatcher Has No Subscriber Error when posting a message to a stream
This has been observed intermittently with Redis transport by myself and others when sending a message to a valid stream. Not sure how to recreate it yet.    11:27:10,082 ERROR ThreadPoolTaskScheduler-1 redis.RedisQueueInboundChannelAdapter:126 - Error sending message  org.springframework.integration.MessageDeliveryException: Dispatcher has no subscribers for channel 'org.springframework.context.support.GenericApplicationContext@3f73865d.input'.  ",5
"Deploying with twittersearch source throws Jackson ClassDefNotFound exception
The upgrade to Jackson 2.2 included the following change to the build script  {code}  project('spring-xd-dirt') {  	description = 'Spring XD DIRT'  	configurations {  	  [runtime,testRuntime]*.exclude group: 'org.codehaus.jackson'  	}  {code}    Spring social twitter template depends on these classes  ",1
"Document queue channel capacity configurable when using local transport
nan",3
"Create a abstract base class for rest controllers
nan",3
"RedisAggregateCounterRepository doesn't give proper results back
Both Luke's original code and my refactored PR[1] (which uses same code snippet) seem to behave strangely.    Stored values seem fine, but the getCounts() method seems phony.    To test:  1) stream create foo --definition ""time|log""  2) tap create bar --definition ""tap@foo | aggregatecounter""  3) curl -H ""application/json"" http://localhost:8080/metrics/aggregate-counters/bar    this gives default bucketing (hourly) but chances are that they are empty. ",3
"Create AbstractStreamIntegrationTest that will destory streams that were created during test method execution
Keep track of named streams that were create and use @After to destroy them.",1
"Improve build file distribution tasks
The current flow of gradle tasks is confusing.  Suggest the following changes to simplify the flow.    1. Move the current task logic in zipXD to distZip  2. Have distZip depend on dist  3. Update the 'how to build docs' on the wiki  4. Make sure that the distZip task only shows up once in the list of gradle target. ",1
"Problems with advanced tapping
Start of a test program that can be placed in StreamCommandTests:    {code}  @Test  public void testTappingAndChannels() {    executeStreamCreate(""myhttp"",""http --port=9314 | transform --expression=payload.toUpperCase() | log"",true);    executeStreamCreate(""tap"",""tap @myhttp.1 | log"",true);		    executeStreamCreate(""tap_new"",""tap myhttp.1 > log"",true);				    executeCommand(""http post --data Dracarys! --target http://localhost:9314"");    // TODO verify both logs output DRACARYS!  }    {code}    In the test program see two taps. One using the older style and one using the newer style and '>' so that there is no real tap module source, the log module just gets its input channel wired directly to myhttp.1 (the output of transform).  They should be doing the same thing.  However when run the output for tap_new is missing, all I see is:    {code}  11:39:36,055  WARN New I/O worker #28 logger.tap:141 - DRACARYS!  11:39:36,059  WARN New I/O worker #28 logger.myhttp:141 - DRACARYS!  {code}    No errors are reported, there is just no output for tap_new.",8
"Add ""counter delete"" shell command
Add ""counter delete"" shell command. This also requires implementation of DELETE rest end point at CountersController.",1
"Create list/delete commands for all the metrics
We need to add list/delete commands for the metrics:    InMemoryAggregateCounter  FieldValueCounter  Gauge  RichGauge    Currently, the AbstractMetricsController class has the delete method to delete the metric from the repository. We can probably use the same for all the metrics.",2
"Fix wiki documentation to use xd shell command prompt to read ""xd:>""
We need to fix the github wiki to use the xd shell command prompt ""xd:>"".",1
"Add CONTRIBUTING.md file
Add CONTRIBUTING.md file, use the Spring Integration file as the basis.",1
"Ad-Hoc Job needs to have option for launch and forget
When running an ad-hoc job without the use of a trigger (adhoc or named).  The user has to wait for job to complete before receiving a success.  We need to launch a job and get a success back to the user letting them know the job has been launched.    for example --immediate",4
"Gemfire cache closed when a gemfire module is undeployed
Need to investigate why this is happening, normally setting   {code:xml}  <gfe:client-cache close=""false""/>  {code}  prevents the (singleton) cache from closing when the application context is closed. ",4
"Gradle Import Broken by Hadoop Pseudo Projects
When importing Spring-XD as a gradle project, in STS, while building the model, we get    Root exception:  java.lang.IllegalArgumentException: Project location doesn't exist:   .../spring-xd/spring-xd-hadoop/hadoop11    ./gradlew eclipse creates these directories, but the plugin needs them before running that task    The problem seems to be that these ""projects"" are not really projects.    Perhaps a quick fix would be to commit these directories (with a dummy file) ??",1
"Add deploy/undeploy/destroy 'all' commands for all applicable resources (streams, tap, job & trigger,)
nan",5
"./xd-container  --transport local throws NumberFormatException
./xd-container [OK] ./xd-container --transport redis [OK] ./xd-container --transport rabbit [OK] ./xd-container --transport local [FAIL]  wkoh-mbp:bin administrator$ ./xd-container --transport local Exception in thread ""main"" org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.jolokia.jvmagent.spring.SpringJolokiaAgent#0': Invocation of init method failed; nested exception is java.lang.NumberFormatException: For input string: ""${xd.jmx.port}""   org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1488)   org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:524)   org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:461)   org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:295)   org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:223)   org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:292)   org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:194)   org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:626)   org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:932)   org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:479)   org.springframework.xd.dirt.server.ContainerMain.launch(ContainerMain.java:89)   org.springframework.xd.dirt.server.ContainerMain.main(ContainerMain.java:72) Caused by: java.lang.NumberFormatException: For input string: ""${xd.jmx.port}""   java.lang.NumberFormatException.forInputString(NumberFormatException.java:48)   java.lang.Integer.parseInt(Integer.java:449)   java.lang.Integer.parseInt(Integer.java:499)   org.jolokia.jvmagent.JolokiaServerConfig.initConfigAndValidate(JolokiaServerConfig.java:211)   org.jolokia.jvmagent.JolokiaServerConfig.init(JolokiaServerConfig.java:84)   org.jolokia.jvmagent.JolokiaServerConfig.<init>(JolokiaServerConfig.java:68)   org.jolokia.jvmagent.spring.SpringJolokiaAgent.afterPropertiesSet(SpringJolokiaAgent.java:78)   org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1547)   org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1485) 	... 11 more",2
"Fix ChannelRegistry Cleanup During Module Undeploy
nan",2
"NPE on stream destroy
xd:>stream create ticktock --definition ""time | log"" --deploy true  18:45:13,310  WARN Spring Shell client.RestTemplate:524 - POST request for ""http://localhost:8080/streams"" resulted in 400 (Bad Request); invoking error handler  Command failed org.springframework.xd.rest.client.impl.SpringXDException: There is already a stream named 'ticktock'    xd:>stream destroy ticktock  18:45:16,505  WARN Spring Shell client.RestTemplate:524 - DELETE request for ""http://localhost:8080/streams/ticktock"" resulted in 500 (Internal Server Error); invoking error handler  Command failed org.springframework.xd.rest.client.impl.SpringXDException: java.lang.NullPointerException      Caused by: java.lang.NullPointerException    org.springframework.xd.dirt.module.ModuleDeployer.undeploy(ModuleDeployer.java:143)    org.springframework.xd.dirt.module.ModuleDeployer.handleMessageInternal(ModuleDeployer.java:97)      ",2
"Rich Gauge doco is outdated
The RichGauge section does not mention the ""alpha"" parameter in redis output, nor does it explain its meaning.",2
"Document JSON quoting behavior in shell
This may be an issue following the search/replace from curl to Shell, but for example, this documentation line does not work:    http post --target http://localhost:9000 --data ""{\""symbol\"":\""VMW\"",\""price\"":72.04}""    The backslash prior to quote is left in the payload (and hence Jackson chokes on it)    We need clear rules about quoting at the shell level",3
"Integration tests for ""DSL Reference"" examples
nan",5
"Fix text-table rendering
nan",1
"Documentation for rabbit source
http://static.springsource.org/spring-xd/docs/1.0.0.BUILD-SNAPSHOT/reference/html/#sources    should have 'rabbit' added to the list and also the corresponding section that shows some basic usage.",1
"Documentation for jms source
http://static.springsource.org/spring-xd/docs/1.0.0.BUILD-SNAPSHOT/reference/html/#sources    should have 'jms' added to the list and also the corresponding section that shows some basic usage.",1
"Documentation for file source
http://static.springsource.org/spring-xd/docs/1.0.0.BUILD-SNAPSHOT/reference/html/#sources    should have 'file' added to the list and also the corresponding section that shows some basic usage.",1
"Create a rabbit sink module and documentation
https://github.com/springsource/spring-xd/wiki/Sinks    should have 'rabbit' added to the list and also the corresponding section that shows some basic usage.",2
"Deployed streams should be restarted on container start
When using Redis store, stored deployed streams should be deployed on container restart.",5
"Conversion Enhancements
Content-Type during transport transit is not the same as the content-type within modules.    ""Real"" transports always use byte[] which may contain raw byte[] from a source, a byte[] converted from a String (which may or may not already contain JSON), or a byte[] containing JSON converted by the transport on the outbound side.    The transport needs to convey which of these was applied on the outbound side so it can properly reconstruct the message.    Retain any content-type header that already exists in the message, and restore it.    For Rabbit, use normal SI/Rabbit headers to convey this information.    For Redis, add the information to the byte[].",8
"CommandResult return sucess even if hadoop shell command fails
when automating tests for creating http|hdfs stream, I run into an issue where CommandResult object always set success=true even if the actual hadoop shell command fail.  == valid hdfs url getShell().executeCommand(""hadoop config fs --namenode hdfs://localhost:8020""); CommandResult cr = getShell().executeCommand(""hadoop fs ls /"");  == output Found 2 items drwxr-xr-x   - administrator supergroup          0 2013-08-05 17:18 /user drwxr-xr-x   - administrator supergroup          0 2013-08-05 17:18 /xd  CommandResult [success=true, result=null, exception=null]  == invalid hdfs url getShell().executeCommand(""hadoop config fs --namenode hdfs://localhost:8021""); CommandResult cr = getShell().executeCommand(""hadoop fs ls /"");  == output Bad connection to FS. command aborted. exception: Call to localhost/127.0.0.1:8021 failed on connection exception: java.net.ConnectException: Connection refused CommandResult [success=true, result=null, exception=null]  Ideally, we should set success=false if hadoop command fail and if hadoop command succeeds, we should set success=true and populate result= output from hadoop command instead of result=null  ",3
"Making an http post with json double quoted will hang the shell.
Was following the (now updated) directions for gemfire-cq source.    xd:> stream create --name stocks --definition ""http --port=9090 | gemfire-json-server --regionName=Stocks --keyExpression=payload.getField('symbol')""  xd:> stream create --name cqtest --definition ""gemfire-cq --query='Select * from /Stocks where symbol=''VMW''' | file""  xd:> http post --target http://localhost:9090 --data ""{""symbol"":""VMW"", ""price"":73}""    The double quotes were causing a problem with xd-singlenode    Aug 06, 2013 5:38:15 PM org.jboss.netty.channel.SimpleChannelUpstreamHandler  WARNING: EXCEPTION, please implement org.springframework.integration.x.http.NettyHttpInboundChannelAdapter$Handler.exceptionCaught() for proper handling.  org.springframework.integration.transformer.MessageTransformationException: org.springframework.integration.MessageHandlingException: org.springframework.integration.transformer.MessageTransformationException: Expected a ':' after a key at 22 [character 23 line 1]    org.springframework.integration.transformer.MessageTransformingHandler.handleRequestMessage(MessageTransformingHandler.java:73)    org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:137)    org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:73)    Using single quotes inside the json brackets worked, need to investigate.",2
"Update to Spring Shell 1.1.0.M1 release
nan",1
"Add list command for AggregateCounter
nan",2
"Refactor test cases to move away from inheritance model of utility methods for streams, counters 
Model the API more akin to SpringXDOperations api.",2
"Set Default Hadoop Name Node for Shell
Currently, you have to set the default name node every time your start the shell. We should do 2 things:   - Provide a default Name node Set Default Hadoop Name Node for Shell: hdfs://localhost:8020 - Should we provide some form of persistence? It kind of sucks that you have to re-specify the name node every time the shell starts up  {code} xd:>hadoop fs ls / You must set fs URL before run fs commands {code} ",2
"Fix JavaDoc Warnings
/home/gpr/Documents/github/spring-xd/spring-xd-analytics/src/main/java/org/springframework/xd/store/AbstractRedisRepository.java:196: warning - @param argument ""the"" is not a parameter name.  /home/gpr/Documents/github/spring-xd/spring-xd-module/src/main/java/org/springframework/xd/module/AbstractPlugin.java:93: warning - @return tag has no arguments.  /home/gpr/Documents/github/spring-xd/spring-xd-rest-domain/src/main/java/org/springframework/xd/rest/client/util/RestTemplateMessageConverterUtil.java:63: warning - Tag @link: reference not found: StreamDefinitionResource.Page  /home/gpr/Documents/github/spring-xd/spring-xd-rest-client/src/main/java/org/springframework/xd/rest/client/TapOperations.java:40: warning - @param argument ""control"" is not a parameter name.  /home/gpr/Documents/github/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/server/options/OptionUtils.java:29: warning - @parame is an unknown tag.  /home/gpr/Documents/github/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/stream/MissingRequiredDefinitionException.java:38: warning - @param argument ""name"" is not a parameter name.  /home/gpr/Documents/github/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/stream/StreamServer.java:102: warning - @return tag has no arguments.  /home/gpr/Documents/github/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/stream/TapDefinition.java:57: warning - @return tag has no arguments.  /home/gpr/Documents/github/spring-xd/spring-xd-module/src/main/java/org/springframework/xd/module/AbstractPlugin.java:93: warning - @param argument ""group"" is not a parameter name.  /home/gpr/Documents/github/spring-xd/spring-xd-module/src/main/java/org/springframework/xd/module/AbstractPlugin.java:93: warning - @param argument ""index"" is not a parameter name.  /home/gpr/Documents/github/spring-xd/spring-xd-module/src/main/java/org/springframework/xd/module/AbstractPlugin.java:101: warning - @param argument ""group"" is not a parameter name.  /home/gpr/Documents/github/spring-xd/spring-xd-module/src/main/java/org/springframework/xd/module/AbstractPlugin.java:101: warning - @param argument ""index"" is not a parameter name.  /home/gpr/Documents/github/spring-xd/spring-xd-dirt/src/main/java/org/springframework/integration/x/json/TypedJsonMapper.java:133: warning - @return tag has no arguments.  /home/gpr/Documents/github/spring-xd/spring-xd-shell/src/main/java/org/springframework/xd/shell/util/UiUtils.java:60: warning - @CloudApplication is an unknown tag.  /home/gpr/Documents/github/spring-xd/spring-xd-tuple/src/main/java/org/springframework/xd/tuple/TupleJsonMarshaller.java:26: warning - @param argument ""tupleToStringConverter"" is not a parameter name.  /home/gpr/Documents/github/spring-xd/spring-xd-tuple/src/main/java/org/springframework/xd/tuple/TupleJsonMarshaller.java:26: warning - @param argument ""stringToTupleConverter"" is not a parameter name.  /home/gpr/Documents/github/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/stream/dsl/DSLException.java:89: warning - @return tag has no arguments.  /home/gpr/Documents/github/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/stream/dsl/StreamLookupEnvironment.java:60: warning - @return tag has no arguments.  ",1
"catch erroneous hadoop config fs --namenode url early
currently, we can specify any bogus url using 'hadoop config fs --namenode' without any warning.  e.g. hadoop config fs --namenode hdfs://localhost:8888  doing a 'hadoop fs ls /' will catch the error and throw exception. Ideally, we should catch the bogus url error early in the 'hadoop config fs' command. similar to 'admin config server --uri'",2
"Use External Connection Factory in TCP Syslog Source
WARN log emitted because the embedded connection factory does not get an application event publisher.    Will be fixed in SI M3 (INT-3107).",1
"Revert XD-624 When SI 3.0.M3 is Available
nan",1
"Shell: RestTemplate not posting using UTF-8 
The *http post* command uses the default MediaType by the RestTemplate, which in return triggers the default *StringHttpMessageConverter* which itself uses the default charset *ISO-8859-1*.    This creates issues when posting special characters.",1
"shell command ""stream list"" fails
Run the shell command {{stream list}} and you get the following error:  {code} xd:>stream list Command failed org.springframework.http.converter.HttpMessageNotReadableException: Could not read JSON: Unrecognized field ""metadata"" (class org.springframework.xd.rest.client.domain.StreamDefinitionResource$Page), not marked as ignorable (3 known properties: , ""links"", ""content"", ""page""])  at [Source: sun.net.www.protocol.http.HttpURLConnection$HttpInputStream@30721965; line: 1, column: 148] (through reference chain: org.springframework.xd.rest.client.domain.Page[""metadata""]); nested exception is com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException: Unrecognized field ""metadata"" (class org.springframework.xd.rest.client.domain.StreamDefinitionResource$Page), not marked as ignorable (3 known properties: , ""links"", ""content"", ""page""])  at [Source: sun.net.www.protocol.http.HttpURLConnection$HttpInputStream@30721965; line: 1, column: 148] (through reference chain: org.springframework.xd.rest.client.domain.Page[""metadata""]) {code}  You get a similar error when running any of the following:  {code} tap list trigger list job list {code}",1
"Streams created without a '|' (substreams) are being typed by the parser as a Job
nan",3
"Problem with tapping on a module using a named sink channel
{code} mystream = http | transform --payload=expression.toUpperCase() > :foo tap mystream.transform | log {code} This appears to fail because we can't tap into whatever was created to represent the named channel 'foo'. There is an @Ignore test in StreamCommandTests called testTappingModulesVariationsWithSinkChannel() which checks this.  (The parser is currently resolving 'tap mystream.transform' to 'tap --channel=foo'.)",2
"StreamCommandTests - asserting sink contents sometimes failing
There are some asserts in StreamCommandTests that are commented out (see the TODOs in there). These asserts are verifying the contents of the various sinks employed in the tests. I am finding that if the tests are run all together with the asserts enabled (run all StreamCommandTests), some of the assertions fail, with something like:    {code}  org.junit.ComparisonFailure: expected:<[DRACARYS!  ]> but was:<[]>    org.junit.Assert.assertEquals(Assert.java:115)    org.junit.Assert.assertEquals(Assert.java:144)    {code}    When run individually the tests succeed. Not sure if it is timing (checking sinks before they've been written to) or something else...",2
"Pluralize test classes in package org.springframework.xd.shell.command
The classes under test are pluralized. Therefore, the test classes themselves should reflect that. E.g. rename *JobCommandTests* to *JobCommandsTests* as it tests class *JobCommands*. Please check all tests in that package for correct naming.",1
"Shell: HTTP Post - Allow posting of local file contents
E.g. allow for posting of JSON data stored in local files.    * Allow users to specify the *content-type*.  * Ensure that Unicode data (UTF) posts correctly.",4
"Rabbit Source - Make Default QueueName == Stream Name
Consistency with JMS Source.",1
"Fix guava dependency for hadoop20 and phd1 
Spring Xd currently ships with Guava 12.0 while Hadoop 2.0.5 and Pivotal HD 1.0 depends on 11.0.2 - this could lead to classpath problems if we include both.",3
"OOTB source modules with poller should use fixed-delay
pollers should standardize on fixed-delay vs fixed-rate. The value should accept a property with a standard name like 'interval'",1
"x-xd-* Transport Content-Type Leakage
The {{AbstractReplyProducingMessageHandler}} in the Rabbit transport exposes the internal transport content-type, if none existed on the original transported message.",1
"Channel Registry Refactoring
Factor out common Redis/Rabbit {{ChannelRegistry}} code.  Also, factor out common inbound/tap code (very similar).  Change transport nternals to Use AbstractTransformer instead of {{ARPMH}} and {{BridgeHandler}}.",4
"Unclear error message using hadoop fs shell commands without configuring a URL
When using the shell, I forgot to configure the Hadoop URL via ""hadoop config fs --namenode hdfs://pivhdsne:8020""  I then did: xd:>hadoop fs ls /xd/tweets You must set fs URL before run fs commands  The error message should tell me how to set the URL (via the hadoop config command). I had to go back to the documentation to figure it out. It's also not entirely grammatically correct.",1
"Update error message for usage of hadoop rm with --recursive option
I had the following interaction with the shell. It does work if I do ""hadoop fs rm /xd/tweets --recursive"". Either the order shouldn't matter or the doc should be more clear on placement of the option.  xd:>hadoop fs rm /xd/tweets To remove directory, please use fs rm --recursive instead xd:>hadoop fs rm --recursive /xd/tweets java.lang.IllegalArgumentException: Failed to convert '/xd/tweets' to type boolean for option 'recursive' Cannot convert /xd/tweets to type Boolean. ",2
"Cannot start xd-container with the --hadoopDistro option
Trying to use xd-container with PHD, and therefore need to start with --hadoopDistro. I get the following error:    $ bin/xd-container --hadoopDistro phd1  17:11:20,305 ERROR main server.ContainerMain:59 - ""--hadoopDistro"" is not a valid option  ",3
"Problem with tapping and > (source channels)
This is a follow on from XD-592. In that bug we fixed up the ability to use tap with pipe.  Tap when used as a source channel should also work (and should deploy in a more optimized fashion since source channels can be directly connected to the subsequent module, creation of a pass-through tap instance isn't necessary).  This test shows the syntax that should work and the current information about how it fails:  {code} public void testTapSourceChannel() throws IOException {   FileSink sink1 = newFileSink();   FileSink sink2 = newFileSink();    stream().create(""myhttp"",     ""http --port=9314 | transform --expression=payload.toUpperCase() | filter --expression=true > :foo"");    // fails with: java.lang.IllegalArgumentException: bean 'myhttp.1' is already   // registered but does not match the required type   tap().create(""wiretap1"", ""tap myhttp.transform > transform --expression=payload.replaceAll('a','.') | %s"", sink1);    // fails in TapDefinition ctor with: java.lang.IllegalArgumentException:   // streamName cannot be empty or null   tap().create(""wiretap2"", ""tap :foo > transform --expression=payload.replaceAll('a','.') | %s"", sink2);    httpPostData(""http://localhost:9314"", ""Dracarys!""); } {code}  I suspect part of the problem initially lies with the code around EnhancedStreamParser that builds the module deployment requests from the Ast parsed from the input DSL string.  Whether a source channel was originally specified with 'tap' is captured in that Ast but that knowledge doesn't appear to be getting used.  ",4
"Update architecture diagram to show rabbit in addition to redis to communicate between admin and containers
Figure 1 here https://github.com/SpringSource/spring-xd/wiki/Architecture should also show Rabbit as an option, since otherwise people will think we are tied to redis.",1
"Map column names with underscore to camelCase style keys for JDBC sink
We need to add support for matching column names with underscores like ""user_name"" and map them to camel case style keys like ""userName"" in the JdbcMessagePayloadTransformer.",3
"Connection props in rabbit.properties ignored by xd-admin and xd-container
I modified rabbit.hostname in rabbit.properties and xd-container still attempted to find Rabbit at localhost with --transport rabbit. Looks like the PPC for xd-container and xd-admin is not pointing to rabbit.properties",2
"Find a way to contribute redis.properties to Rabbit Container PPC
Need to undo the recent add of rabbit.properties to xd-common.xml. Tried to work around this by configuring rabbit-container with only the PPC it needs (pointing to only rabbit.properties), but this caused issues with redis-analytics later requiring redis.properties. Would be nice to have a way for redis-analytics to contribute redis.properties or something similar...   Also, strictly speaking, the local admin server does not even need redis.properties, let alone rabbit.properties, so we should find a cleaner way to configure this.",3
"Split RichGauge in 2
support of the alpha parameter is awkward and can confuse people who are expecting a simple average mean.  Consider splitting RichGauge in two flavors: arithmetic and exponential.  Involves quite some work at the repository, handler and REST level though... ",5
"HTTP source should emit raw payload
Current implementation converts to a String.    See if we can emit raw payload (given that we also emit content-type header)    Setting to 8 points, as this may have lots of implications down the line though",8
"Regression test on file source
As we overwrote changes to file source by mistake, let's add some regression tests, esp. to the file location.    Plan on extending the utility source and sink functionality",3
"Remove 'substream' from the documentation
This will come back in M3 once we iron out the issues.",1
"Eclipse build path error after running gradle -> refresh source folders in Eclipse
After running gradle -> refresh source folders on the spring-xd-module project in Eclipse, there is an error because the {{src/test/java}} folder is missing.    Solution is to add a placeholder file.",5
"End point to retrieve a list of all XD artifacts of all kinds
Any kind of sophisticated artifact retrieval mechanism in XD will need to grab more than one kind of artifact at once.  For example, if I want to see all taps, streams, triggers, and jobs (ie- everything), I need to make 4 http requests.  I can imagine dashboards that need to display information on artifacts of multiple kinds.  There will also need to be a way to pass a query to return a sub-set of artifacts, but that should be designed separately.",5
"File Source Name and Duplicates options not working as documented
The doc says the name option is ""the absolute path to the directory to monitor for files"" but it actually seems to be the name of a dir in /tmp/xd/input. Not sure which is the correct behavior. Also, ""name"" as an option name seems a little vague. Maybe something like ""--directory""?    Also, if I set --duplicates=true, it actually prevents duplicates (setting prevents-duplicates to true)",2
"Configure Jackson ObjectMappers to Allow Single Quotes 
Allow Json payloads from external sources, e.g., http post to contain single quoted field names and values. This is required where XD uses Jackson to convert payloads from Json to object or Tuples.",1
"Trim output from http post shell command to two lines
Instead of     xd:>http post --target http://localhost:9898 --data ""hello world""  > POST (text/plain;charset=UTF-8) http://localhost:9898 hello world  > 200 OK  > Content-Length: 0  > Connection: keep-alive  >   Success sending data 'hello world' to target 'http://localhost:9898'      have    xd:>http post --target http://localhost:9898 --data ""hello world""  > POST (text/plain;charset=UTF-8) http://localhost:9898 hello world  > 200 OK      or better yet    xd:>http post --target http://localhost:9898 --data ""hello world""  > 200 OK POST (text/plain;charset=UTF-8) http://localhost:9898 hello world             ",1
"Intra-Module ""Pipe"" Naming
Consider 2 Admins/Containers that are using discrete Redis instances but a shared Rabbit instance for the transport.    If two different streams are deployed on each, but with the same stream name, the Rabbit queues will be common (e.g. foo.0), causing crosstalk.    Stream names must be unique across all container instances sharing Rabbit infrastructure.    I am not sure what the solution is; two instances of the *same* stream *do* need common queues but instances of different streams need a qualifier of some kind (container name?). I guess it's not *that* big of an issue because, if they're sharing infrastructure, they're likely to be sharing a stream repo too - in which case you'd need unique stream names.",3
"Update to Spring-Data-Redis 1.1.0.M2
Remove the {{NoOpRedisSerializer}} and use the non-serialization feature of M2.",2
"Rename spring-xd-shell to xd-shell
nan",1
"Batch Jobs: Add the ability to provide JobParameters
nan",4
"No indication of failure in shell when deploying job referencing nonexistent trigger
I see the following output on the shell if I create a job and reference a non-existent trigger. There's a big stack trace in the server log, but nothing on the shell side indicating failure. A subsequent ""jobs list"" also shows the job. The same thing happens if I deploy an undeployed Job after deleting its associated Trigger.    $ job create --name helloWorldJob --definition ""myjob --trigger=nonexistenttrigger""  Successfully created and deployed job 'helloWorldJob'",2
"Use correct FS_DEFAULT_NAME_KEY constant based on Hadoop version used
Keep getting the following warning:    WARN Spring Shell conf.Configuration:817 - fs.default.name is deprecated. Instead, use fs.defaultFS    Should switch to use the runtime value of the FS_DEFAULT_NAME_KEY constant based on Hadoop version used.",3
"File sink filename should default to having a '.out' suffix.
nan",1
"AggregateCounter display command options with ""lastHours"" and ""lastDays""
It would be nice to have ""lastHours"" and ""lastDays"" options for aggregatecounter display command.",1
"Remove Redis Transport Headers from Tapped Stream
Redis transport headers are not removed in taps.",2
"Add Accepted Media Type Support to Tap
Currently, the initial tap module accepted media types are not retrieved from the module when creating the tap.",5
"Rest-Client should not force usage of Joda Time
The Rest-Client project should not impose Joda to the user.",2
"AggregateCounterTemplate should not use Joda
Not only should it not use Joda (see XD-668) but the passing of dates currently relies on default formatting",3
"TAB completion for existing entities
Provide Shell TAB completion when referencing an existing entity",4
"Add support for dynamic routing
It should be possible to create streams like the following which rely upon named channel support and dynamic routing capabilities:    {code}  http | somerouter  :x > xtransformer | hdfs  :y > ytransformer | hdfs  {code}    The 'somerouter' processor could return ""x"" or ""y"" which determines the downstream path for each message.    This should be implemented in such a way that any developer adding a router module would only need to deal with existing Spring Integration semantics (in this case, only considering the return of ""x"" or ""y"" - whether it be SpEL or a POJO method invocation). Perhaps in the plugin that modifies a module context, we could simply add a new ChannelResolver implementation (by adding that ChannelResolver as a bean and/or a BeanPostProcessor that configures that as the resolver for any router, if necessary). That ChannelResolver would have a reference to the ChannelRegistry so that the router actually sends its messages to those shared channels. The shared channels themselves would have been created as long as a valid downstream flow has been defined.",16
"Ugly error messages in shell when not connected
should go thru the list of all commands available and make sure that a simple ""not connected"" message is returned instead of something like this:  {code} org.springframework.web.client.ResourceAccessException: I/O error on GET request for ""http://localhost:8080"":Unexpected end of file from server; nested exception is java.net.SocketException: Unexpected end of file from server     at org.springframework.web.client.RestTemplate.doExecute(RestTemplate.java:498) .... {code} ",5
"http post in shell incorrectly mentions default of --target option
the value for --target is required (there is no default), but the hint for that option states otherwise:  {code} xd:>http post --target http post --target required --target: the location to post to; default if option not present: 'http://localhost:9000' {code} ",1
"Add Spring Batch word-count Sample to Spring XD Samples repo 
nan",2
"Cannot chain json-field-value-filter & json-field-extractor
Because StringToJsonNodeTransformer expects a String as input, one cannot chain json related processors.    A simple solution would be to also accept Jackson IN and forward it directly in that case.",1
"add PropertyAccessor for Tuple fields in SpEL
Example using name:    {code}  filter --expression=""payload.myfield.startsWith('foo')""  {code}    Example using index:    {code}  filter --expression=""payload.2.startsWith('foo')""  {code}    This should support nested keys as well:    {code}  filter --expression=""payload.myfield.subfield.startsWith('foo')""  {code}  ",5
"Cannot destroy tap if tapped stream is already destroyed
xd:>tap destroy mytap  16:44:41,850  WARN Spring Shell client.RestTemplate:524 - DELETE request for ""http://localhost:8080/taps/mytap"" resulted in 400 (Bad Request); invoking error handler  Command failed org.springframework.xd.rest.client.impl.SpringXDException: XD116E:(pos 4): unrecognized stream reference 'foo'  tap foo.http | log    Tap is then still listed when I do a ""tap list""",2
"change accepted-media-types to accpted-content-types
nan",1
"Update settings file and reformat existing codebase.
Please put in suggestions for the current .settings file.  Maybe one suggestion is to not format on save?",1
"Add Simple Batch  Sample to Spring XD Samples repo
This example should require no code. Just the basic XML.",2
"Modify file sink to avoid dot with empty suffix
The expression currently appends ""."" + ${suffix} (where the default suffix is 'out').    If the suffix value were an empty String, this would lead to the file name ending with a dot. We should update the expression so that it only appends the dot if the suffix is not empty. This might be possible with a ternary expression.",1
"Change jmxDisabled option to jmxEnabled and do not enable by default
also, the current behavior is broken; it checks if the property is set but does not actually check whether it's true or false",2
"Refactor Taps to Avoid Transport Hop
Taps are currently source modules.    They could be refactored to simply bridge the tapped module's tap pub/sub topic directly (with conversion) to the first tap module's input channel.    Note - ensure destroy works. Currently the tap is destroyed by the simple fact it is a module; if it's no longer a module we'll need special handling to stop/remove the tap adapter.",16
"Support Named Taps (or Similar)
Provide some syntax allowing multiple tap points to be directed to a named channel.    e.g.   tap foo.4 > namedTap  tap bar.2 > namedTap    or    :tap.foo > counter",8
"provide user friendly messages when dealing with invalid gemfire sink
xd:>stream create --name testgemfire --definition ""http --port=8887 | gemfire""   16:20:28,503  WARN Spring Shell client.RestTemplate:524 - POST request for ""http://localhost:8080/streams"" resulted in 500 (Internal Server Error); invoking error handler Command failed org.springframework.xd.rest.client.impl.SpringXDException: org.springframework.beans.factory.BeanDefinitionStoreException: Invalid bean definition with name 'region' defined in null: Could not resolve placeholder 'regionName' in string value ""${regionName}""",4
"error messages not thrown when creating gemfire sink without starting gemfire server
stream create --name test1 --definition ""http --port=8827 | gemfire-server"" Created new stream 'test1'  stream create --name test2 --definition ""http --port=8828 | gemfire-json-server"" Created new stream 'test2'  even if gemfire server is not started, streams are successfully created. This behavior is inconsistent with hdfs where if hdfs connection is not available, creating stream using 'http | hdfs' will fail.  ",4
"Support serialization/deserialization of Message payloads across JVMs across all transports.
  String -> byte[] (string.getBytes())   byte[] -> byte[] (no serialization)  Pojo -> configured serialization    ",1
"Simplify ""instance"" deployment code 
AbstractDeployer has 4 subclasses, 3 of which override e.g. deploy() making the boilerplate factorization ineffective.    Introduce an intermediate class for those deployers that support the concept of an instance (Stream, Tap, Job to some extent)",8
"Change JMX option to reference 'enableJmx' instead of 'disableJmx'
Make the default value of enableJmx false until we have tested/documented JMX functionality",2
"http source module should copy Content-Type header to SI MessageHeaders.CONTENT_TYPE
nan",1
"Add deploy/undeploy commands for taps
nan",3
"Need to check the deployment requests in StreamsControllerIntegrationTest
we should check the actual deployment requests were built correctly for each module in the testCreateUndeployAndDeleteOfStream test.  Currently we just use the anyListOf check.",2
"Upgrade SDR to get rid of temporary no-op serializer
Spring Data Redis 1.1 M2 added the ability to use RedisTemplate with binary data. We should switch to that instead of the no-op serializer we were forced to implement previously.",3
"Shell integration tests should be able to be run across all transports
Automate running integration tests on all supported transports",10
"Handling tap operations on a tap that has reference to a deleted stream
When trying to undeploy/destroy a tap that has reference to an already deleted stream fails with the following exception: Command failed org.springframework.xd.rest.client.impl.SpringXDException: XD116E:(pos 4): unrecognized stream reference '<stream_name at the tap defintion>'.  As expected, the StreamConfigParser's lookupStream fails to find the stream name as the stream doesn't exist in the repository.   In this scenario, what is a better way to handle the tap operations.  Should we undeploy the tap when the stream is destroyed? ( though I don't see an easy way to find the taps that use a specific stream).",2
"Remove XD UUIDGenerator in favor of the new SI provided one
Remove   <bean id=""idGenerator"" class=""org.springframework.xd.dirt.container.UUIDGenerator"" />  (container.xml)    Delete org.springframework.xd.dirt.container.UUIDGenerator  remove compile dependency on eaio from build.gradle",1
"Refactor Message conversion in ChannelRegistrySupport
After an initial attempt which was not ready for M2 we are rethinking our strategy. One of the fundamental things we have come to realize is that its important to treat serialization and type conversion as separate concerns.  Serialization:  A core principle is the consumer should by default receive exactly what the producer sent:    -   If the producer sends a byte[] payload then no serialization is required.      -   A String payload can use simple byte conversion, taking the Charset into account    -  Transporting an Object uses whatever serialization is configured (json, xml, avro, protocol buffer, java.io, msgpack, etc.).   The actual serialization performed for each message must be shared with the producer and consumer. I.e., the consumer needs to know which case above applies to each payload. Currently we are using the MessageHeaders.CONTENT_TYPE defining custom mime types for this (The designated header is subject to change)  Conversion:     - The consumer optionally defines one or more content-types (read MimeType) it can accept in order of preference. If no conversion succeeds, we can either give them the byte[] payload or throw an exception (configurable?).  Examples:  - Consumer accepts a Java Object (application/x-java-object;type=example.Foo).  Assume for simplicity, the consumer may send a JSON String, or a Foo.  On the receiving end we need to distinguish a String payload containing a JSON representation of Foo from a serialized Foo payload. If the payload is a String, we need to know that its original content is application/json. We are currently using a 2nd ""original-content-type"" message header to supply this information.  So in the first case we have (conceptually) content-type: ""XD plain text"" , original-content-type ""application/json"".  In the second case we have content-type: ""XD Serialized JSON"" original-content-type not used in this case since the serialized JSON includes type information (using Jackson conventions which are a bit brittle).   -If the producer type is different from the accepted type, we use the conversion service and the consumer must register appropriate converters.   A twist for XD that may be generally relevant is that some optimization is possible when we know the bytes represent JSON:  -  Tuple conversion:  Since we serialize using JSON and we know how to transform JSON <->Tuple,  we can convert any Object  payload or any JSON String to a Tuple.  We can avoid the two step deserialization+conversion,  e.g.   1) Foo->JSON->Foo 2) Foo->Tuple. ",10
"stack overflow when trying to create a stream with the same name as a module
(NOTE: even if we do want to prevent the use of module names for stream names, we obviously need to avoid a StackOverflowError)    to reproduce:    start the xd-singlenode container    start the xd-shell, and type the following:    {code}  xd:>stream create time --definition ""time | log""  {code}    that should produce an Internal Server Error output message    check the xd-singlenode console, and find:    {code}  SEVERE: Servlet.service() for servlet [xd] in context with path [] threw exception [Handler processing failed; nested exception is java.lang.StackOverflowError] with root cause  java.lang.StackOverflowError    java.lang.StringValue.from(StringValue.java:24)    java.lang.String.<init>(String.java:178)    org.springframework.xd.dirt.stream.dsl.Token.<init>(Token.java:46)    org.springframework.xd.dirt.stream.dsl.Tokenizer.lexIdentifier(Tokenizer.java:195)    org.springframework.xd.dirt.stream.dsl.Tokenizer.process(Tokenizer.java:62)    org.springframework.xd.dirt.stream.dsl.Tokenizer.<init>(Tokenizer.java:41)    org.springframework.xd.dirt.stream.dsl.StreamConfigParser.parse(StreamConfigParser.java:65)    org.springframework.xd.dirt.stream.dsl.StreamConfigParser.parse(StreamConfigParser.java:55)    org.springframework.xd.dirt.stream.dsl.StreamConfigParser.lookupStream(StreamConfigParser.java:545)    org.springframework.xd.dirt.stream.dsl.StreamNode.resolveModuleNodes(StreamNode.java:135)    org.springframework.xd.dirt.stream.dsl.StreamNode.resolve(StreamNode.java:122)    org.springframework.xd.dirt.stream.dsl.StreamsNode.resolve(StreamsNode.java:53)    org.springframework.xd.dirt.stream.dsl.StreamConfigParser.parse(StreamConfigParser.java:74)    org.springframework.xd.dirt.stream.dsl.StreamConfigParser.parse(StreamConfigParser.java:55)    org.springframework.xd.dirt.stream.dsl.StreamConfigParser.lookupStream(StreamConfigParser.java:545)    org.springframework.xd.dirt.stream.dsl.StreamNode.resolveModuleNodes(StreamNode.java:135)    org.springframework.xd.dirt.stream.dsl.StreamNode.resolve(StreamNode.java:122)    org.springframework.xd.dirt.stream.dsl.StreamsNode.resolve(StreamsNode.java:53)    org.springframework.xd.dirt.stream.dsl.StreamConfigParser.parse(StreamConfigParser.java:74)    org.springframework.xd.dirt.stream.dsl.StreamConfigParser.parse(StreamConfigParser.java:55)    org.springframework.xd.dirt.stream.dsl.StreamConfigParser.lookupStream(StreamConfigParser.java:545)    org.springframework.xd.dirt.stream.dsl.StreamNode.resolveModuleNodes(StreamNode.java:135)    org.springframework.xd.dirt.stream.dsl.StreamNode.resolve(StreamNode.java:122)    org.springframework.xd.dirt.stream.dsl.StreamsNode.resolve(StreamsNode.java:53)  	...ad nauseum  	  {code}  ",2
"JobRepository should be persistent and shared across xd-admin/xd-container
The current code is creating an in-memory job repository for each batch job that is launched.  This makes it impossible to query the tables in the job repository across the cluster.  A single job repository that is backed by a file need to be shared across all jobs that are a launched.    *Implementation Suggestions*  * The XDAdmin server should create the job repository schema, if not found, in a HSQLDB database, when it starts up  * The bean definitions should be added to the same context that the analytics are being loaded in as it is already shared across xd-admin/xd-container.  The â€˜analyticsâ€™ context should be renamed to something more generic, â€˜shared parent contextâ€™ or something.  * There is some clean up (removal) of the code in the current JobPlugin, META-INF/spring-xd/plugins/job/common.xml wouldnâ€™t be needed anymore.  That might be all, not sure.    *How to verify it works*  * A JUnit test that verifies the spring batch tables were created in the job repository when xd-admin is launched.  This would require deleting the backing db file before instantiating singlenode/xd-admin/xd-container.    * If you start the xd-container it should be able to find the necessary DataSource/JobRepository beans information to be able to contact the database.  We donâ€™t have DI style JUnit tests so this will required getting a reference to the xd-container and itâ€™s application context, and performing â€˜getBean(JobRepository.class)â€™        ",8
"User should be able to specify Rabbit virtual host
Need to support Rabbit virtual host property in properties file and as args to Rabbit source and sink",2
"A batch job can be launched by sending a message on a channel
*Description*  When a job is â€˜createdâ€™ in SpringXD, a â€˜control-channelâ€™ for that job is also created.  The listener for that channel will receive a message, be able to take the â€˜jobParametersâ€™ and other launch information from the message, and be able to launch/run the job.    NOTE: I can see a few other stories that should probably be made to break this up after writing it. I put estimate of 10 for now, but we should break this up.  Here are some suggestions    1. create â€˜data onlyâ€™ JobParametersBean equivalent with primitive types  2. create jobLauncher source  3. create jobParameterTransformer processor  4. Refactoring of ChannelRegistryâ€™s aliasHing to use a callback strategy.      *Implementation Suggestions*    job create --name helloWorldJob --definition ""myjob --somePropertyToOverride=someValue    * This would not execute the batch job immediately, but instead register the job definition and deploys a â€œjobLauncherï¿½? and the job definition to an XD-Container.    * The XD-Container that receives the deploy request message will create a module application context, will also create a channel with the Channel registry named after the job, e.g. :myJob.  This should be a pub/sub channel from the point of view of the middleware.  From the point of view of the spring integration channel, it should ideally be of the executor channel.  There is a limitation in the current implementation of ChannelRegistry now as â€˜createInboundâ€™ only creates direct channels.  The boolean â€˜aliasHintâ€™ should probably be extended to some type of callback that creates a channel.  The aliasHint was added to address the case of LocalChannelRegistry creating or looking up a queue backed channel or a direct channel.  There will be a consumer on the SI channel in the module application context that will be responsible for getting the job launch information and launching the job.  The launching of the job may need to be explicitly done in a separate thread if direct channels are created by the ChannelRegistry.  The contents of the message should be something similar to the current â€œJobParametersBeanï¿½?, it needs to be easily serializable with simple types via JSON over the wire.  The current impl of â€œJobParametersBeanï¿½? has ObjectMapper, so that may require a bit of reworking.  The handler of the message will use the jobLauncher to launch the job, using the information in the JobParametersBean.    * The â€˜myjobâ€™ can then be launched by sending a message, perhaps this is handled by having a jobLauncher source    jobLauncher [--jobParameters <jobParameters>] [--dateFormat <dateFormat>] [--numberFormat <numberFormat>] [--makeUnique <makeUnique>]  > :myJob    e.g. with no-args    jobLauncher  > :myJob    *How to verify it works*    With the test HelloSpringXDTasklet, we should be able to create the job    job create --name helloSpringXD --definition ""myjob""    This will not launch the job (as mentioned in the â€˜implementationâ€™ section.    It would then be launched by     jobLauncher  > :myJob    where jobLauncher is a new source.    Ideally would like to be able to test a data driven triggering.  This would require a new file source that doesnâ€™t use the file-to-string-transformer, but lets a File object be the payload.    file | jobParameterTransformer > :myJob  ",16
"Support use of separate control and message transports
Control transport - Deploy/Undeploy requests  Message transport - Inter module communication    Currently complicated because starting a Job for example currently uses message transport vs control transport. Testing scenarios require local control and ability to switch to various message transports.    One option is to change the interpretation of transport command line arg depending on SingleNode, Admin, or Container. e.g.  SingleNode --transport rabbit (always use local for control messages)  Admin (requires --transport, message transport does not apply)  Container (enforces the same transport for message and control. Local optimization done via composite module)    The other option is use a separate transport for control vs messages.   Either way need to rationalize the design with respect to control and module messages  ",2
"Add SingleNodeMain class 
SingleNodeMain(){    	parent = new AC(..)     		AdminMain.launch(parent);     		ContainerMain.launch(parent);  }    This should make startup processing more consistent and symmetrical  ",3
"Support for @Configuration based module definitions
nan",3
"Upgrade Lettuce and Netty
Upgrade Lettuce to 2.3.3 and subsequently Netty to 3.6.6",2
"TapCommandTests hangs when using a lazily instantiated Lettuce connection
A change was made in spring-data-redis to instantiate the shared Lettuce connection lazily instead of when the context is initialized. This caused TapCommandTests to hang due to a Netty worker thread trying to initialize the Lettuce connection (Lettuce uses Netty). The change was temporarily backed out of SDR, but we need to consider using a NettyExecutionHandler in NettyHttpInboundChannelAdapter or making the HTTP module's ""input"" channel an ExecutorChannel to avoid potentially long operations like from happening in an I/O thread.    Also, we need to address why this failure simply hangs the shell. Shell was hung waiting on IO here:    org.springframework.http.client.SimpleClientHttpResponse.getRawStatusCode(SimpleClientHttpResponse.java:47)    org.springframework.http.client.AbstractClientHttpResponse.getStatusCode(AbstractClientHttpResponse.java:32)    org.springframework.xd.shell.command.HttpCommands$1.hasError(HttpCommands.java:93)    org.springframework.web.client.RestTemplate.doExecute(RestTemplate.java:484)    org.springframework.web.client.RestTemplate.execute(RestTemplate.java:460)    org.springframework.web.client.RestTemplate.postForEntity(RestTemplate.java:335)    org.springframework.xd.shell.command.HttpCommands.postHttp(HttpCommands.java:103)    sun.reflect.GeneratedMethodAccessor135.invoke(Unknown Source)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)    java.lang.reflect.Method.invoke(Method.java:597)    org.springframework.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:191)    org.springframework.shell.core.SimpleExecutionStrategy.invoke(SimpleExecutionStrategy.java:64)   ringframework.shell.core.SimpleExecutionStrategy.execute(SimpleExecutionStrategy.java:57)  	- locked <7fd3c7d40> (a java.lang.Class for org.springframework.shell.core.SimpleExecutionStrategy)    org.springframework.shell.core.AbstractShell.executeCommand(AbstractShell.java:127)    org.springframework.xd.shell.AbstractShellIntegrationTest.executeCommand(AbstractShellIntegrationTest.java:99)    org.springframework.xd.shell.AbstractShellIntegrationTest.httpPostData(AbstractShellIntegrationTest.java:112)    org.springframework.xd.shell.command.TapCommandTests.testCreateAndDeployTap(TapCommandTests.java:56)    Full stack trace of server exception:     Aug 19, 2013 9:59:00 AM org.jboss.netty.channel.SimpleChannelUpstreamHandler      WARNING: EXCEPTION, please implement org.springframework.integration.x.http.NettyHttpInboundChannelAdapter$Handler.exceptionCaught() for proper handling.      org.springframework.integration.MessageHandlingException: org.springframework.data.redis.RedisConnectionFailureException: Unable to connect to Redis on localhost:6379; nested exception is com.lambdaworks.redis.RedisException: Unable to connect        org.springframework.integration.handler.MethodInvokingMessageProcessor.processMessage(MethodInvokingMessageProcessor.java:76)        org.springframework.integration.handler.ServiceActivatingHandler.handleRequestMessage(ServiceActivatingHandler.java:67)        org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:137)        org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:73)        org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:115)        org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:102)        org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)        org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:178)        org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:149)        org.springframework.integration.core.MessagingTemplate.doSend(MessagingTemplate.java:304)        org.springframework.integration.core.MessagingTemplate.send(MessagingTemplate.java:165)        org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendMessage(AbstractReplyProducingMessageHandler.java:223)        org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendReplyMessage(AbstractReplyProducingMessageHandler.java:207)        org.springframework.integration.handler.AbstractReplyProducingMessageHandler.produceReply(AbstractReplyProducingMessageHandler.java:172)        org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleResult(AbstractReplyProducingMessageHandler.java:166)        org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:144)        org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:73)        org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:115)        org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:102)        org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)        org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:178)        org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:149)        org.springframework.integration.core.MessagingTemplate.doSend(MessagingTemplate.java:304)        org.springframework.integration.core.MessagingTemplate.send(MessagingTemplate.java:165)        org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendMessage(AbstractReplyProducingMessageHandler.java:223)        org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendReplyMessage(AbstractReplyProducingMessageHandler.java:207)        org.springframework.integration.handler.AbstractReplyProducingMessageHandler.produceReply(AbstractReplyProducingMessageHandler.java:172)        org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleResult(AbstractReplyProducingMessageHandler.java:166)        org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:144)        org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:73)        org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:115)        org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:102)        org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)        org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:178)        org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:149)        org.springframework.integration.core.MessagingTemplate.doSend(MessagingTemplate.java:304)        org.springframework.integration.core.MessagingTemplate.send(MessagingTemplate.java:165)        org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendMessage(AbstractReplyProducingMessageHandler.java:223)        org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendReplyMessage(AbstractReplyProducingMessageHandler.java:207)        org.springframework.integration.handler.AbstractReplyProducingMessageHandler.produceReply(AbstractReplyProducingMessageHandler.java:172)        org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleResult(AbstractReplyProducingMessageHandler.java:166)        org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:144)        org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:73)        org.springframework.integration.dispatcher.BroadcastingDispatcher.invokeHandler(BroadcastingDispatcher.java:121)        org.springframework.integration.dispatcher.BroadcastingDispatcher.dispatch(BroadcastingDispatcher.java:112)        org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)        org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:178)        org.springframework.integration.channel.interceptor.WireTap.preSend(WireTap.java:121)        org.springframework.integration.channel.AbstractMessageChannel$ChannelInterceptorList.preSend(AbstractMessageChannel.java:248)        org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:173)        org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:149)        org.springframework.integration.core.MessagingTemplate.doSend(MessagingTemplate.java:304)        org.springframework.integration.core.MessagingTemplate.send(MessagingTemplate.java:165)        org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendMessage(AbstractReplyProducingMessageHandler.java:223)        org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendReplyMessage(AbstractReplyProducingMessageHandler.java:207)        org.springframework.integration.handler.AbstractReplyProducingMessageHandler.produceReply(AbstractReplyProducingMessageHandler.java:172)        org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleResult(AbstractReplyProducingMessageHandler.java:166)        org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:144)        org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:73)        org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:115)        org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:102)        org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)        org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:178)        org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:149)        org.springframework.integration.core.MessagingTemplate.doSend(MessagingTemplate.java:304)        org.springframework.integration.core.MessagingTemplate.send(MessagingTemplate.java:165)        org.springframework.integration.endpoint.MessageProducerSupport.sendMessage(MessageProducerSupport.java:92)        org.springframework.integration.x.http.NettyHttpInboundChannelAdapter.access$200(NettyHttpInboundChannelAdapter.java:59)        org.springframework.integration.x.http.NettyHttpInboundChannelAdapter$Handler.messageReceived(NettyHttpInboundChannelAdapter.java:122)        org.jboss.netty.handler.codec.http.HttpContentEncoder.messageReceived(HttpContentEncoder.java:81)        org.jboss.netty.handler.codec.http.HttpChunkAggregator.messageReceived(HttpChunkAggregator.java:148)        org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)        org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:459)        org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:536)        org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:485)        org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)        org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)        org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)        org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)        org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312)        org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)        org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)        java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)        java.lang.Thread.run(Thread.java:680)      Caused by: org.springframework.data.redis.RedisConnectionFailureException: Unable to connect to Redis on localhost:6379; nested exception is com.lambdaworks.redis.RedisException: Unable to connect        org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.createLettuceConnector(LettuceConnectionFactory.java:345)        org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.initConnection(LettuceConnectionFactory.java:116)        org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.getSharedConnection(LettuceConnectionFactory.java:325)        org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.getConnection(LettuceConnectionFactory.java:106)        org.springframework.data.redis.core.RedisConnectionUtils.doGetConnection(RedisConnectionUtils.java:81)        org.springframework.data.redis.core.RedisConnectionUtils.getConnection(RedisConnectionUtils.java:53)        org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:173)        org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:153)        org.springframework.data.redis.core.AbstractOperations.execute(AbstractOperations.java:86)        org.springframework.data.redis.core.DefaultZSetOperations.add(DefaultZSetOperations.java:41)        org.springframework.data.redis.core.DefaultBoundZSetOperations.add(DefaultBoundZSetOperations.java:47)        org.springframework.xd.store.AbstractRedisRepository.trackMembership(AbstractRedisRepository.java:202)        org.springframework.xd.analytics.metrics.redis.RedisCounterRepository.increment(RedisCounterRepository.java:88)        org.springframework.xd.analytics.metrics.redis.RedisCounterRepository.increment(RedisCounterRepository.java:82)        org.springframework.xd.analytics.metrics.integration.MessageCounterHandler.process(MessageCounterHandler.java:28)        sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)        sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)        sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)        java.lang.reflect.Method.invoke(Method.java:597)        org.springframework.expression.spel.support.ReflectiveMethodExecutor.execute(ReflectiveMethodExecutor.java:69)        org.springframework.expression.spel.ast.MethodReference$MethodValueRef.getValue(MethodReference.java:97)        org.springframework.expression.spel.ast.CompoundExpression.getValueInternal(CompoundExpression.java:82)        org.springframework.expression.spel.ast.SpelNodeImpl.getTypedValue(SpelNodeImpl.java:102)        org.springframework.expression.spel.standard.SpelExpression.getValue(SpelExpression.java:103)        org.springframework.integration.util.AbstractExpressionEvaluator.evaluateExpression(AbstractExpressionEvaluator.java:126)        org.springframework.integration.util.MessagingMethodInvokerHelper.processInternal(MessagingMethodInvokerHelper.java:230)        org.springframework.integration.util.MessagingMethodInvokerHelper.process(MessagingMethodInvokerHelper.java:129)        org.springframework.integration.handler.MethodInvokingMessageProcessor.processMessage(MethodInvokingMessageProcessor.java:73)      	... 84 more      Caused by: com.lambdaworks.redis.RedisException: Unable to connect        com.lambdaworks.redis.RedisClient.connect(RedisClient.java:176)        com.lambdaworks.redis.RedisClient.connectAsync(RedisClient.java:139)        org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.createLettuceConnector(LettuceConnectionFactory.java:339)      	... 111 more      Caused by: java.lang.IllegalStateException: await*() in I/O thread causes a dead lock or sudden performance drop. Use addListener() instead or call await*() from a different thread.        org.jboss.netty.channel.DefaultChannelFuture.checkDeadLock(DefaultChannelFuture.java:342)        org.jboss.netty.channel.DefaultChannelFuture.await(DefaultChannelFuture.java:231)        com.lambdaworks.redis.RedisClient.connect(RedisClient.java:166)      	... 113 more",5
"cat command doesn't work when same data is listed in file multiple times
$ ./xd-admin --transport redis  $ ./xd-container --transport redis  $ ./xd-container --transport redis    xd:>stream create --name httpStream --definition ""http | file""  Created new stream 'httpStream'  xd:>tap create --name httpTap --definition ""tap httpStream | counter""  Created and deployed new tap 'httpTap'  xd:>http post --target http://localhost:9000 --data ""helloworld""  > POST (text/plain;Charset=UTF-8) http://localhost:9000 helloworld  > 200 OK    xd:>! cat /tmp/xd/output/httpStream.out  command is:cat /tmp/xd/output/httpStream.out  helloworld  xd:>http post --target http://localhost:9000 --data ""helloworld""  > POST (text/plain;Charset=UTF-8) http://localhost:9000 helloworld  > 200 OK    xd:>! cat /tmp/xd/output/httpStream.out  command is:cat /tmp/xd/output/httpStream.out  helloworld  xd:>! cat /tmp/xd/output/httpStream.out  command is:cat /tmp/xd/output/httpStream.out  helloworld  xd:>http post --target http://localhost:9000 --data ""helloworld2""  > POST (text/plain;Charset=UTF-8) http://localhost:9000 helloworld2  > 200 OK    xd:>! cat /tmp/xd/output/httpStream.out  command is:cat /tmp/xd/output/httpStream.out  helloworld  helloworld2  xd:>http post --target http://localhost:9000 --data ""helloworld3""  > POST (text/plain;Charset=UTF-8) http://localhost:9000 helloworld3  > 200 OK    xd:>! cat /tmp/xd/output/httpStream.out  command is:cat /tmp/xd/output/httpStream.out  helloworld  helloworld2  helloworld3  xd:>http post --target http://localhost:9000 --data ""helloworld3""  > POST (text/plain;Charset=UTF-8) http://localhost:9000 helloworld3  > 200 OK    xd:>! cat /tmp/xd/output/httpStream.out  command is:cat /tmp/xd/output/httpStream.out  helloworld  helloworld2  helloworld3  xd:>! cat /tmp/xd/output/httpStream.out  command is:cat /tmp/xd/output/httpStream.out  helloworld  helloworld2  helloworld3  xd:>http post --target http://localhost:9000 --data ""helloworld3""  > POST (text/plain;Charset=UTF-8) http://localhost:9000 helloworld3  > 200 OK    xd:>http post --target http://localhost:9000 --data ""helloworld3""  > POST (text/plain;Charset=UTF-8) http://localhost:9000 helloworld3  > 200 OK    xd:>http post --target http://localhost:9000 --data ""helloworld3""  > POST (text/plain;Charset=UTF-8) http://localhost:9000 helloworld3  > 200 OK    xd:>! cat /tmp/xd/output/httpStream.out  command is:cat /tmp/xd/output/httpStream.out  helloworld  helloworld2  helloworld3  xd:>http post --target http://localhost:9000 --data ""helloworld4""  > POST (text/plain;Charset=UTF-8) http://localhost:9000 helloworld4  > 200 OK    xd:>! cat /tmp/xd/output/httpStream.out  command is:cat /tmp/xd/output/httpStream.out  helloworld  helloworld2  helloworld3  helloworld4  xd:>counter display --name httpTap  9  xd:>    however in the regular shell.    $ cat /tmp/xd/output/httpStream.out   helloworld  helloworld  helloworld2  helloworld3  helloworld3  helloworld3  helloworld3  helloworld3  helloworld4      ",2
"Use ParentLastClassLoader to create the Modules ApplicationContext.
The ParentLastClassloader is located in the spring-hadoop project.  It will resolve classes first looking at the child context and then the parent.  This works well for XD since the we want and dependencies of the module to be considered first and if not found, resolve against the parent. It would be possible to even include other versions of .jars already in the parent classloaders (e.g. Spring Integration jars), but for now, we will not immediately test that case.    SimpleModule needs to change to that we can pass in the classloader to use for creating the application context. The current implementation creates a GenericApplicationContext as a field initializer...that should change to be in the ctor.    ModuleDeployer should implement .BeanClassLoaderAware.  The classloader passed in to the BeanClassLoaderAware callback will be used as the â€˜parentâ€™ when creating the ParentLastClassloader.      The URL[] to pass into ParentLastClassloader should be â€˜nullâ€™ or an empty array in this case of an older style module.  (Hopefully ParentLastClassloader  allows that type of fallback).    *Implementation Suggestions:*  The ModuleDeployer code is where the application context for the module is defined and instantiated.   Here is a possible impl path.    1. Assuming we can always use ParentLastClassloader (even for older style modules), then the ModuleDescriptor needs to return an array of URL[] locations for the module, getURL().  This is passed into the cto for SimpleModule.  The ctor then creates a new application context, creates the parentclassloader, sets the classloader on the application context and then proceeds as normal.   2.AbstractModuleRegistry should try and load the resource from two possible locations, e.g.     ./modules/source/file/config/file.xml    or    ./modules/source/file.xml    The module registry needs to be a bit smarter to know, ah, i see a config directory, let me try ./config/file.xml otherwise just ./file.xml      *How to verify it works.*    1. JUnit test in which one of the ModuleRegistry implementations points to a test directory that contains both old and new style modules.  FileModuleRegistry is probably a good choice here.  Need to test that the new getting for URL[] works as expected.    2. Existing tests should run as they did before, in particular the shell integration tests.",5
"Refactor the file source module to have lib/config directories
Convert a simple module, such as file, to further test that what was done in the previous story, â€œUse ParentLastClassLoader to create the Modules ApplicationContext.ï¿½? works as expected.    *Implementation Suggestions*    Remove from build.gradle the dependency on spring-integration-file and place that jar inside a directory    ./modules/source/file/lib     place the current file.xml inside ./modules/source/file/config    *How to verify it works.*    1. Running tests that currently use the file source, e.g. in spring shell, should work as before  2. When deploying a stream file | log, we should be able to interrogate the channel registry and make sure it found the dependencies for the module, ModuleDescriptor should have a not-null URL[] property.",5
"Deploy a new source module *after* XD-singlenode container has started.
After the xd-singlenode process has started, create a new module that has dependencies not already in the parent application context, and then create a stream that uses the new module.    *Implementation Suggestions*  Develop in the test tree, maybe of the xd-shell project, a new module.  the lib and config should be sititng around in a directory waiting to be copied into the appropriate spot.    Could try http://www.date4j.net/..The config file  would be similar to time.xml but use the date4j class.    *How to verify it works*  In a JUnit test case copy in a new module that has new dependencies, ..copy the lib and config directories into the location where the ModuleRegisty will pick it up.    Deploy a new stream, date4j | file and see if there are contents in the file.  ",5
"Deploy a new job module *after* XD-singlenode container has started.
After the xd-singlenode process has started, create a new job that has dependencies not already in the parent application context, and then create and run a new job that uses the new module.    *Implementation Suggestions*  Develop in the test tree, we can put in the jar and config from https://github.com/SpringSource/spring-xd-samples/tree/master/batch-simple     *How to verify it works.*  In a JUnit test case copy in a new job that has new dependencies, ..copy the lib and config directories into the location where the ModuleRegisty will pick it up.    Deploy a new job and check in the job repository that the job ran and was successful.    ",5
"Batch jobs send job and step events on channels 
This is the other side of launching a job by sending a message.  The Job plugin should add listeners at the job/step level so that the job/step context information can be sent out on a channel.    :myjob.notifications is a suggested channel name that would be created automatically.",8
"Change inconditionnal Thread.sleep() calls in tests to smarter incremental pauses
There are a lot of Thread.sleep() calls with delays chosen in the 1-2 seconds range.  Change to a while loop with smaller pauses until a timeout is reached and give up.  This applies to verification code (e.g. verifying that a counter has expected value) as well as File setup, or http being ready to accept requests etc",8
"Test source module in isolation
Register the module under test and deploy the module  Verify output across all transports  Examples  Be able to start the rabbitmq source just by pointing to modules/source/rabbit.xml, pass in some property file for parameters to be replaced, and outgoing message is placed in a in-memory queue backed channel for use with assertions to verify functionality.  Test that sending json, results in media-type header is set to json  Test that sending POJO -> POJO  Test that sending Tuple ->  Tuple  Test that sending a (JSON) String -> String  Test that sending raw bytes ->  raw bytes  ",5
"Test processor module in isolation
Register the module under test  and have access to a source channel that drives messages into the processor and a output channel where output messages are sent.    Examples  Built-in Message conversion: send JSON to a processor module that accepts Tuples.  ",5
"Test sink module in isolation
Register the module under test Send a message to the sink using a test source and verify the sink contents - this requires checking an external resource - depends on the sink ",5
"Create directory structures and move existing UI code into Spring XD repository
Create a directory structure that best benefits UI development.      The copying of the UI files and other gradle build tasks so that the UI can be run inside the embedded servlet container of XD will be a seperate story",4
"Display the UI from xd-admin container when doing development in eclipse
The UI code will be sitting in one or more top level directories in the repository    This story will address the need to     1) copy over the UI code into a location so that it can be picked up by the embedded servlet container when running inside eclipse",2
"Package up the UI code when building the distribution so that it can be shown by xd-admin
The UI code will be sitting in one or more top level directories in the repository    This story will address the need to     1) copy over the UI code into a location so that it can be picked up by the embedded servlet container when the distribution zip is 'unzipped'.      After running ./xd-admin or ./xd-singlenode one should be able to hit the UI at http://localhost:8999/xd    (just an example)",2
"Add additional embedded servlet container config to load static UI resources
Configure embedded servlet container needs to know where to load the UI code.",2
"Replace 'anonymous' node in XD module bean names
Enhance bean naming strategy or provide a value for the property that binds to this",1
"Add PropertyAccessor for JSON fields in SpEL
{code}  filter --expression=""payload.myfield.startsWith('foo')""  {code}    Example using index:    {code}  filter --expression=""payload.2.startsWith('foo')""  {code}    This should support nested keys as well:    {code}  filter --expression=""payload.myfield.subfield.startsWith('foo')""  {code}    This is related to https://jira.springsource.org/browse/XD-676 and that in turn depends on SI being able to configure SpEL",4
"Create JobLaunchRequest Transformer
The JobLaunchRequest Transformer shall accept the following payloads:    * File  * JSON String  * Properties  * Map  * Tuple    Use/Migrate some of the logic from *JobParametersBean*, e.g. using the *DefaultJobParametersConverter*.    *Special Case File*    When handling a *File*, add special JobParameter *absoluteFilePath* populating it with *message.getPayload().getAbsolutePath()*    * Add unit tests      ",6
"A job will be associated with a named channel when the job is created.  
When creating a job, a named channel will be created with a name of job.<your job name>  i.e. job.foo. ======== Required components: - A Transform See XD-733 - JobPlugin needs to create the NamedChannel for the job and associate the transform.    > Registrar.xml will need an input channel?    > Name channel support will be required. - Add --channel to job rest apis to notify system that a named channel is requested.   *Unit Test  ",5
"Job Repo in container needs access to Admin's HSQLDB
The HSQLDB that stores the JobRepository needs to have its content exposed via TCP (network service) so that container has access to update the status of a job run.   -Needs to have property that enumerates the host and port for the admin that is accessible by the user. -If one is not specified it should default to localhost:9500.  *Testing: - Unit Tests - Bring up module and admin.       ^Verify that default host and port work     ^Verify that container on different machine has access to admin   ",5
"Expose restful services that allow users to view job statuses
This story utilizes BatchAdmin and its restful interfaces to show the state of jobs in Spring XD.      *Steps    - Create a Branch in the BatchAdmin (we don't want to lose history)    - Update the restful API's to XD standards.    - Create bamboo task to push jars to artifactory    - Update gradle.build to pull in the Batchadmin jars.    - Expose the restful calls.",5
"Trigger can send a message to a named channel
Trigger can send a message to a named channel.  For example: trigger create --name mytrigger --definition ""trigger --cron='*/10 * * * * *' --message='Good Luck, we are all counting on you'""  --channel foo   Where the --message contains the message that will be sent to foo job/component.",3
"Rename XDContainer and associated classes to Node
Rename XDContainer, ContainerMain, *ContainerLauncher, ContainerLauncherFactory and any variable or methodNames, bean names, etc. that refer to container in favor of the term 'Node'. Eliminate the dirt.container package, and move Node into .server",2
"Eliminate internal dependencies on System properties
Remove System.setProperty() or System.getProperty() for internal xd properties. Use spring Environment abstraction instead. Also, replace ""."" in property names with '_' (XDPropertyKeys). This is compatible with environment variable names.     As a result, XD should accept System properties or environment variables or command line options. Command line options should have highest precedence. Retain StandardEnvironment order wrt to System properties and environment variable. ",4
"Remove Option parsing code used for tests from Servers
Currently *Main class provide alternate static methods for parsing CLI options. One used for testing does not call System.exit() just throws an exception. This code should be moved to spring-xd-test to support integration testing.",3
"Rename xd-global-beans.xml
The above config contains beans that must be in a common parent context for the AdminServer and Modules. Hence not really global since the (Node) doesn't need them itself. So the name is a bit misleading. Come up with something better.",2
"REST API for Job Management
Spring Batch Admin provides a complete, but outdated implementation style, which covers the full administrative lifecycle of batch jobs, their creation, stop/start, and retrieving information about previous job executions and the status of currently executing job executions.  SpringXD has a different way of deploying, starting, and stopping jobs - by sending messages to containers that run the batch job.  However, the reporting state of a job is still stored in a common job repository.  The purpose of this story is to take the first step to merging in the existing code base that focuses only on the retrieval of information from Spring Batch Adminâ€™s Job controller.  The current â€˜REST APIâ€™ style of these commands should stay as close to the original spring batch admin style as possible.  There are several reasons for this 1. It works, and time to springone is short, and we mgmt has expectations around deliverables that we must strive to meet. 2. It gives Andrew a working contract to start developing a UI 3. We can take on this technical debt, but refactor after RC1 and before GA while and deliver end-user functionality.    Attached is the list of endpoints in spring batch admin ",20
"Ensure that when batch jobs are created, they are created with the job bean definition id equal to the â€˜stream nameâ€™
Unlike in spring-batch-admin, in SpringXD all the jobs the /modules/jobs directory is not â€˜visibleâ€™ to query when the server starts.  Jobs only become visible to XDâ€™s â€˜jobs listâ€™ command once they have been â€˜createdâ€™.      Creating a Job in XD is an opportunity to specify additional values to any property placeholders in the job bean definition.  This isnâ€™t part of spring-batch-admin.    We will not worry about the creation of job definition in this story.  Assume that they have been created already and that the GET for /jobs works as it does now for Spring XD.     We should however, make sure that there is always a replacement of the job name in the job bean definition to match the â€˜--nameâ€™ specified in the command line.  That is â€œjob create --name myjob --description â€œthisfunkyjobï¿½?    will use â€˜myjob to replace <job id=""${xd.stream.name}"" in the file thisfunkyjob.xml    *Implementation Suggestions*    This should hopefully just be a matter of changing job definition files to follow the naming pattern.    <job id=""${xd.stream.name}"" â€¦ />    *How to verify it works*    1. Create a JUnit integration style test that has â€˜job create --name myjob --defintion â€œtestJobï¿½?â€™ and then deploy the job.  The name â€˜myjobâ€™ should appear in the job execution table  ",1
"Add dependency to spring batch admin in spring-xd-dirt
We should depend on  		<dependency> 			<groupId>org.springframework.batch</groupId> 			<artifactId>spring-batch-admin-manager</artifactId> 			<version>${project.parent.version}</version> 		</dependency> 		<dependency> 			<groupId>org.springframework.batch</groupId> 			<artifactId>spring-batch-admin-resources</artifactId> 			<version>${project.parent.version}</version> 		</dependency>  we are using spring batch 2.2.0.RELEASE.   We need to depend on spring-batch-admin version 1.3.0.BUILD-SNAPSHOT   ",2
"Return the list of Jobs from spring-batch-admin
The current XD JobController that returns a list of jobs has quite a different API signature than what is in spring-batch-admin.  To simplify the UI development, a new controller JobAdminController, will be created that lives under the request path /jobs/admin.  The goal is to return a the current JSON structure of spring-batch-admin /jobs/ request and make only minimal changes to implementations of controllers as found .  See #1 in the Doc (link to json output doc for spring batch).    *Implementation Suggestions*    There will need to be some SpringMVC setup that will enable the current style of spring-batch-admin controller requests to co-exist with the existing XD Controllers, e.g. the use of .json for json marshalling etc.  This may in fact be the bulk of time spend in this first story to integration spring-batch-admin style controllers into XD.    A new controller named JobAdminController that in the spring-xd-dirt project in the package org.springframework.xd.dirt.rest.  The JobAdminController will not need to follow the same HATEOAS style as the other controllers at this time.    The current controller in Spring Batch Admin looks like this  {code}      @RequestMapping(value = ""/jobs"", method = RequestMethod.GET)      public void jobs(ModelMap model, @RequestParam(defaultValue = ""0"") int startJob,              @RequestParam(defaultValue = ""20"") int pageSize) {          int total = jobService.countJobs();          TableUtils.addPagination(model, total, startJob, pageSize, ""Job"");          Collection<String> names = jobService.listJobs(startJob, pageSize);          List<JobInfo> jobs = new ArrayList<JobInfo>();          for (String name : names) {              int count = 0;              try {                  count = jobService.countJobExecutionsForJob(name);              }              catch (NoSuchJobException e) {                  // shouldn't happen              }              boolean launchable = jobService.isLaunchable(name);              boolean incrementable = jobService.isIncrementable(name);              jobs.add(new JobInfo(name, count, null, launchable, incrementable));          }          model.addAttribute(""jobs"", jobs);      }  {code}    Something like  {code}  @RequestMapping(value = ""/jobs/admin/jobs"", method = RequestMethod.GET)   public void jobs(ModelMap model, @RequestParam(defaultValue = ""0"") int startJob,              @RequestParam(defaultValue = ""20"") int pageSize) {           // We do *not* have to query the Spring Batch Admin â€œJobServiceï¿½? at this time, but        //  instead use the  JobDeployer to get information about jobs launched by Spring XD          Iterable<JobDefinition> jobDefinitions =  dobDeployer.findAll()         // copy these over to a List<JobInfo> as best as possible, copy name over.         // not sure how â€˜descriptionâ€™ is getting added to the JSON        // pari      }  {code}  *How to verify it works*  A sample job needs to be in the modules/job directory. JobCommandTests/AbstractJobIntegrationTest seems to have what is need to stage a job for   ",5
"Gradle Launch needs to use singlenodemain  vs. admin main
nan",1
"Bootstrap XD on Yarn
1. How XD Yarn application should be packaged and bootstrapped? 2. Where the code should be? Within xd itself or separate repo?",1
"Interacting with XD on Yarn
1. How we talk to the XD instance(s) on Yarn 2. There is a rest interface which location can be exposed either via resource manager or appmaster 3. Technically appmaster could also expose interface which could eihter be proxy for xd rest or dedicated interface implementation(i.e. thrift or spring int)",1
"Comm protocol for appmaster
We need to be able to talk to appmaster which will control the whole xd yarn app.  1. Choose the implementation? Thrift? Spring Int? Something else? ",1
"Container and Grid Control
1. We'll need a system which give better control of what yarn/xd containers are out there and what is a status of those containers. 2. We also need grouping of containers order to choose, prioritize and scale tasks. 3. We need heartbeating of the grid nodes. Hadoop Yarn itself doesn't give enough tools to know if container is ""alive"".",1
"XD UI on Yarn
Technically speaking of we want to integrate XD UI on Hadoop tools we should do it so that the proxy on resource manager works with XD UI. From Hadoop Yarn resource manager point of view this proxied url is the applications tracking url(which is registered when application is deployed).",1
"Restrict Job launcher with more than one batch job configured in job module
Currently the Job launcher launches all the batch jobs configured in the job module.    Please refer, ModuleJobLauncher's executeBatchJob().    This makes the JobRegistry registers with multiple batch jobs under the same Spring XD job name (group name).    Also, it is understood that having multiple jobs configuration under the same config xml is uncommon.",2
"JDBC property settings need to be made externally configurable
We need to have a properties section (documented as well) so that users can setup their jdbc connections for the various components.",2
"Fix Class/Package Tangle Introduced by XD-353
{{container}} and {{event}}. {{XDContainer}} references and is referenced by {{ContainerStartedEvent}} (and stopped).    https://sonar.springsource.org/drilldown/measures/7173?metric=package_cycles&rids%5B%5D=7717    ",1
"Reactor Environment Improvements
Use a profile or similar to only include the {{Environment}} conditionally (currently in module-common.xml.  Also  Jon Brisbin one thing to keep in mind: we talked about having a properties file for XD that configured the RingBuffer et al in a non-default way  Jon Brisbin e.g. no event loop Dispatchersâ€¦a ThreadPoolDispatcher with a large thread pool size (50 threads? 100?)â€¦and maybe even two RingBufferDispatchers: input and output  Jon Brisbin so we might want to change from strictly a default Environment bean to an EnvironmentFactoryBean with a specific configurationâ€¦thinking about it now I maybe should add a namespace element for the Environment",3
"Create a splitter module
The splitter functionality in Spring Integration should be exposed to XD as a processing module.  The splitter should use a SpEL expression to specify how to split the message up.      *Implementation Suggestions*    This should be a simple XML based module definition that has input/output channels and has the SpEL expression parameterized.  The default value of the SpEL expression should result in the message not being split.    *How to check it works*    The current file or tail input source can be used to split up the text in a file into words.  The tail module should be checked to see how many lines of text it will read into memory at once.  The file module node with the file-to-string transformer will only work for small files as it keep the whole file in memory.      If there is a big memory inefficiency in using the tail file input source, create a new story to investigate how to have a file based input source that creates a message per line of text or something that is will not result in excessive memory usage.  ",2
"Create aggregator module
Create a processing module based on SI's aggregator component.  The completion criteria for the aggregator should be a simple count of messages (e.g. received 50 messages) and a timeout so that messages don't stay in the aggregator module for more than 30 seconds.     *Implementation suggestions*    Create an XML based processing module definition using the SI aggregator namespace.  Only the options to support the features in the description should be exposed as property placeholders.    *How to know it works*  A shell style integration test that has a source that sends a known amount of messages.  A ticktock like module would perhaps be a good example.  10 messages sent every 100ms with an aggregator set to a 'aggregate count' of 10, should have 1 message output (perhaps to file sink whose name is based on time as well, is that possible.).  A ticktock example with a 1 second delay and with the aggregator module set to have a timeout of 0.5 seconds will have only one message in the file (again assumign the file name has a timestamp/counter in the filename). ",3
"Create Syslog -> Tuple Reactor Codec; Change UDP Syslog Adapter to Emit a Tuple
UDP and Legacy syslog sources emit a {{Map}}; reactor emits a POJO. Make them consistent and emit {{Tuple}}s.",2
"The xd-singlenode script should have execute permissions
The xd-singlenode script currently has '644' permissions unlike xd-admin and xd-container (which have '755'):     {code}  -rwxr-xr-x  1 mark  staff  5899 Aug 26 16:19 xd-admin  -rwxr-xr-x  1 mark  staff  5955 Aug 26 16:19 xd-container  -rw-r--r--  1 mark  staff  5919 Aug 26 16:19 xd-singlenode  {code}  ",1
"Fix Package Tangle between o.s.xd.dirt.event and o.s.xd.dirt.container
https://sonar.springsource.org/drilldown/measures/7173?metric=package_cycles&rids%5B%5D=7717",3
"Make Spring XD buildable with Java 8
JavaDoc issues are causing the build to fail with Java 8",8
"Add Spring XD Build Plan for Java 8 to Bamboo  
This is issue depends on XD-761  https://build.springsource.org/browse/XD-JDK8",2
"Remove Trigger Module Code 
Triggers will be a source and no longer as a unique module.  * The following have to be removed:  ** spring-xd-dirt:    -- package: org.springframework.xd.dirt.plugins.trigger  -- META-INF: spring-xd/plugins/triggers.xml  -- org.springframework.xd.dirt.stream.TriggerPlugin    *The following beans will require updates to remove the trigger code  ** spring-xd-dirt:    --META-INF: spring-xd/internal/deployers.xml - Remove Triggerdeployer  --org.springframework.xd.dirt.plugins.job.JobPlugin - Remove the registrars for fixedDelay, fixedRate, Cron.  As well as the component selection, only need the job-modules-bean  --Update the tests to use the trigger as a source, instead of the trigger module.  ** spring-xd-shell:  Remove trigger commands and associated tests    ** xd controllers:  Remove trigger controllers and their associated tests   This list cover most but not all the components affected.  --Success criteria--  Successful unit and integration tests.    ",5
"Consolidate Trigger Sources into a single Source
Currently we have 2 trigger sources: trigger & cron-trigger.  The preference is to have a user to just use a single trigger source.  for example: * trigger > :myjob * trigger --cron='...' >:myjob * trigger --fixedDelay='...' > :myjob  One option to handle this is to use spel to reference a bean and then have different trigger beans defined. i.e. trigger='cronTriggerBean'.  Each trigger bean would setup the channel with the correct poller.  ",5
"Remove AutoLaunch feature from batch jobs
Jobs will be started via trigger.  So we won't need the JobTriggerBean.",1
"Parser needs to handle a  ':' embedded in a name.
Also drop the enhanced portion of the EnhancedStreamParser.",8
"Add Email Source
nan",8
"Add Email Sink
nan",8
"Add XMPP Sink
nan",5
"Update Batch Job docs to cover triggers as a source
nan",2
"Shell: Add named channel list command
User shall have the ability to get a listing of available named channels (order by name ascending) from the shell  * Add support to controllers * Add tests",10
"Factor out duplicated SpEL / script logic
See discussion at https://github.com/SpringSource/spring-xd/pull/250/files#r6034885",3
"Tab support inconsistent for http post
When doing *xd:> http post* and press the *tab* key. One should get a list of available options. Right now nothing happens. I have to press *--* and then tab to get the options.  Interestingly, this works for *stream create* + *tab* key",2
"Need to create a Persistent-Job-Registry 
In order to hook up the to get access to all the jobs available the job registry has to be shared.  currently the only implmentation is is the MapJobRegistry.    ====  Testability.  ====  The admin will need to be see all jobs created by its containers.",2
"Document router processor module
for an example, see comments here:  https://jira.springsource.org/browse/XD-671  ",2
"Shell: Remove ""taps list"" command
We should only allow ""tap list"" - currently ""tap list"" AND ""taps list"" are allowed but ""tap list"" does not show up under help.",1
"Add validation on tap definitions that checks for module names that are part of the stream definition
Try:  {code}  stream create --name aa --definition ""time | log""  tap create --name t1 --definition ""tap aa.log | log""  {code}    Results in:    {code}  Command failed org.springframework.xd.rest.client.impl.SpringXDException: java.lang.NullPointerException  {code}",2
"${xd.home}/data shows up after gradlew build
When the XD starts up and does not see its job repo it will create one in the${xd.home}/data directory.  When xd.home is not set in system properties the job repo creates a literal ${xd.home}/data directory.  ",2
"Infinite recursion (StackOverflowError) when trying to process JobLaunchingMessageHandler's ""notifications"" channel output
Transport used: Redis    It looks like when the job is launched, the RedisChannelRegistry's composite handler tries to transform the JobLaunchingMessageHandler's output-channel (notifications) payload which is of type ""org.springframework.batch.core.JobExecution"".  and, This results in Infinite recursion (StackOverflowError).    Please see the stack trace here:    01:09:44,827 ERROR task-scheduler-1 redis.RedisQueueInboundChannelAdapter:148 - Error sending message  org.springframework.integration.MessageHandlingException: org.springframework.context.ApplicationContextException: Failed to start bean 'jobTriggerBean'; nested exception is org.springframework.integration.MessageHandlingException: error occurred in message handler [org.springframework.integration.x.redis.RedisChannelRegistry$CompositeHandler@26f8f92e]    org.springframework.integration.handler.MethodInvokingMessageProcessor.processMessage(MethodInvokingMessageProcessor.java:76)    org.springframework.integration.handler.ServiceActivatingHandler.handleRequestMessage(ServiceActivatingHandler.java:67)    org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:137)    org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:73)    org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:115)    org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:102)    org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)    org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:178)    org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:149)    org.springframework.integration.core.MessagingTemplate.doSend(MessagingTemplate.java:304)    org.springframework.integration.core.MessagingTemplate.send(MessagingTemplate.java:165)    org.springframework.integration.endpoint.MessageProducerSupport.sendMessage(MessageProducerSupport.java:92)    org.springframework.integration.x.redis.RedisQueueInboundChannelAdapter.access$3(RedisQueueInboundChannelAdapter.java:1)    org.springframework.integration.x.redis.RedisQueueInboundChannelAdapter$ListenerTask.run(RedisQueueInboundChannelAdapter.java:145)    org.springframework.scheduling.support.DelegatingErrorHandlingRunnable.run(DelegatingErrorHandlingRunnable.java:53)    java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)    java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)    java.util.concurrent.FutureTask.run(FutureTask.java:166)    java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)    java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)    java.lang.Thread.run(Thread.java:722)  Caused by: org.springframework.context.ApplicationContextException: Failed to start bean 'jobTriggerBean'; nested exception is org.springframework.integration.MessageHandlingException: error occurred in message handler [org.springframework.integration.x.redis.RedisChannelRegistry$CompositeHandler@26f8f92e]    org.springframework.context.support.DefaultLifecycleProcessor.doStart(DefaultLifecycleProcessor.java:170)    org.springframework.context.support.DefaultLifecycleProcessor.access$200(DefaultLifecycleProcessor.java:51)    org.springframework.context.support.DefaultLifecycleProcessor$LifecycleGroup.start(DefaultLifecycleProcessor.java:339)    org.springframework.context.support.DefaultLifecycleProcessor.startBeans(DefaultLifecycleProcessor.java:143)    org.springframework.context.support.DefaultLifecycleProcessor.start(DefaultLifecycleProcessor.java:89)    org.springframework.context.support.AbstractApplicationContext.start(AbstractApplicationContext.java:1278)    org.springframework.xd.module.SimpleModule.start(SimpleModule.java:152)    org.springframework.xd.dirt.module.ModuleDeployer.deployModule(ModuleDeployer.java:162)    org.springframework.xd.dirt.module.ModuleDeployer.handleDeploy(ModuleDeployer.java:149)    org.springframework.xd.dirt.module.ModuleDeployer.handleMessageInternal(ModuleDeployer.java:120)    org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:73)    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)    java.lang.reflect.Method.invoke(Method.java:601)    org.springframework.expression.spel.support.ReflectiveMethodExecutor.execute(ReflectiveMethodExecutor.java:69)    org.springframework.expression.spel.ast.MethodReference$MethodValueRef.getValue(MethodReference.java:97)    org.springframework.expression.spel.ast.CompoundExpression.getValueInternal(CompoundExpression.java:82)    org.springframework.expression.spel.ast.SpelNodeImpl.getTypedValue(SpelNodeImpl.java:102)    org.springframework.expression.spel.standard.SpelExpression.getValue(SpelExpression.java:103)    org.springframework.integration.util.AbstractExpressionEvaluator.evaluateExpression(AbstractExpressionEvaluator.java:144)    org.springframework.integration.util.MessagingMethodInvokerHelper.processInternal(MessagingMethodInvokerHelper.java:231)    org.springframework.integration.util.MessagingMethodInvokerHelper.process(MessagingMethodInvokerHelper.java:130)    org.springframework.integration.handler.MethodInvokingMessageProcessor.processMessage(MethodInvokingMessageProcessor.java:73)  	... 22 more  Caused by: org.springframework.integration.MessageHandlingException: error occurred in message handler [org.springframework.integration.x.redis.RedisChannelRegistry$CompositeHandler@26f8f92e]    org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:79)    org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:115)    org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:102)    org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)    org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:178)    org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:149)    org.springframework.integration.core.MessagingTemplate.doSend(MessagingTemplate.java:304)    org.springframework.integration.core.MessagingTemplate.send(MessagingTemplate.java:165)    org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendMessage(AbstractReplyProducingMessageHandler.java:223)    org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendReplyMessage(AbstractReplyProducingMessageHandler.java:207)    org.springframework.integration.handler.AbstractReplyProducingMessageHandler.produceReply(AbstractReplyProducingMessageHandler.java:172)    org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleResult(AbstractReplyProducingMessageHandler.java:166)    org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:144)    org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:73)    org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:115)    org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:102)    org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)    org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:178)    org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:149)    org.springframework.integration.core.MessagingTemplate.doSend(MessagingTemplate.java:304)    org.springframework.integration.core.MessagingTemplate.send(MessagingTemplate.java:165)    org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendMessage(AbstractReplyProducingMessageHandler.java:223)    org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendReplyMessage(AbstractReplyProducingMessageHandler.java:207)    org.springframework.integration.handler.AbstractReplyProducingMessageHandler.produceReply(AbstractReplyProducingMessageHandler.java:172)    org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleResult(AbstractReplyProducingMessageHandler.java:166)    org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:144)    org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:73)    org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:115)    org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:102)    org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)    org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:178)    org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:149)    org.springframework.xd.dirt.plugins.job.JobTriggerBean.executeBatchJob(JobTriggerBean.java:74)    org.springframework.xd.dirt.plugins.job.JobTriggerBean.start(JobTriggerBean.java:63)    org.springframework.context.support.DefaultLifecycleProcessor.doStart(DefaultLifecycleProcessor.java:167)  	... 45 more  Caused by: org.springframework.integration.x.json.TypedJsonMapper$SmartJsonConversionException: Infinite recursion (StackOverflowError) (through reference chain: org.springframework.batch.core.JobExecution[""stepExecutions""]->java.util.UnmodifiableRandomAccessList[0]->org.springframework.batch.core.StepExecution[""jobExecution""]->org.springframework.batch.core.JobExecution[""stepExecutions""]->java.util.UnmodifiableRandomAccessList[0]->org.springframework.batch.core.StepExecution[""jobExecution""]->org.springframework.batch.core.JobExecution[""stepExecutions""]->java.util.UnmodifiableRandomAccessList[0]->org.springframework.batch.core.StepExecution[""j  ...  ...  >java.util.UnmodifiableRandomAccessList[0]->org.springframework.batch.core.StepExecution[""jobExecution""]->org.springframework.batch.core.JobExecution[""stepExecutions""])    com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:611)    com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeWithType(BeanSerializerBase.java:492)    com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:571)    com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:597)    com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeWithType(BeanSerializerBase.java:492)    com.fasterxml.jackson.databind.ser.std.CollectionSerializer.serializeContents(CollectionSerializer.java:119)    com.fasterxml.jackson.databind.ser.std.CollectionSerializer.serializeContents(CollectionSerializer.java:23)    com.fasterxml.jackson.databind.ser.std.AsArraySerializerBase.serializeWithType(AsArraySerializerBase.java:197)    com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:571)    com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:597)    com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeWithType(BeanSerializerBase.java:492)",2
"Avoid use of module name twice in location when using a custom modules
See https://github.com/SpringSource/spring-xd/pull/240#discussion_r6045724",2
"Support a dependencies-manifest aware Module Registry
It would be nice to be able to have modules that are simply made of:  - Their context xml file  - Some kind of manifest that expresses dependencies  and have the runtime take care of the deps",8
"Add CompositeModuleRegistry
nan",3
"Support higher level structure for complex module registry
See discussion at https://github.com/SpringSource/spring-xd/pull/240#discussion_r6045724",8
"Build needs to override $XD_* environment variables
export XD_HOME=foo  gradle clean test     build fails. Need to detected environment variables and override for the build",2
"Enable profile selection from module options
This came up when working on email source. There is <int-mail:imap-idle-channel-adapter> and <int-mail:inbound-channel-adapter>  It would be nice to be able to put those in two profiles and have one of the profile being activated from module options (e.g. email --polling=true|false)  Don't know the runtime cost of activating profiles, but we could blindly activate profiles from all options passed explicitly :  <beans profile=""profile-[optionname]-[optionvalue]"">  Not sure if this is the same as XD-132",5
"Add Warning-level log to postProcessAfterInitialization if Job name is not ""job""
Should it be fatal vs. warning?",1
"Add a JobExecution DTO Object
related to XD-779.   * We need the ability to provide JSON serializable JobExecution information. * Change from using JavaSerialization back to returning objects ",2
"Add Integration Tests to run JobCommands Tests against all transports
similar to ChannelRegistry:    - AbstractChannelRegistryTests that has the real tests  - subclasses for each impl provide the registry to be tested    Thus one test can run against multiple transports.",8
"Add index-based access to TuplePropertyAccessor
nan",2
"Cryptic gradle error running tests when XD SingleNode is running
SingleNodeMain.launchSingleNodeServer(options) calls System.exit() causing a gradle buffer underflow. This is called from SingleNodeMainIntegrationTests. System.exit() should be called from the main method instead. ",1
"Document mail related sources & sinks
nan",5
"Update twittersearch to use Spring Integration support
nan",2
"Upgrade Spring Data Redis to 1.1 RC1
nan",1
"Add integration tests for SpEL and Groovy based routing
nan",4
"Refactor mail and imap source into one ""mail"" module, leveraging Profiles
Once XD-785 is merged",3
"Create separate commands for ""--all"" shell commands
Commands like ""stream deploy"" have changed over time to allow passing a ""--all"" option.    So it's either {{stream deploy foo}} or {{stream deploy --all}}. This has a number of drawbacks, given that these are the only 2 alternatives:  - Implementation code is cumbersome  - None of the options can be marked mandatory, yet one of them is required. This has to be checked in the command code itself  - TAB completion is less powerful as the shell doesn't know if we want the first or the second form.      Consider splitting those commands into two distinct commands, one as before and one literally named {{stream deploy all}}.",5
"Package Tangle Introduced by XD-790
https://sonar.springsource.org/drilldown/measures/7173?metric=package_tangle_index&rids%5B%5D=7717",1
"Add support for ( ) grouping in Parser
nan",3
"Change rabbitmq sink to use routing-key-expression instead of routing-key
The current rabbitmq sink uses the attribute routing-key, which defaults to the name of the stream.  This should be change to use the attribute routing-key-expression so that the routing-key can be determined using SpEL.  This will enable a dynamic evaluation of the routing-key based on message payload/header.    *Implementation Suggestions*    This hopefully should be changing the XML description of the sink to 		    routing-key-expression=""${routingKey:'${xd.stream.name}'}    This way, the ${xd.stream.name} is surrounded by a single quote to indicate a string literal to SpEL in the default case.      *How to verify it works.*    One of the simple uses of this is to create a routing key based on payload.  In a distributed word-count example, the hashcode of a word would be sent to a certain number of processing modules that would perform the count. The idea is that the same word is sent to the same node over and over again, in particular if in-memory counters or state is computed - using centralized redis counters this wouldn't be necessary in the case of only counter state.    The stream     http | rabbit --routingKey=""'word-' + payload.hashCode() % 3""    is an example of a stream that can be used to verify that messages published to a direct exchange will have routing keys of the value word-0, word-1, and word-2.  Binding a queue to each of these routing keys, one can observe the contents of messages in the queue to make sure that words are being routed to the appropriate queue, e.g. publishing ""hello"" as the payload of an http request should always appear in the same queue.  The rabbitmq admin console can be used for this purpose.  ",3
"Job channels need to denote a namespace
Job channels need to have a namespace.    i.e. job-somejobname.  Where the - is the delimiter for the namespace.    The preference is to use the : instead of the -.  But XD-766 needs to be completed in order to support this.",2
"Turning on Regex Pattern multiline mode in JsonStringToObjectTransformer
Based on the PR discussion: https://github.com/SpringSource/spring-xd/pull/270#commitcomment-4003291  We need to consider turning on multiline mode for pattern matching.",1
"Document splitter & aggregator processors
nan",3
"Batch Jobs need container & admin profiles
This is to set the appropriate data source, so that the container will use admins batch repository.",3
"Add Named Channel API
We need an abstraction in place to retrieve messages from a ""named channel"" programmatically.    Right now there is no implementation agnostic way of doing this (such as receiveMessage(), queueSize()).    This could be quite useful for integration tests of streams. E.g. to do more focussed tests without resorting to ""temp-files"" and non-essential sinks or sources etc. - e.g.     {code}  :routeit > router --expression=payload.contains('a')?':foo':':bar'  {code}",8
"Get notified when created named channel ""is ready""
For testing purposes it would be super-helpful if there be a hook to get notified when a named channel is up and running. In current tests one may have to resort to ""Thread.sleep"".",8
"Validate module properties
Currently it's possible to do something like   {code}    http --prot=8888  {code}    It is possible to validate property names by parsing the module definition file(s) directly and matching property placeholders (or profile declarations that may be mapped to properties, etc). This must account recursively for imports as well. (I have some code in a branch that does this).   ",3
"Shell: Standardize counter name parameter
The parameters are not optimal for the counter name between ""Aggregate Counter"" ""Field Value Counter""  --counterName versus --name",2
"Update to spring-data-hadoop 1.0.1.RELEASE
This might mean we should adjust our hadoopDistro options to the ones supported in the new release - hadoop12 (default), cdh4, hdp13, phd1 and hadoop21",3
"Shell integration with XD on Yarn
We should provide a better shell integration when XD is run on Yarn.  1. yarn kill --id TAB completion 2. yarn submit, more options like app name 3. yarn list, filter by app states, etc 4. admin config server TAB completion for running xd apps on yarn",1
"Deleting a stream with reference to named channel disconnects channel from all streams
The following sequence results in ""Dispatcher has no subscribers"" error  (stack trace below), because deleting stream2 disconnects stream1 from the foo channel. Current work on XD-685 has infrastructure for disconnecting just the channels involved in a stream, so should make it easier to fix this issue once merged.     stream create stream1 --definition ""time > :foo""  stream create stream2 --definition ""http > :foo""  stream create stream3 --definition "":foo > file""  stream destroy stream2  // expect file sink to still get time, but instead blows up b/c  // deleteOutbound(""foo"") killed links b/w foo and both local output channels    Server stack trace:  10:47:11,921 ERROR task-scheduler-6 handler.LoggingHandler:140 - org.springframework.integration.MessageDeliveryException: Dispatcher has no subscribers for channel 'SimpleModule [name=time, type=source, group=stream1, index=0].output'.    org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:81)    org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:178)    org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:149)    org.springframework.integration.core.MessagingTemplate.doSend(MessagingTemplate.java:304)    org.springframework.integration.core.MessagingTemplate.send(MessagingTemplate.java:165)    org.springframework.integration.endpoint.SourcePollingChannelAdapter.handleMessage(SourcePollingChannelAdapter.java:97)    org.springframework.integration.endpoint.AbstractPollingEndpoint.doPoll(AbstractPollingEndpoint.java:199)    org.springframework.integration.endpoint.AbstractPollingEndpoint.access$000(AbstractPollingEndpoint.java:51)    org.springframework.integration.endpoint.AbstractPollingEndpoint$1.call(AbstractPollingEndpoint.java:143)    org.springframework.integration.endpoint.AbstractPollingEndpoint$1.call(AbstractPollingEndpoint.java:141)    org.springframework.integration.endpoint.AbstractPollingEndpoint$Poller$1.run(AbstractPollingEndpoint.java:273)    org.springframework.integration.util.ErrorHandlingTaskExecutor$1.run(ErrorHandlingTaskExecutor.java:52)    org.springframework.core.task.SyncTaskExecutor.execute(SyncTaskExecutor.java:49)    org.springframework.integration.util.ErrorHandlingTaskExecutor.execute(ErrorHandlingTaskExecutor.java:49)    org.springframework.integration.endpoint.AbstractPollingEndpoint$Poller.run(AbstractPollingEndpoint.java:268)    org.springframework.scheduling.support.DelegatingErrorHandlingRunnable.run(DelegatingErrorHandlingRunnable.java:53)    org.springframework.scheduling.concurrent.ReschedulingRunnable.run(ReschedulingRunnable.java:81)    java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)    java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)    java.util.concurrent.FutureTask.run(FutureTask.java:138)    java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)    java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:206)    java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)    java.lang.Thread.run(Thread.java:680)  Caused by: org.springframework.integration.MessageDispatchingException: Dispatcher has no subscribers    org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:109)    org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:102)    org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)  	... 24 more    10:47:12,924 ERROR task-scheduler-6 handler.LoggingHandler:140 - org.springframework.integration.MessageDeliveryException: Dispatcher has no subscribers for channel 'SimpleModule [name=time, type=source, group=stream1, index=0].output'.    org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:81)    org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:178)    org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:149)    org.springframework.integration.core.MessagingTemplate.doSend(MessagingTemplate.java:304)    org.springframework.integration.core.MessagingTemplate.send(MessagingTemplate.java:165)    org.springframework.integration.endpoint.SourcePollingChannelAdapter.handleMessage(SourcePollingChannelAdapter.java:97)    org.springframework.integration.endpoint.AbstractPollingEndpoint.doPoll(AbstractPollingEndpoint.java:199)    org.springframework.integration.endpoint.AbstractPollingEndpoint.access$000(AbstractPollingEndpoint.java:51)    org.springframework.integration.endpoint.AbstractPollingEndpoint$1.call(AbstractPollingEndpoint.java:143)    org.springframework.integration.endpoint.AbstractPollingEndpoint$1.call(AbstractPollingEndpoint.java:141)    org.springframework.integration.endpoint.AbstractPollingEndpoint$Poller$1.run(AbstractPollingEndpoint.java:273)    org.springframework.integration.util.ErrorHandlingTaskExecutor$1.run(ErrorHandlingTaskExecutor.java:52)    org.springframework.core.task.SyncTaskExecutor.execute(SyncTaskExecutor.java:49)    org.springframework.integration.util.ErrorHandlingTaskExecutor.execute(ErrorHandlingTaskExecutor.java:49)    org.springframework.integration.endpoint.AbstractPollingEndpoint$Poller.run(AbstractPollingEndpoint.java:268)    org.springframework.scheduling.support.DelegatingErrorHandlingRunnable.run(DelegatingErrorHandlingRunnable.java:53)    org.springframework.scheduling.concurrent.ReschedulingRunnable.run(ReschedulingRunnable.java:81)    java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)    java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)    java.util.concurrent.FutureTask.run(FutureTask.java:138)    java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)    java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:206)    java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)    java.lang.Thread.run(Thread.java:680)  Caused by: org.springframework.integration.MessageDispatchingException: Dispatcher has no subscribers    org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:109)    org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:102)    org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)  	... 24 more    10:47:13,926 ERROR task-scheduler-4 handler.LoggingHandler:140 - org.springframework.integration.MessageDeliveryException: Dispatcher has no subscribers for channel 'SimpleModule [name=time, type=source, group=stream1, index=0].output'.    org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:81)    org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:178)    org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:149)    org.springframework.integration.core.MessagingTemplate.doSend(MessagingTemplate.java:304)    org.springframework.integration.core.MessagingTemplate.send(MessagingTemplate.java:165)    org.springframework.integration.endpoint.SourcePollingChannelAdapter.handleMessage(SourcePollingChannelAdapter.java:97)    org.springframework.integration.endpoint.AbstractPollingEndpoint.doPoll(AbstractPollingEndpoint.java:199)    org.springframework.integration.endpoint.AbstractPollingEndpoint.access$000(AbstractPollingEndpoint.java:51)    org.springframework.integration.endpoint.AbstractPollingEndpoint$1.call(AbstractPollingEndpoint.java:143)    org.springframework.integration.endpoint.AbstractPollingEndpoint$1.call(AbstractPollingEndpoint.java:141)    org.springframework.integration.endpoint.AbstractPollingEndpoint$Poller$1.run(AbstractPollingEndpoint.java:273)    org.springframework.integration.util.ErrorHandlingTaskExecutor$1.run(ErrorHandlingTaskExecutor.java:52)    org.springframework.core.task.SyncTaskExecutor.execute(SyncTaskExecutor.java:49)    org.springframework.integration.util.ErrorHandlingTaskExecutor.execute(ErrorHandlingTaskExecutor.java:49)    org.springframework.integration.endpoint.AbstractPollingEndpoint$Poller.run(AbstractPollingEndpoint.java:268)    org.springframework.scheduling.support.DelegatingErrorHandlingRunnable.run(DelegatingErrorHandlingRunnable.java:53)    org.springframework.scheduling.concurrent.ReschedulingRunnable.run(ReschedulingRunnable.java:81)    java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)    java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)    java.util.concurrent.FutureTask.run(FutureTask.java:138)    java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)    java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:206)    java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)    java.lang.Thread.run(Thread.java:680)  Caused by: org.springframework.integration.MessageDispatchingException: Dispatcher has no subscribers    org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:109)    org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:102)    org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)  	... 24 more    10:47:14,928 ERROR task-scheduler-4 handler.LoggingHandler:140 - org.springframework.integration.MessageDeliveryException: Dispatcher has no subscribers for channel 'SimpleModule [name=time, type=source, group=stream1, index=0].output'.    org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:81)    org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:178)    org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:149)    org.springframework.integration.core.MessagingTemplate.doSend(MessagingTemplate.java:304)    org.springframework.integration.core.MessagingTemplate.send(MessagingTemplate.java:165)    org.springframework.integration.endpoint.SourcePollingChannelAdapter.handleMessage(SourcePollingChannelAdapter.java:97)    org.springframework.integration.endpoint.AbstractPollingEndpoint.doPoll(AbstractPollingEndpoint.java:199)    org.springframework.integration.endpoint.AbstractPollingEndpoint.access$000(AbstractPollingEndpoint.java:51)    org.springframework.integration.endpoint.AbstractPollingEndpoint$1.call(AbstractPollingEndpoint.java:143)    org.springframework.integration.endpoint.AbstractPollingEndpoint$1.call(AbstractPollingEndpoint.java:141)    org.springframework.integration.endpoint.AbstractPollingEndpoint$Poller$1.run(AbstractPollingEndpoint.java:273)    org.springframework.integration.util.ErrorHandlingTaskExecutor$1.run(ErrorHandlingTaskExecutor.java:52)    org.springframework.core.task.SyncTaskExecutor.execute(SyncTaskExecutor.java:49)    org.springframework.integration.util.ErrorHandlingTaskExecutor.execute(ErrorHandlingTaskExecutor.java:49)    org.springframework.integration.endpoint.AbstractPollingEndpoint$Poller.run(AbstractPollingEndpoint.java:268)    org.springframework.scheduling.support.DelegatingErrorHandlingRunnable.run(DelegatingErrorHandlingRunnable.java:53)    org.springframework.scheduling.concurrent.ReschedulingRunnable.run(ReschedulingRunnable.java:81)    java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)    java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)    java.util.concurrent.FutureTask.run(FutureTask.java:138)    java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)    java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:206)    java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)    java.lang.Thread.run(Thread.java:680)  Caused by: org.springframework.integration.MessageDispatchingException: Dispatcher has no subscribers    org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:109)    org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:102)    org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)  	... 24 more    10:47:15,930 ERROR task-scheduler-1 handler.LoggingHandler:140 - org.springframework.integration.MessageDeliveryException: Dispatcher has no subscribers for channel 'SimpleModule [name=time, type=source, group=stream1, index=0].output'.    org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:81)    org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:178)    org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:149)    org.springframework.integration.core.MessagingTemplate.doSend(MessagingTemplate.java:304)    org.springframework.integration.core.MessagingTemplate.send(MessagingTemplate.java:165)    org.springframework.integration.endpoint.SourcePollingChannelAdapter.handleMessage(SourcePollingChannelAdapter.java:97)    org.springframework.integration.endpoint.AbstractPollingEndpoint.doPoll(AbstractPollingEndpoint.java:199)    org.springframework.integration.endpoint.AbstractPollingEndpoint.access$000(AbstractPollingEndpoint.java:51)    org.springframework.integration.endpoint.AbstractPollingEndpoint$1.call(AbstractPollingEndpoint.java:143)    org.springframework.integration.endpoint.AbstractPollingEndpoint$1.call(AbstractPollingEndpoint.java:141)    org.springframework.integration.endpoint.AbstractPollingEndpoint$Poller$1.run(AbstractPollingEndpoint.java:273)    org.springframework.integration.util.ErrorHandlingTaskExecutor$1.run(ErrorHandlingTaskExecutor.java:52)    org.springframework.core.task.SyncTaskExecutor.execute(SyncTaskExecutor.java:49)    org.springframework.integration.util.ErrorHandlingTaskExecutor.execute(ErrorHandlingTaskExecutor.java:49)    org.springframework.integration.endpoint.AbstractPollingEndpoint$Poller.run(AbstractPollingEndpoint.java:268)    org.springframework.scheduling.support.DelegatingErrorHandlingRunnable.run(DelegatingErrorHandlingRunnable.java:53)  ",3
"Add REST endpoint for launching Job
We need a REST endpoint to launch a job.    Given the constraint on JobRegistry being not persistent and not available outside the container JVM, we can not use the batch job admin controller/service to launch the job.    One possible way is to use the trigger source to launch the job at XD.",2
"Re-enable support for tapping labels and named channels
As of XD-685, we no longer have the ability to:  1) Tap a named channel, ala stream1=:foo > sink stream2= :tap:foo > sink  2) Tap a stream whose source is a named channel ala stream2=:tap:stream1 > sink  3) Tap a label ala stream1=http | obfuscator: transform --expression=payload.replaceAll('password','*') | file stream2=:tap:stream1.obfuscator > sink    Loss of named channel support is due to the fact that we are creating a WireTap on a module's local output channel only (thus we never tap named channels). We are not supporting labels because we only create a named channel called ""tap:stream.module"" on stream creation, so later creation of tap on stream1.obfuscator is referring to a non-existent named channel.",8
"Destroying XD job should remove job's entries at batch job repositories/batch job locator
When an XD job is destroyed/deleted, the batch jobRepository entries for the job (associated JobInstances, JobExecutions etc.,) and the BatchJobLocator entries.",4
"Rebase UI on top of new batch admin API
Now that the new batch admin api is taking shape, we need to rebase the XD web UI to use this.  It's more than just changing the http urls sent to the xd server since the new API is not identical to the old one.",5
"Updgrade to use spring-batch-admin 1.3.0 M1 when available
Once spring-batch-admin 1.3.0.M1 is available, update the build to use it.  Likely to be Sept 7 or 9",1
"Support for composed streams
Some recent changes caused this to be turned off. Basically the change was to police whether a stream is well formed at create time, rather than deploy time.  By deferring that check we can create composed streams that are not deployable by themselves but that are when used as building blocks in proper streams.",2
"REST API for job listing should provide details on last execution 
A user should be able to view some important details of the last execution of a job from a job list.  The {{/batch/jobs}} REST endpoint should provide extra fields not currently available in the {{JobInfo}} class.  At a minimum, I would like to see:  * startTime * startDate * last job parameters * duration * last job status",5
"UI should poll server for latest on job info
The admin UI should be polling the server to automatically pick up any new jobs, executions, and instances.",5
"Add Service Activator Processor
Would be nice to have a ServiceActivator Processor available so that if one had an existing Spring bean they could simply describe the bean id and method name - without going through the full complexity of creating a processing module.",3
"Jobs and taps should not require a leading : since they have name spaces.
nan",3
"Change url to access UI from browser
Currently, the url for accessing the XD UI is {{http://localhost:8080/admin-ui/index.html}}.  This feels messy and dated.  We should be able to access the ui without explicitly including the index.html, like this:  {code} http://localhost:8080/admin-ui {code}  ",5
"TupleBuilder.fromString() should not overwrite original id and timestamp fields
When converting a JSON string to a tuple the JSON may contain id. This method should handle this. Same with timestamp",2
"add discardDeletes property to twitterstream source
If true(default): filter for delete messages in the twitter stream and route to a discard channel. This creates a twitter stream including only new tweets and no references to deleted ones.",1
"Rename ChannelRegistry
My current thinking is...       ChannelRegistry -> MessageBus     RabbitChannelRegistry -> RabbitMessageBus     ...    Then, method names change like so:       createInbound -> registerConsumer     createOutbound -> registerProducer      ",3
"Create object-to-json transformer processor
<int:channel id=""input""/> <int:object-to-json-transformer input-channel=""input"" output-channel=""output""/> <int:channel id=""output""/>",1
"Investigate failing tests in MailCommandTest
org.springframework.xd.shell.command.MailCommandTests in spring-xd-shell project.  I get failures relating to invalid username/password  INFO: Stream Name Stream Definition Status ----------- ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- -------- mailstream imap --port=1044 --protocol=imap --folder=INBOX --username=johndoe --password=secret | file --dir=/tmp --name=FileSink1280066074228960855 --suffix=txt --charset=UTF-8 --binary=false deployed 13/09/04 11:32:11 WARN mail.ImapIdleChannelAdapter: error occurred in idle task javax.mail.AuthenticationFailedException: LOGIN failed. Invalid login/password at com.sun.mail.imap.IMAPStore.protocolConnect(IMAPStore.java:663) ",3
"provide a property on twittersearch to enable the object-to-json transformer
Twitter search source should produce JSON or Pojo. The Pojo requires a custom wrapper class that is JSON friendly (e.g., zero arg constructor). The twittersearch module should have a parameter --json true/false (default true) to control the output type.",2
"rename Bridge to Binding and add direction
org.springframework.integration.x.bus.Bridge should now be called Binding    we can also move the INBOUND/OUTBOUND direction (or possibly the CONSUMER/PRODUCER role) into this class; that should simplify its usage in conditional code within the MessageBus code and also reduce the use of ""in"" and ""out"" as Strings in that same code",2
"Upgrade hsqldb version on XD batch admin to the latest
We also would like to upgrade the hsqldb version on spring batch admin so that both are compatible.",3
"Expose Job parameters for JobExecutions
Currently, the way that we are accessing JobExecutions from batch admin, the JobParameters are not being filled in.  We are using a {{org.springframework.batch.admin.service.JdbcSearchableJobExecutionDao}} to grab the job executions from the database. The database queries that it uses does not include the JobParameters (which is stored in a different table).  I think that this will require a change to batch admin in order to properly expose the parameters.",5
"Update docs to cover Module config and lib directory structure
nan",2
"Update docs for new Tap syntax
Now that taps are just channels, we need to update the docs.    The preceding colon is no longer needed (and will be removed altogether), so all examples should be like this:    {code}  tap:foo > bar  {code}    ",2
"Allow tapping a stream prior to stream creation without specifying module name
Currently this works: stream create foo --definition ""tap:baz.time > log"" stream create baz --definition ""time | file""  But this doesn't: stream create foo --definition ""tap:baz > log"" stream create baz --definition ""time | file""  This is because the parser translates references to ""tap:baz"" to named channel ""tap:baz.time"" (the name of the stream's first module). If the stream is not yet created, the parser cannot perform this translation. A fix for this will likely be related to the fix needed for XD-812.",5
"Handle XD admin server shutdown cleanly
There are couple of issues here:  1) The admin server destroy() - close event's onApplicationEvent(ContextClosedEvent) listener has stop() to stop the admin server's tomcat instance. The stop() also calls the applicationContext's destroy() which loops again to stop.  2) With HSQLServer or any batch db server(in future), the admin server stop() also needs to handle the batch db server shutdown.",2
"REST - Listing of FieldValueCounter not working
http://localhost:8080/metrics/field-value-counters  {code} <errors xmlns:atom=""http://www.w3.org/2005/Atom""> <error logref=""HttpMessageNotWritableException""> <message> Could not marshal [PagedResource { content: [links: [<http://localhost:8080/metrics/field-value-counters/hashtags>;rel=""self""]], metadata: Metadata { number: 0, total pages: 1, total elements: 1, size: 0 }, links: [] }]: null; nested exception is javax.xml.bind.MarshalException - with linked exception: [com.sun.istack.SAXException2: unable to marshal type ""org.springframework.xd.rest.client.domain.metrics.MetricResource"" as an element because it is not known to this context.] </message> </error> </errors> {code}",4
"Tipsy tooltip hovers are not responsive after jobs list is refreshed from server
In the XD UI, the list of jobs is refreshed from the server every 5 seconds.  There are also tooltips that are activated when hovering over a job execution. These tooltips are no longer responsive (ie- they remain on the page and don't disappear) after the list of jobs is refreshed.",5
"Refactor out Trigger docs from the Batch Job chapter
nan",2
"Close parent contexts when shutting down
nan",1
"Fix Compiler Warnings
As of M3...  {code} javadoc: warning - Error fetching URL: http://static.springsource.org/spring-shell/docs/current/api/package-list /Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/module/CompositeModuleRegistry.java:40: warning - @param argument ""cp"" is not a parameter name. /Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/module/CompositeModuleRegistry.java:40: warning - @param argument ""file"" is not a parameter name. /Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/plugins/job/JobFactoryBean.java:62: warning - Tag @link: can't find jobName in org.springframework.xd.dirt.plugins.job.JobFactoryBean /Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/rest/XDController.java:56: warning - @param argument ""<V>"" is not a type parameter name. /Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/rest/XDController.java:56: warning - @param argument ""<T>"" is not a type parameter name. /Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/plugins/job/batch/BatchJobAlreadyExistsException.java:32: warning - @param argument ""message"" is not a parameter name. /Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/plugins/job/batch/BatchJobLocator.java:32: warning - @DistributedJobService is an unknown tag. /Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/plugins/job/batch/BatchJobLocator.java:32: warning - @DistributedJobService is an unknown tag. /Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/plugins/job/batch/NoSuchBatchJobException.java:31: warning - @param argument ""message"" is not a parameter name. /Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/server/options/ContainerOptions.java:30: warning - @param argument ""defaultTransport"" is not a parameter name. /Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/server/options/ContainerOptions.java:30: warning - @param argument ""defaultAnalytics"" is not a parameter name. /Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/server/options/OptionUtils.java:35: warning - @return tag has no arguments. /Users/gpr/Documents/workspace-si/spring-xd/spring-xd-module/src/main/java/org/springframework/xd/module/Plugin.java:32: warning - Tag @see: reference not found: ModuleDeployer /Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/integration/x/bus/MessageBus.java:53: warning - @param argument ""moduleInputChannel"" is not a parameter name. /Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/integration/x/bus/MessageBus.java:72: warning - @param argument ""moduleOutputChannel"" is not a parameter name. /Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/integration/x/bus/MessageBus.java:102: warning - @param argument ""moduleOutputChannel"" is not a parameter name. /Users/gpr/Documents/workspace-si/spring-xd/spring-xd-reactor/src/main/java/org/springframework/xd/integration/reactor/config/ReactorNamespaceUtils.java:46: warning - @return tag has no arguments. /Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/plugins/job/batch/BatchJobLocator.java:32: warning - @DistributedJobService is an unknown tag. /Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/plugins/job/batch/BatchJobLocator.java:32: warning - @DistributedJobService is an unknown tag. {code}  and  {code} warning: [options] bootstrap class path not set in conjunction with -source 1.6 /Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/integration/x/bus/LocalMessageBus.java:132: warning: [rawtypes] found raw type: LocalMessageBus.SharedChannelProvider 		SharedChannelProvider channelProvider = aliasHint ? queueChannelProvider 		^   missing type arguments for generic class LocalMessageBus.SharedChannelProvider<T>   where T is a type-variable:     T extends AbstractMessageChannel declared in class LocalMessageBus.SharedChannelProvider /Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/integration/x/bus/LocalMessageBus.java:159: warning: [rawtypes] found raw type: LocalMessageBus.SharedChannelProvider 		SharedChannelProvider channelProvider = aliasHint ? queueChannelProvider 		^   missing type arguments for generic class LocalMessageBus.SharedChannelProvider<T>   where T is a type-variable:     T extends AbstractMessageChannel declared in class LocalMessageBus.SharedChannelProvider /Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/integration/x/twitter/XDEntities.java:93: warning: [rawtypes] found raw type: List 	public List getUrls() { 	       ^   missing type arguments for generic class List<E>   where E is a type-variable:     E extends Object declared in interface List /Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/integration/x/twitter/XDEntities.java:101: warning: [rawtypes] found raw type: List 	public List getHashTags() { 	       ^   missing type arguments for generic class List<E>   where E is a type-variable:     E extends Object declared in interface List /Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/integration/x/twitter/XDEntities.java:109: warning: [rawtypes] found raw type: List 	public List getMentions() { 	       ^   missing type arguments for generic class List<E>   where E is a type-variable:     E extends Object declared in interface List /Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/integration/x/twitter/XDEntities.java:117: warning: [rawtypes] found raw type: List 	public List getMedia() { 	       ^   missing type arguments for generic class List<E>   where E is a type-variable:     E extends Object declared in interface List /Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/integration/x/twitter/XDEntities.java:126: warning: [rawtypes] found raw type: List 	public List getTickerSymbols() { 	       ^   missing type arguments for generic class List<E>   where E is a type-variable:     E extends Object declared in interface List /Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/integration/x/twitter/XDEntities.java:37: warning: [serial] serializable class XDEntities has no definition of serialVersionUID public class XDEntities implements Serializable {        ^ /Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/integration/x/twitter/XDUrlEntity.java:31: warning: [serial] serializable class XDUrlEntity has no definition of serialVersionUID public class XDUrlEntity implements Serializable {        ^ /Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/integration/x/twitter/XDHashTagEntity.java:32: warning: [serial] serializable class XDHashTagEntity has no definition of serialVersionUID public class XDHashTagEntity implements Serializable {        ^ /Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/integration/x/twitter/XDMentionEntity.java:32: warning: [serial] serializable class XDMentionEntity has no definition of serialVersionUID public class XDMentionEntity implements Serializable {        ^ /Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/integration/x/twitter/XDMediaEntity.java:32: warning: [serial] serializable class XDMediaEntity has no definition of serialVersionUID public class XDMediaEntity implements Serializable {        ^ /Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/integration/x/twitter/XDTickerSymbolEntity.java:31: warning: [serial] serializable class XDTickerSymbolEntity has no definition of serialVersionUID public class XDTickerSymbolEntity implements Serializable {        ^ /Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/integration/x/twitter/XDTweet.java:34: warning: [serial] serializable class XDTweet has no definition of serialVersionUID public class XDTweet implements Serializable {        ^ {code}",1
"Create a packaging model for custom modules
Like just discussed after the Spring One session, it would be nice to have a packaging model for custom modules. Instead of putting all libs in one directory and an XML in another, it could be a fat jar including some metadata file pointing to either an XML or to a JavaConfig configuration class inside the jar. Some more words why I think it's important to have something like that: Spring XD can be very well used in the enterprise, and at least for job modules it's likely to have some dependencies. So what do you have to do now to deploy such a module? Build a jar with your build process, then get it from your repository, unpack it and somehow find the relevant XML to copy it into the appropriate folder, then inspect the POM to find out what all the dependencies are, direct and transient, and get them from your repository to copy them into the appropriate folder. And of course, copy the jar. And when using Spring XD in distributed mode, you have to do it several times. With the approach mentioned above you just build your fat jar with your build process and distribute it to all your nodes, nothing complicated.  And if you implement it like that it's easy to have a JavaConfig support that's almost the same.",9
"Add back classifier = 'dist' to distZip build target
Add back ""classifier = 'dist'"" to distZip build target - it was was accidentally removed.",1
"Initial XD on CloudFoundry support
First take on this involves - being able to deploy the two separate applications: xd-admin & xd-container - being able to CF-service provided redis & rabbit for internal needs of XD - to some extent, make modules smart and CF aware (e.g. http source uses correct port)",20
"Enable Spring Boot Loader support
As part of running in Cloud Foundry, one quick workaround for the lack of ""classpath"" support would be to use the Spring Boot Loader special ClassLoader and jar-inside-a-jar support: https://github.com/spring-projects/spring-boot/tree/master/spring-boot-tools/spring-boot-loader",3
"Remove support for the leading : on items that have a declared namespace
When using jobs, taps we no longer need to have the leading :.  i.e. :tap:foo.  We should only support tap:foo.",3
"Remove deprecated tap syntax from the parser.
Tap '@' and using numbers instead of module names.",2
"Revise the available hadoopDistro options
We should adjust our --hadoopDistro options to the ones supported in the new spring-data-hadoop 1.0.1.RELEASE - hadoop12 (default), cdh4, hdp13, phd1, hadoop20  This includes updating the wiki pages",5
"ICLA website is missing Spring XD project option
The README.md at:    https://github.com/spring-projects/spring-xd/blob/master/README.md  ...says:  ""Before we accept a non-trivial patch or pull request we will need you to sign the contributor's agreement""  Navigating to the <contributor's agreement> link takes the user to the ICLA web page at:  https://support.springsource.com/spring_committer_signup  //exp: The Project field's dropdown should have an option for ""Spring XD"" //act: There's no ""Spring XD"" option",1
"Gemfire modules should support connection via locator
The gemfire modules currently accept server host and port. Provide an option to specify a locator host and port",2
"JAR version mismatches
Looks like there are some version mismatch issues with the build/packaging of the XD components. Looking in xd/lib I see the following which looks suspicious:  mqtt-client-0.2.1.jar mqtt-client-1.0.jar  jackson-core-asl-1.9.13.jar jackson-mapper-asl-1.9.12.jar  spring-integration-core-3.0.0.M3.jar spring-integration-http-2.2.5.RELEASE.jar  spring-data-commons-1.6.0.M1.jar spring-data-commons-core-1.4.0.RELEASE.jar ",3
"Refactor DSL for Taps and Jobs Usage
The syntax for both taps and job channels will be prefixed with the word tap. * The parser will search both the all (stream and job for now) registries to find the ""name"".   * If the name is present in only in one registry then that definition will be used.   * If the name is  present both registries a syntax exception will be issued.    - The user then can optionally specify which registry to use by adding a second token to the definition that specifies the type of channel the user wants. 	- i.e. 	tap:{stream}:streamName > mySink * If the user attempts to place a job on the greater (left hand) side without specifying a notification a syntax error will be thrown. Format:  -- tap:[type]:name > $ -- $ > tap:[type]:name   -- Example:    tap:streamName >mySink    tap:stream:streamName > mySink    mySource|myProcessor > tap:stream:mySink    myEmailSource > tap:job:jobName.step1    myEmailSource > tap:jobName.step1    tap:job:jobName-notifications > myEmailSink",4
"DSL needs to have wildcard support for taps
Wildcard support for associating inbound and outbound channels with modules.   The wild card will be represented by an asterisk '*'. Example:  myEmailSource > tap:job:*	send message to all jobs myEmailSource > tap:*		send message to all stream/job taps myEmailSource > :*foo*		send message to all channels that contain the channels that contains the word 'foo' tap:*bar > myEmailSource  ",5
"Add definition of serialVersionUID to Twitter classes
Add serialVersionUID to the objects in package org.springframework.integration.x.twitter:  * XDEntities * XDUrlEntity * XDHashTagEntity * XDMentionEntity * XDMediaEntity * XDTickerSymbolEntity * XDTweet  The absence creates warnings during compile time.",2
"Update doc about modules and spring
The doc at http://docs.spring.io/spring-xd/docs/1.0.0.M3/reference/html/#_modules_and_spring refers to an old version of the counter sink, when it was still hardwired to use redis.  The text next to it that explains placeholders is out of date (with respect to the redis placeholders)",2
"Change metrics assertions in integration tests to use smart Thread.sleep
Similar to what has been done for e.g. FileSink, refactor metrics related sinks to use smart Thread.sleep() timings",5
"Refactor FileModuleRegistry as ""ResourceModuleRegistry""
Apart from sanity checks, there is not much that ties FileModuleRegistry to actual java.io.Files. Using the Resource abstraction would work just the same, and would allow loading modules from the classpath in constrained environments or other file systems/locations. (HDFS /HTTP)",5
"Upgrade to Spring Data Redis 1.1.0.RELEASE
nan",1
"Support for listing of modules in the REST API
From the CLI, one should be able to get a listing of modules and be able to specifically ask for jobs, sources, sinks, and processors.  A brief description of them would also be nice - this might come from adding some metadata into the definition.    Finer grained description/implementation suggestion TBD",8
"Type Conversion across modules
This story will need to be broken down further.    The current code mixes together the type conversion that happens within a single JVM (for data that is passed on a local transport between modules) and serialization/deserialization between JVMs.  This should be separated.    There was a suggestion that we could perhaps use typed data channels in SI as a means implement the type conversion between modules.  The media-type conversion support in Spring 4 is another part of this solution.",10
"First class JSON Path support
Similar to xpath with XML, there are now some initial support in SI that enable the use of filter/routers based on JSON Path expressions.  That support needs to be reviewed and then brought up to the level of exposure in Spring XD so that router/filter modules could use JSON Path.  json-path-filter/router are components that need to be created, perhaps others.    This story needs to be broken down further.",10
"Serialization of Spring Batch Context Objects
Needs some investigation on how the update information from Spring Batch listeners can be sent as a message across modules in a single node configuration as well as across JVMs in a distributed node configuration.",4
"Move BatchJobExecutionsByJobName to BatchJobsController from BatchJobExecutionsController
We need to move the BatchJobExecutionsByJobName method to BatchJobsController as that seems appropriate",1
"Add a test suite to the admin-ui
The admin-ui currently has no unit tests.  Need to add a test suite and hook it up to the build so that tests are run on every build.",5
"Remove remaining Thread.sleeps from the job tests
Get rid of all the thread.sleeps and code that supported them.",1
"Support the ability for a user to create composite modules that can accept parameters
Parser creates a module compose command that allows a user to create a module of other modules.  This composed module can accept parameters.",5
"Create microbenchmark performance test of reactor syslog adapter vs standard syslog adapter
We need to verify that we are seeing improved throughput when using the reactor based syslog adapter.      A suggestion on a basic stream to perform a microbenchmark this would be using in-memory counters, singlenode with the stream definition ""syslog | counter"".    Based on the results of this microbenchmark, other stories may need to be created.",4
"Colocate Modules using labels.
i.e.  http | group1: filter | group1: transform | file   then specifying anything labelled group1 goes to machineX",6
"Support for listing of modules in the Shell
Commands that pair up with the functionality described in XD-859    module list  (would list all modules in a table format)  module list --type=source  (would list only source modules)    and so on.",4
"No errors in Shell when creating stream with HTTP Source + already used port 
2 issues exist:    1)     Current this does not create an error in the shell    {code}  stream create --name s1 --definition ""http | log""  stream create --name s2 --definition ""http | log""  {code}    On the server-side I see:    {code}  Caused by: java.net.BindException: Address already in use    sun.nio.ch.Net.bind0(Native Method)    sun.nio.ch.Net.bind(Net.java:344)    sun.nio.ch.Net.bind(Net.java:336)    sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:199)    sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)    org.jboss.netty.channel.socket.nio.NioServerBoss$RegisterTask.run(NioServerBoss.java:193)    org.jboss.netty.channel.socket.nio.AbstractNioSelector.processTaskQueue(AbstractNioSelector.java:366)    org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:290)    org.jboss.netty.channel.socket.nio.NioServerBoss.run(NioServerBoss.java:42)  	... 3 more  {code}    2)  The Stream should not be saved to the StreamDefinitionRepo in case of an error.    ",4
"Make in-memory meta data stores persistent
Just wanted to create story for this - so we can consider whether this should be addressed.  In at least 2 modules we use non-persisted states. We may want to consider making them persistent:   *Twitter Search* uses an in-memory *MetadataStore* that keeps track of the twitter ids. There exists a corresponding issue for Spring Integration:  ""Create a Redis-backed MetadataStore"" See: https://jira.springsource.org/browse/INT-3085  *File Soure*'s File Inbound Channel Adapter uses a AcceptOnceFileListFilter, which uses an in-memory Queue to keep track of duplicate files.  ",8
"File Source - Provide option to pass on File object
This story may need to be broken into several stories  Particularly for Batch scenarios one may not want to run a ""file-to-string-transformer"" on the payload file in the file source but rather handle/pass the file reference itself (local SAN etc.) - e.g. in case somebody drops a 2GB or in scenarios where one wants to push those large files into HDFS and run hadoop jobs on the data.  This is important for Batch Jobs as they need to access the file itself for the reader.   We need to *keep in mind the various transports we support*. Not sure how Kryo handles file serialization. I would think we only need the File Meta Data to be persisted not the file-data itself (make that configurable??). ",8
"For file based item reader jobs, step/job completion message should have name of file sent on named channel
It looks like we don't handle deletion of source files currently. We should provide some support for that - Maybe there is a way to into Spring Integration's PseudoTransactionManager support:    http://docs.spring.io/spring-integration/api/org/springframework/integration/transaction/PseudoTransactionManager.html    The *File Source* should possibly also support File archival functionality (But that might also be a dedicated processor?). Not sure where we want to set the semantic boundaries for the File Source.  ",8
"Handle AdminServer shutdown cleanly
The admin Server's tomcat is not shutdown properly. There is an existing method shutdownCleanly() on AdminServer but the spring-xd-shell tests hang when we use this method to shutdown the admin server.   ",2
"Create FsShell based module to copy a File to HDFS
Spring batch workflows are great for complex hadoop operations but if I want to create a simple processor that executes some hadoop fs in groovy, it would be nice to do this:  {code} <service-activator output-channel=""output"" input-channel=""input"">    <hadoop:script id=""loadScript"" language=""groovy""> 	 def outputPath = ""${hdfsPath}""	 	 fsh.put(""-"", outputPath)      </hadoop:script> </service-activator> {code}  The goal of this hadoop script is to use in a stream like this:""file | script"" that puts a file byte for byte to hdfs. An enhanced hdfs sink that's optimized for binary data like images/pdfs might be more elegant but I was hoping that this would work. This script gets ignored by Spring XD. But even if it didn't, I am not sure the ""-"" stdin put would work as hoped. ",1
"Taps do not work when JMX is enabled
When JMX is enabled, the Module's output channel processed by StreamPlugin is a Proxy. Thus it fails the ""instanceof"" test used to apply the WireTap to the output channel. ",3
"Remove code in StreamPlugin to extract target output channel from proxy
The SI JMX MBeanExporter proxies the output channel created in a module context when JMX is enabled. We have to unwrap the proxy in StreamPlugin in order to add the tap. XD-877 temporarily introduced this code, but a more elegant solution is called for. Perhaps this will involve SI making addInterceptor an interface method.",3
"Cleanup hsqldb data directory used by tests after each test completion
Currently, the ""data"" directory created by the HSQLDB process during the tests run is not cleaned up and may cause issues. We should delete the ""data"" directory after each test completion.",2
"Update Java Version to 7
nan",1
"Introduce wire.js into the XD admin UI
We should consider moving to wire.js to encourage dependency injection in the UI Javascript code.  See here: https://github.com/cujojs/wire/blob/master/docs/get.md",5
"Disable the JMX setting in SingleNodeMainIntegrationTests.testConfiguration
Set the enableJmx to false because contexts are not getting destroyed properly, and in some cases prevents testSystemPropertiesOverridesDefault from running successfully.",1
"Refactor the tests so that contexts are destroyed properly 
In our tests the context is destroyed at the end of each module.  It should be destroyed at the close of the container.",5
"Do not initialize spring batch schema on each test run
If the spring batch database has already been initialized do not re-initialize for each test run.    ",3
"Add Batch Job Listeners Automatically
Add Batch Job Listeners Automatically    * Each major listener category should send notifications to own channel (StepExecution, Chunk, Item etc.)  * Add attribute to disallow automatic adding of listeners",8
"Fix package tangle between org.springframework.xd.dirt.plugins.job and org.springframework.xd.dirt.job
nan",1
"Package modules with their support jars
Currently, a lot of jars that are on the classpath of xd-dirt are there to support modules.  We should move those to the lib/ construct of a module and remove them from the CP of dirt.    But this should not be done by simple ""mv"", as we'd lose version tracking and dependency management offered by gradle.    Pending a ""dependency-aware"" ModuleRegistry, we should be able to alter the build.gradle file so that it  - knows about individual modules (maybe handles them as project)  - copies its libs into the appropriate directory  - does not copy dependencies that are already legitimate dependencies of xd-dirt (this can be achieved by runtime introspection of the dependeny tree of both projects)    ",6
"Remove org. in hsqldb dependency
nan",1
"Change default admin port from 8080
This conflicts with and 'out of the box' hadoop installation that uses 8080 as the 'map reduce shuffle port'.    8088 sound ok?    ",1
"Run JavaScript tests (Jasmine) as part of the build process
We probably need to look into some options to run our JavaScript tests (Jasmine) as part of the build process - some possibilities:    * Jasmine Gradle Plugin https://github.com/dzhaughnroth/jasmine-gradle-plugin  * Saga - http://timurstrekalov.github.io/saga/    Looks like Maven has slightly better support: http://searls.github.io/jasmine-maven-plugin/index.html    See also: XD-865",8
"Provide a way to access currently deployed modules
For testing, it would be useful to access the deployed Module instances to connect sources and or sinks to a module's input and output channels, etc. This could be a simple as exposing the deployedModuleMap on ModuleDeployer or possibly something more elaborate if this level of granularity is generally useful for runtime administration.",3
"Spring Batch Behavior change from M2 to M3
In M3, the batch job behavior has changed. In M2, it was much easier to create an invoke a batch job. In M3, a trigger is required. Figuring that change out isn't a big deal but the behavior of this batch job in M3 throws a stack trace, yet it executes.   In M2, this same batch job runs fine with no stack trace.   Logs are attached. I can't see a difference in the container log property files from M2 to M3. Turning the log settings down will suppress the traces, but I was not expecting the traces since they did not show up in M2.  Stream Definitions:  job create --name pdfLoadBatchJob --definition ""batch-pdfload --inputPath='LOCAL_PDF_PATH' --hdfsPath='REMOTE_HDFS_PATH'""   stream create --name pdfloadtrigger --definition ""trigger > job:pdfLoadBatchJob""",1
"XD UI: Job Parameters tool tips display needs to aligned
When clicking on a specific job execution from the job executions bar chart, the tool tips display isn't aligned with the job parameters. Please see the attachment.",2
"Create an easier short-cut for launching adhoc Batch Jobs 
Currently, for adhoc launching of Batch jobs you have to use:  {code} stream create --name myTriggerStream --definition ""trigger > job:helloSpringXD"" {code}  For renewed triggering of the job you have to undeploy and then redeploy the job. It would be nice if there was possibly a slightly simpler way of doing this.  Just FYI - As a different approach you can also use the HTTP source:  {source} job create --name myjob --definition ""myjob"" stream create --name myjobhttp --definition ""http > job:http"" http post --data ""{}"" {source}  ",8
"Add --inputType and --outputType module parameters
The ability to configure message conversion via parameters. Consider programatic configuration of data type channels. Values can be media type, e.g., application/json or a java class name.",8
"The HDFS Sink should support copying File payloads
We should support *java.io.file* payloads in order to support non-textual file and large text file payloads being uploaded to HDFS.   Currently text file payloads are converted to a text stream in memory and, non-String payloads are converted to JSON first, using an ""object-to-json-transformer"".   Ultimately we need to support streams such as ""file | hdfs"" where the actually payload being copied to HDFS is not necessarily JSON or textual.  Need to be able to support headers in the message that will indicate which HDFS file the data should be stored in.  ",8
"Create Integration Tests for Batch Notifications
nan",5
"xd-container should start even if xd-admin is not running
currently xd-container will not start due to a DB connection failure if the xd-admin is not already running    In fact, if someone is not using Batch jobs at all with XD, they should not even need a DB connection for either xd-admin or xd-container to run    so... consider using LazyConnectionDataSourceProxy so a connection failure would only occur when the DataSource is actually invoked to retrieve a connection",2
"Move SpEL PropertyAccessors to Module Parent Context
When INT-3133 is resolved, SpEL {{PropertyAccessor}} s are inherited from parent contexts.    Instead of adding the {{JsonPropertyAccessor}} to each module's context, add it to the parent instead.",1
"Wrong Jetty Util on classpath for WebHdfs
We currently include jetty-util-6.1.26.jar but we need to add correct jar for different distributions - PHD uses jetty-util-7.6.10.v20130312.jar    Need to check hadoop-hdfs dependencies for the distros and add jetty-util-* to the jar copy for each distro  ",3
"Properly close Redis/Rabbit connection factories in tests
Tests that leverage [Redis|Rabbit]AvailableRule often create another connection factory in the test body but fail to close it.  Tests should properly close the resource. As an added benefit, the rule itself can expose the resource that it created for deciding whether to skip the test or not",2
"Split xd-dirt in 3 (or 5)
The xd-dirt project should be split in at least 3 parts:  - Classes and resources pertaining to the admin-server  - Container server  - Shared classes  Additionally, we may consider splitting the first two in half as well, having a separate project for CLI handling (and hence introduce 2 other projects for YARN, etc) ",8
"Fix hardcoded redis port from tests
kparikh-mbpro:spring-xd kparikh$ grep -r 6379 * | grep java spring-xd-analytics/src/test/java/org/springframework/xd/analytics/metrics/common/RedisRepositoriesConfig.java:			cf.setPort(6379); spring-xd-analytics/src/test/java/org/springframework/xd/analytics/metrics/integration/GaugeHandlerTests.java:			cf.setPort(6379); spring-xd-analytics/src/test/java/org/springframework/xd/analytics/metrics/integration/RichGaugeHandlerTests.java:			cf.setPort(6379); spring-xd-dirt/src/test/java/org/springframework/xd/dirt/listener/RedisContainerEventListenerTest.java:			cf.setPort(6379); ",1
"Container start/stop publish events are not getting processed
  It looks like Container's ContainerStartedEvent and ContainerStoppedEvent are published from ContainerLauncher's context whereas the ContainerEventListeners are running in XDContainer's context. This makes the container start/stop events not getting processed.  ",2
"Add aggregate counter monthly resolution query support
nan",5
"Add aggregate counter year resolution query support
nan",3
"Add aggregate counter query by number of points
It should be possible to supply a start or end date (or none for the present), plus a ""count"" value for the number of points required (i.e. after or prior to the given time).",3
"Support additional aggregate counter query options
nan",5
"Add a Processor for Restful webservices
Offers the functionality to make http request to a web service. i.e. outbound http gateway.  Example implementations:  stream create --name foo --definition ""trigger |rest --reply-timeout=1 --url=http://earthquake.usgs.gov/earthquakes/feed/geojson/all/day|log""    stream create --name foos --definition ""trigger --payload=lat=34.0567006&lon=-84.34368810000001&site=all&smap=1&searchresult=Roswell%2C%20GA%2030076%2C%20USA#.UktzaWSG1Dd | rest --url=http://forecast.weather.gov/MapClick.php? |log""    ",1
"Reduce Sonar Critical Errors 
Should keep the critical error count as close to zero as possible.",1
"Support for registering custom message converters
Users need to register custom message converters used by modules.",5
"The XD build breaks with Gradle 1.8
The XD build breaks with Gradle 1.8 due to some changes in dependency resolution.",1
"Add documentation for #jsonPath functionality with SpEL based processors
See issue https://jira.springsource.org/browse/XD-862    The docs should be updated to include examples that show how to use the standard 'SpEL' based splitter, transformer, filters with #jsonPath expressions.",2
"Convert modules to be CP-aware
Once XD-887 is merged, gradually convert more modules.    Recipe:  1) Move the <module>.xml file to <module>/config/<module>.xml  2) Declare a :module.<type>.<module> gradle project  3) Move dependencies from dirt project to newly created module project  4) gradle build picks it up.    gradle clean build + manual test  Also have a look at gradle cleanEclipse eclipse",8
"File source should be able to produce file contents or file reference
File source should output either the File itself (serialized File object) or the contents as a byte[]. This option is configured by a parameter --contents=true.  The byte[] may be converted to a String using XD Message Conversion, e.g., --output = text/plain;charset=UTF-8",4
"Make the parser aware of message conversion configuration
Enhance the stream parser to take message conversion into account in order to validate or automatically configure converters. For example:   {noformat:nopanel=true} source   --outputType=my.Foo  | sink --inputType=some.other.Bar   is likely invalid since XD doesn't know how to convert Foo->Bar.  {noformat}",8
"Refine project (mainly dirt) dependencies
See also XD-903, XD-915  A lot of dependencies have been added with the ""compile"" scope as an oversight over time. Some of them are only required at runtime, some may not be required anymore.",1
"Remove json parameter from twittersearch source
json parameter is no longer required. Use --outputType=application/json instead",2
"MQTT source module does not cleanly undeploy
Was attempting to test mqtt, turns out I don't have the proper rabbitmq thing installed. So far so good, I get these kinds of exceptions:  {noformat}  Unable to connect to server (32103) - java.net.ConnectException: Connection refused    org.eclipse.paho.client.mqttv3.internal.TCPNetworkModule.start(TCPNetworkModule.java:75)    org.eclipse.paho.client.mqttv3.internal.ClientComms$ConnectBG.run(ClientComms.java:521)    java.lang.Thread.run(Thread.java:724)  Caused by: java.net.ConnectException: Connection refused    java.net.PlainSocketImpl.socketConnect(Native Method)    java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)    java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)    java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)    java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)    java.net.Socket.connect(Socket.java:579)    org.eclipse.paho.client.mqttv3.internal.TCPNetworkModule.start(TCPNetworkModule.java:66)  	... 2 more  {noformat}    Problem is, I still get them after undeploying my ""mqtt | log"" stream",2
"Add more ""hands on"" example to MQTT doco
Not everyone may be familiar with MQTT, or esp. with MQTT inside Rabbit",3
"Handle SingleNodeServer's stop()  method cleanly
SingleNode server needs to stop cleanly with stopping both the admin server & container server.     Also, all the tests that require SingleNode main server needs to handle the server shutdown appropriately.",2
"Error Channel for streams modules that fail to process a message
As a user, I'd like to be notified when a exception is thrown in a module so that I can tap into an error channel to receive the failures for each stream/module.    ",8
"Split integration.x in dedicated XD projects where appropriate
Similar to what is done for e.g. hadoop, reactor, and http, some of the classes in the .x package (namely gemfire, splunk, twitter) should go in dedicated (albeit small) projects. This would enable further modularization (see XD-915)",3
"Investigate Swagger to generate REST API Documents
nan",3
"Update Core Spring Dependency to 4.0.0.M3
nan",8
"ModuleType  Refactor
Remove ModuleType.getModuleTypeByTypeName.  All code should use the enum.  ",3
"Refactor src/test/resources in Dirt
* In the testmodules.source  ** Rename source-config to packaged-source  ** Rename source-config to packaged-source-no-lib  * All xml files should be prefixed with test.  i.e. testsource, testsink  * Make sure all tests pass with new configuration",1
"Remove the Tabulation characters should not be used from Sonar
nan",1
"Return rounded interval values from aggregate counter queries
The aggregate counter query result currently returns the interval that is passed in, whether it is aligned with the bucket resolution requested or not. It would be more intuitive if the time values returned are rounded (down) to the resolution of the query (i.e. whole minutes, hours, days or whatever).",2
"Format option to display runtime module properties in shell
The runtime module properties requires a format option when displayed in the Shell     Based on the PR (https://github.com/spring-projects/spring-xd/pull/340), the module properties are stored as String and displayed as is.  ",2
"Tests Fail because HSQL not started
This is cause generally by someone having port 9100(hsqldb port) in use. It is recommended that setup checks to see if port is in use.  If it is throw an exception stating that hsqldb port (9100) is in use and suggest options like.  Free up the port or change the hsqldb port.",2
"Remove work around Spring HATEOAS#89
See https://github.com/spring-projects/spring-hateoas/issues/89    Updating HATEOAS version and removing in a lot of controllers should be possible now. See eg https://github.com/spring-projects/spring-xd/blob/4919ea2498a13ef47aaa9437937308fb26a7a24f/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/rest/XDController.java#L219",2
"Support JSON to Object
Register a JSON to Object converter in the DefaultContentTypeAwareConverterRegistry. Currently we only support Object to JSON but not the other way. This may or may not work for an arbitrary object but is useful in many cases.",2
"Implement Kryo serialization for Tuple
Currently TupleCodec uses JSON for serialization/deserialization. It should use Kryo. This will require some customization and potentially changes to Tuple to address the Tuple's conversionService field. ",3
"Find runtime modules by type and/or name
We need a way to find the runtime module info by module type(""source"", ""sink"", ""processor"", ""job""). ",3
"Inject ModuleDefinitionRepository into ModulesController
Currently, ModulesController creates the ModuleDefinitionRepository instance with ModuleRegistry. Instead, we should inject the moduleDefinitionRepository into ModulesController directly.",1
"List runtime modules by wrong containerId should throw exception
With PR#340, listing of runtime modules with a non-existent containerId will display empty table. Instead, we can throw exception saying Container doesn't exist.",1
"Make Runtime modules listing by ContainerId pageable
The RuntimeContainersController (from PR#340) returns the list of runtime modules. Instead we need make it pageable.",2
"Splunk Pulls in an Old SI Jar into STS
nan",1
"Convert remaining modules to be CP-aware
Now that XD-924 is merged, we can convert splunk, twitter and gemfire modules to rely on the newly created projects.    The hadoop module will get its own story, as classpath handling is a bit more tricky for that one.    From there on, no new dependency should be added to DIRT for the sole purpose of a module. Rather, it should directly be created as a CP-aware module and project.",3
"Change repo links in build.gradle repo.spring.io
nan",1
"JobPlugin should not add properties that are not needed
If you create a simple ""http | log"" stream and list the modules, you'll see that JobPlugin adds its numberFormat, makeUnique, etc properties.  Even though they do no harm, it's really strange for users. Plus, it could conflict with e.g. activation of profiles given present properties",3
"Clean up unused JSON mapping classes
There are a few obsolete classes lurking around, TypedJsonMapper comes to mind but there are likely some others",1
"Spike for writing to HDFS
See Epic https://jira.springsource.org/browse/XD-234",10
"Spike for EC2 deployments
nan",20
"Spike for Deployment SPI
SPI for deployment on to YARN + Local 'dirt' cluster.",20
"Spike for job that imports data from CSV file to HDFS
TBD",10
"Spike for job that exports HDFS CSV data to JDBC
nan",10
"Spike for advanced Job Orchestration features
nan",20
"Spring-XD shell can't run commands with kerberized CDH 4.3.0
Basically when I launch the spring-xd shell I can't interact with the namenode I receive security violations despite the fact that I try and set the proper configs. Authentication/Authorization (true/kerberos)  details of this issue can be found here:  http://stackoverflow.com/questions/19258321/the-spring-xd-xd-shell-cant-run-the-hadoop-fs-ls-command-the-command-returns  ",5
"Richer module options metadata
Module options are currently implemented using PropertySourcesPlaceholderConfigurer, doing simple ""text"" replacements.    This story proposes to introduce a richer model for module options, which could take the following xml representation (keeping in mind that there would be an underlying set of classes that could also apply to e.g. @Configuration approach):    {code:xml}  <xd:params>    <xd:param name=""port"" help=""the http port to listen on"" />  </xd:params>  {code}    So, the very first obvious benefit is providing help metadata.  The second one is an easy way to list available option names, without resorting to brittle PropertySourcesPlaceholderConfigurer hacks (we wouldn't for example detect xd.stream.name or XD_TRANSPORT as a valid option)    One could also specify defaults/computations, this time benefiting from SpEL everywhere:  {code:xml}  <xd:params>    <xd:param name=""directory"" default=""headers.directory"" />  </xd:params>  {code}    Lastly, this opens the door to better type checking / combinations / optionality support:  {code:xml}  <xd:params valid=""fixedDelay || cron"">    <xd:param name=""fixedDelay"" type=""int"" />    <xd:param name=""cron"" type=""string"" />  </xd:params>  {code}    The 'valid' attribute can e.g. be evaluated by SpEL.    Some final remarks:  - That construct can be compatible with our current approach by behaving as a PropertySource itself (instead of creating a PropertySource, the StreamPlugin would give this new bean the java.util.Properties)  - Plugins could benefit from a hook in that construct and advertise the fact that they expect/provide new options (e.g. --inputType)    There is a slight problem though, which is that if this construct lives in the same AppContext as the module definition itself, then the AppContext needs to be refreshed for the logic to kick in. One could circumvent that using profiles, or we could rely on a different filename convention (e.g. <module>-params.xml)      ",10
"Stream definition completion REST layer + Shell adapter
Provide the infrastructure for HTTP GET /completions?start='http | file --d"" that would return a list of possible completions (in this case returning the file option names that start with ""d"")    This story is about (and only about):  - Having that REST controller, delegating to some ""CompletionsEngine""  - Implementing the Spring Shell Converter that talks to that    It's an empty shell, useless (but easy to do) without the actual ""CompletionsEngine""",3
"Update Jobs documentation to include ""job launch"" command
This is currently missing and probably supersedes some of the stuff that's in there now.",1
"Refactor DSL parser according to latest syntax proposals
We have recently revised the syntax for stream definitions, this issue covers that refactoring.",12
"Update SI Dependency to 4.0.0.BUILD-SNAPSHOT
nan",3
"Build failure on Ubuntu
Build fails:  gradle clean build  ...  :spring-xd-dirt:test  org.springframework.xd.dirt.stream.FileSourceModuleTests > classMethod FAILED     java.io.IOException at FileSourceModuleTests.java:53  org.springframework.xd.dirt.stream.FileSourceModuleTests > classMethod FAILED     java.lang.IllegalArgumentException at FileSourceModuleTests.java:130  328 tests completed, 2 failed, 11 skipped :spring-xd-dirt:test FAILED  FAILURE: Build failed with an exception.   Looks like it is trying to create a directory under the local filesystem /  java.io.IOException: Unable to create directory /tmpfilesourcetests   org.apache.commons.io.FileUtils.forceMkdir(FileUtils.java:2024)   org.springframework.xd.dirt.stream.FileSourceModuleTests.createTempDir(FileSourceModuleTests.java:53)   sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)   sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)   sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)   java.lang.reflect.Method.invoke(Method.java:606)   org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)   org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)   org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)   org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)   org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)   org.junit.runners.ParentRunner.run(ParentRunner.java:309)   org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecuter.runTestClass(JUnitTestClassExecuter.java:80)   org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecuter.execute(JUnitTestClassExecuter.java:47)   org.gradle.api.internal.tasks.testing.junit.JUnitTestClassProcessor.processTestClass(JUnitTestClassProcessor.java:69)   org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.processTestClass(SuiteTestClassProcessor.java:49)   sun.reflect.GeneratedMethodAccessor23.invoke(Unknown Source)   sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)   java.lang.reflect.Method.invoke(Method.java:606)   org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35)   org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)   org.gradle.messaging.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32)   org.gradle.messaging.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93)   com.sun.proxy.$Proxy2.processTestClass(Unknown Source)   org.gradle.api.internal.tasks.testing.worker.TestWorker.processTestClass(TestWorker.java:103)   sun.reflect.GeneratedMethodAccessor22.invoke(Unknown Source)   sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)   java.lang.reflect.Method.invoke(Method.java:606)   org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35)   org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)   org.gradle.messaging.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:355)   org.gradle.internal.concurrent.DefaultExecutorFactory$StoppableExecutorImpl$1.run(DefaultExecutorFactory.java:66)   java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)   java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)   java.lang.Thread.run(Thread.java:724) ",3
"Running gradle idea creates project configured with source 1.6
The gradle idea task creates a project configured with source 1.6  This results in compile failures on Java 7 specific code",3
"Accessing xd-admin URLs in the browser return XML and not JSON
Here is an example:  the following request for streams:  http://ec2-23-20-25-30.compute-1.amazonaws.com:9393/streams    Returns:  This XML file does not appear to have any style information associated with it. The document tree is shown below.  <errors xmlns:atom=""http://www.w3.org/2005/Atom"">  <error logref=""HttpMessageNotWritableException"">  <message>  Could not marshal [PagedResource { content: [links: [<http://ec2-23-20-25-30.compute-1.amazonaws.com:9393/streams/ticktock>;rel=""self""]], metadata: Metadata { number: 0, total pages: 1, total elements: 1, size: 20 }, links: [] }]: null; nested exception is javax.xml.bind.MarshalException - with linked exception: [com.sun.istack.SAXException2: unable to marshal type ""org.springframework.xd.rest.client.domain.StreamDefinitionResource"" as an element because it is not known to this context.]  </message>  </error>  </errors>",1
"fix gradle clean test
Currently ./gradlew clean test fails since the module dependencies are not packaged before the test task.  ",1
"Features and bug fixes for named channels
nan",10
"Support for common use cases using quartz
nan",40
"Low story point/time improvements or bug fixes that can easily be picked up in a sprint in an ad-hoc manner
nan",20
"User requests that XD team members are issued EC2 accounts
* Setup groups for xd user  * Setup privileges so users can only see their instances  * Setup user accounts  ** Send created access key to users  ** Send username and passwords to user  ",5
"User requires wiki page on how to use the XD EC2 Installer
Add instructions to github wiki on the usage of the installer ",5
"Create google doc with instructions on managing EC2 Instances
* Logging into your XD Account For Example: https://946513944028.signin.aws.amazon.com/console * Discuss how to terminate running instances. ** Users can terminate all instances using the UI on the EC2 admin page * Usage monitoring via CloudWatch * Investigate what metadata in each instance is required so that cloudwatch can track ",3
"Add functionality to provision EC2 instance and mount EBS
* The Application should  **  Create a Spring Application Context. **  Should take a command line parameter --config that will point to a property file with key/value pairs specified below.  By default the file xd-ec2.properties will be looked for on the classpath ** Associate EBS shared volume for each machine instance  * Packaging  ** Use gradle application plugin to generate a â€˜binâ€™ directory with a script to start the application.  Seehttp://www.gradle.org/docs/current/userguide/application_plugin.html ** The config file xd-ec2.properties should be in a directory (ideally a 'config' directory parallel to 'bin' and will be loaded by the application by default - loading via the CP is probably the easiest way. ** Create a POJO to easily reference these properties, vs. using a raw java Properties object.  How to verify it works  * Integration testing ** Create JUnit based tests.  JClouds itself has extensive testing, can look at those for structure. ** Verify what you created has been installed. ** Verify ports are open  Instance Information ** EBS of 50GB Base for each instance. * Report successful and failed Instances.  Key-Value pairs in configuration file properties may include:  * cluster-name= a name describing the cluster you are creating * aws-access-keys= the access key assigned to you by admin * aws-secret-key= the secret key assigned to you by admin * private-key-file= the private key file assigned to you by admin.  Used for ssh-ing into  * multi-node=true/false if true then installer will run the number of nodes as enumerated in the number-nodes property value.  if false the installer will start a single node server * number-nodes=The number of nodes(containers) to deploy for this cluster.  Value is an integer > 0 * machine-size=The size of machines to be assigned for admin and nodes.  small, medium, large * redis-port=6379 * rabbit-port=5279 * xd-dist-url=The url to download the XD to install.  for example: http://blahblahblah.zip * ami = The ami image to use for your cluster. for example: ami-dfadsfdadf  ",10
"Install XD admin instance on EC2
*Update the Deployer class to add the following methods * ** RunningInstance  deployAdminServer  * The install script steps: ** Using XD-977 install distribution ** Setup XD_HOME variable ** start up redis and rabbit using ports as specified in xd-ec2.properties on admin server ** Create configurator directory ** Copy the configurator to containers ** Use port watch to make sure they started ** start admin server ** use port watch to make sure the admin started on 9393 ** Report if admin server started.  If it didn't start abort install. ** Report public DNS name of admin-server    * Integration Testing ** Verify XD admin has been started *** Create a basic stream (trigger>log)and make sure we get a success code from xd admin was received. *** Query the redis to see if the stream was created.  ",10
"Create CI for XD-EC2 project
Automated test will use directly use the Deployer class  * asserts on basic info of RunningInstance  ** check that EBS was mounted  ** that application was unzipped  ** redis and rabbit are running via port checks    * http requests on admin port for   ** root path  ** list of modules  * @AfterClass that will look for the cluster name and terminate all instances  Look at â€˜liveâ€™ tag in JClouds tests for some additional tactics  ",5
"The HDFS Sink should support compressing files as they are copied
Get a java.io.File and copy it into HDFS.  Could be text or binary.  Write compressed with Hadoop and third party codecs   see: (XD-277, XD-279)  should initially support:  - bzip2    - LZO  ",8
"Users need the ability to provision an XD cluster on EC2 via command line tool.
we need the ability to run an XD cluster to get a handle on general issues and missing features based on running the system in a 'true' clustered environment.  We donâ€™t need to make this an end-user facing feature in the short term, e.g. set a few keys in the shell and then install via a shell command. ",40
"Bootstrap Project for XD AWS Installer
* Create a Spring application context.  (XML or Java, dealers choiceâ€¦)  * Use XD eclipse code format policy  * Create Source Package structure  * Create Test Package Structure  * Gradle build  * JClouds and Spring deps    ",5
"Add functionality to download XD distribution zip to each EC2 instance as specified by user 
* User specifies download distribution zip file from properties file.  See XD-969 for key-value pairs * Copy the distribution from the shared EBS/S3  the ebs volume assigned to the each node/admin instance  .   ** If not on shared ebs/s3 pull from http site specified by user and put on the shared EBS volume.  ** Unzip the distribution from on the ebs volume for the instance to the /home/ubuntu directory ** make sure privileges are set to ubuntu not root.   How to verify it works * Create a JUNit style integration test that ** Deletes a known .zip distribution from EBS/S3. ** Invoke the application functionality (should be a 1 liner) that will start up the instance and download the .zip distribution from the URI provided in xd-ec2.properties.  Verify the file is now in EBS/S3 and also on the instance ** Tear down created instance ** Create new instance passing in the same URI of the .zip distribution.  Verify as before ",5
"Installer needs to launch a single node XD instance
* Create a Deployer class has methods  ** RunningInstance  deploySingleNode  *** takes into account machine size as specified in properties file  ** void  destroyAllInstances()    *** or whatever JClouds returns from the destroy call  ** ctor gets passed in the root boostrapping credentials.  * Install Script Steps  ** Setup XD_HOME variable  ** Make sure privileges are set to ubuntu not root.  ** start up redis and rabbit using ports as specified in xd-ec2.properties  ** Use port watch to make sure they started  ** Start singlenode after configuration.  ** Display hostname of singlenode server  ** Report successful and failed startup  ** Hit root of xd-admin to see if there is a response on 9393  * Integration Testing  ** Verify that config files have been setup  ** Verify XD has been started  ** Verify XD can process a basic http post    ",10
"Batch Wordcount Sample to use File Source
nan",3
"Install XD Container on EC2
*Update the Deployer class to add the following methods* * RunningInstance  deployContainer  * For each of the containers: ** Using XD-977 install distribution ** Setup XD_HOME variable ** Create configurator directory ** Copy the configurator to containers ** Run Configurators ** Verify that configuration files are setup correctly ** start container server ** Report if container started. (using jmx/Jolokia, if available) If it didn't start report failure but continue. ** Report public DNS name of container   * Integration Testing ** Verify that config files have been setup properly ** For each container: *** Verify container has been started *** Verify that container is working by creating a stream in admin (trigger|log).  Verify that the log on the container is being updated.  ",10
"Missing guava-11.0.2.jar dependency for hadoop distros
We used to have a shared guava-11.0.2.jar dependency in the lib dir. That's no longer there so hadoop distros that require this now fail (at least any hadoop 2.0.x based ones)    We should also upgrade to current Hadoop versions (Hadoop 2.2 stable)",3
"OOTB batch jobs for common cases
nan",20
"Writing to HDFS - 1.x
nan",1
"Create OOTB batch job for import and processing multiple files to JDBC
Create a sample batch job for inclusion in the distribution that will perform the following tasks.  ItemReader * Read a from a directory with multiple files (configurable) * Support for CSV (assume first line has header values) * Convert to tuple data structure ItemProcessor * Provide groovy based no-op ItemProcessor.  (configurable) ItemWriter * Write to JDBC. ** Provide  ** Assume there is an DB instance running somewhere, specify connection info (configurable)  **The sample job should be documented** ",10
"Create OOTB 'file to HDFS' batch import job that is launched by a stream.
Same processing as XD-984, but the job instacne is launched via an event from the input file source.  Supporting a single file per job launch is OK.  > job create blah --definition ""filehdfs""  > stream create csvStream --definition ""file --ref=true --dir=/Users/luke --pattern=*.csv > job:blah""     *the job should be documented*",5
"Create OOTB batch job that uses the Hadoop Shell to copy multiple files from a local directory into HDFS
Create a new tasklet implementation that will use the ANT globbing like syntax found in MultiItemResourceReader to specify the input files, and use FsShell to invoke a 'copyFromLocal' command with a specified HDFS output directory.  The batch job would call the tasklet.  *the job should be documented*",10
"Create OOTB batch job for export and processing multiple files from HDFS to MongoDB
The ItemReader will read multiple files from HDFS and the data will be converted to a tuple data structure The ItemProcessor will be a no-op groovy script. The ItemWriter will write the data to a MongoDB collection ** A TupleToDBObject converter will need to be developed.  *the sample job should be documented*",8
"Create OOTB batch job for export and processing multiple files from HDFS to JDBC
Same setup as XD-987 for ItemReader and ItemProcessor, but should write to HDFS.      One can assume that the table structure has been created already external to the batch job execution.",5
"Create OOTB batch job that uses the Hadoop Shell to copy multiple files from HDFS to a local directory
This is the inverse of XD-986 and will require creating a custom tasklet but with the input/output reversd.",5
"The HDFS Store Library should support writing text with delimiter
Support writing lines of text separated by a delimiter  Support writing a CSV (comma-separated variables), TSV (tab-separated variables),  No compression",8
"The HDFS Store Library should support compression when writing text
Need to support writing text in compressed format    should initially support:   - bzip2   - LZO",8
"The HDFS Store Library should support writing to Sequence Files
Support for writing Sequence Files  Without Compression  Need a means to specify the key/value to be used ",8
"The HDFS Store Library should support compression when writing to Sequence Files
Support for using compression when writing Sequence Files  Either block or record-based compression. ",8
"The HDFS Sink should support writing POJOs to HDFS using Parquet
Writing POJOs using Kite SDK ",8
"Refactor tests with FileSink|FileSource to use eventually() matcher
Some tests (esp. ModuleClasspathTests.testModuleWithClasspathAfterServerStarted) seem to fail because of a race condition.    Add a Hamcrest matcher that knows how to read the content of a FileSink|Source and refactor those to read like e.g.    assertThat(fileSink, eventually(hasContent(""foo)))    ",5
"The HDFS Sink should support writing POJOs to HDFS using Avro/Kite SDK with support for partitioning
Support for partitioning on a field, e.g. date.",4
"Add documentation for gemfire cache-listener source
Need some sample usage, docs for     https://github.com/spring-projects/spring-xd/tree/master/modules/source/gemfire     ",1
"Return the step execution information in the current job execution controller
Related to https://jira.springsource.org/browse/BATCH-2109    DistributedJobService#listJobExecutionsForJob overrides   SimpleJobService#listJobExecutionsForJob    and does not include the *StepExecution*s. This is due to serializion issues with Jackson.     In order to fix this, we need to add a Jackson MixIn.  ",4
"User should be able to view the list of all the job definitions
Create a tab view with tabs â€œJob Definitionsï¿½?, â€œRuntime Jobsï¿½? (Deployed Jobs?, â€œJob Instancesï¿½? (Is Runtime Jobs a better name here?) and â€œJob Executionsï¿½?. On clicking â€œJob Definitionsï¿½? tab, we can have a table view of job definitions. Since we bootstrap.js, we can have a responsive table layout to list all the available job definitions. At the REST layer, â€œ/jobsï¿½? provides the list of job definitions. We can expand the JobsController list()â€™s QueryOptions to add more criteria (especially to list JobDefinitionâ€™s status (Deployed/Undeployed).  Also, this is the UI implementation for the shell command â€œjob listï¿½?  ",4
"User should be able to view the list of all Deployed Jobs
On clicking the â€œDeployed Jobsï¿½?, we can have a table view of all the deployed jobs.  This is again a responsive table layout with all the job definitions with status â€œdeployedï¿½?. The deployed XD job corresponds to a single batch Job Instance.     This story addresses the UI layout changes to display existing JobInstance information.",2
"UI: User should be able to get all the job executions on a given job at deployed jobs page
On clicking a specific job name in the deployed jobs list, we need to redirect the user to show the list of all the job executions on that job.  User should be able to navigate back to the deployed jobs list.",3
"UI: User should be able to launch a job from Deployed jobs page
From the Deployed jobs page, user should be able to click on the ""Launch"" button on a specific job and specify the Job parameters as key value pairs in the text box and we will convert that into JSON string as JobParameters into JobLaunch request.    ",2
"UI: User should be able to schedule a job from Deployed jobs page
From the Deployed jobs page, by clicking the ""Schedule"" button on a specific deployed job row, user should be able to schedule this job with:  1) Cron trigger (with cron expression) as a source to job launching named channel  2) Fixed rate/delay trigger as a source to job launching named channel    ",4
"UI: User should be able to filter the list of executions on the execution tab
On clicking the â€œExecutionsâ€ tab, user should see the list of all batch job executions. There should be options to filter job executions by few criteria such as by â€œJob nameâ€, â€œexecution timeâ€ etc., ",3
"UI: User should be able to view job detail from a specific job execution at Job Executions page
On clicking ""details"" link on a job execution row, user should see the job details.    Job detail page will show all the information about the job, where as the table listing of jobs on the Execution tab may have omitted some columns or aggregated values to convey information more easily.",3
"UI: User should be able to see step execution info in a table below job detail
On clicking the job detail page, we should display all the step executions associated with the specific job execution in a table view.",3
"Jobs list REST endpoint should include deployed/undeployed status
Currently, the jobs definition list REST endpoint doesn't include deployed/undeployed status on a given job.",4
"Don't perform error-level logging for normal application behavior in batch admin functionality
Assumption: The batch job *wordCountJob* does not exist. Executing the REST endpoint:  http://localhost:9393/batch/jobs/wordCountJob/executions  yields the correct response:  {code} <errors xmlns:atom=""http://www.w3.org/2005/Atom""> <error logref=""NoSuchBatchJobException""> <message> Batch Job with the name wordCountJob.job doesn't exist </message> </error> </errors> {code}  One could argue, though, that this is technically not an error but a status/application message, no??  However, the console/log logs at error level with a full stacktrace. I think the full stacktrace should only be available when debugging is allowed. In normal mode, I think we should log only at info or warn level without the full stacktrace. ",2
"Convert remaing FileSink assertions to eventually() constructs
See discussion at https://github.com/spring-projects/spring-xd/pull/362",5
"Spring Shell --host option does not work
When trying to use the --host command line parameter, spring shell reports the following error for this command, ""xd-shell --host localhost"" : Oct 25, 2013 10:35:25 AM org.springframework.shell.core.SimpleParser commandNotFound WARNING: Command '--host localhost' not found (for assistance press TAB) ",2
"DataSource for batch infrastructure should be configurable, not hardcoded to hsqldb
Currently HSQLDB is the only option for batch jobs.  This should be configurable so that a user may select another JDBC data store option.      Steps:  * hsqldb.properties needs to be changed to batch-jdbc.properties  * hsql prefixes should be changed to batch-jdbc  * JDBC Connection String needs to be configurable  * JDBC Driver needs to be configurable  * The Setup Scripts to be used for spring batch need to be configurable.  * HSQLServerBean should be renamed to something batch-jdbc.properties    Tests:  Should be able write tests that support HSQLDB",8
"Add additional REST endpoint that return the XML definition of a job
nan",8
"Command to show the XML of the job definition
nan",8
"Create a reusable responsive UI layout to render the XD PagedResoures in tabular view
Create a reusable responsive UI layout to render the PagedResources returned from REST endpoint.     As part of this, try upgrading the bootstrap to 3.0.0 and use the responsive styles offered in it.",5
"Provide an option to pretty print JSON output
Probably the cleanest approach is to provide a properties file in the xd config directory that enables this globally, e.g., json.pretty.print=true.  This will require some refactoring of the ModuleTypeConversion plugin, i.e., use DI in streams.xml",3
"Spring Batch Admin UI looks to localhost when getting status updates
Currently when you bring up a admin-ui on a browser that is not on the XD admin server, the page reports an error.    This is because the javascript is looking to localhost to get its updates.  ",5
"JobDeployer hides root exceptions on failure
When deploying jobs, the following code (line 103), hides the root cause of deployment failure:      if (exceptionClassName.equals(BEAN_CREATION_EXCEPTION) || exceptionClassName.equals(BEAN_DEFINITION_EXEPTION)) {      throw new MissingRequiredDefinitionException(definition.getName(), cause.getMessage());    }    For example:    org.springframework.xd.dirt.stream.MissingRequiredDefinitionException: Error creating bean with name 'dataSourceInitializer' defined in file [/Users/luke/Work/spring-xd/build/dist/spring-xd/xd/modules/job/hdfsjdbc/config/hdfsjdbc.xml]: Cannot resolve reference to bean 'databasePopulator' while setting bean property 'databasePopulator'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'databasePopulator' defined in file [/Users/luke/Work/spring-xd/build/dist/spring-xd/xd/modules/job/hdfsjdbc/config/hdfsjdbc.xml]: Initialization of bean failed; nested exception is java.lang.NullPointerException    org.springframework.xd.dirt.stream.JobDeployer.deploy(JobDeployer.java:103)    org.springframework.xd.dirt.stream.JobDeployer.deploy(JobDeployer.java:67)    org.springframework.xd.dirt.rest.XDController.save(XDController.java:242)",1
"Add Spring Retry to Rabbit Message Bus
Log and purge bad messages",2
"Cleanup ModuleDeployer
ModuleDeployer has many methods with very similar names that are hard to understand.    Moreover, there is a substantial amount of duplicated code that should be extracted in sub methods with descriptive names.    One should even consider splitting the class    ",5
"Fix undeploy of stream with a composed module
Create a composed module  Create a stream that uses that module  Try to undeploy the stream.  Kaboom    The dispatch is not correctly implemented in ModuleDeployer",5
"Switch ""module list"" to horizontal display
The module list command currently has a very simplistic two column display of (module name, module type). The is not very readable.  Switch to a 4 column display: (Sources, Processors, Sinks, Jobs)  Additionally, mark composed module [e.g. ""myhttp (c)""]",5
"Refactor job deploy to go through XDController.deploy(name)
Job deployment currently goes through an overloaded version of deploy() that takes 4 parameters. This prohibits job handling code from benefiting from common behavior (and eg currently breaks deployAll)    Given that the 3 additional parameters are in fine handled as module parameters, let's push them to the job definition, known at creation time, rather than at deployment time (as it does not really make sense to change those between deploys)",4
"Kryo Redis Serializer
The current (XD) uses an inadequate {{MessageRedisSerializer}} when {{extractPayload}} is false (not currently being used). When porting this to SI, the serializer will be dropped.    Suggest creation of Kryo serializer for Messages for when Redis source/sinks are created.",5
"UI: Implement Job Deploy/Undeploy from the Job Definitions page
From XD-1023, the job status(deployed/undeployed) is available from JobInstance Repository and a job can be deployed/undeployed correctly.     Implement Job deploy/undeploy for a given job from JobDefinitions page and indicate status of the job definition (deployed/undeployed).",2
"UI: Responsive layout for supported user agents(Mobile, Tablet and Desktop)
The current admin UI uses bootstrap 3.0.0 which provides responsive design. We need to expand our scope to support all supported user agents.  This requires changes to use user agent specific layout for the UI views.",10
"Create script-based batch ItemProcessor 
This would be included in the OOTB batch jobs to optionally process the loaded Tuple with a configured script.",10
"UI - Do not hard-code server url
Specify the default URL as:    {code}  urlRoot: location.protocol+'//'+location.hostname+(location.port ? ':'+location.port: '')  {code}",1
"Migrate to SI Redis Queue and Topic Adapters
nan",3
"Refactoring to JdbcMessagePayloadTransformer to extends from AbstractPayloadTransformer not JsonToObjectTransformer
nan",1
"UI - Launch a job with parameters
Use a modal dialog to specify runtime parameters. There should be a little text are that gives hints as to the spring batch parameter key/value conventions, e.g. for type.  Might be a good idea to have a checkbox that lets you select to 'auto increment' job instance number.  4 columns key, value, type, identifying and an 'add parameter' button that adds a new row.  This would appear as a modal dialog box, polling of the state of the deployments would be suspended while the job parameter modal dialog box is shown.  ",8
"Convert hadoop module to isolated classloader scheme
* rename spring-xd-extension-hdfs to something else, as it seems it is all spring ""data"" stuff and is not coupled to xd. But leave it in extensions/ for now  * rename and move spring-xd-hadoop inside extensions (maybe to spring-xd-extension-hadoop (or hdfs))  * make hadoop related modules depend on the latter (which itself will depend on the former)",8
"Add a tcp-client source module
Add a module that can act as a tcp *client* (as opposed to our current tcp module, which acts as a server, waiting for an incoming connection)    Also, the module should allow to send ""commands"" to the remote server. The typical minimal case for such a protocol is to send ""PING"" messages, but a stateful mechanism should be put in place for more complex cases.",5
"Stream definitions should not be saved when auto-deploy results in an error
This relates to XD-871 which provides a good scenario.     >stream create s1 --definition ""http | log --deploy""   >stream create s2 --definition ""http | log --deploy""    The second command results in an error message that the port is in use but the stream definition is still saved. Since create + deploy is a logical unit of work, it should follow transactional semantics. In other words if the deploy fails, the repository should be rolled back (or a compensating destroy should be performed).  Note this should not be handled the same way if create and deploy happen separately. In that case, the stream definition should remain.",4
"Attempt to compose a module with same name+type as existing should fail
As per discussion, any attempt to create something that would collide with existing should fail:    II) Attempt to create a composed module with same name as an already existing, not composed module, with same type  should fail (EB, MF)  should work and shadow previous module from now on    III) Attempt to create a composed module with same name as an already existing composed module, with same type  should fail (EB, MF)  should work and shadow previous module from now on  should work and retroactively change definitions of streams with that module  ",5
"Support composed module deletion
Provided it is not currently used in any stream:    V) Attempt to destroy a composed module  Should not be supported at all  Should not be supported if involved in at least one stream (EB?, MF!)  Should be supported and have no other consequences whatsoever (see IV) (EB?)  Should be supported and invalidate/destroy streams involving it  ",10
"Support ModuleOptions for composed modules
With the following semantics (assuming http | filter | transform composition):      * Fully qualified names always available (eg filter.expression)     * using module type as key     * or label if ambiguity  * Simple names available if no ambiguity (eg here ""port"" refers to http.port)    As per discussion:    VIII) Use of â€œ--expressionï¿½? option in composed module â€œfilter | filterï¿½?  Should not be supported (require FQN always)  Should pertain to first filter module (always)  Should pertain to all modules with an â€œexpressionï¿½? option (here: both filters)  Should fail if not qualified and ambiguity. Use label.expression (or type.expression, eg if we had â€œfilter | transform) (see VII â‡’ nesting1.nesting2.nesting3.expression) (MF)    VIII) Use of â€œ--expressionï¿½? option in composed module â€œhttp | filter --expression=something | transformï¿½?  Should not be supported (already valued)  Should pertain to first filter module (always)  Should pertain to all modules with an â€œexpressionï¿½? option (here: both filters)  Should fail if not qualified and ambiguity. Use label.expression (or type.expression, eg if we had â€œfilter | transform) (see VII â‡’ nesting1.nesting2.nesting3.expression) (MF)  ",15
"Source that produces JSON String as byte[] with --outputType application/json should produce JSON string
nan",2
"Composed of Composed fails at stream deployment time
Although composition of a module out of an already composed module seems to work at the 'module compose' level, trying to deploy a stream with that more complex module fails with      org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:589)    org.apache.tomcat.util.net.JIoEndpoint$SocketProcessor.run(JIoEndpoint.java:312)    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)    java.lang.Thread.run(Thread.java:724)  Caused by: java.lang.IllegalArgumentException: each module before the last must provide 'output'    org.springframework.util.Assert.notNull(Assert.java:112)    org.springframework.xd.module.CompositeModule.initialize(CompositeModule.java:132)    org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:234)    org.springframework.xd.dirt.module.ModuleDeployer.deployModule(ModuleDeployer.java:224)    org.springframework.xd.dirt.module.ModuleDeployer.handleCompositeModuleDeployment(ModuleDeployer.java:180)    org.springframework.xd.dirt.module.ModuleDeployer.handleMessageInternal(ModuleDeployer.java:129)    org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:73)  	... 63 more",5
"Remove ""legacy"" application code following Spring bootification
See the discussion in https://github.com/spring-projects/spring-xd/pull/370    There are now various superseded classes and tests which we no longer need.",2
"Upgrade to Spring for Apache Hadoop 1.0.2.RELEASE and Pivotal HD 1.1
Make sure the sinks and jobs work against Pivotal HD 1.1",3
"Support short names for types in ModuleOptions
The ModuleOptions PR currently uses FQN for types (eg java.lang.String)    Would be nice to have support for short names for common types, both in the properties files and the annotation",3
"Encapsulate list of ModuleDeploymentRequests within a parsed Stream result object
Currently the parser returns a List<ModuleDeploymentRequest>, and the deployer works with that list directly. We need a higher level parser result (e.g. DeployableStream - or probably a better name after some thought) that can encapsulate that list while also enabling metadata to be added. That metadata may be helpful for composite module information as well as the module dependencies of a given stream (including any composed modules within that stream).",8
"refactor module dependency tracking to be closer to stream deployment
see comments on this PR (which is part of the code that needs to be refactored):  https://github.com/spring-projects/spring-xd/pull/390  ",8
"Create project for model that is common between client and server
this would elminate dependencies that are currently in the codebase, such as:  * RESTModuleType and ModuleType enums * ModuleOption and DetailedModuleDefinitionResource.Option  ",5
"Consider a shared project for POJOs that are shareable betweed model and REST layer
nan",4
"Allow Aggregate Counter to use timestamp field in data.
Currently the aggregate counter aggerates by the current time. However the data may already have a timestamp in it (eg streams from activity events on a website).   It would be useful as an alternative approach to be able to specify this field to aggregate on.     This would have the following benefits:    1) The aggregate counts would be more accurate as they would reflect the acutal event times and not have any lag from an intermediate messgaging system they might have passed through.  2) If for whatever reason XD is down, comes back up and starts pulling queued messages from the messaging system the aggregate counter will reflect the correct event time. Currently you would get a gap and then a spike as a backlog of messages would get allocated to the current aggregate count.  3) Old data could be rerun through XD still creating the correct aggregate counts.    Configuration would be something like     stream create --name mytap --definition ""tap:mystream > aggregatecounter --name=mycount --timestampField=eventtime""    without the timestampfield it would behave as currently. ",5
"Extend aggregate counter to dynamically aggregate by field values in addition to time.
This would be a combination of the existing aggregate counter and field value counter functionality.  For example if the stream data was for car purchases some fields might be colour, make and model.  When analysing the aggregate data I dont just want to know how many were sold on Monday, but how many of each make or how many of each colour,  or how many of a particular colour, make AND model. This would allow a dashboard type client to 'drill down' into each dimension or combination of dimensions (in real time without executing batch queries against the raw data)  Ideally the aggregate counter would be specified as   stream create --name mytap --definition ""tap:mystream > aggregatecounter --name=mycount --fieldNames=colour,make,model""  The keys would be dynamically created according to the field values in each record (ie in a similar way to the field value counter you would not need to predefine field values) and keys would be created for all combinations of the fields specified eg the record   { ""colour"":""silver"" , ""make"":""VW"" , ""model"" : ""Golf"" }   would increment the following key counters (in addtion to the existing time buckets)  <existing base counter - ie all fields for this time bucket> colour:black make:VW model:Golf colour:black.make:VW colour:black.model:Golf make:VW.model:Golf colour:black.make:VW.model:Golf  ie the actual keys would look something like  aggregatecounters.mycount.make:VW.model:Golf.201307 etc  This may seem like it would generate a lot of key combinations but in practice the data generated will still be massively less than the raw data, and keys will only be created if that combination occurs in a time period.  Also some fields may be dependent on each other (such as make and model in the above example) so the amount of possibilites for those composite keys would be a lot less that the number of one times the number of the other.  ",5
"Create documentation for composed modules
nan",3
"Improve Module Options support
note from PR #365 - which has been merged - providing the initial level of support...    Pending issues (to be addressed in another PR?):  - [x] complex case  - [x] default values for complex case, when option is not surfaced back to the module (eg ""suffix"" in our canonical example)  - [ ] plugin provided options and values  - [ ] descriptive defaults instead of actual defaults (e.g. \<use stream name\>)  - [ ] JSR303 Validation    ",12
"Rename or reconsider the ""module display"" command
note from PR #365:    {quote}  We should probably create a new story to (at least) rename ""module display"" to ""module showconfig"" or something, but possibly even reconsider it altogether. In some sense, it's even violating the encapsulation of ModuleRegistry. It wont work for java-config style modules or spring-integration Groovy DSL modules. Personally, if anything, I'd rather see those config files themselves exposed as part of an admin UI. What you are doing with the options here fits better with the encapsulation principle and the fact that typical usage should not require detailed knowledge of the actual underlying configuration of a Module.  {quote}    ",1
"Enforce consistent naming across CLI options, and command/template/operations method names
e.g. see comment on PR #390:  https://github.com/spring-projects/spring-xd/pull/390/files#r7563787    In that case, it's ""delete"" in one place and ""destroy"" in another. There are other cases as well.",4
"Add ""http get"" command to shell
This will allow for some easy demonstration of how 'HATEOS' works via links in our REST API.  There are probably some quite useful commands here that could be used from the Spring Data REST shell longer term, but a good place to take a look at now.    Goal is to show how metrics such as counters are accessible w/o having to switch tabs to use wget.  The pretty printing of the returned JSON is an important feature to help understand the response, this functionality can be taken from/reused from the Spring Data REST shell",3
"Deployment tab on admin-ui lags behind with changes on other pages
The deployment tab does not reflect the current state of the jobs.  User must hit browser refresh button to make it work.",4
"Schedule button shows up but does not perform any task
I know that the feature is not ready, but we either should post a message to the user that the feature is not available or hide the button",5
"Empty parameter sent to job when launched from UI
Job gets a empty key:value pair when launching the job from the admin-ui.",5
"Colors on Job Definition tab are different than other tabs
The table background color on the Job Definition Tab is green while the others tabs have a white background.  They should be consistent.",1
"Remove unnecessary LESS files from XD UI styles
Currently, the bootstrap.less file has all the styles that the bootstrap supports. But we should only add/compile the LESS that are needed by XD UI.",2
"Batch Job's step execution count is always '0'
The batch job's step execution count is retrieved from org.springframework.batch.admin.web.JobExecutionInfo in batch job repository.    But, the JobExecutionInfo always have the stepExecutionCount set to '0'.",2
"Add support for Hortonworks Data Platform 2.0
(apologies if a ticket already exists for this, but I didn't see one)    I spun up the Hortonworks Data Platform 2.0 sandbox, but see it isn't supported by Spring XD yet.    How hard would it be to add these Distro's in?  Is it just a matter of dropping in a lib folder for hadoop22 and/or hdp20, and allowing those and options to be passed in via the --hadoopDistro option?    I'm currently trying to work through the following tutorial, but using the HDP 2.0 sandbox instead of the 1.3 sandbox    http://hortonworks.com/hadoop-tutorial/using-spring-xd-to-stream-tweets-to-hadoop-for-sentiment-analysis/    Thanks!",1
"Upgrade asciidoctor-gradle-plugin to 0.7.0
Looks like we need to spend a cycle on Asciidoc - as we still have the author-tag-issue - I thought we can simply upgrade the asciidoctor-gradle-plugin to 0.7.0 (currently 0.4.1) but that breaks the docs being generated.",2
"User would like to see request/response details (headers URLs etc) in shell
Add debug flag for logging globally maybe or else --trace per command?",1
"Fix mqtt module properties
Use of dot in property name prevents the user from specifying a value in stream definition  Also, defaults are repeated at .xml and .properties level",2
"Update router sink logic to match new channel syntax
for example, the following should work:    {code}  xd:>stream create a1 --definition ""queue:a > transform --expression=payload+'-a' | log""  Created new stream 'a1'    xd:>stream create b1 --definition ""queue:b > transform --expression=payload+'-b' | log""  Created new stream 'b1'    xd:>stream create s1 --definition ""http | router --expression=payload.contains('a')?'queue:a':'queue:b'""  Created new stream 's1'    xd:>http post --data ""ha""  > POST (text/plain;Charset=UTF-8) http://localhost:9000 ha  > 200 OK    // log shows: ""ha-a""    xd:>http post --data ""hi""  > POST (text/plain;Charset=UTF-8) http://localhost:9000 hi  > 200 OK    // log shows: ""ha-b""  {code}    This needs to be tested against all transports (local, redis, and rabbit)  ",4
"Extra ""job"" queues being created for all streams
This is most likely an issue for any transport, since it's probably happening within the JobPlugin, but I noticed when using Rabbit that every stream I created also triggered creation of ""job:[streamname]"" Queues.",2
"Topic channels are not broadcasting
This needs to work for all transports (local, rabbit, and redis), and we need to ensure that we have test coverage for each of those to avoid any regressions.    The incorrect behavior was observed with all three transports:    {code}  xd:>stream create a --definition ""topic:foo > transform --expression=payload+'-a' | log""  Created new stream 'a'    xd:>stream create b --definition ""topic:foo > transform --expression=payload+'-b' | log""  Created new stream 'b'    xd:>stream create s --definition ""http > topic:foo""  Created new stream 's'    xd:>http post --data hi  > POST (text/plain;Charset=UTF-8) http://localhost:9000 hi  > 200 OK    // only one line in log!  {code}  ",5
"Container with redis as transport emits stack trace on shutdown
This is ""harmless"" (the attempt to brpop continues even after the connection itself has been closed), but ugly:    {code}  12:45:15,084 ERROR redisInboundAdapter-redis:queue-inbound-channel-adapter1 inbound.RedisQueueMessageDrivenEndpoint:181 - Failed to execute listening task. Will attempt to resubmit in 5000 milliseconds.  org.springframework.data.redis.RedisSystemException: Redis exception; nested exception is com.lambdaworks.redis.RedisException: Connection closed    org.springframework.data.redis.connection.lettuce.LettuceExceptionConverter.convert(LettuceExceptionConverter.java:46)    org.springframework.data.redis.connection.lettuce.LettuceExceptionConverter.convert(LettuceExceptionConverter.java:36)    org.springframework.data.redis.connection.lettuce.LettuceConverters.toDataAccessException(LettuceConverters.java:159)    org.springframework.data.redis.connection.lettuce.LettuceConnection.convertLettuceAccessException(LettuceConnection.java:253)    org.springframework.data.redis.connection.lettuce.LettuceConnection.bRPop(LettuceConnection.java:1508)    org.springframework.data.redis.core.DefaultListOperations$12.inRedis(DefaultListOperations.java:163)    org.springframework.data.redis.core.AbstractOperations$ValueDeserializingRedisCallback.doInRedis(AbstractOperations.java:51)    org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:185)    org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:153)    org.springframework.data.redis.core.AbstractOperations.execute(AbstractOperations.java:86)    org.springframework.data.redis.core.DefaultListOperations.rightPop(DefaultListOperations.java:160)    org.springframework.data.redis.core.DefaultBoundListOperations.rightPop(DefaultBoundListOperations.java:105)    org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.popMessageAndSend(RedisQueueMessageDrivenEndpoint.java:178)    org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.access$300(RedisQueueMessageDrivenEndpoint.java:51)    org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint$ListenerTask.run(RedisQueueMessageDrivenEndpoint.java:286)    org.springframework.integration.util.ErrorHandlingTaskExecutor$1.run(ErrorHandlingTaskExecutor.java:52)    java.lang.Thread.run(Thread.java:724)  Caused by: com.lambdaworks.redis.RedisException: Connection closed    com.lambdaworks.redis.RedisAsyncConnection.await(RedisAsyncConnection.java:1079)    com.lambdaworks.redis.RedisConnection.await(RedisConnection.java:820)    com.lambdaworks.redis.RedisConnection.brpop(RedisConnection.java:101)    org.springframework.data.redis.connection.lettuce.LettuceConnection.bRPop(LettuceConnection.java:1506)  	... 12 more  {code}    We should shutdown the consumer gracefully (i.e. before the connection is closed).",3
"Remove existing 'purpose built' json processors and ensure all functionality is still available with #jsonPath based SpEL expression based processors
nan",2
"File to HDFS batch job fails due to ""/data"" directory not available in HDFS
The batch job for File to HDFS will try to check for the default '/data/' directory even if the target directory in HDFS is something else.  If the /data directory isn't there, the job will fail.    This should be fixed so there isn't a check on the directory that isn't the final HDFS target directory and the target directory should be created if it doesn't exist.",4
"Close HDFS file when Batch job ends
The File to HDFS batch job will not close the file being written to in HDFS when the job completes.  The ItemWriter for HDFS needs to incorporate functionality that is present in the standard FlatFileWriter, perhaps inheriting from AbstractItemStreamItemWriter",6
"Add bridge module
Add a bridge module per XD-956 to support definitions like topic:foo > queue:bar . Convenient for testing for XD-1066",1
"Change default container port from 9000 to something else
Change the default port since it conflicts with the default port for the http source",1
"Multiple SLF4J bindings on the classpath
Summary says it all. When starting, we now get {noformat} SLF4J: Class path contains multiple SLF4J bindings. SLF4J: Found binding in [jar:file:/Users/ebottard/.gradle/caches/artifacts-24/filestore/org.slf4j/slf4j-log4j12/1.7.5/jar/6edffc576ce104ec769d954618764f39f0f0f10d/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/Users/ebottard/.gradle/caches/artifacts-24/filestore/org.slf4j/slf4j-log4j12/1.6.1/jar/bd245d6746cdd4e6203e976e21d597a46f115802/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory] {noformat}  Most certainly introduced by 1c4817ee60ae9325f6394dcc78aa803c47818546 or 72baec92f2e7bbe860b4a9dc6c994536c1670881 ",2
"Restore XD Banner
Migrating to boot dropped the XD banner and its info.  Can be restored using eg a boot Initializer and removing the default boot banner",3
"Create an FTP tasklet to get remote files and put them in the local file system.
This is along the lines of what is in this blog post    http://coreyreil.wordpress.com/2012/12/21/spring-batch-creating-an-ftp-tasklet-to-get-remote-files/    Note that there have been some new developments in SI to get at the underlying stream for FTP.    The way to test this is to create a new batch job in XD that has this as it's tasklet.  Going forward the target file system will also be HDFS.",16
"Proof of Concept for module contributions based off Boot
Develop out a Proof of concept for review that allows users to easily create a new module to Spring XD. Key features to be enabled:    1. Allow existing modules to be included as a classpath dependency  2. Consolidate shared libraries in existing modules (e.g. a shared lib for all gemfire modules)  3. User develops a module as a normal Java project  4. User's project can then be added to a runtime container in the same way (as a dependency)    Nice to have:     1. at development time container can run continuously and pick up changes from user's module project.  2. Java config for a module.  3. command line functionality to create project scaffolding.    ",40
"Add ""spring-xd-exec"" directory to Git repo
Without the directory, Spring XD cannot be imported into STS using its own Gradle support.",1
"Small method name refactorings and add Javadoc in batch controllers
BatchJobsController.listForJob should be 'executionsForJob'  BatchJobsController.jobInstances should be 'instancesForJob'    The JavaDoc for the class and each method should be more descriptive about their functionality      ",1
"Make deploy=false as the default when creating a new job
The automatic deployment of the job makes it harder to understand the lifecycle of the job and also does not allow for the opportunity to define any additional deployment metadata for how that job runs, e.g is it partitioned etc.",1
"Create documentation on the lifcycle of a job in XD
This should go in the first section of the Batch Job documentation.    Here is a rough suggestion    The lifecycle of a job in XD.    1. Register a job module with the MoudleRegistry by putting a XML/jar files in the $XD_HOME/modules/jobs directory.  2. Create a job definition from a job module by providing a name and properties that apply to all job instances can be configured at this point.  The job is not yet deployed  3. Deploy the job to an xd-container.  A job instance can not be created by sending a message to a job queue that contains optional runtime job parameters  4. Launch a job by sending a message to the job queue with job parametres.  A Job Instance is created, representing a specific run of the job.  A Job Instance is the Job Definition plus the runtime job parameters.  You can query for the Job Instances associated with a given job name  5. The job is executed creating a Job Exection object that captures the success or failure of the job.  You can query for the Job Executions associated with a given job name.  6. Undeploy a job.  This removes the job from xd-container.",1
"Create REST API for getting information on a job execution for a given execution id
Adopted functionality from Spring Batch admin Should include springmvc test framework style tests  GET /batch/jobs/executions/{executionId} - Get information on all executions of a given job name.",2
"Create REST API for getting information on all steps of a given job execution
Adopted functionality from spring batch admin Should include springmvc test framework style tests  GET /batch/jobs/executions/{executionId}/steps - Get information on all steps of a given job execution",4
"Create REST API for getting information on a given step execution
Functionality adopted from spring batch admin Should include springmvc test framework style tests  GET /batch/jobs/executions/{executionId}/steps/{stepId} - Get information on a given step execution",4
"Create REST API for stopping a specific job
Functionality adopted from spring batch admin Should include springmvc test framework style tests  DELETE /batch/jobs/executions/{executionId} Stop a specific job",3
"Create REST API for stopping all job executions
Adopted functionality from spring batch admin  Should include SpringMVC test framework style tests.    DELETE /batch/jobs/executions/ - stop all job executions",3
"Create REST API for restarting a specific job instance
Functionality adopted from spring-batch admin  Should include springmvc test framework style tests    POST /batch/jobs/{jobName}/{jobInstanceId}/executions - restart a specific job instance    ",3
"Create REST API for getting information on the progress of a given step execution
There is a NPE in the current spring batch admin functionality.    Should be springmvc test framework style tests.    GET /batch/jobs/executions/{jobExecutionId}/steps/{stepExecutionId}/progress",4
"Create shell command for getting information on all job executions for a given name
nan",2
"Create shell command for restarting a specific job instance
nan",2
"Create shell command for getting information on the progress of a given step execution
nan",2
"Create shell command for getting information on all steps of a given job execution
nan",2
"Create shell command for getting information on a given step execution
nan",2
"Create shell command for stopping a specific job
nan",2
"Create a shell command for stopping all job executions
nan",2
"Make Batch Job controllers HATEOAS compliant
Currently, the BatchJobsController and BatchJobExecutionsController are not HATEOAS compliant and we need make them so.",3
"Redo Hadoop distribution dependency management
The way we now include various Hadoop distributions is cumbersome to maintain. Need a better way of managing and isolating these dependencies on a module level rather than container level.",8
"Integration tests for XD Installer
nan",8
"Controllers - Disallow negative pageSize values
Currently you can specify negative pageSize values - The controllers should validatate that. Right now an internal exception is being return, leaking internal details to the caller -   E.g.:   **http://localhost:9393/batch/executions?pageSize=-1** results in:  {code} <errors> <error logref=""BadSqlGrammarException""> <message> StatementCallback; bad SQL grammar [SELECT TOP -1 E.JOB_EXECUTION_ID, E.START_TIME, E.END_TIME, E.STATUS, E.EXIT_CODE, E.EXIT_MESSAGE, E.CREATE_TIME, E.LAST_UPDATED, E.VERSION, I.JOB_INSTANCE_ID, I.JOB_NAME FROM BATCH_JOB_EXECUTION E, BATCH_JOB_INSTANCE I WHERE E.JOB_INSTANCE_ID=I.JOB_INSTANCE_ID ORDER BY E.JOB_EXECUTION_ID DESC]; nested exception is java.sql.SQLSyntaxErrorException: user lacks privilege or object not found: TOP </message> </error> </errors> {code}  ",2
"Create aggregator module that uses redis as a message store
There is a need for a persistent store for messages so that they can survive the crash of a container node.  The redis implementation in SI is useful since users may already be using redis for other features such as analytics.    This module would sit alongside the current in-memory based aggregator.  It brings in redis as a dependency and there should be configuration options exposed so that one can use a different version of redis as compared to the one that maybe used for analytics.      Due to the need to have redis on the CP, and the large number of different options that each message store implementation provides and the different 3rd party library dependencies, I would like to avoid going down the path of using a profile here as it would seem to go beyond what we had discussed for profile support.  We can revisit as the module contribution story based on boot unfolds.",4
"Create aggregator module that uses an embedded database stored in the local filesystem
Similar to XD-1100.  SI has the jdbc based message store.    <int-jdbc:message-store id=""messageStore"" data-source=""dataSource""      table-prefix=""MY_INT_""/>    The configuration of this aggregator would be configured so that it uses an embedded database, hsqldb or H2 depending if there is any real perf benefit to one or the other, and store the data on the local file system.",8
"Create microbenchmark for performace of redis and jdbc based aggregators
would be good to have a general feel for the general performance of these two options.  Redis can run on the same node as the benchmark.",8
"JDBC sink is broken - looks like some config options got booted
The JDBC sink is broken. Simple ""time | jdbc"" results in:    org.springframework.jdbc.BadSqlGrammarException: PreparedStatementCallback; bad SQL grammar [insert into test (payload) values(?)]; nested exception is java.sql.SQLSyntaxErrorException: user lacks privilege or object not found: TEST    Looks like some config options got clobbered during bootification. ",5
"Create Shell Integration test fixture for jdbc related sink
Would be nice to have some kind of regression testing on the jdbc sink, as it becomes more prominent in XD.    Use of an in memory db where we expose eg a JdbcTemplate to assert state",5
"Add some test coverage to mqtt modules
Even though it may be hard to come up with a mqtt broker, an easy test that should be automated is    somesource | mqtt --topic=foo    with     mqtt --topics=foo | somesink      And asserting that what is emitted to somesource ends up in somesink.    ",3
"Create OOTB batch job that moved data from JDBC to HDFS
Basic inverse of current hdfsjdbc job.",6
"Create Mock on DistributedJobService instead of instantiating object
Currently, the tests use instantiated DistributedJobService object instead of a simple mock. We can just use mock object for the tests.",1
"Restore lax command line options
Restore --foo=bar as well as --foo bar    Validation of values should be done as a separate story",2
"Provide cmdline options validation
Wherever they come from (cmd line args or ENV_VARS), options such as transport, analytics, etc should be validated and issues should be reported to users",2
"Fix JobRepoTests to use different batch job repo
Currently, the JobRepoTests use the same batch job repository that the XD runtime uses. Since the batch job repo doesn't delete the job instances, there would be stale data from this test. ",2
"Clear Redis after tests
The following keys remain after running the test suite:  {code} redis 127.0.0.1:6379> keys * 1) ""containers"" 2) ""containers.application:9292"" {code}  ",2
"ApplicationContext ID generation
ApplicationContext ID generation is difficult in general.  Cloud Foundry solves this problem for us by providing unique instance ids to all running instances of an app. I don't suppose that helps much in the general case though, and we need something (a rule of thumb) that is unique and preferably deterministic, so that nodes retain their ID across process and connector restarts.  In Cloud Foundry the instance id plays a vital role (and will be automatically picked up by the app and applied - need to look at how that plays in a context hierarchy). User can set the context id manually using {{spring.application.name}} and {{spring.application.index}}.",2
"Investigate dropped Module Deployment Requests
We have observed in unit tests (see AbstractSingleNodeStreamIntegrationTests) that(Redis/SingleNode) occasionally fail. The root cause must be investigated further but there is some evidence to suggest that the control messages (ModuleDeploymentRequests) are not always received and handled by the ModuleDeployer. This does not produce an error but results in runtime stream failures. This problem may be resolved as part of the planned Deployment SPI but is being tracked here until we are certain that it has been resolved.",5
"We no longer validate the --hadoopDistro options in the xd scripts
We no longer validate the --hadoopDistro options in the xd scripts. Seem sthe classes doing this validation were removed for boot.  We do this validation in the xd-shell script",3
"Add documentation about message store to aggregator doco
nan",2
"Add bash based scripts of simple module create to src/main/scripts
nan",1
"Add starting of newly build XD server, running of smoketest bash script, and killing of XD server to CI test
nan",5
"Create scripts for batch DB creation
We need to create a shell script that calls the batch DB creation sql files for the JDBC option selected.  ",3
"Standardize Date/Time/TimeZone handling
We should we centrally standardize on date/time formats so that we don't create inconsistencies, and follow ISO 8601 internally. Internally we should only work with UTC (or make that the default config option).    Ultimately, whatever the user sees is just a formatting concern.",8
"Better error messages following XD-1109
2 forces at heand here: * Spring binding/validation itself * jsr303 (for eg @NotNull)",2
"Add jmxPort to list of coerced cmd line options
Following merge of XD-1109.    See discussion at https://github.com/spring-projects/spring-xd/commit/eaf886eab3b2ef07da55575029ccabb2c8a36af9#commitcomment-4701947",2
"Add profile activation on top of XD-953
nan",5
"Turn off RestTemplate logging in shell
Example:  {noformat}  xd:>module delete --name sink:file  12:31:19,495  WARN Spring Shell client.RestTemplate:566 - DELETE request for ""http://localhost:9393/modules/sink/file"" resulted in 500 (Internal Server Error); invoking error handler  Command failed org.springframework.xd.rest.client.impl.SpringXDException: Cannot delete non-composed module sink:file  {noformat}    The WARN log is redundant with the command result and should be silenced",1
"Provide ModuleOptionsMetadata using simple approach
Following merge of XD-953, provide module options using the ""simple"" approach where applicable",5
"Switch CAPITAL_LETTERS to system.property style in application config
nan",1
"Add support for deploying a batch job with partitioning across multiple XD nodes.
This is a very big story, needs some planning/discussion before starting work.  Should be able to be implemented as a plugin. ",20
"Disable JMX by default (and extend to other contexts than modules)
nan",1
"Enable --refresh-dependencies into be present when executing gradlew
This could be inside gradlew or a .settings file.",2
"Update the EC2 deployer 
EC2 Deployer   Needs to change its configuration behavior to use environment variables vs. the property files  * Remove ConfigureSystem class, since we will use environment variables instead  * Allow users to set environment variables via the xd-ec2 properties.  * If properties are not present use those that are available in the application.yml  * Utilize JClouds environment variable setup features to implement this behavior.",5
"Basic support for Plugin contributed Module Options Metadata
Pending a better approach and extension point (see XD-1050), provide a way to factor out common recurring options for modules:  * inputType  * outputType  * job parameters",4
"JMS Module - add support for TOPICS
As a Spring XD user I need to listen on a JMS Topic and ingest the messages, so I can process the messages.    Currently the module only allows for Queues",2
"Create test script that reproduces failure in writing to HDFS when undeploying and redeploying a stream
See https://github.com/spring-projects/spring-xd/pull/415#issuecomment-29024329    This test should be created so that it reproduces the failure and then turned off, so that when we switch to the new HDFS writing code    ",2
"Logging is not producing a log file
* need to add a -Dxd.home=$XD_HOME to the start scripts else all files will write to the /logs directory.  * Need to update .bat files to use % for env variables instead of $.  * Need to rename logger.config to logging.config so that boot will pick up the log config files.  * Admin needs to use xd-admin-logger configs instead of xd-container-logger  * Renamed logging file for singlenode from admin.log to singlenode.log",1
"Local control channels enabled in SingleNodeApplication when alternate transport is selected.
This causes intermittent test failures when testing streams with other transports since ModuleDeployer receives duplicate requests from multiple threads. Fix is to check for local transport before invoking setUpControlChannels()",1
"Failure in writing to HDFS when undeploying and redeploying a stream with numbers in directory and/or file name
nan",3
"Have the REST info about a module advertise enum properties
When a ModuleOption is backed by an enum, change the (currently String) type representation to be the possible values  ie java.lang.String -> String but traffic.Light -> Red | Green | Orange",3
"Review and Optimize the Serialized JSON Nodes for Batch Objects
For several Batch Job related JSON endpoints, we serialize too much information.",4
"Add TaskLet to Stream from (S)FTP to HDFS
nan",3
"Add (S)FTP Gateway and Batch Partitioner to List/Process Remote Files
nan",16
"Spike to model the cluster nodes.
Each node in the cluster advertises itself and the admin node listens to these ads and creates groups out of them.  The deployment of jobs and stream processing can then be deployed onto a specific group and specific nodes within the group.  The project https://github.com/spring-projects/spring-data-grid is the start of this model.",8
"Spike for matching algorithm for stream/job deployments and nodes
Base matching algorithm on the model used in http://research.cs.wisc.edu/htcondor/   Jobs/Stream - specify their requirements and preferences Nodes - specify their requirements and preferences  e.g.    A job/stream module ""requires"" a linux x85-64 platform and ""prefers"" the machine with the most free memory  A node ""requires"" that only only can run jobs when there is 25% or more free memory and it ""prefers"" to run stream modules over job modules.  The requirements and preferences are represented as SpEL expressions in a ""Advertisement"" data structure.  There is a 'require' expression and a 'preference' expression.   Matching occurs between Node Ads and Job/Stream Ads so that the requirements of both Ads evaluate to true and the matching nodes are ranked according to the preference expression.  The Job/Streams Ads can make use of well defined attributes about the nodes, such as it's memory/cpu usage.",10
"Spike for transformation of the parsed stream/job definition to a Physical Deployment Model taking into account a Deployment Manifest
The initial parsing of a stream/job definitions into a list of ModuleDeploymentRequests needs to be transformed into a Physical Deployment Model that takes into account     1) module co-location  2) partitioning  3) number of instances  4) node assignment    an potentially other data related to the runtime properties of a module (e.g. concurrency settings)     The an external DeploymentManifest will be used to capture this information.    A deployment of a stream or job will then need to optionally provide a reference to deployment manifest.",10
"Add --date option to the trigger module
The TriggerSourceOptionsMetadata class should be able to use an actual Date object, thanks to Spring binding conversion.  BUT, the ${date} construct will receive a toString version of it. Make sure this works properly",4
"With Partitioned Jobs, Wire Partitioner and StepExecutionHandlers with the MessageBus
nan",16
"Allow local data transport option for the container
Currently local is not a supported data transport for the container. It should be an option. Note that local is not valid for control transport for a standalone container. So we need to revisit the current semantics that default the control transport to be the same as the data transport, i.e. ,--transport=local should fail. ",2
"source jms --- connect to another jms provider
As a system administrator I need to connect to SonicMQ as jms provider    When setting up the correct spring xml file and added the correct jar files to the lib directory I received the following exception---      Question: is there a spot I should be defining the conversion strategy?    {code}    .   ____          _            __ _ _   /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \  ( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \   \\/  ___)| |_)| | | | | || (_| |  ) ) ) )    '  |____| .__|_| |_|_| |_\__, | / / / /   =========|_|==============|___/=/_/_/_/   :: Spring Boot ::             (v0.5.0.M6)    15:04:36,092 ERROR http-bio-9393-exec-1 rest.RestControllerAdvice:157 - Caught exception while handling a request  org.springframework.integration.MessageHandlingException: error occurred in message handler [moduleDeployer]    org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:79)    org.springframework.integration.config.ServiceActivatorFactoryBean$1.handleRequestMessage(ServiceActivatorFactoryBean.java:83)    org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:142)    org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:73)    org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:115)    org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:102)    org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)    org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:178)    org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:149)    org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:94)    org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:42)    org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:86)    org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendMessage(AbstractReplyProducingMessageHandler.java:228)    org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendReplyMessage(AbstractReplyProducingMessageHandler.java:212)    org.springframework.integration.handler.AbstractReplyProducingMessageHandler.produceReply(AbstractReplyProducingMessageHandler.java:177)    org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleResult(AbstractReplyProducingMessageHandler.java:171)    org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:149)    org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:73)    org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:115)    org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:102)    org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)    org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:178)    org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:149)    org.springframework.xd.dirt.stream.DeploymentMessageSender.sendDeploymentRequests(DeploymentMessageSender.java:57)    org.springframework.xd.dirt.stream.AbstractDeployer.sendDeploymentRequests(AbstractDeployer.java:137)    org.springframework.xd.dirt.stream.AbstractDeployer.basicDeploy(AbstractDeployer.java:157)    org.springframework.xd.dirt.stream.AbstractInstancePersistingDeployer.deploy(AbstractInstancePersistingDeployer.java:78)    org.springframework.xd.dirt.rest.XDController.save(XDController.java:242)    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)    java.lang.reflect.Method.invoke(Method.java:606)    org.springframework.web.method.support.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:214)    org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:132)    org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:104)    org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandleMethod(RequestMappingHandlerAdapter.java:748)    org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:689)    org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:83)    org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:947)    org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:878)    org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:946)    org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:848)    javax.servlet.http.HttpServlet.service(HttpServlet.java:647)    org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:822)    javax.servlet.http.HttpServlet.service(HttpServlet.java:728)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:305)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)    org.springframework.web.filter.HttpPutFormContentFilter.doFilterInternal(HttpPutFormContentFilter.java:88)    org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:108)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)    org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:222)    org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:123)    org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:472)    org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:171)    org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:99)    org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:118)    org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:407)    org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1004)    org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:589)    org.apache.tomcat.util.net.JIoEndpoint$SocketProcessor.run(JIoEndpoint.java:310)    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)    java.lang.Thread.run(Thread.java:744)  Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'connectionFactory' defined in file [/Users/dmarley/sandbox/spring-xd/build/dist/spring-xd/xd/modules/source/jms/config/../../../common/jms-sonic-infrastructure-context.xml]: Initialization of bean failed; nested exception is org.springframework.beans.ConversionNotSupportedException: Failed to convert property value of type 'progress.message.jclient.ConnectionFactory' to required type 'javax.jms.ConnectionFactory' for property 'targetConnectionFactory'; nested exception is java.lang.IllegalStateException: Cannot convert value of type [progress.message.jclient.ConnectionFactory] to required type [javax.jms.ConnectionFactory] for property 'targetConnectionFactory': no matching editors or conversion strategy found    org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:547)    org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:475)    org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:300)    org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:228)    org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:296)    org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:195)    org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:660)    org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:760)    org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:482)    org.springframework.boot.SpringApplication.refresh(SpringApplication.java:552)    org.springframework.boot.SpringApplication.run(SpringApplication.java:293)    org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:130)    org.springframework.xd.module.SimpleModule.initialize(SimpleModule.java:135)    org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:239)    org.springframework.xd.dirt.module.ModuleDeployer.deployModule(ModuleDeployer.java:229)    org.springframework.xd.dirt.module.ModuleDeployer.handleDeploy(ModuleDeployer.java:214)    org.springframework.xd.dirt.module.ModuleDeployer.handleDeploymentRequest(ModuleDeployer.java:196)    org.springframework.xd.dirt.module.ModuleDeployer.handleMessageInternal(ModuleDeployer.java:137)    org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:73)  	... 63 more  Caused by: org.springframework.beans.ConversionNotSupportedException: Failed to convert property value of type 'progress.message.jclient.ConnectionFactory' to required type 'javax.jms.ConnectionFactory' for property 'targetConnectionFactory'; nested exception is java.lang.IllegalStateException: Cannot convert value of type [progress.message.jclient.ConnectionFactory] to required type [javax.jms.ConnectionFactory] for property 'targetConnectionFactory': no matching editors or conversion strategy found    org.springframework.beans.BeanWrapperImpl.convertIfNecessary(BeanWrapperImpl.java:474)    org.springframework.beans.BeanWrapperImpl.convertForProperty(BeanWrapperImpl.java:505)    org.springframework.beans.BeanWrapperImpl.convertForProperty(BeanWrapperImpl.java:499)    org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.convertForProperty(AbstractAutowireCapableBeanFactory.java:1497)    org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyPropertyValues(AbstractAutowireCapableBeanFactory.java:1456)    org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1192)    org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:537)  	... 81 more  Caused by: java.lang.IllegalStateException: Cannot convert value of type [progress.message.jclient.ConnectionFactory] to required type [javax.jms.ConnectionFactory] for property 'targetConnectionFactory': no matching editors or conversion strategy found    org.springframework.beans.TypeConverterDelegate.convertIfNecessary(TypeConverterDelegate.java:267)    org.springframework.beans.BeanWrapperImpl.convertIfNecessary(BeanWrapperImpl.java:459)  	... 87 more    {code}",4
"Update docs for separate control and data transport
nan",1
"Apply Composite GoF pattern to ModuleDefinition
nan",8
"Step execution progress Shell command to use coherent Id (jobExecutionId:stepExecutionId)
Instead of using jobExecutionId and stepExecutionId as two separate options for the ""job execution step progress"" command, we can have a single option with id mentioned as (jobExecutionId:stepExecutionId)",2
"Fix Tail Source to Use Native Adapter by Default
Support all available options",3
"Update doc about trigger changes
Trigger has been changed to be one single modules, taking params (as opposed to 3 before)    Update doc at https://github.com/spring-projects/spring-xd/wiki/Batch-Jobs    Also, arguably, the module could be advertised as a source itself (it is only mentioned in the context of batch)",3
"The lib directory for hadoop12 contains mix of hadoop versions
This causes issues depending on which version of the core/common jar gets loaded first - like:    xd:>hadoop fs ls  -ls: Fatal internal error  java.lang.UnsupportedOperationException: Not implemented by the DistributedFileSystem FileSystem implementation  â€‚â€‚at org.apache.hadoop.fs.FileSystem.getScheme(FileSystem.java:213)  â€‚â€‚at org.apache.hadoop.fs.FileSystem.loadFileSystems(FileSystem.java:2401)  â€‚â€‚at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2411)  â€‚â€‚at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2428)  â€‚â€‚at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:88)  â€‚â€‚at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2467)  â€‚â€‚at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2449)  â€‚â€‚at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:367)  â€‚â€‚at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:166)  â€‚â€‚at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:351)  â€‚â€‚at org.apache.hadoop.fs.Path.getFileSystem(Path.java:287)  â€‚â€‚at org.apache.hadoop.fs.shell.PathData.expandAsGlob(PathData.java:325)  â€‚â€‚at org.apache.hadoop.fs.shell.Command.expandArgument(Command.java:224)  â€‚â€‚at org.apache.hadoop.fs.shell.Command.expandArguments(Command.java:207)  â€‚â€‚at org.apache.hadoop.fs.shell.Command.processRawArguments(Command.java:190)  â€‚â€‚at org.apache.hadoop.fs.shell.Command.run(Command.java:154)  â€‚â€‚at org.apache.hadoop.fs.FsShell.run(FsShell.java:255)  â€‚â€‚at org.springframework.xd.shell.hadoop.FsShellCommands.run(FsShellCommands.java:412)  â€‚â€‚at org.springframework.xd.shell.hadoop.FsShellCommands.runCommand(FsShellCommands.java:407)  â€‚â€‚at org.springframework.xd.shell.hadoop.FsShellCommands.ls(FsShellCommands.java:110)  â€‚â€‚at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  â€‚â€‚at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)  â€‚â€‚at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  â€‚â€‚at java.lang.reflect.Method.invoke(Method.java:606)  â€‚â€‚at org.springframework.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:191)  â€‚â€‚at org.springframework.shell.core.SimpleExecutionStrategy.invoke(SimpleExecutionStrategy.java:64)  â€‚â€‚at org.springframework.shell.core.SimpleExecutionStrategy.execute(SimpleExecutionStrategy.java:48)  â€‚â€‚at org.springframework.shell.core.AbstractShell.executeCommand(AbstractShell.java:127)  â€‚â€‚at org.springframework.shell.core.JLineShell.promptLoop(JLineShell.java:483)  â€‚â€‚at org.springframework.shell.core.JLineShell.run(JLineShell.java:157)  â€‚â€‚at java.lang.Thread.run(Thread.java:724)  ",3
"Refactor/Simplify JobPlugin
Currently, the JobPlugin extends AbstractPlugin and the AbstractPlugin has got lots of unused code (like it doesn't do anything with preProcessSharedContext()) in there. ",2
"Update docs for gemfire sink to include locator configuration
nan",1
"Create integration test script for JMS & MQTT
Create a script to sanity check JMS and MQTT",3
"Add a MongoDB Sink
This should be quite straightforward, since the Spring Data Mongo jars are already included. We have this working by just adding the attached sink context file and the spring-integration-mongodb jar.    (This works for JSON string streams, but a mongo converter probably needs added to support Tuple conversion)  ",5
"Standardize naming and unit for options across modules
We should standardize on the options between modules:  idleTimeout - timeout rolloverSize - rollover  Also, need to standardize on unit used for timeout - should this be s or ms? ",8
"Re-deployment of hdfs sink reuses filename of first deployment
Need to check for existing files with the same file counter",3
"Column option of JDBC sink should not convert underscore to property name.
Current implementation of column option of JDBC sink convert underscore to java property name. If database column contains underscore, there is no way to store data.  So JdbcMessagePayloadTransformer should not use JdbcUtils.convertUnderscoreNameToPropertyName even if column contains ""_"".",1
"Update Spring Framework dependency to 4.0 GA
nan",1
"Update Spring Integration version to 4.0.M2
nan",1
"Enhancements to Gemfire CQ Source
The Gemfire CQ source needs some enhancements:  * enable locator configuration * consider decoupling from JSON. Currently designed to work with gemfire-json-server to avoid dependence on specific domain objects on the client and server side. So produces json strings from PdxInstance(s) stored in the cache.  ",2
"Create Gemfire Integration Test Scripts
nan",3
"Mail Source ModuleOptions (+ profiles)
nan",2
"Tapping a stream with multiple labelled filters causes duplicate messages
Test case is here:  https://github.com/spring-projects/spring-xd/blob/master/src/test/scripts/basic_stream_tests#L49  We would expect one message in the counter, but get 3.",5
"Column name of JDBC sink module should not hard code to ""payload"".
Current implementation of JDBC sink module insert data into ""payload"" column. But I can't just change the default column ""payload"" to something else using --columns option. Because JdbcMessagePayloadTransformer compare the columnName to ""payload"" and it hard coded.   ",1
"Splunk module is broken
Splunk sink module doesn't work at all. It throws java.lang.VerifyError exception like following.  nested exception is java.lang.VerifyError: class org.springframework.integration.splunk.outbound.SplunkOutboundChannelAdapter overrides final method onInit.()V  This is because SplunkOutputChannelAdapter refers old spring integration jar, but recent AbstractReplyProducingMessageHandler (which SplunkOutputChannelAdapter extends) set final to onInit method. Hence it doesn't work.  SplunkOutboundChannelAdapter should be fixed to not override onInit method and replace the jar file spring-integration-splunk-1.0.0.M1.jar.",2
"Container (Launcher) id is not unique
XD container's id is set to use its application context id which is derived from:    ${vcap.application.name:${spring.application.name:${spring.config.name:application}}}:${vcap.application.instance_index:${spring.application.index:${server.port:${PORT:null}}}}    With the *default* values[the PORT is not set and embedded tomcat uses local port], the launcher id is set to ""application:0""    When I have multiple launchers then all the launchers have the same id as ""application:0"" which doesn't seem correct.    Do we need to use the Id that is generated at the XDContainer's constructor here?    public XDContainer() {  		this.id = UUID.randomUUID().toString();  	}    ",3
"Restore previous CmdLine library to populate options
Expected benefits are  --key<space>value as well as --key=value on the command line (eg XD-1108)    nice usage screen   ",4
"Duplicate messages on tap
If I fiddle with the testTappingWithLabels method I can reproduce the same issue:    HttpSource source = newHttpSource();    FileSink sink = newFileSink().binary(true); FileSink tapsink1 = newFileSink().binary(true); stream().create(""myhttp"", ""%s | flibble: transform  --expression=payload.toUpperCase() | flibble2: transform  --expression=payload.toUpperCase() | %s"", source, sink); stream().create(""mytap4"", ""tap:stream:myhttp.flibble > transform  --expression=payload.replaceAll('A','.') | %s"", tapsink1); source.ensureReady().postData(""Dracarys!"");    assertThat(sink, eventually(hasContentsThat(equalTo(""DRACARYS!""))));    assertThat(tapsink1, eventually(hasContentsThat(equalTo(""DR.C.RYS!""))));      java.lang.AssertionError:  Expected: ""DR.C.RYS!"", trying at most 10 times        but: failed after 10*100=1000ms:  ""DR.C.RYS!DR.C.RYS!"",  ",3
"Update HDFS sink documentation to reflect new functionality introduced in XD-990 and XD-991
nan",2
"Support external defaults for PojoModuleOptions
The current behavior when there are ""global"" external defaults to module options is to set them in the placeholder construct: ${foo:${the_default}} where ${the_default} is sourced from some properties file.  The downside is that the module options infrastructure is unaware of them.  Provide support for such defaults (at least when using PojoModuleOptions) in the form of  * Annotate a field (or the setter?) with @Value(""${the_default}"") * (maybe) Annotate the pojo with @PropertySource to indicate the location of the properties file * (maybe) come up with a general naming scheme for the properties file",5
"Update to spring-data-hadoop 2.0.0.M4
Update dependencies to spring-data-hadoop 2.0.0.M4",1
"Change module naming strategy in parser taking into account no duplicate labels and repeated modules
Labels should only be used help uniquely identify a module in a stream.  The stream definition a | x: b | x: c | d should throw an error since the label x is applied to two modules.  Also ambiguity exists with a stream definition such as   http | transform | transform | filter | hdfs  since a module with the name 'transform' will exist twice, meaning a tap on transform will result in message from both transform modules.  To make the naming unique and the usage with taps, the following naming strategy is proposed demonstrated by example.  http | transform | transform | filter | hdfs http, transform.0, transform.1, filter, hdfs  If a use tries to tap on transform, an error would be shown saying that  you could try to tap on transform.0 or transform.1 or alternatively use labels.  http | transform | filter | transform | hdfs http, transform.0, filter, transform.1, hdfs  http | x: transform | filter | transform | hdfs http, x, filter, transform, hdfs  ",4
"Make avro sink options consistent with hdfs and add docs
nan",3
"Nodes can not connect with Admin using Redis as transport
This has happened more than once, where a node fails for whatever reason, and when it is restarted, it does not receive requests from the admin server.  This could be file handle count based.  Since this is not Rabbit as a transport I'm not chasing this down yet.  But felt it needed to be recorded.",5
"Enabling of JMX support is broken
However this is triggered (depending on whether https://github.com/spring-projects/spring-xd/pull/477/files is merged yet or not), jmx seems to be broken because of duplicate beans / mbeans names",4
"Surface the provenance of a default to the user
When using @Value for providing a default value in a module options POJO, make it so that the REST API (and hence the module info command) advertises that   1) the expression was ${foo.something} 2) to the best extent possible (value may come from another property source), tell which config file it came from (introspecting the @PropertySource annotation)",3
"Update to spring-data-hadoop 2.0.0.M5
Update to spring-data-hadoop 2.0.0.M5 when it is released and remove the temporary DatasetTemplateAllowingNulls in spring-xd-hadoop  We should also review the supported hadoop distros - think we should support anything that is current/stable: - hadoop12 - hadoop22 - phd1 (PHD 1.1) - hdp13 - hdp20 - cdh4 ",3
"topic in mqtt source was marked as topics
Changed field back to topic",1
"Admin & Launcher startup fails when XD_JMX_ENABLED is set to true
When exporting of MBeans are enabled via XD_JMX_ENABLED (also, jmxEnabled as in application.yml), the Admin and Lancher server application fail to start.    Since the admin applications has the same 'integrationMbeanExporter' bean name for IntegrationMBeanExporter as that its ParentConfiguration, there is a naming conflicts and the exception thrown as:    (Same is the case for launcher and its parent configuration)    Exception in thread ""main"" org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'mbeanExporter' defined in class org.springframework.context.annotation.MBeanExportConfiguration: Invocation of init method failed; nested exception is org.springframework.jmx.export.UnableToRegisterMBeanException: Unable to register MBean [org.springframework.integration.monitor.IntegrationMBeanExporter@1de40d3e] with key 'integrationMBeanExporter'; nested exception is javax.management.InstanceAlreadyExistsException: org.springframework.integration.monitor:name=integrationMBeanExporter,type=IntegrationMBeanExporter    org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1553)    org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:539)    org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:475)    org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:304)    org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:228)    org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:300)    org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:195)    org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:700)    org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:760)    org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:482)    org.springframework.boot.context.embedded.EmbeddedWebApplicationContext.refresh(EmbeddedWebApplicationContext.java:124)    org.springframework.boot.SpringApplication.refresh(SpringApplication.java:609)    org.springframework.boot.SpringApplication.run(SpringApplication.java:321)    org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:130)    org.springframework.xd.dirt.server.AdminServerApplication.run(AdminServerApplication.java:60)    org.springframework.xd.dirt.server.AdminServerApplication.main(AdminServerApplication.java:42)  Caused by: org.springframework.jmx.export.UnableToRegisterMBeanException: Unable to register MBean [org.springframework.integration.monitor.IntegrationMBeanExporter@1de40d3e] with key 'integrationMBeanExporter'; nested exception is javax.management.InstanceAlreadyExistsException: org.springframework.integration.monitor:name=integrationMBeanExporter,type=IntegrationMBeanExporter    org.springframework.jmx.export.MBeanExporter.registerBeanNameOrInstance(MBeanExporter.java:610)    org.springframework.jmx.export.MBeanExporter.registerBeans(MBeanExporter.java:535)    org.springframework.jmx.export.MBeanExporter.afterPropertiesSet(MBeanExporter.java:417)    org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1612)    org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1549)  	... 15 more  Caused by: javax.management.InstanceAlreadyExistsException: org.springframework.integration.monitor:name=integrationMBeanExporter,type=IntegrationMBeanExporter    com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)    com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)    com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)    com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)    com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)    com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:513)    org.springframework.jmx.support.MBeanRegistrationSupport.doRegister(MBeanRegistrationSupport.java:195)    org.springframework.jmx.export.MBeanExporter.registerBeanInstance(MBeanExporter.java:663)    org.springframework.jmx.export.MBeanExporter.registerBeanNameOrInstance(MBeanExporter.java:600)  	... 19 more  ",2
"Add redisConnectionFactory with connection pool
We need to add a connection pool to the Redis connection factory used for the transport, otherwise we'll see exceptions like these:    12:57:54,842 ERROR inbound.tictoc.0-redis:queue-inbound-channel-adapter1 inbound.RedisQueueMessageDrivenEndpoint:183 - Failed to execute listening task. Will attempt to resubmit in 5000 milliseconds.  org.springframework.data.redis.RedisSystemException: Redis exception; nested exception is com.lambdaworks.redis.RedisException: Unable to connect  â€‚â€‚at org.springframework.data.redis.connection.lettuce.LettuceExceptionConverter.convert(LettuceExceptionConverter.java:46)  â€‚â€‚at org.springframework.data.redis.connection.lettuce.LettuceExceptionConverter.convert(LettuceExceptionConverter.java:36)  â€‚â€‚at org.springframework.data.redis.connection.lettuce.LettuceConverters.toDataAccessException(LettuceConverters.java:159)  â€‚â€‚at org.springframework.data.redis.connection.lettuce.LettuceConnection.convertLettuceAccessException(LettuceConnection.java:253)  â€‚â€‚at org.springframework.data.redis.connection.lettuce.LettuceConnection.bRPop(LettuceConnection.java:1508)  â€‚â€‚at org.springframework.data.redis.core.DefaultListOperations$12.inRedis(DefaultListOperations.java:163)  â€‚â€‚at org.springframework.data.redis.core.AbstractOperations$ValueDeserializingRedisCallback.doInRedis(AbstractOperations.java:51)  â€‚â€‚at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:185)  â€‚â€‚at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:153)  â€‚â€‚at org.springframework.data.redis.core.AbstractOperations.execute(AbstractOperations.java:86)  â€‚â€‚at org.springframework.data.redis.core.DefaultListOperations.rightPop(DefaultListOperations.java:160)  â€‚â€‚at org.springframework.data.redis.core.DefaultBoundListOperations.rightPop(DefaultBoundListOperations.java:105)  â€‚â€‚at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.popMessageAndSend(RedisQueueMessageDrivenEndpoint.java:178)  â€‚â€‚at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.access$300(RedisQueueMessageDrivenEndpoint.java:51)  â€‚â€‚at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint$ListenerTask.run(RedisQueueMessageDrivenEndpoint.java:291)  â€‚â€‚at java.lang.Thread.run(Thread.java:724)  Caused by: com.lambdaworks.redis.RedisException: Unable to connect  â€‚â€‚at com.lambdaworks.redis.RedisClient.connect(RedisClient.java:176)  â€‚â€‚at com.lambdaworks.redis.RedisClient.connectAsync(RedisClient.java:139)  â€‚â€‚at org.springframework.data.redis.connection.lettuce.LettuceConnection.getAsyncDedicatedConnection(LettuceConnection.java:2924)  â€‚â€‚at org.springframework.data.redis.connection.lettuce.LettuceConnection.getDedicatedConnection(LettuceConnection.java:2932)  â€‚â€‚at org.springframework.data.redis.connection.lettuce.LettuceConnection.bRPop(LettuceConnection.java:1506)  â€‚â€‚... 11 more  Caused by: java.net.BindException: Cannot assign requested address  â€‚â€‚at sun.nio.ch.Net.connect0(Native Method)  â€‚â€‚at sun.nio.ch.Net.connect(Net.java:465)  â€‚â€‚at sun.nio.ch.Net.connect(Net.java:457)  â€‚â€‚at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:639)  â€‚â€‚at org.jboss.netty.channel.socket.nio.NioClientSocketPipelineSink.connect(NioClientSocketPipelineSink.java:108)  â€‚â€‚at org.jboss.netty.channel.socket.nio.NioClientSocketPipelineSink.eventSunk(NioClientSocketPipelineSink.java:70)  â€‚â€‚at org.jboss.netty.channel.Channels.connect(Channels.java:634)  â€‚â€‚at org.jboss.netty.channel.AbstractChannel.connect(AbstractChannel.java:207)  â€‚â€‚at com.lambdaworks.redis.RedisClient.connect(RedisClient.java:165)  â€‚â€‚... 15 more  ",1
"Support use of application.yml fragments
Since spring boot (by default) looks for property sources from file:./config/application.yml and file:./application.yml    In XD bundles, the CLASSPATH of our XD startup scripts'(admin, container, singlenode startup scripts) set to use APP_HOME and APP_HOME/config.    But, if we have an application.yml fragment on $APP_HOME/config, then it is considered as classpath resource and the actual ""application.yml"" (from dirt lib/ classpath) is not loaded.     Also, we need to separate out the properties files we have inside $APP_HOME/config as by default, boot uses ""config"" directory as well.",3
"xd-admin server to --transport as an option.
--transport as an alias for --controlTransport",3
"Add ModuleOptions support for Rabbit sink
We need to add ModuleOptions support for Rabbit sink.",2
"Could not override rabbit sink module's rabbit connection factory properties
Currently RabbitMQ sink module's connection properties could not be overridden by ""${xd.config.home}/${configProperties:rabbit}.properties"" even if ""local-override"" is set to true.    It looks like the AmqpTemplate used by the AMQP outbound channel adapter doesn't use the connection factory defined in the sink module.",2
"Setup precedence order for module properties' property resolver
The PropertyResolver needs to follow the below precedence order on PropertySources when resolving the module properties:    From lowest to the highest order,    0 application.yml  1 applicaiton.yml fragment  2 property placeholders  2a  property placeholder under 'shared' config directory   2b property placeholder under module/(source/sink/processor)/config directory  3. environment variables  4. system properties  5. command line      ",5
"JDBC sink destroys existing table
The jdbc sink deletes existing table and creates a single column payload one even if properties file has 'initializeDatabase=false'",3
"Add documentation for JDBC to HDFS batch job
Add docs to section     https://github.com/spring-projects/spring-xd/wiki/Batch-Jobs#pre-packaged-batch-jobs",2
"Update to Spring Batch 2.2.4
nan",1
"CommandLine default values from container & admin options can not be overridden
If we have default values from Container/Admin options, then they can not be overridden by the system properties or system environment.     Currently, the only default we have for the Container/Admin options is for ""jmxEnabled"" option and since it is a boolean it can never be overridden by sys/env property XD_JMX_ENABLED.    I think we need to make sure there are no default values assigned for the non-boolean Container/Admin options and handle the boolean type option separately.",2
"Add paging and sorting to Field Value Counter API
see discussion at https://github.com/spring-projects/spring-xd/commit/2f0e80b5e337b71c9c70de510a44d2f050d10fa7",5
"Align filehdfs batch job defaults with those of corresponding hdfs sink
The batch jobs use different defaults compared to some of the sink/source modules.    filehdfs puts data in a /data directory with files named after the stream using a .log file extension. The hdfs sink puts files in an /xd/<streamname> directory using .txt as the default file extension.    filehdfs needs a more descriptive naming",5
"job launch doesn't do tab completion of --name
In the shell:  job launch doesn't do completion of --name, this is different behavior compared to job destroy  typing 'job destroy' and hitting tab completes with '--name'  typing 'job launch' and hitting tab does nothing",5
"Shell: Job Parameters are whitespace sensitive
This works:  {code} job launch helloSpringXD --params ""{""myStringParameter"":""foobar"",""secondParam"":""hello""}"" {code}  This fails:  {code} job launch helloSpringXD --params ""{""myStringParameter"":""foobar"", ""secondParam"":""hello""}"" {code}  Notice the space between the parameters. This fails with:  {code} spring> job launch helloSpringXD --params ""{""myStringParameter"":""foobar"", ""secondParam"":""hello""}"" 14/01/06 12:00:07 WARN client.RestTemplate: PUT request for ""http://localhost:9393/jobs/helloSpringXD/launch"" resulted in 500 (Internal Server Error); invoking error handler  Command failed org.springframework.xd.rest.client.impl.SpringXDException: org.springframework.integration.MessageHandlingException: java.lang.IllegalArgumentException: Unable to convert provided JSON to Map<String, Object>  {code}  Internally the parameters are parsed to:  {code} {""myStringParameter"":""foobar"",secondParam"":""hello""} {code}  The curly brace is missing.  ",4
"Fix filejdbc batch job
filejdbc throws an exception:   {code}  java.lang.IllegalArgumentException: Could not resolve resource location pattern [/mycsvdir/*.csv]: class path resource [mycsvdir/] cannot be resolved to URL because it does not exist  {code}    This can be solved by using a file:// prefix    Maybe just update the docs?  ",3
"Fix hdfsjdbc batch job
hdfsjdbc throws an exception:  {code}  org.springframework.beans.factory.BeanDefinitionStoreException: Invalid bean definition with name 'itemReader' defined in URL [file:/Users/trisberg/Projects/spring-xd/build/dist/spring-xd/xd/modules/job/hdfsjdbc/config/hdfsjdbc.xml]: Could not resolve placeholder 'columns' in string value ""${columns}""  {code}    The hdfsjdbc job uses 'columns' instead of 'names' as the parameter for the column-names. Should we make this usage consistent between jobs?    There is a comment in the docs - ""there is also a limitation in that the database table must be created manually. This is due to a bug in Spring Hadoop and will be fixed in the future."" Think this is this solved in Spring Hadoop now?    initializeDatabase should default to false now to be consistent with jdbc sink    Rename batch-jdbc/mongo-import.properties to batch-jdbc/mongo.properties since these aren't just for import",5
"Create custom help command for xd-shell so that hadoopDistro option is listed
xd-shell is using the default spring-shell help command.  Need to create a help command specific to XD so that it can list the --hadoopDistro command line option.  Note, the hadoopDistro command line option is actually processed by the xd-shell bash script",2
"Update build script to use correct version of spring-data-hadoop based on distro
nan",1
"JobPlugin - Use containsKey when checking for Parameters
Use containsKey when checking for Parameters Add tests",2
"Update jdbchdfs properties and defaults to better match hdfs sink  
We should use fileName, fileExtension properties and default to /xd/jobname as directory",3
"Add test for filejdbc to test scripts
The filejdbc jobs isn't included in the test scripts",3
"Garbage in Job Repo Causes List Failure
For some reason, the partitioned batch jobs are storing  {{StepExecutions}}s in the DB with a null ID.  This causes {{JobCommandTests.testListStepExecutionsForSpecificJobExecution()}} to fail because the HATEOS code asserts not null...  {code} 18:25:26,185 ERROR http-nio-9393-exec-3 rest.RestControllerAdvice:186 - Caught exception while handling a request java.lang.IllegalArgumentException: [Assertion failed] - this argument is required; it must not be null   org.springframework.util.Assert.notNull(Assert.java:112)   org.springframework.util.Assert.notNull(Assert.java:123)   org.springframework.hateoas.mvc.ResourceAssemblerSupport.createResourceWithId(ResourceAssemblerSupport.java:87)   org.springframework.xd.dirt.rest.StepExecutionInfoResourceAssembler.toResource(StepExecutionInfoResourceAssembler.java:39)   org.springframework.xd.dirt.rest.BatchStepExecutionsController.list(BatchStepExecutionsController.java:107) {code}  For some reason, the query for objects with the {{jobExecutionId}} also returned these two objects with null keys.  Blowing away my data directory fixes the problem (until I run another partitioned batch job).  I need to figure out why spring batch is creating these bad records, but we should probably add some defensive code to protect against null IDs.  You can reproduce by building against my XD-1146 branch. ",1
"Job Plugin - Notification Channel not correctly bound to MessageBus
nan",4
"Create Module options metadata for OOTB jobs
nan",5
"Add support for XD_CONFIG environment variable in windows shell scripts
This was added in bash scripts as part of XD-1186.",4
"Shell - 'makeUnique' Job Parameter is true by default 
Currently the shell assumes that the 'makeUnique'job parameter is by default *false*. That is not true. As a consequence the parameter has currently no impact/does not work.",2
"Update hadoop instructions in the xd-samples
batchHashtagCount and batchWordCount  projects need ""hadoop fs ls"" instructions need to be updated.  ",1
"Boot updates post 0.5 M7
Some boot classes we compile against have changed or been replaced.",3
"Allow conditional validation for module options
Using JSR303 groups, which should be derived from injected values",4
"Make job names unique across tests that use the same JobRepository
The job names used by the tests should be unique across the tests when use the same JobRepository.    ",3
"Distributed JobLocator should return a valid job
In distributed batch processing in XD, the JobLocator implementation for getJob(String jobName) should return a valid Job (FlowJob/SimpleJob).    Since we won't we relying on the MapJobRegistry's joblocator implementation which doesn't work in distributed use case, we need to have an appropriate way to return FlowJob/SimpleJob using XD's BatchJobLocator.",5
"Serialization of ChunkContext fails using Kryo
nan",2
"twittersearch and twitterstream should support compatible formats
Currently twitterstream emits native twitter json whereas twittersearch uses SI/Spring Social and emits spring social Tweet types. This makes it difficult to replace twitter sources and reuse XD stream definitions.  This requires coordination with SS 1.1.0 and SI 4.0 GA releases. NOTE: I think it's a good idea to continue to support native twitter JSON, keep as an option for twitterstream, but the default should be Tweet types.",5
"Base server implementation on Spring Boot
Server startup is done using Spring  Boot as well as starting a module application context at runtime.",20
"Release 1.0.0.M5
nan",8
"Batch jobs should use application.yml provided connection as default
Batch jobs should use application.yml provided connection as default. They now have their own configuration in batch-jdbc.properties. This config needs to account for any changes made to application.yml settings so the data is written to the batch metadata database by default.",5
"XD modules should not have 'build' directory upon running gradle build
Currently, the 'modules' project is marked as java project to enable eclipse/idea metadata files generation. But it generates a 'build' directory with a jar that has empty MANIFEST file.    This build directory also gets copied into the bundle after 'dist'.",2
"Duplicate MBean server definition by MBeanExportingPlugin
The MBeanServer is referred by XD admin/launcher when JMX is enabled (by setting --jmxEnabled option) and this is defined in xd-global-beans.xml.    The MBeans that are exposed by the modules also use the same MBeanServer define above and there is a duplicate MBeanServer definition (from jmx/common.xml) which the MBeanExportingPlugin adds as a component to the module which doesn't seem to be needed.    Also, currently the flag ""jmxEnabled"" is generic and used by adminserver/launcher as well as the module MBeanExportingPlugin. If we have a separate flag to enable the JMX *only* for modules then, a separate definition of MBeanServer could be necessary.  ",1
"Unit/Integration tests need appropriate cleanup of test data during teardown
Currently, there are test data(e.g: stream name) not being cleaned up during teardown especially when there is a test case failure. This breaks the test suite when the same test data is used by other tests.    We need to move the cleanup logic at an appropriate level so that the test data is always cleaned up irrespective of the test result.",5
"Add code coverage to gradle build
Build should be able to generate code coverage reports.    After a quick tour of the intertubes it seems that JaCoCo is a well maintained project and has first class support inside gradle.    http://www.gradle.org/docs/current/userguide/jacoco_plugin.html  ",3
"Integrate code coverage reports into the CI process
Not sure if this is best done via Sonar our sonar build plan, the nightly one, or the frequent one off master...    Open question is if we want to fail a build do to code coverage levels.",4
"Review abstract base classes used in test cases to ensure proper resource cleanup.
nan",6
"Investigate long running tests and create refactoring issues
https://sonar.springsource.org/dashboard/index/7173?did=3    Shows which of our current tests take the most time to execute.    xd.dirt.stream and xd.shell.command are where the most time is spent.    In xd.dirt.stream it seems likely that time can be reduced by not restarting a new single node server per test, but sharing it across tests, e.g.     RabbitSingleNodeStreamDeploymentIntegrationTests 	   LocalSingleNodeStreamDeploymentIntegrationTests   RedisSingleNodeStreamDeploymentIntegrationTests 	    As a first pass, the test that take longer than 15 seconds in that report should be investigated.  ",6
"Provide a easy, prescriptive means to perform unit and basic stream integration tests.
AbstractSingleNodeStreamDeploymentIntegrationTests is the basis of 'state of the art' testing for a stream that allows you to get a reference to the input and output channel of the stream    http | filter | transform | file.    One can send messages to the channel after the http module, but before filter and one can retrieve the messages that were sent to the channel after the transform module but before file.    The current implementation inside AbstractSingleNodeStreamDeploymentIntegrationTests can be improved in terms of ease of use for end-users.      The issue is to create as simple a way as possible for a user to test their processing modules/stream definitions without having to actually do a real integration test by sending data to the input module.    Either as a separate issue or as part of this one, the documentation     https://github.com/spring-projects/spring-xd/wiki/Creating-a-Processor-Module    should be updated to explicitly show how to use this issue's test functionality.  ",8
"Create build infrastructure for Angular based UI
Create a project that includes build and test automation for a new Angular based UI.  This work is independent of calling the UI build step from gradle.  A super minimal UI, just to have some basic code, is all that is required.     This should use     Grunt  Jasmine  Karma  Bower  YUIdoc (separate story?)    UI Components from backbone should be available in the base project.    ",8
"Integrate grunt based UI build into the XD's gradle build
Blog post http://naleid.com/blog/2013/01/24/calling-gruntjs-tasks-from-gradle/  seems to be the definitive reference....",5
"Investigate if we should use RequreJS with Angular
nan",6
"Create EC2 AMI for single-node install of Apache Hadoop 1.2.1
nan",4
"Create EC2 AMI for single-node install of Apache Hadoop 2.2.0
nan",3
"Create EC2 AMI for single-node install of  Pivotal HD 1.1
nan",8
"Create EC2 AMI for single-node install of Cloudera CDH 4.5.0
nan",5
"Create EC2 AMI for single-node install of Hortonworks Data Platform 1.3
nan",5
"Add option to stop all running XD EC2 instances that match a given naming pattern
This functionality should be added as a command line option to the main app in the spring-xd-ec2 project",4
"Add to Acceptance Test EC2 CI build plan a stage that stops any existing CI EC2 deployments
Add a stage to the plan that will stop any instances that the CI process may have started before and relaunch a 2 node install based on rabbit.    https://build.springsource.org/browse/XD-ATEC2 was created as an empty shell.    The running of across different transports will be handled in a separate story along with adding a stage to run a 'hello world' acceptance test.    ",8
"Add stage to Acceptance Test EC2 build plan that runs a basic acceptance test application against the single-node deployment
Run test application developed in XD-1245    ",2
"Add to Acceptance Test EC2 CI build plan a stage that uses XD distributed mode with rabbit
See https://quickstart.atlassian.com/download/bamboo/get-started/bamboo-elements    ""Stages are comprised of one or more Jobs, which run in parallel""    we would like the tests across the rabbit and redis transport to occur in parallel.  ",8
"Add to Acceptance Test EC2 job a stage that uses XD distributed mode with redis
See https://quickstart.atlassian.com/download/bamboo/get-started/bamboo-elements    ""Stages are comprised of one or more Jobs, which run in parallel""    we would like the tests across the rabbit and redis transport to occur in parallel.",8
"Optimize deployment of xd-admin and multiple nodes in spring-xd-ec2 to occur in parallel.
The deployment of nodes is sequential, we can reduce the time to deploy a cluster greatly by having these tasks execute in parallel.",6
"Create App for CI to Shutdown XD EC2 Cluster
Application will shutdown all servers with a specific name.    Application will take a --cluster-name parameter.     Generate artifact to state success or failure  ",5
"Create CI Plan for XD EC2 deployment
* Use the cleanup app from XD-1243 to stop previous CI runs of XD on EC2.  * Build XD-EC2 deployer from github.  * Use XD-EC2 Deployer to deploy the CI XD-Instance  * Should produce artifact that contains the URL      * admin server of the XD cluster.     * container servers of the XD cluster",8
"Develop basic acceptance test application to exercise based XD-EC2 deployment from CI
Create a first pass at an acceptance test app for a stream definition of http | log.      This will involve creating two new projects in xd    1. spring-xd-integration-test  2. spring-xd-acceptance-tests    #1 will contain generally useful utility methods for acceptance test, such as sending data over http, obtaining and asserting JMX values of specific modules.  #2 will contain tests that use #1 to test the various out of the box modules provides in XD.",5
"To be able to run the tests without conflicting with an existing XD admin server/launcher
In line with what we address at https://jira.springsource.org/browse/XD-1223, there are cases where tests fail when there is an existing instance of hsqldb running.    Since hsqldb uses the same port and database, it causes issues.",3
"Remove RedisStreamDeploymentIntegrationTests and RabbitStreamDeploymentIntegrationTests
These are duplicates of *SingleNodeDeploymentIntegrationTests",1
"Merge AbstractStreamDeploymentIntegrationTests and AbstractSingleNodeStreamDeploymentIntegrationTests
We do not need two base classes for this",2
"AbstractShellIntegrationTests should start and stop server once.
e.g., make application static and check for initialization. Need to ensure each test restores the initial state of the server",5
"Investigate RabbitSingleNodeStreamDeploymentIntegrationTests performance
This test is very slow (2x the Redis version). Lots of stacktraces when running. Could be related. ",5
"Access-Control-Allow-Origin header should not be hard-coded
See also XD-451 as reference.",4
"Allow processor script variables to be passed as module parameters
Currently, if we want to bind values to script variables we need to put them in a properties file like so:  xd:> stream create --name groovyprocessortest --definition ""http --port=9006 | script --location=custom-processor.groovy --properties-location=custom-processor.properties | log  Ideally it should be:  xd:> stream create --name groovyprocessortest --definition ""http --port=9006 | script --location=custom-processor.groovy --foo=bar --baz=boo | log   ",5
"RabbitMessageBus queue name prefix
I see the topics/exchanges created for Redis/Rabbit message buses have the prefix ""topic."" Is there any reason why we didn't have the prefix ""queue."" for the name of the queue created in Rabbit message bus in compared with the queue created in Redis message bus which has prefix ""queue.""  ",1
"Optimize AbstractSingleNodeStreamDeploymentIntegrationTests
XD singlenode currenly initialized in @Before, should be @BeforeClass. In this case must be re-initialized for each transport, but not for each @Test.",2
"Create assertion to get count of messages processed by a specific module in a stream
The modules are exposed via JMX and in turn exposed over http via jolokia.  See https://jira.springsource.org/browse/XD-343.  This issue is to develop a helper method that given a stream id and/or module name, assert that the number of messages processed after sending stimulus messages is as expected. e.g.  int originalCount = getCount(""testStream"", ""file"");  //do stuff that generates 100 messages assertCount(""testStream"", ""file"", 100, originalCount)  For now we can assume we know the location of where the modules are located by assuming we have only one container deployed.",5
"Running XD as service
It is useful to configure operating system so that it will start Spring XD automatically on boot.    For example, in Linux it would be great if Spring XD distro contains init.d script to run it as service. A typical init.d script gets executed with arguments such as ""start"", ""stop"", ""restart"", ""pause"", etc. In order for an init.d script to be started or stopped by init during startup and shutdown, the script needs to handle at least ""start"" and ""stop"" arguments.  ",2
"Support default values for options derived out of ${} placeholders
The logic can be found in DefaultModuleOptionsMetadataCollector but caused problems in the initial PR. Revisit if needed",4
"Provide a clean way to get a reference to the MessageBus running in SingleNodeApplication
Currently the message bus is only obtained via Module.getComponent(MessageBus.class). Stream testing scenarios that depend on sending and receiving payloads via named channels do not require a deployed module instance per se, but any stream flow control uses the MessageBus directly. Getting a deployed module instance in general is expensive, e.g., you have to wait for the module to deploy asynchronously, whereas the MessageBus implementation could be known a priori when the application starts. An improvement would be to ask the container for its MessageBus.",3
"Copy latest build to S3 after a XD Build
nan",5
"Update SI to latest 4.0 M3 and Spring AMQP to 1.3.0.M2
The Rabbit endpoint suffers from a problem similar to XD-1067.  Seems like spring-[rabbit/amqp] needs to be bumped to 1.3.0.M1 to fix it.  Sadly, we get this error:  noformat  java.lang.NoSuchMethodError: org.springframework.amqp.core.MessageProperties.getContentLength()J  at org.springframework.integration.amqp.support.DefaultAmqpHeaderMapper.extractStandardHeaders(DefaultAmqpHeaderMapper.java:102)  at org.springframework.integration.amqp.support.DefaultAmqpHeaderMapper.extractStandardHeaders(DefaultAmqpHeaderMapper.java:53)  at org.springframework.integration.mapping.AbstractHeaderMapper.toHeaders(AbstractHeaderMapper.java:205)  at org.springframework.integration.mapping.AbstractHeaderMapper.toHeadersFromRequest(AbstractHeaderMapper.java:148)  at org.springframework.integration.amqp.inbound.AmqpInboundChannelAdapter$1.onMessage(AmqpInboundChannelAdapter.java:75)  at org.springframework.amqp.rabbit.listener.AbstractMessageListenerContainer.doInvokeListener(AbstractMessageListenerContainer.java:584)  at org.springframework.amqp.rabbit.listener.AbstractMessageListenerContainer.invokeListener(AbstractMessageListenerContainer.java:482)  at org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer.access$001(SimpleMessageListenerContainer.java:69)  at org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer$1.invokeListener(SimpleMessageListenerContainer.java:144)  at org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer.invokeListener(SimpleMessageListenerContainer.java:920)  at org.springframework.amqp.rabbit.listener.AbstractMessageListenerContainer.executeListener(AbstractMessageListenerContainer.java:454)  at org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer.doReceiveAndExecute(SimpleMessageListenerContainer.java:728)  at org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer.receiveAndExecute(SimpleMessageListenerContainer.java:712)  at org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer.access$400(SimpleMessageListenerContainer.java:69)  at org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer$AsyncMessageProcessingConsumer.run(SimpleMessageListenerContainer.java:812)  at java.lang.Thread.run(Thread.java:724)  noformat  Updating to latest SI snapshot does not help (as of Jan 23rd)",4
"Surface better exception information to client
In trying to upgrade to latest SI, I encountered a failing test because it expects an error message to contain something, but SI changes make it disappear (the problem is an SI exception now has an explicit message and so does not expose its cause message anymore)  This, however is the manifest of a deeper ""problem"". We currently expose the getMessage() of any generic Exception caught, in the VndError REST construct. But this is not enough.  Things that we can consider are: 1) adding the whole stacktrace of the caught exception, as a String. This is not very good at it leaks java specific details 2) unwrap the caught exception to get to the deepest cause. This may not be what we want everytime 3) construct a VndErrors (note the 's') made of each layered exception 4) similar to 1), but not using the stacktrace, only the messages of each cause  etc ",7
"Twitterstream is broken
nan",3
"Remove unused code related to 'accepted media type' in MessageBus
nan",4
"Investigate and remove explicit thread sleep in AbstractSingleNodeStreamDeploymentIntegrationTests
There are explicit Thread.sleep() calls after deploy() in some of the test methods in  AbstractSingleNodeStreamDeploymentIntegrationTests.  Also, the test method deployAndUndeploy() doesn't have explicit Thread.sleep() and fails inconsistently with this error:  java.lang.AssertionError: stream test0 not undeployed    org.junit.Assert.fail(Assert.java:88)   org.junit.Assert.assertTrue(Assert.java:41)   org.springframework.xd.dirt.stream.AbstractSingleNodeStreamDeploymentIntegrationTests.waitForUndeploy(AbstractSingleNodeStreamDeploymentIntegrationTests.java:332)   org.springframework.xd.dirt.stream.AbstractSingleNodeStreamDeploymentIntegrationTests.deployAndUndeploy(AbstractSingleNodeStreamDeploymentIntegrationTests.java:221)   sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)   sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)   sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)   java.lang.reflect.Method.invoke(Method.java:601)   org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)   org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)   org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)   org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)   org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)   org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)   org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)   org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)   org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)   org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)   org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)   org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)   org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)   org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)   org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)   org.springframework.xd.test.AbstractExternalResourceTestSupport$1.evaluate(AbstractExternalResourceTestSupport.java:71)   org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)   org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)   org.junit.rules.RunRules.evaluate(RunRules.java:20)   org.junit.runners.ParentRunner.run(ParentRunner.java:309)   org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecuter.runTestClass(JUnitTestClassExecuter.java:86)   org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecuter.execute(JUnitTestClassExecuter.java:49)   org.gradle.api.internal.tasks.testing.junit.JUnitTestClassProcessor.processTestClass(JUnitTestClassProcessor.java:69)   org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.processTestClass(SuiteTestClassProcessor.java:50)   sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)   sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)   sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)   java.lang.reflect.Method.invoke(Method.java:601)   org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35)   org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)   org.gradle.messaging.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32)   org.gradle.messaging.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93)   com.sun.proxy.$Proxy2.processTestClass(Unknown Source)   org.gradle.api.internal.tasks.testing.worker.TestWorker.processTestClass(TestWorker.java:103)   sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)   sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)   sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)   java.lang.reflect.Method.invoke(Method.java:601)   org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35)   org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)   org.gradle.messaging.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:355)   org.gradle.internal.concurrent.DefaultExecutorFactory$StoppableExecutorImpl$1.run(DefaultExecutorFactory.java:64)   java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)   java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)   java.lang.Thread.run(Thread.java:722)",3
"Add states to the deployment of stream
Improve how the state of the stream is managed.    A deploy command moves the stream from the undeployed state to the deploying state. If all modules in the stream are successfully deployed, the stream state is â€˜deployedâ€™ If one or more module deployments failed, the stream state is failed.  Any modules that were successfully deployed, are still running.      Sending an undeploy command will stop all modules of the stream and return the stream to the undeployed state.    For the individual modules that failed, we will be able to find out which ones failed.  Not yet sure if we can try to redeploy just those parts of the stream that failed.    See the [design doc|https://docs.google.com/a/gopivotal.com/document/d/1kWtoH_xEF1wMklzQ8AZaiuhBZWIlpCDi8G9_hAP8Fgc/edit#heading=h.2rk74f16ow4i] for more details.    Story points for this issue are the total of all the story points for the subtasks.",20
"Investigate missing boot's actuator endpoints in XD
Currently, few of the boot's actuator endpoints go missing in the EndpointHandler mapping.  They are: BeansEndpoint, dumpEndpoint, traceEndpoint, healthEndpoint, infoEndpoint.    Also, the EndpointHandler mapping doesn't even happen in case of LauncherApplication.  I think this is because the LauncherApplication context starts with port '0' and the TomcatEmbeddedServletContainer sets the local port for it later. With port '0', the Endpointhandler mapping is disabled during the EndpointHandler mapping bean creation.",3
"Remove job options that are handled at module level from shell
Since makeUnique, dateFormat and numberFormat now have their own module options now, they are nicely advertised as options to a job (and will benefit from code completion soon), so they can be removed from the shell (where they currently allow for a misconfiguration if set at both the job and shell level)",4
"The use of labelled modules and taps needs more explanation
https://github.com/spring-projects/spring-xd/wiki/Taps mentions this but the explanation needs more elaboration and example, e.g.   mystream ->   ""http | flibble: transform --expression=payload.toUpperCase() | file""    ""tap:stream:mystream.flibble > transform --expression=payload.replaceAll('A','.') | log"");",1
"spring-integration-hadoop.xsd (and reactor) Imports the SI 3.0 Instead of SI 4.0 Schema
nan",1
"Out of the box batch jobs should add xdJobExecutionListener and xdStepExecutionListener
To show best practice, our batch jobs should include these listeners so that notifications can be sent.  In particular it is desirable that jobs with file item readers can send the files that were processed so another stream can be sent the file name when the job completes to move/delete the file",8
"Default option values broken for composed modules
May want to fix ""properly"" when tackling the module options for composed modules, but this is currently broken (and wasn't at some point):    module compose upperHttp --definition ""http | transform --expression=payload.toUpperCase()""  stream create foo --definition ""upperHttp | log""    This will fail saying that ${port} can't be resolved    This will work though:  module compose upperHttp --definition ""http --port=9000 | transform --expression=payload.toUpperCase()""  stream create foo --definition ""upperHttp | log""      Note that   stream create foo --definition ""upperHttp --port=xxx | log"" should work too wut won't, but that's another bug (will create after this one)",4
"Rename avro sink to hdfs-dataset and add support for parquet format
The kite SDK is used to write Avro records in a Kite specific format and also it support the parquet format for which we will eventually support.",2
"The HDFS Sink should roll over based on the number of events.
nan",4
"JMX endpoints not functioning 
I cannot access the JMX/Jolokia endpoints using the Spring Boot RC1 and Snapshot.  Works with XD-M5 and Boot-M7",3
"Create Better UI instead of Boot's default ""Whitelabel Error Page""
Create a better UI instead of Boot's default ""Whitelabel Error Page""",6
"Add caching to ModuleOptionsMetadataResolver
Will likely involve having the module identity (type+name) be part of the OptionsMetadata identity/cache key",5
"Allow for late-binding of module options defaults
This is about computing the value to support expressions such as ${xd.stream.name} as a default.  Initial discussion suggested to leverage the work done in XD-1175 by having a custom @LateValue (or @DeployTimeValue, etc) be resolved at deployment time",4
"Support shell completions for closed set of values in stream definitions
When doing dsl completion in a stream definition and a module option accepts a value that has a closed set of possible values (eg booleans, enums), we can provide completions for those.  ",8
"Support shell completions for module names
see CompletionProviderTests#testUnfinishedModuleNameShouldReturnCompletions()    Ideally, would require a change in the parser so that it knows which kind of module was expected when it failed.",4
"Use dedicated modules for code completion fingerprinting
see discussion at https://github.com/spring-projects/spring-xd/pull/495#discussion-diff-9291037",6
"Tap definitions should verify stream name
xd:>stream create --name simple --definition ""http | transform | filter | transform | file"" Created new stream 'simple' xd:>stream create --name tapSimple --definition ""tap:stream:mystream.transform > file"" Created new stream 'tapSimple'  There isn't a stream named ""mystream""... I don't remember if we want to allow for this (set up taps before there is a stream) or if it should be an error.  Otherwise, works as expected  xd:>stream create --name tapSimple2 --definition ""tap:stream:simple.transform > file"" Command failed org.springframework.xd.rest.client.impl.SpringXDException: XD144E:(pos 11): Reference to 'transform' is not unique in the target stream 'http | transform | filter | transform | file', please label the relevant module and use the label, or use a suffix index to indicate which occurrence of the module, e.g. 'transform.0' tap:simple.transform",4
"Remove references to XD-1050 in documentation
nan",2
"Use descriptive texts for some module options defaults
Need a way to tell the user that this option will be determined at runtime,late bindings.  In the module info command, references to ${xd.stream.name} could read ""<use stream name>"" for example)",4
"Module context PropertyPlaceholderAutoConfiguration should have allowNulls = true
The current configuration prevents modules to have default values that evaluate to null.  The workaround is to either:    - have the module have its own PPC (which allows nulls)  - rid the placeholders with ${foo:}",4
"Handle container shutdown gracefully
Currently, ModuleDeployer is a disposable bean. When the container context is closed, the ModuleDeployer bean is destroyed along with its associated common context and deployed modules. Issue arises, if the connectionfactory bean associated with the deployed modules' message bus bindings  is destroyed before the ModuleDeployer bean, there is exception stacktrace (at least in case of Redis MessageBus) saying ""Connection closed"".     Adding SmartLifecycle support to ModuleDeployer will make sure all the beans are destroyed during Lifecycle processor's stop() method before any of the singletonbeans are destroyed.    The stacktrace (when using Redis MessageBus is):    org.springframework.data.redis.RedisSystemException: Redis exception; nested exception is com.lambdaworks.redis.RedisException: Connection closed  at org.springframework.data.redis.connection.lettuce.LettuceExceptionConverter.convert(LettuceExceptionConverter.java:45)  at org.springframework.data.redis.connection.lettuce.LettuceExceptionConverter.convert(LettuceExceptionConverter.java:35)  at org.springframework.data.redis.connection.lettuce.LettuceConverters.toDataAccessException(LettuceConverters.java:158)  at org.springframework.data.redis.connection.lettuce.LettuceConnection.convertLettuceAccessException(LettuceConnection.java:237)  at org.springframework.data.redis.connection.lettuce.LettuceConnection.bRPop(LettuceConnection.java:1449)  at org.springframework.data.redis.core.DefaultListOperations$12.inRedis(DefaultListOperations.java:154)  at org.springframework.data.redis.core.AbstractOperations$ValueDeserializingRedisCallback.doInRedis(AbstractOperations.java:50)  at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:181)  at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:149)  at org.springframework.data.redis.core.AbstractOperations.execute(AbstractOperations.java:84)  at org.springframework.data.redis.core.DefaultListOperations.rightPop(DefaultListOperations.java:151)  at org.springframework.data.redis.core.DefaultBoundListOperations.rightPop(DefaultBoundListOperations.java:92)  at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.popMessageAndSend(RedisQueueMessageDrivenEndpoint.java:178)  at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.access$300(RedisQueueMessageDrivenEndpoint.java:51)  at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint$ListenerTask.run(RedisQueueMessageDrivenEndpoint.java:291)  at org.springframework.integration.util.ErrorHandlingTaskExecutor$1.run(ErrorHandlingTaskExecutor.java:52)  at java.lang.Thread.run(Thread.java:722)  Caused by: com.lambdaworks.redis.RedisException: Connection closed  at com.lambdaworks.redis.RedisAsyncConnection.await(RedisAsyncConnection.java:1079)  at com.lambdaworks.redis.RedisConnection.await(RedisConnection.java:820)  at com.lambdaworks.redis.RedisConnection.brpop(RedisConnection.java:101)  at org.springframework.data.redis.connection.lettuce.LettuceConnection.bRPop(LettuceConnection.java:1447)  ... 12 more  13:19:00,897 WARN Thread-5 support.DefaultLifecycleProcessor:257 - Failed to stop bean 'org.springframework.integration.redis.inbound.RedisInboundChannelAdapter#0'  com.lambdaworks.redis.RedisException: Connection is closed  at com.lambdaworks.redis.RedisAsyncConnection.dispatch(RedisAsyncConnection.java:1065)  at com.lambdaworks.redis.pubsub.RedisPubSubConnection.unsubscribe(RedisPubSubConnection.java:82)  at org.springframework.data.redis.connection.lettuce.LettuceSubscription.doUnsubscribe(LettuceSubscription.java:68)  at org.springframework.data.redis.connection.util.AbstractSubscription.unsubscribe(AbstractSubscription.java:186)  at org.springframework.data.redis.connection.util.AbstractSubscription.unsubscribe(AbstractSubscription.java:146)  at org.springframework.data.redis.listener.RedisMessageListenerContainer$SubscriptionTask.cancel(RedisMessageListenerContainer.java:836)  at org.springframework.data.redis.listener.RedisMessageListenerContainer.stop(RedisMessageListenerContainer.java:210)  at org.springframework.integration.redis.inbound.RedisInboundChannelAdapter.doStop(RedisInboundChannelAdapter.java:127)  at org.springframework.integration.endpoint.AbstractEndpoint.stop(AbstractEndpoint.java:100)  at org.springframework.integration.endpoint.AbstractEndpoint.stop(AbstractEndpoint.java:115)  at org.springframework.context.support.DefaultLifecycleProcessor.doStop(DefaultLifecycleProcessor.java:229)  at org.springframework.context.support.DefaultLifecycleProcessor.access$300(DefaultLifecycleProcessor.java:51)  at org.springframework.context.support.DefaultLifecycleProcessor$LifecycleGroup.stop(DefaultLifecycleProcessor.java:363)  at org.springframework.context.support.DefaultLifecycleProcessor.stopBeans(DefaultLifecycleProcessor.java:202)  at org.springframework.context.support.DefaultLifecycleProcessor.onClose(DefaultLifecycleProcessor.java:118)  at org.springframework.context.support.AbstractApplicationContext.doClose(AbstractApplicationContext.java:888)  at org.springframework.boot.context.embedded.EmbeddedWebApplicationContext.doClose(EmbeddedWebApplicationContext.java:157)  at org.springframework.context.support.AbstractApplicationContext$1.run(AbstractApplicationContext.java:809)",2
"module delete command should only provide completions with composed modules
There is no point in providing completion with something that will fail when the user tries it. The information about a module being a composed is now available at the REST layer, so should be used",4
"Remove the constraint on job module batch job id to be ""job""
Currently, the job module's batch job's bean id should be ""job"". This also causes the job name to be 'actual-job-name + "".job""' and the batch job controllers require to search for job with suffix "".job"".     Removing this constraint would help us avoiding these.",3
"Update spring-xd-extension-reactor dependency
Currently, the reactorEnv bean is defined in module-common context and the spring-xd-dirt has the runtime dependency over spring-xd-extension-reactor project.     This enables boot's ReactorAutoConfiguration to initialize the reactor environment, we have the reactor setup configured for both admin and container server applications.    Since reactor environment is not being used by container and only used by the reactor-syslog module, we can move the reactorEnv bean definition in reactor-syslog module.    There is one caveat in this approach as the reactor environment gets setup everytime a new reactor-syslog module is deployed.",1
"Module message conversion fails to work if JMX is enabled
If JMX is enabled for modules (enabling the IntegrationMBeanExporter), the ModuleTypeConversionPlugin fails to get the reference to input/output channel (for adding the ContentTypeHeaderInterceptor) and results in exception.    It seems like the IntegrationMBeanExporter (when JMX is enabled) creates JdkDynamicAopProxy for all the integration components and thereby the following check on ModuleTypeConversionPlugin to retrieve the AbstractMessageChannel fails.    AbstractMessageChannel channel = null;  			if (isInput) {  				channel = module.getComponent(""input"", AbstractMessageChannel.class);  			}  			else {  				channel = module.getComponent(""output"", AbstractMessageChannel.class);  			}    I see the reason why AbstractMessageChannel is used here (to use some of the methods in the implementing class that didn't exist in the interfaces) but IntegrationMBeanExporter creating JdkDynamicAopProxy for the channel fails to resolve as AbstractMessageChannel here.    Following is the full stack trace:    To replicate,    stream create testing --definition """"http --outputType=text/plain | log""  3:45,238 ERROR redisInboundAdapter-redis:queue-inbound-channel-adapter1 handler.LoggingHandler:145 - org.springframework.messaging.MessageHandlingException: error occurred in message handler [moduleDeployer]    org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:79)    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)    java.lang.reflect.Method.invoke(Method.java:601)    org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)    org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)    org.springframework.integration.monitor.SimpleMessageHandlerMetrics.handleMessage(SimpleMessageHandlerMetrics.java:106)    org.springframework.integration.monitor.SimpleMessageHandlerMetrics.invoke(SimpleMessageHandlerMetrics.java:86)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)    org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)    com.sun.proxy.$Proxy62.handleMessage(Unknown Source)    org.springframework.integration.config.ServiceActivatorFactoryBean$1.handleRequestMessage(ServiceActivatorFactoryBean.java:83)    org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:152)    org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:73)    org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:121)    org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:108)    org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)    org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:218)    org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:188)    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)    java.lang.reflect.Method.invoke(Method.java:601)    org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)    org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)    org.springframework.integration.monitor.DirectChannelMetrics.monitorSend(DirectChannelMetrics.java:113)    org.springframework.integration.monitor.DirectChannelMetrics.doInvoke(DirectChannelMetrics.java:97)    org.springframework.integration.monitor.DirectChannelMetrics.invoke(DirectChannelMetrics.java:91)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)    org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)    com.sun.proxy.$Proxy61.send(Unknown Source)    org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:114)    org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:44)    org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:92)    org.springframework.integration.endpoint.MessageProducerSupport.sendMessage(MessageProducerSupport.java:96)    org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.popMessageAndSend(RedisQueueMessageDrivenEndpoint.java:212)    org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.access$300(RedisQueueMessageDrivenEndpoint.java:51)    org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint$ListenerTask.run(RedisQueueMessageDrivenEndpoint.java:291)    org.springframework.integration.util.ErrorHandlingTaskExecutor$1.run(ErrorHandlingTaskExecutor.java:52)    java.lang.Thread.run(Thread.java:722)  Caused by: org.springframework.xd.dirt.plugins.ModuleConfigurationException: Bean named 'output' must be of type [org.springframework.integration.channel.AbstractMessageChannel], but was actually of type [com.sun.proxy.$Proxy70]    org.springframework.xd.dirt.plugins.stream.ModuleTypeConversionPlugin.configureModuleConverters(ModuleTypeConversionPlugin.java:144)    org.springframework.xd.dirt.plugins.stream.ModuleTypeConversionPlugin.postProcessModule(ModuleTypeConversionPlugin.java:70)    org.springframework.xd.dirt.module.ModuleDeployer.postProcessModule(ModuleDeployer.java:378)    org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:282)    org.springframework.xd.dirt.module.ModuleDeployer.deployAndStore(ModuleDeployer.java:271)    org.springframework.xd.dirt.module.ModuleDeployer.handleDeploy(ModuleDeployer.java:266)    org.springframework.xd.dirt.module.ModuleDeployer.handleSingleModuleMessage(ModuleDeployer.java:244)    org.springframework.xd.dirt.module.ModuleDeployer.handleMessageInternal(ModuleDeployer.java:171)    org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:73)  	... 42 more  Caused by: org.springframework.beans.factory.BeanNotOfRequiredTypeException: Bean named 'output' must be of type [org.springframework.integration.channel.AbstractMessageChannel], but was actually of type [com.sun.proxy.$Proxy70]    org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:376)    org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)    org.springframework.context.support.AbstractApplicationContext.getBean(AbstractApplicationContext.java:979)    org.springframework.xd.module.core.SimpleModule.getComponent(SimpleModule.java:156)    org.springframework.xd.dirt.plugins.stream.ModuleTypeConversionPlugin.configureModuleConverters(ModuleTypeConversionPlugin.java:100)  	... 50 more",3
"Few integration tests fail if JMX is enabled
If JMX is enabled, some of the integration tests fail.    This is similar to what we see in XD-1295.    One example of this case is, the test classes that extend StreamTestSupport.    In StreamTestSupport, the @BeforeClass has this line:    moduleDeployer = containerContext.getBean(ModuleDeployer.class);    When JMX is enable, the IntegrationMBeanExporter creates JdkDynamicProxy for the ModuleDeployer (since it is of type MessageHandler) and thereby the above line to get bean by the implementing class type (ModuleDeployer) fails.    There are few other places where we use to refer the implementing classes on getBean().  Looks like we need to fix those as well.    ",2
"Fix module type guessing heuristics
Commit 96de9fcfaf32719413015a1a6bace1b30b6b9610 strengthened module type inference, but some corner cases remain (marked as TODOs and commented out assertions in tests).    To be effective, we need to look at the whole deployed stream (or composed module). Modify ParsinContext accordingly.",5
"Commands that prompt the user are now broken
Most likely due to https://github.com/spring-projects/spring-shell/commit/296e4d2ff0e6e91d91209ab8717335357c587de0  When the user submits with ENTER, the shell appears to hang",5
"Move --deleteFiles out of ResourcesIntoJdbcJobModuleOptionsMetadata
Currently, the fileDeletion listeners are added to filepollhdfs and filejdbc OOTB job modules so that the files are deleted after successful completion of jobs that write the file into hdfs/jdbc.    We have ""--deleteFiles"" option in ResourcesIntoJdbcJobModuleOptionsMetadata (from https://github.com/spring-projects/spring-xd/pull/562)  which makes it available for hdfsjdbc job module as well. But it is not supported yet as it involves deletion of HDFS files.    We need the file deletion listeners for the hdfsjdbc and hdfsmongodb job modules so that if opted to delete files, it can be supported.    ",3
"Handling boolean type module option properties defaults in option metadata
There are few boolean type module option properties whose default values are specified in the module definitions than their corresponding ModuleOptionsMetaData.     Also, when using boolean we need to have module option using primitive type boolean than Boolean type.    Currently, these are some of the module options that require this change:    ""initializeDatabase"" in modules filejdbc, hdfsjdbc job modules, aggregator processor module, jdbc sink module    ""restartable"" in all the job modules    ""deleteFiles"" in filejdbc, filepollhdfs job modules              ",3
"MBeans are not destroyed if stream is created and destroyed with no delay
Problem: The container that the stream was deployed to, will not allow new streams to be deployed.  Once the error occurs, the only solution is to terminate the XD Container and restart it.  To reproduce create a stream foo and destroy the stream, then create the stream  foo again.  This best done programmatically, taking the same steps using the ""shell"" may not reproduce the problem.  i.e. if you put a Sleep of 1-2 seconds between the destroy and the next create, it works fine  ",5
"Add documentation for using FTP->HDFS partitioned jobs
nan",5
"Create REST API for getting information on a given job instance
nan",3
"Create shell command for getting information on a given job instance
nan",3
"File Sink should support Replace as an option
currently the file sink only supports append.  User should support an overwrite feature.",5
"Move ftp support from .x package to spring-xd-dirt batch package
Commit https://github.com/spring-projects/spring-xd/commit/761cd5e8250c055878caf3a789ab5b3254ba48e8 introduced support for FTP and added a bunch of .x classes.    These should not belong to DIRT proper though, and should be added to an extension style project. The job(s) module would then depend on them",1
"Use HATEOAS Link templates
HATEOAS 0.9 introduced some support for templated links. This should be leveraged to properly handle eg /streams/{id} instead of using string concatenation",5
"Weird behavior of the transform module
When trying some of the examples of XD-159, came up with weird behavior of the transform module.    This boils down to:  {noformat}  stream create foo --definition ""http |transform --expression='new java.lang.Integer(payload)' |  transform --expression=payload.getClass() | log""  http post --data 42   ==> Integer (OK)  http post --data WTH => WTH (!)  {noformat}    Seems that when the expression can not be evaluated, the incoming payload is transmitted as is",4
"JSR303 validation of options interferes with dsl completion
When using a JSR303 annotated class for module options, the binding failures should be bypassed, as they interfere with completion proposals.  ",5
"Misleading error message when trying to restart a job exec
Disregard the missing date that is caused by another problem.  Here is the setup:  {noformat}  xd:>job execution list    Id  Job Name  Start Time                        Step Execution Count  Status    --  --------  --------------------------------  --------------------  ---------    13  foo         Europe/Paris                    0                     STARTING    12  foo       2014-02-12 15:39:46 Europe/Paris  1                     FAILED    11  foo       2014-02-12 15:39:29 Europe/Paris  1                     COMPLETED    10  foo       2014-02-12 15:38:36 Europe/Paris  1                     COMPLETED    9   foo       2014-02-12 15:38:21 Europe/Paris  1                     COMPLETED    8   foo         Europe/Paris                    0                     STARTING    7   foo       2014-02-12 15:25:41 Europe/Paris  1                     COMPLETED    6   foo       2014-02-12 15:25:04 Europe/Paris  1                     FAILED    5   foo       2014-02-12 15:14:32 Europe/Paris  1                     FAILED    4   foo       2014-02-12 15:14:13 Europe/Paris  1                     FAILED    3   foo       2014-02-12 15:13:54 Europe/Paris  1                     FAILED    2   foo       2014-02-12 15:13:18 Europe/Paris  1                     FAILED    1   foo       2014-02-12 15:12:58 Europe/Paris  1                     FAILED    0   foo       2014-02-12 15:11:44 Europe/Paris  1                     FAILED    xd:>job execution restart --id 12  Command failed org.springframework.xd.rest.client.impl.SpringXDException: Job Execution 12 is already running.  {noformat}    while the server exception is a bit better:  {noformat}  Caused by: org.springframework.batch.core.repository.JobExecutionAlreadyRunningException: A job execution for this job is already running: JobInstance: id=11, version=0, Job=[foo]    org.springframework.batch.core.repository.support.SimpleJobRepository.createJobExecution(SimpleJobRepository.java:120)  {noformat}    I'd argue we should not speak in terms of execution ids if possible, but rather in terms of job names  ",1
"Job execution list should mention jobs that have been deleted
Create a job, execute it a couple of times, destroy it and then invoke job execution list.    The job name column should mention that a job is defunct (even though a job with the same name could have been re-created in the interim)",3
"Job execution restart fails with NPE
Create a job, launch it but make it fail (eg filejdbc with missing file)  job execution list => it's there, as FAILED. Good  job execution restart <theid> ==> Fails with NPE:  {noformat} 16:59:42,160 ERROR http-nio-9393-exec-7 rest.RestControllerAdvice:191 - Caught exception while handling a request java.lang.NullPointerException   org.springframework.batch.core.job.AbstractJob.execute(AbstractJob.java:351)   org.springframework.batch.core.launch.support.SimpleJobLauncher$1.run(SimpleJobLauncher.java:135)   org.springframework.core.task.SyncTaskExecutor.execute(SyncTaskExecutor.java:50)   org.springframework.batch.core.launch.support.SimpleJobLauncher.run(SimpleJobLauncher.java:128)   sun.reflect.GeneratedMethodAccessor157.invoke(Unknown Source)   sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)   java.lang.reflect.Method.invoke(Method.java:606)   org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)   org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)   org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)   org.springframework.batch.core.configuration.annotation.SimpleBatchConfiguration$PassthruAdvice.invoke(SimpleBatchConfiguration.java:117)   org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)   org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)   com.sun.proxy.$Proxy39.run(Unknown Source)   org.springframework.batch.admin.service.SimpleJobService.restart(SimpleJobService.java:179)   org.springframework.xd.dirt.plugins.job.DistributedJobService.restart(DistributedJobService.java:77)   org.springframework.xd.dirt.rest.BatchJobExecutionsController.restartJobExecution(BatchJobExecutionsController.java:146)   sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)   sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)   sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)   java.lang.reflect.Method.invoke(Method.java:606)   org.springfram {noformat}",5
"Create XD .zip distribution for YARN
Create XD .zip distribution for YARN that adds an additional sub-project to the spring-xd repo for building the xd-YARN.zip    Link into main build file    Produce a new artifact spring-xd-v-xyz-yarn.zip as part of the nightly CI process -- will now have 2 artifacts, main xd.zip distribution and xd-yarn.zip    Does not include any Hadoop distribution libraries    Does include spring-hadoop jars for Apache22 â€˜unflavoredâ€™  ",3
"Create POJO based FileSink module metadata
This is so that we can have an ENUM that can show the possible values and autocomplete.  Using just the XML the user has a greater chance to enter an invalid mode.   ",5
"UI:Fix E2E test warning
When running E2E tests the following warning may be observed:    {code}  Running ""karma:e2e"" (karma) task  INFO [karma]: Karma v0.10.9 server started at http://localhost:7070/_karma_/  INFO [launcher]: Starting browser PhantomJS  TypeError: Cannot read property 'verbose' of undefined      at enableWebsocket (/Users/hillert/dev/git/spring-xd/spring-xd-ui/node_modules/grunt-connect-proxy/lib/utils.js:101:18)      at Object.utils.proxyRequest [as handle] (/Users/hillert/dev/git/spring-xd/spring-xd-ui/node_modules/grunt-connect-proxy/lib/utils.js:109:5)      at next (/Users/hillert/dev/git/spring-xd/spring-xd-ui/node_modules/grunt-contrib-connect/node_modules/connect/lib/proto.js:193:15)      at Object.livereload [as handle] (/Users/hillert/dev/git/spring-xd/spring-xd-ui/node_modules/grunt-contrib-connect/node_modules/connect-livereload/index.js:147:5)      at next (/Users/hillert/dev/git/spring-xd/spring-xd-ui/node_modules/grunt-contrib-connect/node_modules/connect/lib/proto.js:193:15)      at Function.app.handle (/Users/hillert/dev/git/spring-xd/spring-xd-ui/node_modules/grunt-contrib-connect/node_modules/connect/lib/proto.js:201:3)      at Server.app (/Users/hillert/dev/git/spring-xd/spring-xd-ui/node_modules/grunt-contrib-connect/node_modules/connect/lib/connect.js:65:37)      at Server.EventEmitter.emit (events.js:98:17)      at HTTPParser.parser.onIncoming (http.js:2108:12)      at HTTPParser.parserOnHeadersComplete [as onHeadersComplete] (http.js:121:23)      at Socket.socket.ondata (http.js:1966:22)      at TCP.onread (net.js:525:27)  {code}  ",2
"XD container can not be started before the admin server
The job single partitioned step support (from singlestep-partitioner-support.xml) has the batch job DAOs (loaded from batch.xml).    During container startup, when the jobExecutionDao bean is initialized it makes the db connection to the underlying batch database (which admin server initializes).     Here is the exception:    15:30:03,600  INFO main xml.XmlBeanDefinitionReader:316 - Loading XML bean definitions from class path resource [META-INF/spring-xd/batch/batch.xml]  15:30:06,154  WARN main annotation.ConfigurationClassEnhancer:318 - @Bean method StepScopeConfiguration.stepScope is non-static and returns an object assignable to Spring's BeanFactoryPostProcessor interface. This will result in a failure to process annotations such as @Autowired, @Resource and @PostConstruct within the method's declaring @Configuration class. Add the 'static' modifier to this method to avoid these container lifecycle issues; see @Bean Javadoc for complete details  15:30:06,705  INFO main support.PostProcessorRegistrationDelegate$BeanPostProcessorChecker:309 - Bean 'org.springframework.xd.dirt.server.ParentConfiguration$JmxConfiguration' of type [class org.springframework.xd.dirt.server.ParentConfiguration$JmxConfiguration] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)  15:30:07,266  INFO main annotation.AnnotationMBeanExporter:416 - Registering beans for JMX exposure on startup  15:30:07,291  INFO main annotation.AnnotationMBeanExporter:896 - Bean with name 'XDParentConfigMBeanExporter' has been autodetected for JMX exposure  15:30:07,299  INFO main annotation.AnnotationMBeanExporter:659 - Located managed bean 'XDParentConfigMBeanExporter': registering with JMX server as MBean [org.springframework.integration.monitor:name=XDParentConfigMBeanExporter,type=IntegrationMBeanExporter]  15:30:09,637  INFO main concurrent.ThreadPoolTaskScheduler:165 - Initializing ExecutorService  'scheduler'  15:56:08,788  INFO main concurrent.ThreadPoolTaskScheduler:203 - Shutting down ExecutorService 'scheduler'  15:56:08,806  INFO main annotation.AnnotationMBeanExporter:434 - Unregistering JMX-exposed beans on shutdown  15:56:08,853  INFO main autoconfigure.AutoConfigurationReportLoggingInitializer:118 -     Error starting ApplicationContext. To display the auto-configuration report enabled debug logging (start with --debug)      15:56:08,854  INFO main listener.ClasspathLoggingApplicationListener:54 - Application failed to start with classpath: [file:/Users/iperumal/workspace/spring-xd/modules/processor/scripts/, file:/Users/iperumal/workspace/spring-xd/spring-xd-dirt/bin/, file:/Users/iperumal/workspace/spring-xd/spring-xd-test/bin/, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.integration/spring-integration-test/4.0.0.M3/74e696bad60aab349c74f52839eb43ed0e1ce0e2/spring-integration-test-4.0.0.M3.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.integration/spring-integration-amqp/4.0.0.M3/32dd5001acffd82391d756cf3b5ba73ca4075aed/spring-integration-amqp-4.0.0.M3.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.integration/spring-integration-redis/4.0.0.M3/ed5e47b6844212bb88c112c559556b4cb3d6b087/spring-integration-redis-4.0.0.M3.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.data/spring-data-hadoop/2.0.0.M4/9f1acbf66f3a97d42a8f5b00eb0c0cad11562730/spring-data-hadoop-2.0.0.M4.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.data/spring-data-redis/1.1.1.RELEASE/e2d5e9cfdaaa3fbcc2a8d4bdbe06daf771cb4e39/spring-data-redis-1.1.1.RELEASE.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework/spring-context/4.0.1.RELEASE/cb996939c8d48ae55ec933041f17e7fba4d9e27d/spring-context-4.0.1.RELEASE.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework/spring-context-support/4.0.1.RELEASE/94dc23c49a74f3f4b894b29416b08202e5976f49/spring-context-support-4.0.1.RELEASE.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework/spring-tx/4.0.1.RELEASE/b93b2c39b09ff858a42db85a0a9a8ce232a6779/spring-tx-4.0.1.RELEASE.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework/spring-test/4.0.1.RELEASE/367212c3b84c63a48220efa0fe8e9a3a937fcf68/spring-test-4.0.1.RELEASE.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/com.lambdaworks/lettuce/2.3.3/1366615be02807a568c5f2d3a4475a3d27a879a6/lettuce-2.3.3.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.hsqldb/hsqldb/2.3.0/93306187b1a782f2b929d12536022185487037d2/hsqldb-2.3.0.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.apache.tomcat/tomcat-jdbc/7.0.42/3827da9ca05ff115f239a2372bd44cfd729c692d/tomcat-jdbc-7.0.42.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/junit/junit/4.11/4e031bb61df09069aeb2bffb4019e7a5034a4ee0/junit-4.11.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/commons-io/commons-io/2.4/b1b6ea3b7e4aa4f492509a4952029cd8e48019ad/commons-io-2.4.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.codehaus.groovy/groovy-all/2.1.0/a14306a090eec2fa91017b77ac079361f68e1830/groovy-all-2.1.0.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.hamcrest/hamcrest-library/1.3/4785a3c21320980282f9f33d0d1264a69040538f/hamcrest-library-1.3.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.hamcrest/hamcrest-core/1.3/42a25dc3219429f0e5d060061f71acb49bf010a0/hamcrest-core-1.3.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.objenesis/objenesis/1.0/9b473564e792c2bdf1449da1f0b1b5bff9805704/objenesis-1.0.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.mockito/mockito-core/1.9.5/c3264abeea62c4d2f367e21484fbb40c7e256393/mockito-core-1.9.5.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/commons-logging/commons-logging/1.1.1/5043bfebc3db072ed80fbd362e7caf00e885d8ae/commons-logging-1.1.1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework/spring-core/4.0.1.RELEASE/e39774d97c9dadfe49e6dfd16e3868bc1e390554/spring-core-4.0.1.RELEASE.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework/spring-beans/4.0.1.RELEASE/605582e95fb62b43fb4a843babdcf739f3497e92/spring-beans-4.0.1.RELEASE.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/aopalliance/aopalliance/1.0/235ba8b489512805ac13a8f9ea77a1ca5ebe3e8/aopalliance-1.0.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework/spring-aop/4.0.1.RELEASE/ff68e4cfdbb2be3e8d8a7f34e7cbacc1860dfe75/spring-aop-4.0.1.RELEASE.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework/spring-expression/4.0.1.RELEASE/452cb22401e868a1e79677dd22b6a3097fc603fa/spring-expression-4.0.1.RELEASE.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.retry/spring-retry/1.0.3.RELEASE/33b967f6abaa0a496318bff2ce96e6da6285a54d/spring-retry-1.0.3.RELEASE.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework/spring-messaging/4.0.1.RELEASE/829829afd9135368faa1e3a5261404f602a2e939/spring-messaging-4.0.1.RELEASE.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.integration/spring-integration-core/4.0.0.M3/12b445cfa896b906facd2be289adcdfe839f6104/spring-integration-core-4.0.0.M3.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.hamcrest/hamcrest-all/1.3/63a21ebc981131004ad02e0434e799fd7f3a8d5a/hamcrest-all-1.3.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.amqp/spring-amqp/1.3.0.M2/e668db16a4206e96531b978e5978868ba0ebf4e9/spring-amqp-1.3.0.M2.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/com.rabbitmq/amqp-client/3.2.2/9e4485e734415e84ea3caea25650f8651f388a3a/amqp-client-3.2.2.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.amqp/spring-rabbit/1.3.0.M2/ceb54c437d2d00c3a22d59982922f24fbf78c8a/spring-rabbit-1.3.0.M2.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.slf4j/slf4j-api/1.6.6/ce53b0a0e2cfbb27e8a59d38f79a18a5c6a8d2b0/slf4j-api-1.6.6.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.slf4j/jcl-over-slf4j/1.6.6/ec497945fdcaf7fd970ae9931b9bbfaf735d385e/jcl-over-slf4j-1.6.6.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework/spring-jdbc/4.0.1.RELEASE/7d46d07d44f56af7cdcbba53ff671c5487f9547/spring-jdbc-4.0.1.RELEASE.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/commons-cli/commons-cli/1.2/2bf96b7aa8b611c177d329452af1dc933e14501c/commons-cli-1.2.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/xmlenc/xmlenc/0.52/d82554efbe65906d83b3d97bd7509289e9db561a/xmlenc-0.52.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/commons-codec/commons-codec/1.4/4216af16d38465bbab0f3dff8efa14204f7a399a/commons-codec-1.4.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/commons-httpclient/commons-httpclient/3.0.1/d6364bcc1b2b2aa69d008602d36a700453648560/commons-httpclient-3.0.1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.apache.commons/commons-math/2.1/b3c4bdc2778ddccceb8da2acec3e37bfa41303e9/commons-math-2.1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/commons-collections/commons-collections/3.2.1/761ea405b9b37ced573d2df0d1e3a4e0f9edc668/commons-collections-3.2.1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/commons-lang/commons-lang/2.4/16313e02a793435009f1e458fa4af5d879f6fb11/commons-lang-2.4.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/commons-beanutils/commons-beanutils/1.7.0/5675fd96b29656504b86029551973d60fb41339b/commons-beanutils-1.7.0.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/commons-digester/commons-digester/1.8/dc6a73fdbd1fa3f0944e8497c6c872fa21dca37e/commons-digester-1.8.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/commons-beanutils/commons-beanutils-core/1.8.0/175dc721f87e4bc5cc0573f990e28c3cf9117508/commons-beanutils-core-1.8.0.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/commons-configuration/commons-configuration/1.6/32cadde23955d7681b0d94a2715846d20b425235/commons-configuration-1.6.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/oro/oro/2.0.8/5592374f834645c4ae250f4c9fbb314c9369d698/oro-2.0.8.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/commons-net/commons-net/1.4.1/abb932adb2c10790c1eaa4365d3ac2a1ac7cb700/commons-net-1.4.1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/commons-el/commons-el/1.0/1df2c042b3f2de0124750241ac6c886dbfa2cc2c/commons-el-1.0.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.eclipse.jdt/core/3.1.1/88c83ce444cf46d02494da37c9fa1eebc9ce9cea/core-3.1.1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.apache.hadoop/hadoop-core/1.2.1/3e5874122a26a735162a380627210779b41bfd59/hadoop-core-1.2.1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.apache.hadoop/hadoop-streaming/1.2.1/4baac190cf4cd4a6d085780cbcab1a89493f932b/hadoop-streaming-1.2.1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.apache.hadoop/hadoop-tools/1.2.1/b08c16bd0448fbcadab67c4f8df837c094fdc91e/hadoop-tools-1.2.1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.data/spring-data-hadoop-core/2.0.0.M4/ff4cefb0870d61fdc9efe26d118310c02b5eafbb/spring-data-hadoop-core-2.0.0.M4.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.data/spring-data-hadoop-batch/2.0.0.M4/47de250d5d9b48ed1319a747e3b06fdc46d939ef/spring-data-hadoop-batch-2.0.0.M4.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/io.netty/netty/3.6.6.Final/e4e40738ce9bee0a92389cb739c94d7839778647/netty-3.6.6.Final.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.apache.tomcat/tomcat-juli/7.0.42/f0049ac94514d69231c41ed96238efb94ffdd9cf/tomcat-juli-7.0.42.jar, file:/Users/iperumal/workspace/spring-xd/spring-xd-analytics/bin/, file:/Users/iperumal/workspace/spring-xd/spring-xd-tuple/bin/, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.core/jackson-databind/2.2.2/3c8f6018eaa72d43b261181e801e6f8676c16ef6/jackson-databind-2.2.2.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.batch/spring-batch-infrastructure/3.0.0.BUILD-SNAPSHOT/cfaea737589c43c54ff338ae27e1bee477620176/spring-batch-infrastructure-3.0.0.BUILD-SNAPSHOT.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/com.eaio.uuid/uuid/3.2/77ba5105d949cd589aff75400d9f7d3676691a46/uuid-3.2.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.core/jackson-annotations/2.2.2/285cb9c666f0f0f3dd8a1be04e1f457eb7b15113/jackson-annotations-2.2.2.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.core/jackson-core/2.2.2/d20be6a5ddd6f8cfd36ebf6dea329873a1c41f1b/jackson-core-2.2.2.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.data/spring-data-commons/1.6.2.RELEASE/e96a0458cdc3179ca70c880f42315bb75df4faf5/spring-data-commons-1.6.2.RELEASE.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/joda-time/joda-time/2.1/8f79e353ef77da6710e1f10d34fc3698eaaacbca/joda-time-2.1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/log4j/log4j/1.2.17/5af35056b4d257e4b64b9e8069c0746e8b08629f/log4j-1.2.17.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.slf4j/jcl-over-slf4j/1.7.5/cd5970bd13fa85f7bed41ca606d6daf7cbf1365/jcl-over-slf4j-1.7.5.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.slf4j/slf4j-log4j12/1.7.5/6edffc576ce104ec769d954618764f39f0f0f10d/slf4j-log4j12-1.7.5.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/nl.jqno.equalsverifier/equalsverifier/1.1.3/60cd685f314a9cebfd0595d88fea45fba2f47918/equalsverifier-1.1.3.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.slf4j/slf4j-api/1.7.5/6b262da268f8ad9eff941b25503a9198f0a0ac93/slf4j-api-1.7.5.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.objenesis/objenesis/1.1/63db1176f16448172611266154e4f6d39a0e1e68/objenesis-1.1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/cglib/cglib-nodep/2.2/59afed7ab65e7ec6585d5bc60556c3cbd203532b/cglib-nodep-2.2.jar, file:/Users/iperumal/workspace/spring-xd/spring-xd-rest-domain/bin/, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.hateoas/spring-hateoas/0.8.0.RELEASE/819c25e1ff12b7fca483d76b4e7d20221f621fcd/spring-hateoas-0.8.0.RELEASE.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.batch/spring-batch-admin-manager/1.3.0.M1/5afc7442417af8c46ae51480ed2b83943283d449/spring-batch-admin-manager-1.3.0.M1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.batch/spring-batch-core/3.0.0.BUILD-SNAPSHOT/8168f58716cd305040eaa87c82dc61822b03415c/spring-batch-core-3.0.0.BUILD-SNAPSHOT.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.codehaus.jackson/jackson-core-asl/1.9.13/3c304d70f42f832e0a86d45bd437f692129299a4/jackson-core-asl-1.9.13.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework/spring-web/4.0.1.RELEASE/2ace92025f042e1d3ddfdbba093172e3572ac130/spring-web-4.0.1.RELEASE.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework/spring-webmvc/4.0.1.RELEASE/2dbc91a6413115f7ffbe94f0fa9bc9fda3281d90/spring-webmvc-4.0.1.RELEASE.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.objenesis/objenesis/1.3/dc13ae4faca6df981fc7aeb5a522d9db446d5d50/objenesis-1.3.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.slf4j/slf4j-api/1.7.2/81d61b7f33ebeab314e07de0cc596f8e858d97/slf4j-api-1.7.2.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.aspectj/aspectjrt/1.6.6/ff58f520e1a304b8a02b8cea8b96b1b8e5b25b0/aspectjrt-1.6.6.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.aspectj/aspectjweaver/1.6.6/c0383be877cfa4ec6b62202c942a89a6264a2be6/aspectjweaver-1.6.6.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/commons-pool/commons-pool/1.3/3231230c1d7631b66a74d1c4653cfd65a6f9ea0/commons-pool-1.3.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/commons-dbcp/commons-dbcp/1.2.2/4fd4c6110e9bca3a655b717eb2e5920febb8403d/commons-dbcp-1.2.2.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/commons-io/commons-io/1.4/a8762d07e76cfde2395257a5da47ba7c1dbd3dce/commons-io-1.4.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/commons-lang/commons-lang/2.1/4763ecc9d78781c915c07eb03e90572c7ff04205/commons-lang-2.1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/commons-fileupload/commons-fileupload/1.2.1/384faa82e193d4e4b0546059ca09572654bc3970/commons-fileupload-1.2.1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/net.sf.ehcache/ehcache-core/2.3.0/e59473c71a31e8e19da4fbc7028585c8ed51d69f/ehcache-core-2.3.0.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/commons-collections/commons-collections/3.2/f951934aa5ae5a88d7e6dfaa6d32307d834a88be/commons-collections-3.2.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.freemarker/freemarker/2.3.15/c8cfe522476fcec8da5c980d58bf62d6ab0cf27c/freemarker-2.3.15.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.batch/spring-batch-admin-resources/1.3.0.M1/bdf7d5afc02397385fce8731409f606e54d4d033/spring-batch-admin-resources-1.3.0.M1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.retry/spring-retry/1.0.2.RELEASE/d673c90a9fd8f0de5f20d53d61047849f707f42b/spring-retry-1.0.2.RELEASE.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/javax.batch/javax.batch-api/1.0/65392d027a6eb369fd9fcd1b75cae150e25ac03c/javax.batch-api-1.0.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/com.ibm.jbatch/com.ibm.jbatch-tck-spi/1.0/8ac869b0a60bff1a15eba0fb6398942410396938/com.ibm.jbatch-tck-spi-1.0.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/xpp3/xpp3_min/1.1.4c/19d4e90b43059058f6e056f794f0ea4030d60b86/xpp3_min-1.1.4c.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/com.thoughtworks.xstream/xstream/1.3/3f755b1a46744302712b1b962c4ab64de392f477/xstream-1.3.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.codehaus.jettison/jettison/1.1/1a01a2a1218fcf9faa2cc2a6ced025bdea687262/jettison-1.1.jar, file:/Users/iperumal/workspace/spring-xd/spring-xd-module/bin/, file:/Users/iperumal/workspace/spring-xd/spring-xd-module-spi/bin/, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-autoconfigure/1.0.0.RC1/7830d0dd26f75841d8b5c2c72c42b864b1192ddb/spring-boot-autoconfigure-1.0.0.RC1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/javax.validation/validation-api/1.0.0.GA/b6bd7f9d78f6fdaa3c37dae18a4bd298915f328e/validation-api-1.0.0.GA.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.hibernate/hibernate-validator/4.3.1.Final/49b31d8ea51fa21cc78a89e9d4ddb11d6bfb4669/hibernate-validator-4.3.1.Final.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot/1.0.0.RC1/7e53b72a368c495a482d3a213ad6338f8f7afcfa/spring-boot-1.0.0.RC1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.jboss.logging/jboss-logging/3.1.0.CR2/28725380c07f917ace4e511db21cc45e9ae5a72b/jboss-logging-3.1.0.CR2.jar, file:/Users/iperumal/workspace/spring-xd/spring-xd-hadoop/bin/, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.data/spring-data-hadoop-store/2.0.0.M4/1d3d691c0e6952ba26724339668e17040c368683/spring-data-hadoop-store-2.0.0.M4.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.xerial.snappy/snappy-java/1.1.0/71c46e2313e9288...",3
"Allow mixins of ModuleOptionsMetadata
A lot of modules have similar options. Moreover, job modules often have options that belong to at least two domains (eg jdbc + hdfs).  I think that by using FlattenedCompositeModuleOptionsMetadata, we could come up with a way to combine several options POJOs into one. Something like:  public class JdbcHdfsOptionsMetadata {    @OptionsMixin   private JdbcOptionsMetadata jdbc;    @OptionsMixin   private HdfsOptionsMetadata hdfs; }  this would expose eg ""driverClass"" as well as ""rolloverSize"" as top level options. Values could be actually injected into the fields, so that eg custom validation could occur (default validation for the mixin class would occur by default)",5
"Add XD deployment for YARN
Add YARN specific code based on Janne's prototyping    Add YARN Client and AppMaster implementations and startup config files    This includes shell scripts to deploy XD to YARN    Test working on Apache 2.2 distribution    We can modify config files, everything should be possible to override by providing command-line args or env variables.  ./xd-yarn-deploy --zipFile /tmp/spring-xd-yarn.zip --config /tmp/spring-xd-yarn.yml    ",8
"Add way to provide module config options for XD on YARN
There seems to be some intersection with the work for this issue and the rationalization of how module properties are handled.  There will be changes to configuration/property management support such that each module (source, sink, etc) will be able to also be overridden in spring-xd.yml (or wherever -Dspring.config.location points to.  The HDFS sink module for example, will have default values based on it's OptionsMetadata and will be of the form <type>.<module>.<option>      That means in the configuration for hdfs.xml sink, there would be a config section such as    {code:xml}      <configuration>        fs.default.name=${sink.hdfs.hd.fs}        mapred.job.tracker=${sink.hdfs.hd.jt}        yarn.resourcemanager.address=${sink.hdfs.hd.rm}        mapreduce.framework.name=${sink.hdfs.mr.fw}      </configuration>  {code}    With default values defined by a HdfsSinkOptionsMetadata class.  The hdfs.xml module file would not contain any references to a properties file.    A file specified by -Dspring.config.location could override the values in a config section such as    sink:    hdfs:      hd.fs : hdfs://foobarhost:8020      hd.jt : 10.123.123.123:9000    etc.    ",5
"Create shell script to retrieve list of xd components running on YARN
nan",5
"Make Hadoop22 the default for the build
nan",4
"Deploy XD on YARN for a distribution other than Apache Hadoop 2.2
Suggest trying with Hortonworks 2.0",8
"Provide xd-shell integration for deploying XD on YARN
Command such as  yarn app list yarn deploy-xd --zipFile /tmp/myapp.zip --config /tmp/myconfig.yml ",8
"Rabbit source module with outputType fails to deploy
To replicate the issue:    Create stream:   stream create rabbittest --definition ""rabbit --queues=test --outputType=text/plain | log""    Stacktrace thrown:    17:59:56,436 ERROR http-nio-9393-exec-3 rest.RestControllerAdvice:191 - Caught exception while handling a request  java.lang.IllegalArgumentException: Module option named outputType is already present    org.springframework.xd.module.options.FlattenedCompositeModuleOptionsMetadata.<init>(FlattenedCompositeModuleOptionsMetadata.java:56)    org.springframework.xd.module.options.DelegatingModuleOptionsMetadataResolver.resolve(DelegatingModuleOptionsMetadataResolver.java:49)    org.springframework.xd.dirt.stream.XDStreamParser.parse(XDStreamParser.java:117)    org.springframework.xd.dirt.stream.AbstractDeployer.save(AbstractDeployer.java:73)    org.springframework.xd.dirt.rest.XDController.save(XDController.java:227)    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)    java.lang.reflect.Method.invoke(Method.java:601)    org.springframework.web.method.support.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:214)    org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:132)    org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:104)    org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandleMethod(RequestMappingHandlerAdapter.java:749)    org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:690)    org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:83)    org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:945)    org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:876)    org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:961)    org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:863)    javax.servlet.http.HttpServlet.service(HttpServlet.java:647)    org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:837)    javax.servlet.http.HttpServlet.service(HttpServlet.java:728)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:305)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)    org.springframework.boot.actuate.trace.WebRequestTraceFilter.doFilter(WebRequestTraceFilter.java:114)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)    org.springframework.boot.actuate.autoconfigure.EndpointWebMvcAutoConfiguration$ApplicationContextFilterConfiguration$1.doFilterInternal(EndpointWebMvcAutoConfiguration.java:131)    org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:108)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)    org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:77)    org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:108)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)    org.springframework.web.filter.HttpPutFormContentFilter.doFilterInternal(HttpPutFormContentFilter.java:88)    org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:108)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)    org.springframework.boot.actuate.autoconfigure.MetricFilterAutoConfiguration$MetricsFilter.doFilter(MetricFilterAutoConfiguration.java:97)    org.springframework.boot.actuate.autoconfigure.MetricFilterAutoConfiguration$MetricsFilter.doFilter(MetricFilterAutoConfiguration.java:82)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)    org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:222)    org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:123)    org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:472)    org.apache.catalina.valves.RemoteIpValve.invoke(RemoteIpValve.java:680)    org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:171)    org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:99)    org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:118)    org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:407)    org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1004)    org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:589)    org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.run(NioEndpoint.java:1680)    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)    java.lang.Thread.run(Thread.java:722)  ",1
"Modularize XD UI
From https://jira.springsource.org/browse/XD-1231 we understand the importance of modularizing client side javascript code. This story tracks modularization of XD UI.",8
"Add a Kafka Source
This would use the Kafka Spring Integration Extension. We have a version of this working but had to modify the adapter code as its not currently compatible with Spring Integration 4. See INTEXT-97    ",8
"Enhance HadoopFileSystemTestSupport to obtain resource for a specific hadoop distro
It looks like the HadoopFileSystemTestSupport test rule by default runs against hadoop 1.2 and we can add a way to support running the hadoop centric tests to run against a given hadoop distro.     Currently, if the test is run against a version other than 1.2, the rule says:    15:47:34,469 ERROR main hadoop.HadoopFileSystemTestSupport:95 - HADOOP_FS IS NOT AVAILABLE, SKIPPING TESTS  org.apache.hadoop.ipc.RemoteException: Server IPC version 9 cannot communicate with client version 4    org.apache.hadoop.ipc.Client.call(Client.java:1113)    org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)    com.sun.proxy.$Proxy8.getProtocolVersion(Unknown Source)    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)    java.lang.reflect.Method.invoke(Method.java:601)    org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)    org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)    com.sun.proxy.$Proxy8.getProtocolVersion(Unknown Source)    org.apache.hadoop.ipc.RPC.checkVersion(RPC.java:422)    org.apache.hadoop.hdfs.DFSClient.createNamenode(DFSClient.java:183)    org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:281)    org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:245)    org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:100)    org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1446)    org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:67)    org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1464)    org.apache.hadoop.fs.FileSystem.get(FileSystem.java:263)    org.apache.hadoop.fs.FileSystem.get(FileSystem.java:124)    org.springframework.xd.test.hadoop.HadoopFileSystemTestSupport.obtainResource(HadoopFileSystemTestSupport.java:49)    org.springframework.xd.test.AbstractExternalResourceTestSupport.apply(AbstractExternalResourceTestSupport.java:58)    org.junit.rules.RunRules.applyAll(RunRules.java:26)    org.junit.rules.RunRules.<init>(RunRules.java:15)    org.junit.runners.BlockJUnit4ClassRunner.withTestRules(BlockJUnit4ClassRunner.java:379)    org.junit.runners.BlockJUnit4ClassRunner.withRules(BlockJUnit4ClassRunner.java:340)    org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:256)    org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)    org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)    org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)    org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)    org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)    org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)    org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)    org.junit.runners.ParentRunner.run(ParentRunner.java:309)    org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:50)    org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)    org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467)    org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683)    org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390)    org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197)  ",3
"Make Batch Job Restarts Work using Single Node
See also XD-1320.",4
"XD Integration pauses should be configurable
While the current setting work while running from your laptop to local deployments.  Or running from your laptop to ec2, they are not long enough for CI to Ec2. This should have good defaults and have CI set them to what it needs.",5
"Add config file fragment support configuration in XD windows bat scripts
The external configuration fragment file support by setting spring.config.location in the XD startup scripts are not updated in xd-admin, xd-container and xd-singlenode .bat scripts.   Please refer: https://github.com/spring-projects/spring-xd/issues/582 ",2
"Ensure XD Samples share common version dependencies
Currently samples use separate build scripts, so the XD versions, etc. may all be different. There should be a top level build script or at least a way to ensure the same version dependencies",3
"STS Gradle Import Broken
When importing XD as a gradle project into STS, it fails with    Missing directories spring-xd-hadoop/hdp20 and spring-xd-yarn    mkdir on these directories solves the problem.    The hdp20 case relates to XD-599 - it is not clear why spring-xd-yarn is needed.  ",1
"Allow easy integration with other types of message transports - remove enums for transport layers
If a third party messaging solution wants to be the transport layer in SpringXD they must currently fork the SpringXD code base and change the enums.  Example: CommonDistributedOptions.ControlTransport currently limits to the following options (rabbit, redis).  So if a third party like messaging system, like ZeroMQ, wanted to plug-in they would have to add to the enum.  Here is another example where GemFire was used as the messaging system:   https://github.com/charliemblack/spring-xd/blob/master/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/server/options/CommonDistributedOptions.java#L38  All messaging enums should be removed for an extensible model.",1
"Stream partitioning metadata should allow updating at runtime - dynamically / anytime
In a running system some times the algorithm for partitioning the data might overload a given server with work.  When that happens we might need to ""rebalance"" the partitioned work / data to achieve a even balance of stream throughput across servers in a given compute group.  We can think of this dynamic rebalancing behavior as an extension of a failure use case.   In the failure scenario we need to re-partition the stream to other servers in the group.  We should allow third parties to plug-in to help with this capability.  As an example GemFire will report the new partitioning meta-data when this type failure / rebalance happens. ",8
"Deployment manifest to support partitioning a stream
We need a methodology for providing partitioning hints.    A current proposal uses message headers to provide:    * partition_key â€“ the item to partition on  * destination_region â€“ what to target    In the proposal the developer used ""partition_key"" to route the stream message to the node where data was stored in process.  This was done so downstream stream operations could work on the data with out suffering any network IO.    The ""destination_region"" was used to target the type of data the downstream streams were going to use in their stream processing.      ",16
"Deployment manifest to support directing deployment to run on a group of servers
Need some kind of hint that a given deployment is to be run on a group of servers.  That deployment would then be part of a partitioned work flow.    Another item on this would be the ""group"" that the server is running in can be added to removed dynamically.  The use case on this would be if the group of servers are running a max CPU capacity we can easily add another compute node.  Likewise we can remove a server from the group if the servers are not being fully utilized.       This issue is lightly linked to:  https://jira.springsource.org/browse/XD-1337",8
"XD container doesn't need embedded servlet container if it uses specific management port
Currently, the XD container uses embedded tomcat only to support management server that acts as the RESTful server for boot's actuator MVC and jolokia endpoints.    If the configuration is set to use a specific management port (using the fixes addressed via XD-1122), then we don't need to have embedded tomcat servlet container in XD container. ",4
"Support oracle jdbc configuration for XD batch job repository
Currently, hsqldb, postgres and mysql job repositories are supported. We need to add configurable oracle jdbc settings.",2
"Configuration for RabbitMQ message bus concurrent consumers
By having the configuration option for concurrent consumers would help improve the performance of message consumption by the consumer modules when the ordering of the incoming messages don't matter.",1
"Provide a conventional way to extend XD Container configuration
Provide an easy way for users to add beans (e.g., Gemfire cache configuration) or modify default XD configuration such as serializers, and message converters. A simple approach is to add a well known resource selector such as classpath*:META-INF/spring/xd/extensions or include this path in an extensible @Configuration base class.  In addition, we should adopt conventional names for beans that are meant to be extended, e.g. use an xd. prefix.",8
"Cannot undeploy stream that was created and deployed with a ""."" in the name
eserrano-mbp:spring-xd-1.0.0.M5 eserrano$ ./shell/bin/xd-shell   _____                           __   _______  /  ___|          (-)             \ \ / /  _  \  \ `--. _ __  _ __ _ _ __   __ _   \ V /| | | |   `--. \ '_ \| '__| | '_ \ / _` |  / ^ \| | | |  /\__/ / |_) | |  | | | | | (_| | / / \ \ |/ /  \____/| .__/|_|  |_|_| |_|\__, | \/   \/___/        | |                  __/ |        |_|                 |___/  eXtreme Data  1.0.0.M5 | Admin Server Target: http://localhost:9393  Welcome to the Spring XD shell. For assistance hit TAB or type ""help"".  xd:>stream list    Stream Name    Stream Definition                                          Status    -------------  ---------------------------------------------------------  --------    eesstream.log  http | transform --expression=payload.toUpperCase() | log  deployed    httptest       http | file    tictac         time | log    xd:>stream     stream all         stream create      stream deploy      stream destroy     stream list        stream undeploy        xd:>stream undeploy --name   stream undeploy --name   required --name: the name of the stream to un-deploy; no default value  xd:>stream undeploy --name eesstream.log   Command failed org.springframework.xd.rest.client.impl.SpringXDException: The stream named 'eesstream' is not currently deployed    xd:>  ",1
"Use dot as the composed module option separator
Following merge of https://github.com/spring-projects/spring-xd/pull/601, use dot as the separator for a composed module option.    Need change to the parser to accept dots",4
"Add aliases concept to module options and use in composed modules
Following merge of https://github.com/spring-projects/spring-xd/pull/601, allow a module option to be known by several names and elect a short name in composed module options when there is no ambiguity",6
"Expose a getter for a module unique id inside a stream definition
Following merge of https://github.com/spring-projects/spring-xd/pull/601, expose a unique id under which a module is known inside a stream. That id (which defaults to the module name) is what should be used as the qualifier for an option name inside a composed module, ie    {noformat}  module compose foo --definition ""f1: filter | filter""    ==>  f1.expression and filter.expression are available  {noformat}",8
"Allow end users to configure Rabbit MQ properties on the MessageBus (for acks, txs, etc).
This requires exposing properties to the ListenerContainer. Probably cleaner to inject the ListenerContainer into the RabbitMessageBus and expose property placeholders on the LC. Maybe do the same for RabbitAdmin as well. ",1
"Make hdfs configurable via application.yml
We currently pull all hdfs config properties from hdfs.properties - change that to use application.yml",3
"Remove enum for transport options
The enum unnecessarily restricts the ability to add new transport (MessageBus implementations) whereas the config location path is open for extensions since it uses wildcards (in message-bus.xml):    {code}  <import resource=""../../transports/${XD_TRANSPORT}-bus.xml""/>  {code}  ",4
"Replace BeanDefinitionAddingBeanPostProcessor with Ordered Plugins
This will allow us to control the order of plugins and use plugin(s) to manage the common module context, replacing BeanDefinitionAddingBeanPostProcesser",8
"Job Launch should throw exception if the job is not deployed in the container
While processing the ""Job Launch"" request, the ModuleDeployer checks if the job is already deployed, if not it deploys the job before launching it.  This approach causes issue in case of multiple containers environment where the job is deployed in one container(1) but the ""job launch"" request is picked up by other container(2). Because the container(2) that processes ""job launch"" request deploys the job again, it conflicts with an existing job that is deployed in conatiner(1) with the same name in the JobRepository.    Initially, the idea behind 'deploy before launch' was to enable launch shell command to also deploy. Because of the issue mentioned above, it is ok to assume that job launch needs to be performed on an existing deployed job.",1
"Switch to use Jedis driver for Redis
The Spring Data team recommends using the Jedis driver since the Lettuce driver hasn't had any update activity for several months. Jedis is actively maintained.    We might also want to investigate Redisson which is a fork of Lettuce - https://github.com/mrniko/redisson",4
"Remove XDContainer and rename LauncherApplication
Post-boot refactoring. XDContainer lifecycle methods are not being used. Refactor by merging relevant functionality into LauncherApplication. Rename LauncherApplication to ContainerServerApplication (consistent with AdminServerApplication).  ",5
"Publish ContainerStoppedEvent when the container shutsdown
When the container shuts down, ContainerStoppedEvent should be published so that appropriate listeners would act on.  Please refer to this discussion here:  https://github.com/spring-projects/spring-xd/pull/612",2
"Hadoop distro option hdp20 is broken
Starting the shell with --hadoopDistro hdp20 causes this:    Exception in thread ""main"" org.springframework.beans.factory.parsing.BeanDefinitionParsingException: Configuration problem: Unable to locate Spring NamespaceHandler for XML schema namespace [http://www.springframework.org/schema/hadoop]  Offending resource: URL [jar:file:/Users/trisberg/Demo/spring-xd-1.0.0.BUILD-SNAPSHOT/shell/lib/spring-xd-shell-1.0.0.BUILD-SNAPSHOT.jar!/META-INF/spring/spring-shell-plugin.xml]    Creating a stream ""time | hdfs"" in xd-singlenode started with --hadoopDistro hdp20 causes this:    java.lang.IllegalStateException: Can't find class used for type of option 'codec': org.springframework.data.hadoop.store.codec.Codecs  ",3
"Container nodes should write attributes to ZooKeeper
The /xd/containers node is the parent where each Container will write an ephemeral child node. The node name should be the Container's ID, and the node data should be the Container's attributes (host, pid, and much more to be added later).    When a Container shuts down cleanly, it should eagerly delete the ephemeral node so that watchers are notified immediately. For any other case (including a network partition), the ephemeral node will disappear after the timeout elapses.",4
"Remove launcher.xml and [transport]-launcher.xml configuration.
These are no longer used post boot.  ",2
"Json information returned by curl does not reflect deployed status correctly
The Json information returned by curl does not reflect deployed status correctly.  To recreate:  1. Start xd-singlenode  2. start xd-shell  In the xd-shell      (i). stream create --definition ""time | log"" --name ticktock     (ii). stream list  Note the status of the ticktock stream is deployed  3. open a new command prompt & type curl http://localhost:9393/streams/ticktock  4. Note the returned json stream:    {""name"":""ticktock"",""deployed"":null,""definition"":""time | log"",""links"":[{""rel"":""self"",""href"":""http://localhost:9393/streams/ticktock""}]}  5. I would expect the json attribute ""deployed"" to be ""true"", but it is null.",3
"Create an embedded ZooKeeper server process
This will be used by SingleNodeApplication if no ZooKeeper process is available based on the provided client connect string. It will still be recommended that user's run ZooKeeper externally, at least in standalone mode, when running SingleNodeApplication. We should clarify that in the documentation and logs.    It should implement SmartLifecycle so that it can be managed as a bean within an ApplicationContext.",5
"Classpath issue with homebrew version on MacOSX 10.9.1
I've installed spring xd for the first time today on my mac, using the homebrew distribution.  I tried to create the following stream :  {{xd> stream create --definition ""http --port=6666 | log"" --name httptest}}  This produced the following error :   org.springframework.integration.MessageHandlingException: error occurred in message handler [moduleDeployer]   org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:79)   org.springframework.integration.config.ServiceActivatorFactoryBean$1.handleRequestMessage(ServiceActivatorFactoryBean.java:83)   org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:142)   org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:73)   org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:115)   org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:102)   org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)   org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:183)   org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:153)   org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:114)   org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:44)   org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:92)   org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendMessage(AbstractReplyProducingMessageHandler.java:228)   org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendReplyMessage(AbstractReplyProducingMessageHandler.java:212)   org.springframework.integration.handler.AbstractReplyProducingMessageHandler.produceReply(AbstractReplyProducingMessageHandler.java:177)   org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleResult(AbstractReplyProducingMessageHandler.java:171)   org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:149)   org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:73)   org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:115)   org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:102)   org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)   org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:183)   org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:153)   org.springframework.xd.dirt.stream.DeploymentMessageSender.sendDeploymentRequests(DeploymentMessageSender.java:57)   org.springframework.xd.dirt.stream.AbstractDeployer.sendDeploymentRequests(AbstractDeployer.java:163)   org.springframework.xd.dirt.stream.AbstractDeployer.basicDeploy(AbstractDeployer.java:204)   org.springframework.xd.dirt.stream.AbstractInstancePersistingDeployer.deploy(AbstractInstancePersistingDeployer.java:78)   org.springframework.xd.dirt.rest.XDController.save(XDController.java:229)   sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)   sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)   sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)   java.lang.reflect.Method.invoke(Method.java:606)   org.springframework.web.method.support.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:214)   org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:132)   org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:104)   org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandleMethod(RequestMappingHandlerAdapter.java:748)   org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:689)   org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:83)   org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:945)   org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:876)   org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:931)   org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:833)   javax.servlet.http.HttpServlet.service(HttpServlet.java:647)   org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:807)   javax.servlet.http.HttpServlet.service(HttpServlet.java:728)   org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:305)   org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)   org.springframework.boot.actuate.trace.WebRequestTraceFilter.doFilter(WebRequestTraceFilter.java:114)   org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)   org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)   org.springframework.boot.actuate.autoconfigure.EndpointWebMvcAutoConfiguration$1.doFilterInternal(EndpointWebMvcAutoConfiguration.java:126)   org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:108)   org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)   org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)   org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:77)   org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:108)   org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)   org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)   org.springframework.web.filter.HttpPutFormContentFilter.doFilterInternal(HttpPutFormContentFilter.java:88)   org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:108)   org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)   org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)   org.springframework.boot.actuate.autoconfigure.MetricFilterAutoConfiguration$MetricsFilter.doFilter(MetricFilterAutoConfiguration.java:97)   org.springframework.boot.actuate.autoconfigure.MetricFilterAutoConfiguration$MetricsFilter.doFilter(MetricFilterAutoConfiguration.java:82)   org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)   org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)   org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:222)   org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:123)   org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:472)   org.apache.catalina.valves.RemoteIpValve.invoke(RemoteIpValve.java:680)   org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:171)   org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:99)   org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:118)   org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:407)   org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1004)   org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:589)   org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.run(NioEndpoint.java:1680)   java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)   java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)   java.lang.Thread.run(Thread.java:744) Caused by: org.springframework.beans.factory.CannotLoadBeanClassException: Cannot find class [org.springframework.integration.x.http.NettyHttpInboundChannelAdapter] for bean with name 'org.springframework.integration.x.http.NettyHttpInboundChannelAdapter#0' defined in URL [file:/Users/philippe/springsource/spring-xd-1.0.0.M1/xd/modules/source/http.xml]; nested exception is java.lang.ClassNotFoundException: org.springframework.integration.x.http.NettyHttpInboundChannelAdapter   org.springframework.beans.factory.support.AbstractBeanFactory.resolveBeanClass(AbstractBeanFactory.java:1327)   org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.predictBeanType(AbstractAutowireCapableBeanFactory.java:594)   org.springframework.beans.factory.support.AbstractBeanFactory.isFactoryBean(AbstractBeanFactory.java:1396)   org.springframework.beans.factory.support.AbstractBeanFactory.isFactoryBean(AbstractBeanFactory.java:959)   org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:680)   org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:760)   org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:482)   org.springframework.boot.SpringApplication.refresh(SpringApplication.java:609)   org.springframework.boot.SpringApplication.run(SpringApplication.java:321)   org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:130)   org.springframework.xd.module.core.SimpleModule.initialize(SimpleModule.java:181)   org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:264)   org.springframework.xd.dirt.module.ModuleDeployer.deployAndStore(ModuleDeployer.java:254)   org.springframework.xd.dirt.module.ModuleDeployer.handleDeploy(ModuleDeployer.java:249)   org.springframework.xd.dirt.module.ModuleDeployer.handleSingleModuleMessage(ModuleDeployer.java:227)   org.springframework.xd.dirt.module.ModuleDeployer.handleMessageInternal(ModuleDeployer.java:154)   org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:73) 	... 79 more Caused by: java.lang.ClassNotFoundException: org.springframework.integration.x.http.NettyHttpInboundChannelAdapter   org.springframework.boot.context.embedded.tomcat.TomcatEmbeddedWebappClassLoader.loadClass(TomcatEmbeddedWebappClassLoader.java:68)   org.apache.catalina.loader.WebappClassLoader.loadClass(WebappClassLoader.java:1559)   org.springframework.util.ClassUtils.forName(ClassUtils.java:238)   org.springframework.beans.factory.support.AbstractBeanDefinition.resolveBeanClass(AbstractBeanDefinition.java:392)   org.springframework.beans.factory.support.AbstractBeanFactory.doResolveBeanClass(AbstractBeanFactory.java:1348)   org.springframework.beans.factory.support.AbstractBeanFactory.resolveBeanClass(AbstractBeanFactory.java:1319) 	... 95 more  I managed to create the exact same stream successfully after installing the release manually from the zip file and setting up XD_HOME and my path as documented, so I guess it's a problem specific to the homebrew installation.",1
"Make Job notification channels subscribable
Currently, the job notification channels are direct channels. We need to make these pub/sub channels.    With XD-885 (allowing automatic job listeners registration),  this would allow us to create named channel syntax like:    topic:job:myjobname-jobExecution > log  topic:job:myjobname-stepExecution > log     ",5
"Tap XD batch Job output
Currently, job module only has an input channel that receives the job launching requests. If we have an output channel for the job module, we get the following benefits:    1) The output channel reflects the JobExecution from the job launching message handler's reply channel  2) This can be different from JobExecution listener's notification channel where we would get both the beforeJob() and afterJob() notification.  3) We can tap the output channel to send the job results to some other sinks.    Along with this improvement, planning to do some more refactoring on the Plugins so that some of the common implementation methods are handled in the base class.  ",5
"Upgrade to SHDP 2.0 M6
The YARN support in M6 changes most of the config properties, need to update XD to use new ones.",5
"StreamDeployer.deleteAll() does not handle dependency tracking
create a composed module, use it in a stream, delete ALL streams.  Try to delete the composed module => fails thinking that it's still used by the stream    ",2
"Unable to destroy stream when using http source
When destroying a stream that contains an http source, an exception is thrown.  Thus even though the stream is destroyed all resources are not released i.e. port 9000 is still in use.    NettyHttpInboundChannelAdapter is currently setup with child.tcpNoDelay set to true.  And when running on my system and on EC2 the http needs more time to release the port.    My recommendation is to set bootstrap.setOption(""child.tcpNoDelay"", false) instead of true. ",3
"Exclude commons-logging from final distro1
xd/lib includes jcl-over-slf4j so we don't need additional commons-logging jars in the modules or extensions:  https://github.com/spring-projects/spring-xd/pull/610",1
"Refactor container to remove shared module context as a separate context 
The main container context becomes the shared context for modules.",8
"Using hdfs sink throwing an error
I am trying to use HDFS as sink while creating streams and I am encountering the following error :  Please refer attached document : Exception - localhost - 8020.txt   The fs.default.name set in hadoop.properties is : fs.default.name=hdfs://localhost:8020  I have also tried the following variation in the hadoop.properties file : fs.default.name=hdfs://127.0.0.2:8020 fs.default.name=hdfs://127.0.0.2:50070  Using these values are also throwing me exceptions as mentioned in the following files attached :  Exception - 127.0.0.2 - 8020.txt  Exception - 127.0.0.2 - 50070.txt   We are using : Pivotal HD 1.0.1 spring-xd-1.0.0.M5-dist  Kindly let us know the way around for this issue.   ",5
"Serialization over data transport fails for classes that are module specific
Given:    the counter example from the guide is run with redis enabled:  xd-singlenode --transport redis --store redis  When:   a stream is created  stream create --name springtweets --definition ""twittersearch --consumerKey=<your_key> --consumerSecret=<your_secret> --query=spring | file --dir=/tweets/""  Then:  An exception is thrown:  Exception in thread ""inbound.springtweets.0-redis:queue-inbound-channel-adapter35"" org.springframework.integration.MessageHandlingException: error occurred in message handler [springtweets.0.convert.bridge]   org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:79)   org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:115)   org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:102)   org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)   org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:178)   org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:149)   org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:94)   org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:42)   org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:86)   org.springframework.integration.endpoint.MessageProducerSupport.sendMessage(MessageProducerSupport.java:92)   org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.popMessageAndSend(RedisQueueMessageDrivenEndpoint.java:207)   org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.access$300(RedisQueueMessageDrivenEndpoint.java:51)   org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint$ListenerTask.run(RedisQueueMessageDrivenEndpoint.java:286)   java.lang.Thread.run(Thread.java:724) Caused by: org.springframework.integration.x.bus.serializer.SerializationException: unable to deserialize [null]. Class not found.   org.springframework.integration.x.bus.MessageBusSupport.deserializeConsumerPayload(MessageBusSupport.java:247)   org.springframework.integration.x.bus.MessageBusSupport.transformPayloadForConsumer(MessageBusSupport.java:191)   org.springframework.integration.x.bus.MessageBusSupport.transformPayloadForConsumerIfNecessary(MessageBusSupport.java:168)   org.springframework.integration.x.redis.RedisMessageBus.access$300(RedisMessageBus.java:57)   org.springframework.integration.x.redis.RedisMessageBus$ReceivingHandler.handleRequestMessage(RedisMessageBus.java:176)   org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:142)   org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:73) 	... 13 more Caused by: java.lang.ClassNotFoundException: org.springframework.social.twitter.api.Tweet   java.net.URLClassLoader$1.run(URLClassLoader.java:366)   java.net.URLClassLoader$1.run(URLClassLoader.java:355)   java.security.AccessController.doPrivileged(Native Method)   java.net.URLClassLoader.findClass(URLClassLoader.java:354)   java.lang.ClassLoader.loadClass(ClassLoader.java:424)   sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)   java.lang.ClassLoader.loadClass(ClassLoader.java:357)   java.lang.Class.forName0(Native Method)   java.lang.Class.forName(Class.java:190)   org.springframework.integration.x.bus.MessageBusSupport.deserializeConsumerPayload(MessageBusSupport.java:241) 	... 19 more",3
"Clarify API or syntax for managing deployment parameters
Suppose we have 3 environements of Spring XD : - Dev environment  - Test environment  - Prod environement (  Suppose whe develop the script bellow: ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////// stream1 = http | filter --expression=payload.contains('toto') | file --dir=/tmp/toto  stream2 = http | filter --expression=payload.contains('titi') | file --dir=/tmp/titi //////////////////////////////////////////////////////////////////////////////////////////////////////////////////////  When we need to deploy the script in Test and Prod environements , we must modify ""dir"" option of ""file"" sink. This is very easy when there is not a lot of options and when we have a small factory team. But in a big factory environment this will be problematic.   In order to industrialize deployment, it would be convenient to implement in DSL a directory interface API or something equivalent like below:  Suppose we call this directory interface XDDI ... like ""XD Directory Interface"" :-)  The script can be like that: ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////// stream1 = http | filter --expression=payload.contains('toto') | file --dir=XDDI('totoKey')  stream2 = http | filter --expression=payload.contains('titi') | file --dir=XDDI('titiKey')  //////////////////////////////////////////////////////////////////////////////////////////////////////////////////////  The XDDI keys are defined in a centralized directory interface (admin console or XDDI.properties)  The XDDI keys/values in Dev environment: ///////////////////////////////////// totoKey=/tmp/toto titiKey=/tmp/titi /////////////////////////////////////  The XDDI keys/values in Test environment: ////////////////////////////////////////// totoKey=/tartempion/toto titiKey=/petaouchnok/titi /////////////////////////////////////////  The XDDI keys/values in Prod environment: ///////////////////////////////////////////////////// totoKey=/vavoirlabasijysuis/toto titiKey=/vavoirlabasijysuis/titi /////////////////////////////////////////////////////  When the script is deployed in Test or Prod environement, if the script contain a key that is not defined in centralized directory, the deployment fail.   This will reduce errors risks in a big factory environnement (several hundred parameters and signifiant team turnover).  ",3
"HSQL always started, even when using other database
I set the config/xd-config.yml properties to use MySQL including this    â€‚profiles:  â€‚â€‚â€‚â€‚active: default,mysql    When XD ADmin starts I still see HSQL server started and localhost:9393/env shows:    ""profiles"": [  â€‚â€‚â€‚â€‚""adminServer"",  â€‚â€‚â€‚â€‚""hsqldb"",  â€‚â€‚â€‚â€‚""default""  ],  ",5
"Update to use spring-data-hadoop 2.0 M6
nan",5
"Create spike of web app that maps UI design docs to MVC components in Angluar
nan",10
"XD Shell crashes when the stream DSL has ""!""
The XD shell crashes when the following command issued:    stream create test --definition ""http | filter --expression=!payload.contains('test') | log""    It looks like the JLine ConsoleReader's expandEvents is set to true by default and this causes the issue:    Exception in thread ""Spring Shell"" java.lang.IllegalArgumentException: !payload.contains('test') | log"": event not found    jline.console.ConsoleReader.expandEvents(ConsoleReader.java:734)    jline.console.ConsoleReader.finishBuffer(ConsoleReader.java:604)    jline.console.ConsoleReader.accept(ConsoleReader.java:1912)    jline.console.ConsoleReader.readLine(ConsoleReader.java:2537)    jline.console.ConsoleReader.readLine(ConsoleReader.java:2162)    jline.console.ConsoleReader.readLine(ConsoleReader.java:2150)    org.springframework.shell.core.JLineShell.promptLoop(JLineShell.java:517)    org.springframework.shell.core.JLineShell.run(JLineShell.java:178)    java.lang.Thread.run(Thread.java:722)  ",2
"Rename ""node"" references to ""container""
This applies to a few places (when addressing the issue, a search should be done to uncover any others), e.g.:    ContainerServerApplication:      public static final String NODE_PROFILE = ""node"";    *Options classes:       ""The transport to use for data messages (from node to node)""       ""The transport to use for control messages (between admin and nodes)""  ",4
"Change module placeholder names and remove context:property-placeholder usage
e.g. rabbit.xml source.    	<context:property-placeholder  location=""${xd.config.home}/${configProperties:rabbit}.properties""  ignore-resource-not-found=""true"" />    would be removed and    	<rabbit:connection-factory id=""rabbitConnectionFactory"" host=""${host:${spring.rabbitmq.host:localhost}}""  port=""${port:${spring.rabbitmq.port:5672}}"" virtual-host=""${vhost:${spring.rabbitmq.virtualHost:/}}""  username=""${username:${spring.rabbitmq.username:guest}}"" password=""${password:${spring.rabbitmq.password:guest}}""/>    would look like  port=""${port}"" and a property file in the module directory or POJO in the module lib would specify the default value of the port.    For POJO it would be  options_class = org.springframework.xd.dirt.modules.metadata.RabbitSourceOptionsMetadata    For property file it would be  option.port.default=5672  option.port.description=""cool port number""    This needs to be consistently done across all the modules.              ",3
"Avoid false negative test failures related to HSQLDB
When executing tests and failures occur (should be easy to simulate with a forced failure), several other side-effect failures occur with the following error:    {code}  Caused by: java.lang.IllegalArgumentException: HSQLDB could not be started. Maybe another instance is already running on 0.0.0.0:13583 ?  {code}    It seems that the failing tests are not properly cleaning up, so the HSQLDB instance they started is still running thereby causing the other failures.    In the case I last witnessed this on a dev branch, I had a valid failure in LocalControlRedisDataInitializationTests (the environmentMatchesTransport method, specifically) and TypeConvertingStreamTests. So, perhaps a good way to start exploring this issue is to simulate a failure in one or both of those classes. The false negatives I'm seeing are in the following:    {code}  LocalSingleNodeInitializationTests.environmentMatchesTransport  RabbitSingleNodeInitializationTests.environmentMatchesTransport  RedisSingleNodeInitializationTests.environmentMatchesTransport  FileSourceModuleTests.classMethod  LocalSingleNodeStreamDeploymentIntegrationTests.classMethod  RabbitSingleNodeStreamDeploymentIntegrationTests.classMethod  RabbitSingleNodeStreamDeploymentIntegrationTests.classMethod  {code}    They all seem to be tests that start the SingleNodeApplication.",4
"Can't create http source while TCP is used as a source and sink on singlenode
[Problem] Can't use tcp source, sink and http together on Single Node.  While creating tests for CI I tried to create the following: [Steps to Reproduce] xd:>stream create fooOut --definition ""tcp|file"" Created new stream 'fooOut' xd:>stream create fooIn --definition ""http --port=9002|tcp"" Command failed org.springframework.xd.rest.client.impl.SpringXDException: Failed to bind to: 0.0.0.0/0.0.0.0:9000. Possibly the port is already in use. Even if I use different ports for the tcp I still get failures pointing to 9000. [Extra Notes] The stream below is works. xd:>stream create fooOut --definition ""tcp|file"" Created new stream 'fooOut' xd:>stream create fooIN --definition ""time|tcp""  *Stack Trace Attached*",5
"XD does not release ports on failed stream creates.
If stream fails to create while using sources or sinks that need ports.  Ports are not released.  If while creating a stream you see: ""Command failed org.springframework.xd.rest.client.impl.SpringXDException: Failed to bind to: 0.0.0.0/0.0.0.0:9000. Possibly the port is already in use."" then destroy the stream (or all streams for that matter) you will not be able to create another stream that uses that port.    The work around is to cycle the container.",5
"tcp source requires a \r\n to suffix all inbound data
When sending data to the TCP Source if the data is not terminated with a \r\n when the socket is closed by the client, XD throws an exception.  ",1
"Add acceptance tests for stream with sources of TCP, HTTP, and Time and sinks of File and Log
Need to be able to test the following sources:  TCP, HTTP, Time,",5
"Improve job launch functionality with distributed nodes
When sending a launch request, the message is not targeted to the container node that hosts the deployed job.  With RabbitMQ, the message is not ack'd so it will get picked up eventually by the container that hosts the deployed job.  This should change to a targeted message.  ----- Original description from Thomas below  Tried deploying some batch jobs and they all seem to fail when running admin and one container using redis as transport  xd:>job create mongojob --definition ""hdfsmongodb --resources=/data/*.log --names=col1,col2,col3 --idField=col1 --collectionName=test""  fails with this: {quote} 18:19:01,612  WARN redisInboundAdapter-redis:queue-inbound-channel-adapter6 boot.SpringApplication:635 - Error handling failed (Error creating bean with name 'integrationRequestMappingHandlerMapping': Initialization of bean failed; nested exception is java.lang.IllegalStateException: org.springframework.context.annotation.AnnotationConfigApplicationContext@25e7506f has not been refreshed yet) 18:19:01,614 ERROR redisInboundAdapter-redis:queue-inbound-channel-adapter6 handler.LoggingHandler:145 - org.springframework.messaging.MessageHandlingException: error occurred in message handler [moduleDeployer]   org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:84)   org.springframework.integration.config.ServiceActivatorFactoryBean$1.handleRequestMessage(ServiceActivatorFactoryBean.java:83)   org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:170)   org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)   org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:128)   org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)   org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)   org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)   org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:255)   org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:223)   org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:114)   org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:44)   org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:92)   org.springframework.integration.endpoint.MessageProducerSupport.sendMessage(MessageProducerSupport.java:98)   org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.popMessageAndSend(RedisQueueMessageDrivenEndpoint.java:211)   org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.access$300(RedisQueueMessageDrivenEndpoint.java:50)   org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint$ListenerTask.run(RedisQueueMessageDrivenEndpoint.java:290)   org.springframework.integration.util.ErrorHandlingTaskExecutor$1.run(ErrorHandlingTaskExecutor.java:52)   java.lang.Thread.run(Thread.java:724) Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'readResourcesStep': Cannot create inner bean '(inner bean)' of type [org.springframework.batch.core.listener.StepListenerFactoryBean] while setting bean property 'listeners' with key [0]; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name '(inner bean)#4': FactoryBean threw exception on object creation; nested exception is java.lang.IllegalArgumentException: interface org.springframework.batch.core.StepListener is not visible from class loader   org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveInnerBean(BeanDefinitionValueResolver.java:282)   org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:126)   org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveManagedList(BeanDefinitionValueResolver.java:351)   org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:154)   org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyPropertyValues(AbstractAutowireCapableBeanFactory.java:1456)   org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1197)   org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:537)   org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:475)   org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:304)   org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:228)   org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:300)   org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:195)   org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:681)   org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:760)   org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:482)   org.springframework.boot.SpringApplication.refresh(SpringApplication.java:616)   org.springframework.boot.SpringApplication.run(SpringApplication.java:306)   org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:130)   org.springframework.xd.module.core.SimpleModule.initialize(SimpleModule.java:225)   org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:270)   org.springframework.xd.dirt.module.ModuleDeployer.deployAndStore(ModuleDeployer.java:260)   org.springframework.xd.dirt.module.ModuleDeployer.handleDeploy(ModuleDeployer.java:201)   org.springframework.xd.dirt.module.ModuleDeployer.handleMessageInternal(ModuleDeployer.java:172)   org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78) 	... 18 more Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name '(inner bean)#4': FactoryBean threw exception on object creation; nested exception is java.lang.IllegalArgumentException: interface org.springframework.batch.core.StepListener is not visible from class loader   org.springframework.beans.factory.support.FactoryBeanRegistrySupport.doGetObjectFromFactoryBean(FactoryBeanRegistrySupport.java:151)   org.springframework.beans.factory.support.FactoryBeanRegistrySupport.getObjectFromFactoryBean(FactoryBeanRegistrySupport.java:110)   org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveInnerBean(BeanDefinitionValueResolver.java:272) 	... 41 more Caused by: java.lang.IllegalArgumentException: interface org.springframework.batch.core.StepListener is not visible from class loader   java.lang.reflect.Proxy.getProxyClass0(Proxy.java:487)   java.lang.reflect.Proxy.newProxyInstance(Proxy.java:722)   org.springframework.aop.framework.JdkDynamicAopProxy.getProxy(JdkDynamicAopProxy.java:121)   org.springframework.aop.framework.JdkDynamicAopProxy.getProxy(JdkDynamicAopProxy.java:111)   org.springframework.aop.framework.ProxyFactory.getProxy(ProxyFactory.java:98)   org.springframework.batch.core.listener.AbstractListenerFactoryBean.getObject(AbstractListenerFactoryBean.java:163)   org.springframework.beans.factory.support.FactoryBeanRegistrySupport.doGetObjectFromFactoryBean(FactoryBeanRegistrySupport.java:144) 	... 43 more {quote}  I'll post more errors as I collect them",8
"Add JDBC Sink to acceptance tests
nan",10
"Add option to xd-admin and xd-container YARN scripts to allow copying ot HDFS and no execution.
nan",4
"Add documentation on how to deploy XD to YARN
nan",5
"Upgrade to Spring Batch 3.0.0 M3
nan",2
"Sometimes getting NPE when master step runs for ftphdfs job
Depending in the ftp server used there seems to be an error condition that generates an NullPointerException.     These are the steps to reproduce this:    {code}  job create --name myftphdfs --definition ""ftphdfs --host=ftp.sunet.se --port=21""  job launch --name myftphdfs --params {""remoteDirectory"":""/pub/music/Abba"",""hdfsDirectory"":""/xd/ftp""}  {code}    Exception:  {code}  16:31:38,385 ERROR redisInboundAdapter-redis:queue-inbound-channel-adapter1 step.AbstractStep:225 - Encountered an error executing the step  java.lang.NullPointerException    org.springframework.batch.integration.partition.MessageChannelPartitionHandler.handle(MessageChannelPartitionHandler.java:140)    org.springframework.batch.core.partition.support.PartitionStep.doExecute(PartitionStep.java:105)    org.springframework.batch.core.step.AbstractStep.execute(AbstractStep.java:198)    org.springframework.batch.core.job.SimpleStepHandler.handleStep(SimpleStepHandler.java:144)    org.springframework.batch.core.job.flow.JobFlowExecutor.executeStep(JobFlowExecutor.java:64)    org.springframework.batch.core.job.flow.support.state.StepState.handle(StepState.java:67)    org.springframework.batch.core.job.flow.support.SimpleFlow.resume(SimpleFlow.java:163)    org.springframework.batch.core.job.flow.support.SimpleFlow.start(SimpleFlow.java:142)    org.springframework.batch.core.job.flow.FlowJob.doExecute(FlowJob.java:134)    org.springframework.batch.core.job.AbstractJob.execute(AbstractJob.java:304)    org.springframework.batch.core.launch.support.SimpleJobLauncher$1.run(SimpleJobLauncher.java:135)    org.springframework.core.task.SyncTaskExecutor.execute(SyncTaskExecutor.java:50)    org.springframework.batch.core.launch.support.SimpleJobLauncher.run(SimpleJobLauncher.java:128)    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)    java.lang.reflect.Method.invoke(Method.java:606)    org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)    org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)    org.springframework.batch.core.configuration.annotation.SimpleBatchConfiguration$PassthruAdvice.invoke(SimpleBatchConfiguration.java:117)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)    org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)    com.sun.proxy.$Proxy40.run(Unknown Source)    org.springframework.batch.integration.launch.JobLaunchingMessageHandler.launch(JobLaunchingMessageHandler.java:50)    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)    java.lang.reflect.Method.invoke(Method.java:606)    org.springframework.expression.spel.support.ReflectiveMethodExecutor.execute(ReflectiveMethodExecutor.java:63)    org.springframework.expression.spel.ast.MethodReference.getValueInternal(MethodReference.java:122)    org.springframework.expression.spel.ast.MethodReference.access$100(MethodReference.java:44)    org.springframework.expression.spel.ast.MethodReference$MethodValueRef.getValue(MethodReference.java:258)    org.springframework.expression.spel.ast.CompoundExpression.getValueInternal(CompoundExpression.java:85)    org.springframework.expression.spel.ast.SpelNodeImpl.getTypedValue(SpelNodeImpl.java:113)    org.springframework.expression.spel.standard.SpelExpression.getValue(SpelExpression.java:111)    org.springframework.integration.util.AbstractExpressionEvaluator.evaluateExpression(AbstractExpressionEvaluator.java:163)    org.springframework.integration.util.MessagingMethodInvokerHelper.processInternal(MessagingMethodInvokerHelper.java:268)    org.springframework.integration.util.MessagingMethodInvokerHelper.process(MessagingMethodInvokerHelper.java:142)    org.springframework.integration.handler.MethodInvokingMessageProcessor.processMessage(MethodInvokingMessageProcessor.java:75)    org.springframework.integration.handler.ServiceActivatingHandler.handleRequestMessage(ServiceActivatingHandler.java:71)    org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:170)    org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)    org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:128)    org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)    org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)    org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)    org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:255)    org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:223)    org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:114)    org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:44)    org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:92)    org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendMessage(AbstractReplyProducingMessageHandler.java:260)    org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendReplyMessage(AbstractReplyProducingMessageHandler.java:241)    org.springframework.integration.handler.AbstractReplyProducingMessageHandler.produceReply(AbstractReplyProducingMessageHandler.java:205)    org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleResult(AbstractReplyProducingMessageHandler.java:199)    org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:177)    org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)    org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:128)    org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)    org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)    org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)    org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:255)    org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:223)    org.springframework.xd.dirt.plugins.job.JobPlugin.launch(JobPlugin.java:176)    org.springframework.xd.dirt.module.ModuleDeployer.launchModule(ModuleDeployer.java:380)    org.springframework.xd.dirt.module.ModuleDeployer.processLaunchRequest(ModuleDeployer.java:330)    org.springframework.xd.dirt.module.ModuleDeployer.handleLaunch(ModuleDeployer.java:316)    org.springframework.xd.dirt.module.ModuleDeployer.handleMessageInternal(ModuleDeployer.java:169)    org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)    org.springframework.integration.config.ServiceActivatorFactoryBean$1.handleRequestMessage(ServiceActivatorFactoryBean.java:83)    org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:170)    org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)    org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:128)    org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)    org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)    org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)    org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:255)    org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:223)    org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:114)    org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:44)    org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:92)    org.springframework.integration.endpoint.MessageProducerSupport.sendMessage(MessageProducerSupport.java:98)    org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.popMessageAndSend(RedisQueueMessageDrivenEndpoint.java:211)    org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.access$300(RedisQueueMessageDrivenEndpoint.java:50)    org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint$ListenerTask.run(RedisQueueMessageDrivenEndpoint.java:290)    org.springframework.integration.util.ErrorHandlingTaskExecutor$1.run(ErrorHandlingTaskExecutor.java:52)    java.lang.Thread.run(Thread.java:724)  {code}",5
"Investigate missing stepExecutions in JobRepository.getLastJobExecution() 
When the job is run with its jobParameters by SimpleJobLauncher, its lastJobExecution's stepExecutions are checked for UNKNOWN status to throw JobRestartException.  It looks like the stepExecutions for the lastJobExecution are never set and the collection 'stepExecutions' is not fetched from job repository.    Hence, not sure if the following condition in SimpleJobLauncher's run(final Job job, final JobParameters jobParameters)  would ever get executed:    for (StepExecution execution : lastExecution.getStepExecutions()) {  				if (execution.getStatus() == BatchStatus.UNKNOWN) {  					//throw  					throw new JobRestartException(""Step ["" + execution.getStepName() + ""] is of status UNKNOWN"");  				}//end if  			}//end for",3
"Rewrite of the existing UI to use AngluarJS
nan",40
"Clean up mismatches and bugs in how mdoule options and server configuration is handled.
nan",20
"Support explicit client connect string for ZooKeeper
This will have an effect on both singlenode and distributed mode as   described below...    *singlenode:*    Currently, when running in singlenode mode, an embedded ZooKeeper server is started, and it discovers an available port for client connections and exposes it via a getClientPort() method.    With this change, IF a client connect string is provided explicitly, the embedded ZooKeeper server should not be started, but instead the client connection should be established using that explicitly provided connect string. SEE the ""todo"" comment above the `SpringApplicationBuilder admin` line within `SingleNodeApplication`.    *distributed:*    Currently, when running in distributed mode, the client connect string is hardcoded to `localhost:2181`. Obviously that is only a short term solution since a real-world distributed runtime would span multiple machines (including the ZooKeeper ensemble itself). That is also why the connect string must support a comma-delimited list of `host:port` values (for redundancy), but the constructor for `ZooKeeperConnection` that accepts a client connect string already supports that via the underlying `CuratorFramework` builder.    With this change, a command-line argument for the client connect string will be passed into the `ZooKeeperConnection` constructor, within the configuration (e.g. SEE the ""todo"" statement within `ContainerServerApplication.DistributedZooKeeperConnectionConfiguration`).",4
"Upgrade to Spring 4.0.2.RELEASE
nan",1
"Container and Single Node do not update their associated log
After the container/singlenode is up and running, the application does not update the singlenode.log or containernode.log.  create a stream ""time|log"".  you will see the timestamp in your console, but it will not be in the log.",5
"Container fails to start if JMX is enabled and manage_port is set
   The container will not start with JMX enabled and the management_port set.  The stacktrace is attached and the settings for the container are enumerated below:      export endpoints_jmx_enabled=true     export endpoints_jmx_uniqueNames=true     export endpoints_jolokia_enabled=true     export XD_JMX_ENABLED=true     export management_port=15005",5
"Admin leader should watch Container nodes in ZooKeeper
This will require a ZooKeeperConnection in AdminServerApplication, based on singlenode vs. distributed (via profiles, see: ContainerServerApplication for an example).    Then a PathChildrenCache should be established for the /xd/containers node.",4
"Admin servers should write streams to and delete them from ZooKeeper
This should also enable removal of any StreamDefinitionRepository code.    The state should be written as a data node at the stream level (e.g. /xd/streams/mystream {state=...})    For now we at least need to support the boolean --deploy=true|false flag. If that is true, then the leader Admin will deploy the modules of the stream across available containers (XD-1399)",8
"Admin leader should watch ZooKeeper for Stream deployment requests
The Stream deployment requests will be written to /xd/streams/streamname and the data on that node will include the state or a boolean indicator (for now) of whether it should be deployed.    When a Stream is deployed, the leader will consult its Container cache and write the modules to the various /xd/deployments child nodes (see XD-1400).",8
"Containers should listen for Module deployment requests and their deletions
The Admin leader will write each Module deployment request to a child node of /xd/deployments for a selected Container (see XD-1399). That Container-specific (persistent) child node needs to be created by the Container at the same time as it creates its ephemeral node under /xd/containers.    The Container should then deploy the Module. If that same node is subsequently deleted, the Container should undeploy the Module.",8
"Add new reactor tcp module
A reactor based TCP module that would support some basic CODECS.    Should evaluate if this new TCP module would subsume the current reactor-syslog module functionality or if the reactor-syslog module should be enhanced/upgradted.",8
"Create a â€˜throughput-samplerâ€™ module for benchmarking
A module that could be used in a stream definition such as     reactor --bind tcp://0.0.0.0:3000/length?codec=bytes | do-stuff | throughput-sampler    where throughput-sampler could start measurements once a key 'START' is found in a Message and stop when the key 'STOP' is found in a Message, listing the number of msgs/sec etc.",4
"Create benchmarking application to demonstrate high performance message processing
The application should live in  in spring-xd-samples repository.    The stream created in https://jira.spring.io/browse/XD-1402 should be documented how to run a benchmark and made easy to execute.  Can use ruby/bash-awk-sed to generate traffic via sendfile in order to saturate the stream.  ",10
"Test against Spring Boot Snapshot build
nan",4
"Version XD dependencies with Spring IO platform versioning
All the XD artifacts dependencies need to be updated with the versions that in line with Spring IO platform's standard versioning.  ",10
"Upgrade groovy version to 2.2.2
Currently, XD hast ""testCompile"" dependency to use org.codehaus.groovy:groovy-all:2.2.1.   The spring-integration-groovy uses ""2.2.2"" and it is what spring IO platform uses as well. To keep it all same, we can change this ""testCompile"" dependency to use ""2.2.2""",1
"Create a throughput sink
The throughput module would expect a payload of the type Message<byte[]> and look for the byte[] to be START or STOP strings to trigger a throughput measurement.    https://github.com/spring-projects/spring-xd/tree/master/extensions would be the place for the module to live.",5
"Container ID is not equal to its application context ID
The ContainerServerApplication has a bean ContainerMetaData whose ID is equal to the ContainerServerApplication context's ID.   The ContainerMetadata is alway referred (using @Autowired) in ContainerConfiguration class and this injects a new bean whose containerID is different from that of the containerID created at ContainerServerApplication. Also, this is the ID that gets stored as the container ID in znode for /xd/containers by ContainerRegistrar.  We should avoid having duplicate container metadata and the container ID needs to be same as that of the ContainerServerApplication's context ID.",3
"XD EC2 needs to bootstrap ZOOKEEPER at installation time.
Startup zookeeper on EC2 cluster instances.",5
"Create xd-yarn script
Create an xd-yarn script that is more ""Cloud Foundry"" like -     xd-yarn push -p <path-to-unzipped-yearn-distro>  xd-yarn start admin  xd-yarn start container  ",5
"Composed options does not trigger profile activation
I was in the process of rewriting transform using profiles (see ExpressionOrScriptMixin).  This broke eg ModuleCommandTests.testComposedModulesValuesInDefinition because basically composed module options activate no profile.  The problem is that there is no real way to know what to activate currently, because when deploying a part of a composed module, its metadata is actually a link to the *whole* metadata, but does not really know which part.  Long story short, something we may be able to do is to activate the union of all profiles, but this breaks very easily : module compose foo --definition ""transform --expr=foo | transform --script=bar""  would try to activate both ""script"" and ""expression"" profiles for both modules.",5
"Create spring-xd-test-fixtures project
Acceptance tests has a direct library reference to spring-xd-shell.  This causes problems with eclipse.  It needs a intermediate main project to resolve the dependencies.  This is based on the conversation I had with Mark Fisher.    you cant depend on another projects' src/test  what is needed is an intermediate projectâ€¦ some test support project  that project would depend on spring-xd-shell (and others) â€¦ but then the spring-xd-integration-test project would depend on the intermediate one  lib dependencies should only be under src/main  src/test is intended to be scoped to the project it sits inâ€¦ tests FOR that project - not reusable base classes, etc",4
"Create one xd-yarn shell script that encompases the functionality of seperate shell scripts
nan",3
"Inconsistent test failure with mqtt script test in CI environment
mqtt_tests under src/tests/scripts fail inconsistently with the following exception stacktrace:  SEVERE: xd.mqtt.client.id.src: Timed out as no activity, keepAlive=60,000 lastOutboundActivity=1,395,174,955,434 lastInboundActivity=1,395,174,895,434 13:36:55,442 ERROR http-nio-9393-exec-7 inbound.MqttPahoMessageDrivenChannelAdapter:66 - Exception while connecting and subscribing, retrying Client is currently disconnecting (32102)   org.eclipse.paho.client.mqttv3.internal.ExceptionHelper.createMqttException(ExceptionHelper.java:27)   org.eclipse.paho.client.mqttv3.internal.ClientComms.disconnect(ClientComms.java:409)   org.eclipse.paho.client.mqttv3.MqttAsyncClient.disconnect(MqttAsyncClient.java:524)   org.eclipse.paho.client.mqttv3.MqttClient.disconnect(MqttClient.java:250)   org.eclipse.paho.client.mqttv3.MqttClient.disconnect(MqttClient.java:243)   org.springframework.integration.mqtt.inbound.MqttPahoMessageDrivenChannelAdapter.connectAndSubscribe(MqttPahoMessageDrivenChannelAdapter.java:104)   org.springframework.integration.mqtt.inbound.MqttPahoMessageDrivenChannelAdapter.doStart(MqttPahoMessageDrivenChannelAdapter.java:63)   org.springframework.integration.endpoint.AbstractEndpoint.start(AbstractEndpoint.java:84)   org.springframework.context.support.DefaultLifecycleProcessor.doStart(DefaultLifecycleProcessor.java:173)   org.springframework.context.support.DefaultLifecycleProcessor.access$200(DefaultLifecycleProcessor.java:51)   org.springframework.context.support.DefaultLifecycleProcessor$LifecycleGroup.start(DefaultLifecycleProcessor.java:346)   org.springframework.context.support.DefaultLifecycleProcessor.startBeans(DefaultLifecycleProcessor.java:149)   org.springframework.context.support.DefaultLifecycleProcessor.start(DefaultLifecycleProcessor.java:91)   org.springframework.context.support.AbstractApplicationContext.start(AbstractApplicationContext.java:1180)   org.springframework.xd.module.core.SimpleModule.start(SimpleModule.java:270)   org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:250)   org.springframework.xd.dirt.module.ModuleDeployer.deployAndStore(ModuleDeployer.java:238)   org.springframework.xd.dirt.module.ModuleDeployer.handleDeploy(ModuleDeployer.java:179)   org.springframework.xd.dirt.module.ModuleDeployer.handleMessageInternal(ModuleDeployer.java:150)   org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)   org.springframework.integration.config.ServiceActivatorFactoryBean$1.handleRequestMessage(ServiceActivatorFactoryBean.java:83)   org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:170)   org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)   org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)   org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)   org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)   org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)   org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:255)   org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:223)   org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:114)   org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:44)   org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:93)   org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendMessage(AbstractReplyProducingMessageHandler.java:260)   org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendReplyMessage(AbstractReplyProducingMessageHandler.java:241)   org.springframework.integration.handler.AbstractReplyProducingMessageHandler.produceReply(AbstractReplyProducingMessageHandler.java:205)   org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleResult(AbstractReplyProducingMessageHandler.java:199)   org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:177)   org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)   org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)   org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)   org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)   org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)   org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:255)   org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:223)   org.springframework.xd.dirt.stream.DeploymentMessageSender.sendDeploymentRequests(DeploymentMessageSender.java:57)   org.springframework.xd.dirt.stream.AbstractDeployer.sendDeploymentRequests(AbstractDeployer.java:163)   org.springframework.xd.dirt.stream.AbstractDeployer.basicDeploy(AbstractDeployer.java:204)   org.springframework.xd.dirt.stream.AbstractInstancePersistingDeployer.deploy(AbstractInstancePersistingDeployer.java:78)   org.springframework.xd.dirt.rest.XDController.deploy(XDController.java:142)   sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)   sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)   sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)   java.lang.reflect.Method.invoke(Method.java:616)   org.springframework.web.method.support.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:215)   org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:132)   org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:104)   org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandleMethod(RequestMappingHandlerAdapter.java:749)   org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:690)   org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:83)   org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:945)   org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:876)   org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:961)   org.springframework.web.servlet.FrameworkServlet.doPut(FrameworkServlet.java:874)   javax.servlet.http.HttpServlet.service(HttpServlet.java:650)   org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:837)   javax.servlet.http.HttpServlet.service(HttpServlet.java:728)   org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:305)   org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)   org.springframework.boot.actuate.trace.WebRequestTraceFilter.doFilter(WebRequestTraceFilter.java:114)   org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)   org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)   org.springframework.boot.actuate.autoconfigure.EndpointWebMvcAutoConfiguration$ApplicationContextFilterConfiguration$1.doFilterInternal(EndpointWebMvcAutoConfiguration.java:128)   org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:108)   org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)   org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)   org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:77)   org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:108)   org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)   org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)   org.springframework.web.filter.HttpPutFormContentFilter.doFilterInternal(HttpPutFormContentFilter.java:85)   org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:108)   org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)   org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)   org.springframework.boot.actuate.autoconfigure.MetricFilterAutoConfiguration$MetricsFilter.doFilterInternal(MetricFilterAutoConfiguration.java:84)   org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:108)   org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)   org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)   org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:222)   org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:123)   org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:472)   org.apache.catalina.valves.RemoteIpValve.invoke(RemoteIpValve.java:680)   org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:171)   org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:99)   org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:118)   org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:407)   org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1004)   org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:589)   org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.run(NioEndpoint.java:1680)   java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)   java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)   java.lang.Thread.run(Thread.java:679) 13:36:55,465  INFO http-nio-9393-exec-7 module.ModuleDeployer:240 - deployed SimpleModule [name=mqtt, type=source, group=mqttSourceTest, index=0 @38946002] HTTP/1.1 200 OK Server: MochiWeb/1.1 WebMachine/1.10.0 (never breaks eye contact) Date: Tue, 18 Mar 2014 20:36:55 GMT content-type: application/json Content-Length: 16 Cache-Control: no-cache  {""routed"":false}cat: /tmp/xdtest/basic/mqttSourceTest.out: No such file or directory bamboo@w1-kodiak-hd006:/data/bamboo-home/xml-data/build-dir/XD-SCRIPTS-RS/spring-xd/src/test/scripts$ Mar 18, 2014 1:38:55 PM org.eclipse.paho.client.mqttv3.internal.ClientState checkForActivity SEVERE: xd.mqtt.client.id.src: Timed out as no activity, keepAlive=60,000 lastOutboundActivity=1,395,175,075,488 lastInboundActivity=1,395,175,015,488 13:38:55,490 ERROR task-scheduler-2 inbound.MqttPahoMessageDrivenChannelAdapter:141 - Exception while connecting and subscribing Client is disconnected (32101)   org.eclipse.paho.client.mqttv3.internal.ExceptionHelper.createMqttException(ExceptionHelper.java:27)   org.eclipse.paho.client.mqttv3.internal.ClientComms.disconnect(ClientComms.java:405)   org.eclipse.paho.client.mqttv3.MqttAsyncClient.disconnect(MqttAsyncClient.java:524)   org.eclipse.paho.client.mqttv3.MqttClient.disconnect(MqttClient.java:250)   org.eclipse.paho.client.mqttv3.MqttClient.disconnect(MqttClient.java:243)   org.springframework.integration.mqtt.inbound.MqttPahoMessageDrivenChannelAdapter.connectAndSubscribe(MqttPahoMessageDrivenChannelAdapter.java:104)   org.springframework.integration.mqtt.inbound.MqttPahoMessageDrivenChannelAdapter.access$300(MqttPahoMessageDrivenChannelAdapter.java:37)   org.springframework.integration.mqtt.inbound.MqttPahoMessageDrivenChannelAdapter$1.run(MqttPahoMessageDrivenChannelAdapter.java:137)   org.springframework.scheduling.support.DelegatingErrorHandlingRunnable.run(DelegatingErrorHandlingRunnable.java:54)   java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)   java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:351)   java.util.concurrent.FutureTask.runAndReset(FutureTask.java:178)   java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:165)   java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:267)   java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)   java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)   java.lang.Thread.run(Thread.java:679) ",3
"When there are no wiretap listeners don't publish messages
Being able to listen to a stream at any point has a significant performance impact.  The reason for the impact is the message needs to be ""serialized + transported + deserialized"" to other members even if there is no one listening.  This ""serialized + transported + deserialized"" processes happens for each step in a flow - source | process | sink.    Recommend creating some kind of protocol for wiretaps that allows members to know if there is someone listening in the grid so they will emit the data.  Likewise we need to deregister the listener if the wiretap is deleted.",8
"Create RPM for distribution
Package SpringXD into an RPM  install path = /opt/pivotal/spring-xd-1.0.0.M5  with symlink /opt/pivotal/spring-xd -> current version  init.d scripts to start/stop/status  service springxd-admin start|stop|status  service springxd-container start|stop|status  user/group = springxd/pivotal  Host springxd rpm in Pivotal repo  yum install springxd  Support RHEL/CentOS version 5 and 6? (tested on latest updates)  Support for 32 and 64 bits  Support Java 1.6 and 1.7  ",8
"Create subproject spring-xd-machine-learning-analytics
This project contains core abstractions that will allow for multiple implementations of a machine learning algorithm to be implemented via integration with various existing libraries or custom code implementations.      The initial code for this has been developed in a separate github repo and is located here     https://github.com/thomasdarimont/spring-xd/tree/feature/advanced-analytics-support/spring-xd-analytics/src/main/java/org/springframework/xd/analytics/model    The model can assume its use in evaluation of the model inside a stream where the data structure is a Tuple.  Note, it maybe useful to consider Message<Tuple> in case any metadata outside the core 'input data' is required to help guide the evaluation.    The build.gradle file should be updated such that there is a new build artifact spring-xd-machine-learning-analytics.jar along the lines of our other build artifacts.  Open to other naming suggestions.",8
"Create subproject spring-xd-machine-learning-analytics-jpmml
Using the jppml evaluator, provide an implementation of the core abstractions in the spring-xd-machine-learning.    The initial code for this has been developed in a separate github repo and is located here    https://github.com/thomasdarimont/spring-xd/tree/feature/advanced-analytics-support/spring-xd-analytics-jpmml/src/main/java/org/springframework/xd/analytics/model/jpmml ",4
"Create a JPMML module that will evaluate a model.
A analytical model should be evaluated as a processor in a stream.  The model evaluation will take the input variables from a Tuple and output variable will be placed into the tuple as well.    A strawman of the stream definition can be     stream create --definition "" SOURCE | jpmml â€˜fraud-detectionâ€™ | PROC1 â€¦ | PROCN "" --name stream1    Using profiles and playing all implementations of the analytical model in the same module lib directory, it maybe possible to select one of multiple implementations in the form     "" SOURCE | analytic --library=jpmml --name=â€˜fraud-detectionâ€™ | PROC1 â€¦ | PROCN ""    such that the core module name is the same but parameterized by what library type to use.  This may be problematic in that different libraries may have incompatible dependencies.    The analytical model can define the names of input and output fields, so at a minimum a name is required, however to easily adapt a given analytic model evaluation to a specific source modules output, it seems desirable to specify which fields are to be used as input, overriding the names of the input fields could be done in a manner such as     jpmml â€“name=linear-regresssion â€“inputFields=a,b,c ",8
"Create documentation for the core analytical model abstractions and use of jpmml processor
nan",5
"Create a simple sample application for the jpmml module
The sample application should primarily show the 'round trip' that is possible in that ""offline"" analysis in R can generate a pmml flle that can be imported and evaluated in an ""online"" stream definition.  The specific use case can be as simple as the IRIS data set or other existing examples, such as the fraud demo.    The sample application resides in https://github.com/spring-projects/spring-xd-samples",4
"Create documentation for how module properties are resolved.
The ordering of the lookup should be described, in particular detail on how environment variables can overrride properties.    Some details will necessarily change based on outcome of current discussion, but the overall ordering is going to remain.",4
"Fix existing Karma unit tests + Migrate E2E tests to Protractor
The grunt build for karma unit tests is currently broken with requireJS support on the XD admin app. ",4
"Minification of XD Admin UI JS files 
With requireJS r.js and ngmin, we need to make sure the XD admin JS files are appropriately minified. Currently, the JS files are not minified.",4
"Upgrade to ZooKeeper 3.4.6
3.4.6 was released on 2014.03.10:  http://zookeeper.apache.org/doc/r3.4.6/releasenotes.html    Especially relevant for us, they updated Netty from 3.2.2 to 3.6.6:  https://issues.apache.org/jira/browse/ZOOKEEPER-1715  ",2
"Log Hadoop Distro and ZK client connect info on Container startup
It would be nice to display container config logging with the hadoop distro and zookeeper client connect being used when the container starts up.  ",3
"Create Shared Server Context
Create a Shared Server Context to the Container hierarchy. This is a child of Global Beans which contains beans that are only shared by the Admin and Container contexts but not Modules. The ZooKeeper components go here to support the ZK deployment architecture. The MessageBus also goes here to support a clean way to launch jobs from the Admin process. ",4
"Add JMS Acceptance Tests
nan",5
"Support multiple admin servers on a same host
By default, XD admin server uses hsqldb as data source for batch job repository and uses the default port 9101, specific database location and dbname. This makes the existence of multiple admin server on a same host not possible. ",5
"Configure servers to use VanillaHealthEndpoint
The standard SimpleHealthIndicator that boot performs a database test that fails in xd-container since it does not require the use of a database.  ",3
"Update to Spring Boot RC5
nan",2
"Improvements to Modules Tab
1. Get listing of job modules  2. Remove version and action column  3. Text to say creating definitions from available modules in the UI is forthcoming, link to https://github.com/spring-projects/spring-xd/wiki/Batch-Jobs#creating-a-job for how to do this in the command line.     4. Hardcode an association between spring xd out of the box module names and a description.   5. Add button to display the XML file that defines the job module  ",8
"Improvements to Executions Tab
1. Add quick filter  2. The table should have columns for                                            name | instance | execution id    Getting the name might require a bit of extra work given some limitations with JSON serialization and cycles in the current object returned from spring batch.    3. The restart action should appear only if the job is restartable and the status was failed.  ",1
"Misc cleanup in UI
0. Remove home page with sign in and upper right hand corner with user login info.  1. Change the word template to modules in the tab  2. Different text for each of the tabs, â€œmodules, definition, deployments, scheduledï¿½?  3. Definitions tab to have text along the lines ""â€œallows you to deploy  and undeploy batch job definitions"" add links to help on how to do that in the CLI.  4. Deployments tab   a   creating new definitions, - parameters needs to be space on parameters,  â€œJob Parameters for Job XYZï¿½? after clicking launch.   b. comment out scheduler button   c. add quick filter  5. Scheduler tab   a. comment out tab  ",5
"Exclude slf4j Transitive Dependencies
nan",3
"Investigate module classloader leakage
See report at https://github.com/spring-projects/spring-xd/issues/661    This should not happen as the module holds the classes that hold the classloader, but who knows. An integration test that verifies this would be nice, albeit tricky.",5
"Allow re-use of a module classloader
See report at https://github.com/spring-projects/spring-xd/issues/661  It would be good indeed to allow this (eg by having a WeakHashMap<Classloader, type+name> map in the global context). The caveat though, is that any statics used by the module would be shared too. We can make this an opt-out though (I think that sharing by default makes sense) by having a flag in the module .properties manifest",8
"Implement XD_MODULE_CONFIG_LOCATION & NAME
The description in the google doc     https://docs.google.com/a/gopivotal.com/document/d/12Cboa7nyVVKVxDIsHLJ-68m5f78ayXn14EJLrclJYVg/edit?usp=sharing    describes the usage of XD_MODULE_CONFIG_LOCATION and XD_MODULE_CONFIG_NAME",5
"Remove Hadoop distro Enum options
Please see the discussion here:    https://github.com/spring-projects/spring-xd/pull/655/files#r10892925",3
"Update Spring Framework dependency to 4.0.3 GA
nan",2
"Update to Spring Boot 1.0 GA
nan",2
"Don't swallow unexpected exceptions in StacktraceFingerprintingCompletionRecoveryStrategy
nan",4
"Update spring-data-hadoop dependency and add new Hadoop distros
Update to Spring for Apache Hadoop 2.0 RC3 Add support for new hadoop distros:  - Pivotal HD 2.0 (phd20) - Hortonworks HDP 2.1 (hdp21) - Cloudera CDH5 (cdh5)  ",8
"Add documentation for a 'stdin' source module
In order to support ingestion from stdin, the suggested approach is to do the following.    xd:>stream create --definition ""tcp --decoder=LF | log"" --name foo    $ cat my.log | netcat localhost 1234    So while this is really a tcp based ingestion case, once can use pipe or redirect of stdin/err in order to achieve the same goal.  It should appear as a source module in the docs on par with other source modules in its own section.  ",2
"SpringXD logs error and large stack trace when metric can't be found. Distracting.
When a REST client of SpringXD (i.e., a dashboard) attempts to query (GET) a metric (e.g., counter, gauge, etc.) that does not exist the admin sever logs an ERROR and a large stack trace (attached).  In usage of Spring XD we see this frequently because a dashboard is running but the streams and counters have not been created quite yet, or initialized by messages flowing through the streams.  With a polling dashboard this results in a lot of distracting and large stack traces in the logs that are not actually issues.      I would suggest logging a one line warning or info message instead of the error and stack trace. ",1
"Update to Spring Shell 1.1 RC1
nan",2
"Support CI build in Travis
Get minimal compilation going in Travis.  Redis/Rabbit services can be done in a separate story.",1
"http module leaks threads
Attempts to create/use/undeploy a stream involving the http module will result in an OOME stating that VM could not create native thread.    Other modules should be checked as well.  {noformat}    java.lang.Thread.run(Thread.java:724)  Caused by: org.springframework.context.ApplicationContextException: Failed to start bean 'org.springframework.integration.x.http.NettyHttpInboundChannelAdapter#0'; nested exception is java.lang.OutOfMemoryError: unable to create new native thread    org.springframework.context.support.DefaultLifecycleProcessor.doStart(DefaultLifecycleProcessor.java:176)    org.springframework.context.support.DefaultLifecycleProcessor.access$200(DefaultLifecycleProcessor.java:51)    org.springframework.context.support.DefaultLifecycleProcessor$LifecycleGroup.start(DefaultLifecycleProcessor.java:346)    org.springframework.context.support.DefaultLifecycleProcessor.startBeans(DefaultLifecycleProcessor.java:149)    org.springframework.context.support.DefaultLifecycleProcessor.start(DefaultLifecycleProcessor.java:91)    org.springframework.context.support.AbstractApplicationContext.start(AbstractApplicationContext.java:1180)    org.springframework.xd.module.core.SimpleModule.start(SimpleModule.java:273)    org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:251)    org.springframework.xd.dirt.module.ModuleDeployer.deployAndStore(ModuleDeployer.java:239)    org.springframework.xd.dirt.module.ModuleDeployer.handleDeploy(ModuleDeployer.java:179)    org.springframework.xd.dirt.module.ModuleDeployer.handleMessageInternal(ModuleDeployer.java:150)    org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)  	... 80 more  Caused by: java.lang.OutOfMemoryError: unable to create new native thread    java.lang.Thread.$$YJP$$start0(Native Method)    java.lang.Thread.start0(Thread.java)    java.lang.Thread.start(Thread.java:693)    java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:949)    java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1371)    org.jboss.netty.util.internal.DeadLockProofWorker.start(DeadLockProofWorker.java:38)    org.jboss.netty.channel.socket.nio.AbstractNioSelector.openSelector(AbstractNioSelector.java:343)    org.jboss.netty.channel.socket.nio.AbstractNioSelector.<init>(AbstractNioSelector.java:95)    org.jboss.netty.channel.socket.nio.AbstractNioWorker.<init>(AbstractNioWorker.java:53)    org.jboss.netty.channel.socket.nio.NioWorker.<init>(NioWorker.java:45)    org.jboss.netty.channel.socket.nio.NioWorkerPool.createWorker(NioWorkerPool.java:45)    org.jboss.netty.channel.socket.nio.NioWorkerPool.createWorker(NioWorkerPool.java:28)    org.jboss.netty.channel.socket.nio.AbstractNioWorkerPool.newWorker(AbstractNioWorkerPool.java:99)    org.jboss.netty.channel.socket.nio.AbstractNioWorkerPool.init(AbstractNioWorkerPool.java:69)    org.jboss.netty.channel.socket.nio.NioWorkerPool.<init>(NioWorkerPool.java:39)    org.jboss.netty.channel.socket.nio.NioWorkerPool.<init>(NioWorkerPool.java:33)    org.jboss.netty.channel.socket.nio.NioServerSocketChannelFactory.<init>(NioServerSocketChannelFactory.java:149)    org.jboss.netty.channel.socket.nio.NioServerSocketChannelFactory.<init>(NioServerSocketChannelFactory.java:131)    org.jboss.netty.channel.socket.nio.NioServerSocketChannelFactory.<init>(NioServerSocketChannelFactory.java:115)    org.springframework.integration.x.http.NettyHttpInboundChannelAdapter.doStart(NettyHttpInboundChannelAdapter.java:114)    org.springframework.integration.endpoint.AbstractEndpoint.start(AbstractEndpoint.java:84)    org.springframework.context.support.DefaultLifecycleProcessor.doStart(DefaultLifecycleProcessor.java:173)  	... 91 more  {noformat}  ",5
"Update Spring-AMQP to 1.3.1.RELEASE 
nan",1
"User should not be required to specify a control channel
With the addition of zookeeper, a user does not have to specify rabbit or redis for the control channel.   This story should allow a user to specify redis, rabbit or no control channel.",2
"Environment checkers in acceptance tests should use Asserts
Replace tests and throws in the XdEc2Validation with asserts in the following methods:  verifyTestContent  verifyLogContent  verifySendCounts    VerifySendCounts should check for the exact number of Jmx events instead of >0.",4
"Allow user to configure tests with DI 
With the addition of sinks and sources that require connections with external entities (hadoop, JMS, JDBC, ...)  the environment setup is getting unwieldy.    * Integrate SpringJUnit4ClassRunner.class into acceptance tests.  * Retrieve environment variables via Dependency injection from application.properties.  * Utilize profiles for     --local single node    --local cluster    --ec2 single node    --ec2 cluster",3
"Remove the S3 XD Jar Cache
Originally it was placed in the code to prevent overtasking our servers with downloads when people want to install XD.  And to allow for faster downloads.  However XD Jars are already placed on S3, so this feature is no longer needed.",3
"Upgrade to Spring Hadoop 2.0 RC2
nan",2
"Prevent accidental pickup of ENV var as module option
Typical case is with a module that contains {noformat} ... user=""${username}"" ... /> {noformat}  (say, as part of a jdbc connection configuration) and no value has been given at deployment time.  The module may pickup a value from the environment by mistake (typically from an environment variable of the same name). This was even more problematic when ordering of property sources was unclear, but should be prevented entirely anyway.",5
"Remove jmxEnabled as a cmdLine option and enable JMX by default
After some discussion and voting, we decided to remove ""jmxEnabled"" as a command line option and have JMX enabled by default.  This can be disabled from xd-config.yml externally.",3
"Use Hadoop mini-cluster test support in XD tests
The tests that use HDFS currently require an external Hadoop installation and is hard to set up/update version in all the environments where we want to run tests, e.g. bamboo, travis.    See if the mini-cluster described in     http://docs.spring.io/spring-hadoop/docs/2.0.0.RC1/reference/html/testing.html#testing:yarn:minicluster    can be used in the test cases instead.",3
"Validate time field processing with AggregateCounter
AggregateCounter needs a test case along the lines of of org.springframework.xd.analytics.metrics.integration.FieldValueCounterHandlerTests that will demonstrate the base functionality of the module as well as use of the  timeField and dateFormat options.",3
"Delete post module and CF profile
This would get rid of the CF specific post module, keeping the general abstraction of 'http' source across CF and non-CF environments.",1
"Upgrade to Spring Integration 4.0.0.M4
Currently on snapshots, which is oddly pulling in groovy 2.1.0",1
"Update management context path to <root>/management
nan",1
"Update XdEc2Validation to reference <root>/management endpoint
change     ""/jolokia/list"";    to     ""/management/jolokia/list"";    etc.",2
"Do not eagerly use repositories in completion's *Strategy
nan",4
"Update to Reactor 1.1.0 M3
nan",1
"Error in correlation strategy in aggregator.xml
{noformat}  	correlation-strategy-expression=""${correlation:'${xd.stream.name}}'""  {noformat}  should be  {noformat}  	correlation-strategy-expression=""${correlation:'${xd.stream.name}'}""  {noformat}  Add a test to make sure correlation expressions here work.",2
"Rationalize inclusion of JSON jars in xd/lib and in modules
To avoid CL conflicts, a short term solution to this problem is to make sure that there is only one copy of jackson related classes and have them live in xd/lib.    using  jackson-core-2.3.2.jar jackson-databind-2.3.2.jar jackson-core-asl-1.9.13.jar jackson-mapper-asl-1.9.13.jar  are the latest versions and the ones to use.  There maybe other libraries that we duplicate in the same manner, we will need to investigate as well as part of another issue.  Typical error is:  java.lang.LinkageError: loader constraint violation: when resolving method ""org.springframework.http.converter.json.MappingJackson2HttpMessageConverter.setObjectMapper(Lcom/fasterxml/jackson/databind/ObjectMapper;)V"" the class loader (instance of org/springframework/xd/module/support/ParentLastURLClassLoader) of the current class, org/springframework/social/twitter/api/impl/TwitterTemplate, and the class loader (instance of sun/misc/Launcher$AppClassLoader) for resolved class, org/springframework/http/converter/json/MappingJackson2HttpMessageConverter, have different Class objects for the type com/fasterxml/jackson/databind/ObjectMapper used in the signature ",3
"Migrate repositories to ZooKeeper
all state about the running system (containers, streams, and jobs) should be available via ZK, and ultimately the --store option should not be needed    1. Refactor ModuleDefinitionRepository to use ZooKeeper     * remove RedisModuleDefinitionRepository     * remove InMemoryModuleDefinitionRepository    2. Refactor ModuleDependencyRepository to use ZooKeeper     * remove RedisModuleDependencyRepository     * remove InMemoryModuleDependencyRepository    3. Refactor RuntimeModuleInfoRepository to use ZooKeeper (rename ModuleMetadata...)     * remove RedisRuntimeModuleInfoRepository     * remove AbstractRedisRuntimeModuleInfoRepository     * remove InMemoryRuntimeModuleInfoRepository    4. Refactor RuntimeContainerModuleInfoRepository to use ZooKeeper (rename ContainerMetadata...)     * remove RedisRuntimeContainerModuleInfoRepository     * remove InMemoryRuntimeContainerModuleInfoRepository    5. Remove support for --store     * remove the memory-store.xml and the redis-store.xml     * instead include just one repositories.xml in shared server config     * remove the associated property key and the *Options properties    6. Remove the events and listeners that were being used",10
"Let modules define a default value for --inputType option
If a module e.g. always expects the payload of the message to be of a certain java type if would be good for documentation and convenience reasons in order to specify a default value for the --inputType option.     documentation = output for module info  convenience = we could e.g. support to always accept a Json payload (or automatic message payload conversion once it is extensible)    currently, adding  options.inputType.default  to the module's property file has no effect    I've also tried to ""redefine"" it using  options.inputType.description  This leads to the following exception:    Command failed org.springframework.xd.rest.client.impl.SpringXDException: Module option named 'outputType' is present in several delegates: [org.springframework.xd.module.options.SimpleModuleOptionsMetadata@3c1d635a, FlattenedCompositeModuleOptionsMetadata  [outputType] => PojoModuleOptionsMetadata backed by class org.springframework.xd.dirt.plugins.stream.ModuleTypeConversionPluginMetadataResolver$OutputOptionsMetadata, defining options [[ModuleOption [name=outputType, type=class org.springframework.util.MimeType, defaultValue=null, description=how this module should emit messages it produces]]]  [inputType] => PojoModuleOptionsMetadata backed by class org.springframework.xd.dirt.plugins.stream.ModuleTypeConversionPluginMetadataResolver$InputOptionsMetadata, defining options [[ModuleOption [name=inputType, type=class org.springframework.util.MimeType, defaultValue=null, description=how this module should interpret messages it consumes]]]]",4
"ZooKeeper runtime cleanup and refactoring
nan",40
"Refactor StreamParser to return a StreamDefinition
{code}  StreamDefinition sd = streamParser.parse(name, dslText);  {code}    We should also consider explicit methods such as parseStream (so that parseJob and parseComposedModule are at least separate methods, if not separate parser classes that share the common parser support class that is the core of today's parser). The parsing for ""completion providers"" should probably be spun off to its own class as well. In the end, there should be no need for a ParsingContext enum but rather, more explicitly named methods and dedicated classes if that seems like the right approach.    the StreamDefinition should be composed of ""ModuleDescriptors"" (that name is not set in stone) and other Stream-level metadata like source/sink channels    consider merging some of StreamFactory code there, and the rest into StreamDeployer    merge ModuleDescriptor and ModuleDeploymentRequest as part of this effort (again, a new name could be considered, but ModuleDescriptor should take precedence over ModuleDeploymentRequest), and note in the process that ModuleDescriptor was originally designed to be immutable (taking constructor args), but as we migrated the prototype code into XD itself, this was violated. We may want to consider a builder approach, and we likely want to avoid the need for a ModuleDefinition within the ModuleDescriptor.",8
"Improve Exception handling for ZooKeeper data access
Currently we have many catch(Exception) blocks that simply wrap and rethrow RuntimeExceptions. We should create at least a top-level RuntimeException of our own, within the XD Exception hierarchy, and possibly a hierarchy of RuntimeExceptions extending from that, and mapping to the various checked Exceptions that can occur in ZooKeeper data access.    Also, we should not be re-wrapping those Exceptions that are already RuntimeExceptions, so we should consider a ZooKeeperExceptionHandler (and although I'm typically hesitant to recommend it, this might be a case where a static util method is the right approach).  ",8
"Update document for the distributed runtime based on RC1 changes
This should be included on the wiki, providing a thorough overview of how the distributed runtime works under-the-hood, including details of the various ZooKeeper nodes and associated watchers, etc. For example, it should describe the role of *any* Admin as a provider of the REST API as well as the specific roles allocated to the single *leader* Admin. It should describe Module recovery after Container failure, as well as some description of various failure scenarios (crashed JVM, network partition, etc) and how the recovery should be expected to occur (e.g. in some cases it will be nearly immediate, and in other cases it will be after a timed out connection). Intentional Container shutdown should be included in that discussion as well. At many points, this doc could link back to items that we also need to add to the main User Guide, such as the description of ContainerMatcher as the strategy for an Admin to determine which Container(s) should deploy a given Module, since that ContainerMatcher is used not only for the initial deployment, but also any redeployment that may be necessary.",6
"Merge Container and ContainerMetadata as well as their ""repositories""
The two classes in question are:  * org.springframework.xd.dirt.cluster.Container  * org.springframework.xd.dirt.container.ContainerMetadata    The former is currently used by the Admin when making decisions about Module deployment. That latter was a replacement for RuntimeContainerInforEntity as we migrated the various Redis/InMemory Repositories to use the data that is now available in ZooKeeper instead.    The ContainerRepository is currently used by the Admin leader, and the ContainerMetadataRepository is used by the REST endpoint that supports the xd-shell's 'runtime containers' command. Perhaps those can also be merged.    In any case, if not addressed by a larger refactoring, the ContainerRepository should probably support an Iterable return rather than an Iterator. Having finders (e.g. for attribute key/values such as ""group""==""foo"") might be convenient for various Module deployment strategies.",4
"Refactor the Module and Container Repositories
All of the following are currently defined in META-INF/spring-xd/internal/repositories.xml (among others, like Stream/StreamDefinition and Job/JobDefinition repos):    {code}  combine some of these (in repositories.xml) and reconsider the use of Repository abstractions everywhere:     <bean id=""containerMetadataRepository"" class=""org.springframework.xd.dirt.container.store.ZooKeeperContainerMetadataRepository""/>     <bean id=""moduleMetadataRepository"" class=""org.springframework.xd.dirt.module.store.ZooKeeperModuleMetadataRepository"" />     <bean id=""moduleDependencyRepository"" class=""org.springframework.xd.dirt.module.store.ZooKeeperModuleDependencyRepository""/>     <bean id=""moduleDefinitionRepository"" class=""org.springframework.xd.dirt.module.store.ZooKeeperModuleDefinitionRepository""/>  {code}    At the very least the Module repositories could be combined, and also we should remove ModuleDefinitionRepositoryUtils that provides static utility methods (since we no longer need that for reuse across multiple repo impls).    Finally, we should reconsider the use of actual Repository interfaces for these, since in most cases there are only one or two methods being called. Thus, it's not necessary to support the full range of CRUD operations for which those Repository interfaces are intended.",8
"DefaultContainerMatcher should make a better attempt at round-robin distribution
Currently the index is used globally but applied to a range of candidates that can differ based on the match criteria per invocation.",2
"Merge Module.Type and ModuleType
Also likely rename, remove, or replace that Module (maybe can be supplanted by ModuleDescriptor when used in the refactored parser).    Also, considering the ""url"" property is not necessary (vestige of the prototype), all we'd be left with here is the Module name and type, which are used to identify a Module uniquely. Therefore this Module could be renamed to ModuleKey or something. It could be used within the StreamDefinition itself (e.g. getDescriptor(moduleKey)).",2
"Remove spring-xd-analytics-ml-pmml project and module xml - to be put in seperate repository
AGPL license issues",2
"Add initial support for DeploymentManifest
The REST API for deploy should accept parameters, which provide manifest key/value pairs (e.g. ?http.instances=5). Ultimately we will want to support passing a named manifest which had been stored previously.    The 'stream deploy' shell command should support passing these as --options.    Initially we should support [modulename].instances and [modulename].group.",5
"StreamListener should watch /xd/deployments/streams
nan",4
"JobListener should watch /xd/deployments/jobs
nan",4
"The Container's DeploymentListener should watch /xd/deployments/modules
Currently it watches /xd/deployments/[containerid], but due to the reuse of that top level node for XD-1483 and XD-1484, we should instead use /xd/deployments/modules/[containerid]",4
"Move resusable analytics repository classes to a new project.
The analytics project has been used as a host for common repository classes because it was easily visible by both -dirt and other stuff (can't remember which)  This should be cleaned and a dedicated project for ""core"", ""utility"", ""re-usable"", ""whatever"" classes should be created  ",5
"eliminate package tangle
see:  https://sonar.springsource.org/drilldown/measures/7173?metric=package_tangle_index&rids%5B%5D=7717",4
"Test scripts on windows that use XD_MODULE_CONFIG_LOCATION/NAME
nan",2
"Create a test case for insulating environment variables in module property lookup
For PR https://github.com/spring-projects/spring-xd/pull/682    see if one can have a test case such that a test module would have a 'PATH' property that overlaps with the environment variable.  It should never resolve to the real unix/windows path.",2
"Add documentation for new module property configuration support
the algorithm and approach in     https://docs.google.com/a/gopivotal.com/document/d/12Cboa7nyVVKVxDIsHLJ-68m5f78ayXn14EJLrclJYVg/edit#    needs to be added to the section on application configuration",3
"SXD's gemfire-server wrapper script can't handle absolute paths w/o extra slash
Absolute paths fail: gemfire-server /Users/dbeauregard/Software/spring-xd/spring-xd-1.0.0.M5/gemfire/config/my-demo.xml  (This failse, see error below)    You have to add an extra forward slash at the beginning (//Users) to get it to work: gemfire-server //Users/dbeauregard/Software/spring-xd/spring-xd-1.0.0.M5/gemfire/config/my-demo.xml (This works)    ---------------- The log and error ----------------------    dbeauregard-mbp:~ dbeauregard$ gemfire-server /Users/dbeauregard/Software/spring-xd/spring-xd-1.0.0.M5/gemfire/config/my-demo.xml  09:39:33,772  INFO main gemfire.CacheServer:50 - Starting Cache Server  09:39:33,884  INFO main support.FileSystemXmlApplicationContext:513 - Refreshing org.springframework.context.support.FileSystemXmlApplicationContext@2ba119b3: startup date [Fri Apr 04 09:39:33 MDT 2014]; root of context hierarchy  09:39:33,949  INFO main xml.XmlBeanDefinitionReader:316 - Loading XML bean definitions from file [/Users/dbeauregard/Users/dbeauregard/Software/spring-xd/spring-xd-1.0.0.M5/gemfire/config/my-demo.xml]  Exception in thread ""main"" org.springframework.beans.factory.BeanDefinitionStoreException: IOException parsing XML document from file [/Users/dbeauregard/Users/dbeauregard/Software/spring-xd/spring-xd-1.0.0.M5/gemfire/config/my-demo.xml]; nested exception is java.io.FileNotFoundException: Users/dbeauregard/Software/spring-xd/spring-xd-1.0.0.M5/gemfire/config/my-demo.xml (No such file or directory)    org.springframework.beans.factory.xml.XmlBeanDefinitionReader.loadBeanDefinitions(XmlBeanDefinitionReader.java:343)    org.springframework.beans.factory.xml.XmlBeanDefinitionReader.loadBeanDefinitions(XmlBeanDefinitionReader.java:303)    org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:180)    org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:216)    org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:187)    org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:251)    org.springframework.context.support.AbstractXmlApplicationContext.loadBeanDefinitions(AbstractXmlApplicationContext.java:127)    org.springframework.context.support.AbstractXmlApplicationContext.loadBeanDefinitions(AbstractXmlApplicationContext.java:93)    org.springframework.context.support.AbstractRefreshableApplicationContext.refreshBeanFactory(AbstractRefreshableApplicationContext.java:129)    org.springframework.context.support.AbstractApplicationContext.obtainFreshBeanFactory(AbstractApplicationContext.java:540)    org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:454)    org.springframework.context.support.FileSystemXmlApplicationContext.<init>(FileSystemXmlApplicationContext.java:140)    org.springframework.context.support.FileSystemXmlApplicationContext.<init>(FileSystemXmlApplicationContext.java:84)    org.springframework.xd.gemfire.CacheServer.main(CacheServer.java:52)  Caused by: java.io.FileNotFoundException: Users/dbeauregard/Software/spring-xd/spring-xd-1.0.0.M5/gemfire/config/my-demo.xml (No such file or directory)    java.io.FileInputStream.open(Native Method)    java.io.FileInputStream.<init>(FileInputStream.java:146)    org.springframework.core.io.FileSystemResource.getInputStream(FileSystemResource.java:114)    org.springframework.beans.factory.xml.XmlBeanDefinitionReader.loadBeanDefinitions(XmlBeanDefinitionReader.java:329)  	... 13 more",1
"Allow users to set attribute values on a container
Allow users to configure arbitrary key value pairs for a container instance that may be referenced in deployment descriptors to target specific container instances. ",2
"Support groups container attribute
A container instance may be designated as belonging to a group. The end user may define this attribute using yaml config as xd.container.groups=<comma delimited string> or an environment variable or --groups command line argument. ",2
"xd-shell tab completion missing for http post/get
xd-shell tab completion missing for 'http post' and 'http get' cli commands.  Typing ""xd:>http post"" <tab> <tab> gives no suggestions event though --file or --data are required.  ",1
"OS commands no longer supports whitespace/arguments in M6
OS commands, i.e., ""!"" doesn't support arguments in M6; it did in M5.      The following gives an error:  xd:>! ls /  You cannot specify option '' more than once in a single command    No arguments or whitespace works:  xd:>! ls  command is:ls  spring-shell.log  xd-shell  xd-shell.bat",1
"xd:>runtime modules gives error from CLI
xd:>runtime modules  Command failed org.springframework.xd.rest.client.impl.SpringXDException: java.lang.RuntimeException: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /xd/deployments/modules/4dc55d87-125b-4e4a-a76e-82bb6980820d/TickTock.sink.log-1/metadata    This is on OSX running in distributed mode with --transport rabbit --hadoopDistro hadoop22, redis 2.8.8, rabbit 3.2.3, hadoop 2.2.0, and zookeeper 3.4.5.",1
"Exception thrown when accessing Jolokia via the management context path
When trying to access Jolokia via the management/jolokia (http://localhost:9393/management/jolokia) I get the following exception.       {""error_type"":""java.lang.IllegalArgumentException"",""error"":""java.lang.IllegalArgumentException : No type with name 'management' exists"",""status"":400,""stacktrace"":""java.lang.IllegalArgumentException: No type with name 'management' exists\n\tat org.jolokia.util.RequestType.getTypeByName(RequestType.java:69)\n\tat org.jolokia.request.JmxRequestFactory.createGetRequest(JmxRequestFactory.java:94)\n\tat org.jolokia.http.HttpRequestHandler.handleGetRequest(HttpRequestHandler.java:78)\n\tat org.jolokia.http.AgentServlet$3.handleRequest(AgentServlet.java:298)\n\tat org.jolokia.http.AgentServlet.handle(AgentServlet.java:229)\n\tat org.jolokia.http.AgentServlet.doGet(AgentServlet.java:194)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:621)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:728)\n\tat org.springframework.web.servlet.mvc.ServletWrappingController.handleRequestInternal(ServletWrappingController.java:158)\n\tat org.springframework.web.servlet.mvc.AbstractController.handleRequest(AbstractController.java:154)\n\tat org.springframework.boot.actuate.endpoint.mvc.JolokiaMvcEndpoint.handle(JolokiaMvcEndpoint.java:120)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.springframework.web.method.support.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:215)\n\tat org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:132)\n\tat org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:104)\n\tat org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandleMethod(RequestMappingHandlerAdapter.java:749)\n\tat org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:689)\n\tat org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:83)\n\tat org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:938)\n\tat org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:870)\n\tat org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:961)\n\tat org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:852)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:621)\n\tat org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:837)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:728)\n\tat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:305)\n\tat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)\n\tat org.springframework.boot.actuate.trace.WebRequestTraceFilter.doFilter(WebRequestTraceFilter.java:115)\n\tat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)\n\tat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)\n\tat org.springframework.boot.actuate.autoconfigure.EndpointWebMvcAutoConfiguration$ApplicationContextFilterConfiguration$1.doFilterInternal(EndpointWebMvcAutoConfiguration.java:137)\n\tat org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:108)\n\tat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)\n\tat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)\n\tat org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:77)\n\tat org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:108)\n\tat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)\n\tat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)\n\tat org.springframework.web.filter.HttpPutFormContentFilter.doFilterInternal(HttpPutFormContentFilter.java:88)\n\tat org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:108)\n\tat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)\n\tat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)\n\tat org.springframework.boot.actuate.autoconfigure.MetricFilterAutoConfiguration$MetricsFilter.doFilterInternal(MetricFilterAutoConfiguration.java:85)\n\tat org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:108)\n\tat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)\n\tat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)\n\tat org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:222)\n\tat org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:123)\n\tat org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:472)\n\tat org.apache.catalina.valves.RemoteIpValve.invoke(RemoteIpValve.java:680)\n\tat org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:171)\n\tat org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:99)\n\tat org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:118)\n\tat org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:407)\n\tat org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1004)\n\tat org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:589)\n\tat org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.run(NioEndpoint.java:1680)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:724)\n""}",5
"Move JMX Endpoints from /jolokia to management/jolokia
Update code to use the management/jolokia endpoint.",3
"Create documentation for module property configuration
Describe the algorithm as defined in https://docs.google.com/a/gopivotal.com/document/d/12Cboa7nyVVKVxDIsHLJ-68m5f78ayXn14EJLrclJYVg/edit?usp=sharing    ",2
"change xd-config.yml and xd-modules-config.yml to servers.yml and modules.yml
nan",1
"Stream deployment race condition
When a container is started, the leader admin will scan the deployed streams to determine if any have modules that need to be deployed on the new container.     When a stream is deployed, the leader admin will select containers to deploy modules to.    If a new container and stream are deployed at the same time, there is the window for a race condition where both attempt to deploy a module to a container. This can be solved by (at least one) of the following:    * Consider using a single thread in the admin leader to handle all ZooKeeper updates. This means that the handling of new containers and stream deployment requests will not happen concurrently.  * Trap the {{NodeExists}} exception when creating the {{/xd/deployments/modules/...}} node in ZooKeeper",5
"IP address used as default data when creating paths
Invoking {{Paths.ensurePath}} is creating a default value of the host IP address instead of the expected ""empty"" value. ",1
"Investigate failing LocalSingleNodeStreamDeploymentIntegrationTests
Investigate the failing test LocalSingleNodeStreamDeploymentIntegrationTests.moduleChannelsRegisteredWithMessageBus:    {noformat}  java.lang.AssertionError: expected:<3> but was:<0>    org.junit.Assert.fail(Assert.java:88)    org.junit.Assert.failNotEquals(Assert.java:743)    org.junit.Assert.assertEquals(Assert.java:118)    org.junit.Assert.assertEquals(Assert.java:555)    org.junit.Assert.assertEquals(Assert.java:542)    org.springframework.xd.dirt.stream.AbstractSingleNodeStreamDeploymentIntegrationTests.moduleChannelsRegisteredWithMessageBus(AbstractSingleNodeStreamDeploymentIntegrationTests.java:270)    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)    java.lang.reflect.Method.invoke(Method.java:606)    org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)    org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)    org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)    org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)    org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)    org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)  {noformat}    This can be most easily reproduced on Ubuntu.",1
"Refactor duplicate code for modules
The admin leader performs module deployment requests by listening to stream deployment requests under {{/xd/deployments/streams}} and by listening to containers joining and leaving the cluster under {{/xd/containers}}. Refactor any duplicate code into a common class/component that can be used by both listeners.    *Edit:* the listener duplication will be handled in XD-1548. This issue is for the duplicate code in {{ModuleDeploymentRequest}} and {{ModuleDescriptor}}.",10
"Avoid duplication when loading streams for deployment
See: ContainerListener.loadStream() and StreamListener.onChildAdded(). Both require the stream definition as well as stream deployment manifest.",10
"Documentation typo in JSON SPEL filter
In the JSON SPEL Filter twitter example here:  http://docs.spring.io/spring-xd/docs/1.0.0.M5/reference/html/#filter    ""hashTags"" should not have a capital 'T'.  Should be ""hashtags"".",1
"Add admin-ui to YARN zip packaging
nan",1
"Prevent submiting jobs that are not currently deployed using Admin UI
Job modules ""Launch"" and ""Schedule"" command buttons are active even if the job module isn't deployed or has been destroyed.    Get errors like:   ""Yikes, something bad happened while launching job myjob4""  ""The job named 'myjob4' is not currently deployed""",3
"All jobs end up on the same container node
The jobs aren't spread evenly across available container nodes as they are created/deployed. I had 3 nodes but only one has the job modules.    [zk: localhost:2181(CONNECTED) 56] ls /xd/deployments/modules/621230e0-a089-4fbe-afc8-611ae527fcbc  [myjob9.job.jdbchdfs-0, myjob5.job.jdbchdfs-0, myjob8.job.jdbchdfs-0, myjob4.job.jdbchdfs-0, myjob6.job.jdbchdfs-0, myjob7.job.jdbchdfs-0]  [zk: localhost:2181(CONNECTED) 57] ls /xd/deployments/modules/6969579c-0cf4-4cc1-8e21-e01d73a70965  []  [zk: localhost:2181(CONNECTED) 58] ls /xd/deployments/modules/d0667cd1-a57a-4279-b7fb-dd63e4dd40d4  []    ",3
"Clean up publishing to maven repositories of empty module projects
See the various module.xyz directories here     https://repo.spring.io/libs-snapshot/org/springframework/xd/",4
"Reduce amount of logging at server startup
 INFO main zookeeper.ZooKeeper:100 - Client environment:java.class.path    produces a huge amount of output and is rather distracting, if that specific entry could be at a log level of debug it would make the startup of the server look cleaner.",1
"Create documentation for ZK runtime
nan",3
"Create documentation for how to extend the XD containers
https://jira.spring.io/browse/XD-1343 and related issues.",4
"Change the default deploy option to false for stream/job deploy commands
With the introduction of the deployment manifest setting this to false makes it easier to use the deployment manifest, which is the main use-case scenario.  ",2
"Create documentation for the deployment manifest
nan",3
"Allow the value of xd.module.number to be used as property placeholder value
This will allow us to use multiple instances of  hdfs/file sinks and not have any filename/path collisions.",3
"Deploy ""orphaned"" jobs to new containers
When a container joins the cluster, the leader admin will assign modules to it that are unassigned. It needs to also check for job modules and deploy them if required.",1
"Change request mapping for removing a stream deployment in XDController
Currently _deployments - with an understore in XDController should be something else.  Need to segment up the url space better for stream/jobs to avoid a clash.",3
"Add test cases for DefaultContainerMatcher
nan",4
"Provide job repository creation schema for additonal databases
oracle, gemfire xd (derby should be the dialect), and sybase",3
"Push ${xd.stream.name} into POJO defaults
See XD-1283.  We've been waiting for 1283 to change constructs like  {noformat}  attr=""${name:${xd.stream.name}}""    to just  {noformat}  attr=""${name}""  {noformat}      Turns out we can simply push down the ${xd.stream.name} bit in the default value (most likely initialization of a field in a POJO metadata class) and it will work just fine.    We can also consider:  - providing a fake value for those placeholders to use when doing ""module info"" (ie user will see  ""<name of the stream>"" instead of ""${xd.stream.name}""      ",8
"Create a dedicated plugin for ${xd.stream.name} and similar
The setting of the xd.stream.name property is currently duplicated in StreamPlugin and JobPlugin, etc  Moreover, there is no strong String constant to reference it    Create a plugin dedicated to those matters (namely making bits of DeploymentMetadata available to the module environment)",4
"parser should only allow one label instance per module
nan",2
"stream list should show ""undeployed"" rather than blank if a stream is not deployed
nan",2
"Create small documentation section on jmx/monitoring functionalty
Should mention jolokia, how to turn on/off boot/jolokia http metric/monitoring and jmx.    Mention the naming strategy to identify modules running in a stream.",1
"Need better error handling for module info shell command
Would be better to provide a more useful message, e.g. ""The module name must be of the form <module-type>:<module-name>"" xd:>module info --name time java.lang.StringIndexOutOfBoundsException: Failed to convert 'time' to type QualifiedModuleName for option 'name,' String index out of range: -1 xd:>module info --name sink/time java.lang.StringIndexOutOfBoundsException: Failed to convert 'sink/time' to type QualifiedModuleName for option 'name,' String index out of range: -1 xd:>module info --name sink:time Command failed org.springframework.xd.rest.client.impl.SpringXDException: NullPointerException  xd:>module info --name source:time Information about source module 'time':",1
"Exception when accessing CDH4 namenode
Get exception when accessing cdh4 from shell -    java.lang.UnsupportedOperationException: This is supposed to be overridden by subclasses.    com.google.protobuf.GeneratedMessage.getUnknownFields    most likely due to protobuf-java-2.5.0.jar being on the main classpath now      Full stack trace:  {code}  trisberg@carbon:~/Test$ ./spring-xd-1.0.0.BUILD-SNAPSHOT/shell/bin/xd-shell --hadoopDistro cdh4  16:55:22,680  WARN main conf.Configuration:824 - fs.default.name is deprecated. Instead, use fs.defaultFS   _____                           __   _______  /  ___|          (-)             \ \ / /  _  \  \ `--. _ __  _ __ _ _ __   __ _   \ V /| | | |   `--. \ '_ \| '__| | '_ \ / _` |  / ^ \| | | |  /\__/ / |_) | |  | | | | | (_| | / / \ \ |/ /  \____/| .__/|_|  |_|_| |_|\__, | \/   \/___/        | |                  __/ |        |_|                 |___/  eXtreme Data  1.0.0.BUILD-SNAPSHOT | Admin Server Target: http://localhost:9393  Welcome to the Spring XD shell. For assistance hit TAB or type ""help"".  xd:>hadoop config fs --namenode hdfs://cdh4:8020  xd:>hadoop fs ls /  Hadoop configuration changed, re-initializing shell...  16:55:28,853  WARN Spring Shell util.NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable  -ls: Fatal internal error  java.lang.UnsupportedOperationException: This is supposed to be overridden by subclasses.    com.google.protobuf.GeneratedMessage.getUnknownFields(GeneratedMessage.java:180)    org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFileInfoRequestProto.getSerializedSize(ClientNamenodeProtocolProtos.java:30108)    com.google.protobuf.AbstractMessageLite.toByteString(AbstractMessageLite.java:49)    org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.constructRpcRequest(ProtobufRpcEngine.java:149)    org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:193)    com.sun.proxy.$Proxy43.getFileInfo(Unknown Source)    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)    java.lang.reflect.Method.invoke(Method.java:606)    org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:164)    org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:83)    com.sun.proxy.$Proxy43.getFileInfo(Unknown Source)    org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:629)    org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1545)    org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:819)    org.apache.hadoop.fs.FileSystem.globStatusInternal(FileSystem.java:1646)    org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1592)    org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1567)    org.apache.hadoop.fs.shell.PathData.expandAsGlob(PathData.java:271)    org.apache.hadoop.fs.shell.Command.expandArgument(Command.java:224)    org.apache.hadoop.fs.shell.Command.expandArguments(Command.java:207)    org.apache.hadoop.fs.shell.Command.processRawArguments(Command.java:190)    org.apache.hadoop.fs.shell.Command.run(Command.java:154)    org.apache.hadoop.fs.FsShell.run(FsShell.java:254)    org.springframework.xd.shell.hadoop.FsShellCommands.run(FsShellCommands.java:412)    org.springframework.xd.shell.hadoop.FsShellCommands.runCommand(FsShellCommands.java:407)    org.springframework.xd.shell.hadoop.FsShellCommands.ls(FsShellCommands.java:110)    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)    java.lang.reflect.Method.invoke(Method.java:606)    org.springframework.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:196)    org.springframework.shell.core.SimpleExecutionStrategy.invoke(SimpleExecutionStrategy.java:64)    org.springframework.shell.core.SimpleExecutionStrategy.execute(SimpleExecutionStrategy.java:48)    org.springframework.shell.core.AbstractShell.executeCommand(AbstractShell.java:127)    org.springframework.shell.core.JLineShell.promptLoop(JLineShell.java:530)    org.springframework.shell.core.JLineShell.run(JLineShell.java:178)    java.lang.Thread.run(Thread.java:744)  {code}",3
"Modify module metadata to exclude default module properties
When the runtime modules command displays the the modules metadata properties, we can exclude the default module properties such as 'xd.stream.name', 'xd.module.index' and show only the other properties such as resolved module options etc.,",2
"Support external datasource for single node application
Currently, single node application assumes to use hsqldb server by default and starts the hsqldb as it starts.  We need a way to provide external datasource and not to start hsqldb by default when the single node starts up.",3
"Correct hadoop classpath versions for the distros
When using hdp13, the XdConfigLoggingInitializer throws this info:  12:02:06,064  INFO main util.XdConfigLoggingInitializer:77 - Hadoop Distro: hdp13 12:02:06,068  WARN main util.XdConfigLoggingInitializer:84 - Hadoop version detected from classpath: 1.2.0 but you provided '--hadoopDistro hdp13'. Did you mean '--hadoopDistro hdp20'? 12:02:06,069  WARN main util.XdConfigLoggingInitializer:84 - Hadoop version detected from classpath: 1.2.0 but you provided '--hadoopDistro hdp13'. Did you mean '--hadoopDistro hadoop12'?  Since hdp13 uses hadoop 1.2.0, we need to fix that in the versions map ContainerOptions.getHadoopDistroVersions()",1
"Error when removing HDFS files in shell
I get this:    {code}  xd:>hadoop fs rm /xd/test/time-3.log  Error: run HDFS shell failed. Message is: org.apache.hadoop.fs.FileStatus.isDirectory()Z  {code}    so far I have seen this with --hadoopDistro hdp13 and hadoop12    same command works fine using shell from M5 release  ",3
"Rename xd-config.yml to servers.yml and add modules/modules.yml to spring-xd-yarn
Make changes to XD on YARN config that correspond to XD-1499 changes",3
"Clean up MBean registration for failed module deployments
When a module fails to deploy (for instance an http module configured with a port that is already bound) subsequent attempts to deploy the module fail due to a JMX exception:  {noformat} java.lang.RuntimeException: org.springframework.context.ApplicationContextException: Failed to start bean 'org.springframework.integration.monitor.IntegrationMBeanExporter#0#beafb1fc-4423-4e4f-a88c-1655ea0fdcc5'; nested exception is org.springframework.jmx.export.UnableToRegisterMBeanException: Unable to register MBean [MessageChannelMonitor: [name=output, sends=0]] with key 'org.springframework.integration:type=MessageChannel,name=output'; nested exception is javax.management.InstanceAlreadyExistsException: xd.foo:module=http.0,component=MessageChannel,name=output   org.springframework.xd.dirt.server.ContainerRegistrar.deployStreamModule(ContainerRegistrar.java:447)   org.springframework.xd.dirt.server.ContainerRegistrar.onChildAdded(ContainerRegistrar.java:346)   org.springframework.xd.dirt.server.ContainerRegistrar.access$700(ContainerRegistrar.java:92)   org.springframework.xd.dirt.server.ContainerRegistrar$DeploymentListener.childEvent(ContainerRegistrar.java:655)   org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:494)   org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:488)   org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)   com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)   org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)   org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:485)   org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)   org.apache.curator.framework.recipes.cache.PathChildrenCache$11.run(PathChildrenCache.java:755)   java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)   java.util.concurrent.FutureTask.run(FutureTask.java:266)   java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)   java.util.concurrent.FutureTask.run(FutureTask.java:266)   java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)   java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)   java.lang.Thread.run(Thread.java:744) Caused by: org.springframework.context.ApplicationContextException: Failed to start bean 'org.springframework.integration.monitor.IntegrationMBeanExporter#0#beafb1fc-4423-4e4f-a88c-1655ea0fdcc5'; nested exception is org.springframework.jmx.export.UnableToRegisterMBeanException: Unable to register MBean [MessageChannelMonitor: [name=output, sends=0]] with key 'org.springframework.integration:type=MessageChannel,name=output'; nested exception is javax.management.InstanceAlreadyExistsException: xd.foo:module=http.0,component=MessageChannel,name=output   org.springframework.context.support.DefaultLifecycleProcessor.doStart(DefaultLifecycleProcessor.java:176)   org.springframework.context.support.DefaultLifecycleProcessor.access$200(DefaultLifecycleProcessor.java:51)   org.springframework.context.support.DefaultLifecycleProcessor$LifecycleGroup.start(DefaultLifecycleProcessor.java:346)   org.springframework.context.support.DefaultLifecycleProcessor.startBeans(DefaultLifecycleProcessor.java:149)   org.springframework.context.support.DefaultLifecycleProcessor.onRefresh(DefaultLifecycleProcessor.java:112)   org.springframework.context.support.AbstractApplicationContext.finishRefresh(AbstractApplicationContext.java:773)   org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:485)   org.springframework.boot.SpringApplication.refresh(SpringApplication.java:648)   org.springframework.boot.SpringApplication.run(SpringApplication.java:311)   org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:130)   org.springframework.xd.module.core.SimpleModule.initialize(SimpleModule.java:240)   org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:184)   org.springframework.xd.dirt.module.ModuleDeployer.deployAndStore(ModuleDeployer.java:174)   org.springframework.xd.dirt.module.ModuleDeployer.deployAndStore(ModuleDeployer.java:164)   org.springframework.xd.dirt.server.ContainerRegistrar.deployModule(ContainerRegistrar.java:227)   org.springframework.xd.dirt.server.ContainerRegistrar.deployStreamModule(ContainerRegistrar.java:429) 	... 18 more Caused by: org.springframework.jmx.export.UnableToRegisterMBeanException: Unable to register MBean [MessageChannelMonitor: [name=output, sends=0]] with key 'org.springframework.integration:type=MessageChannel,name=output'; nested exception is javax.management.InstanceAlreadyExistsException: xd.foo:module=http.0,component=MessageChannel,name=output   org.springframework.jmx.export.MBeanExporter.registerBeanNameOrInstance(MBeanExporter.java:610)   org.springframework.integration.monitor.IntegrationMBeanExporter.registerChannels(IntegrationMBeanExporter.java:837)   org.springframework.integration.monitor.IntegrationMBeanExporter.doStart(IntegrationMBeanExporter.java:459)   org.springframework.integration.monitor.IntegrationMBeanExporter.start(IntegrationMBeanExporter.java:410)   org.springframework.context.support.DefaultLifecycleProcessor.doStart(DefaultLifecycleProcessor.java:173) 	... 33 more Caused by: javax.management.InstanceAlreadyExistsException: xd.foo:module=http.0,component=MessageChannel,name=output   com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)   com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)   com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)   com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)   com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)   com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)   org.springframework.jmx.support.MBeanRegistrationSupport.doRegister(MBeanRegistrationSupport.java:195)   org.springframework.jmx.export.MBeanExporter.registerBeanInstance(MBeanExporter.java:663)   org.springframework.jmx.export.MBeanExporter.registerBeanNameOrInstance(MBeanExporter.java:606) 	... 37 more {noformat}",2
"Admin needs to clean up failed deployment attempts
If a container fails to deploy a module, the admin needs to clean up the {{/xd/deployments/modules/CONTAINER-ID/module}} path so that another attempt can be made to deploy that module to that container.",2
"Twitter Search results are not deserialized
Data is returned, but not human readable i.e. ""org.springframework.social.twitter.api.Tweet@4b20a104"".  Needs to be readable in log and or file.  ",3
"TwitterSearch does not deploy correctly if previous deploy fails
if you mess up you consumerSecret,  then deploy the stream, it will throw a 403, which is correct. Then you destroy, recreate, and redeploy with the correct key, source will not retrieve  any results.  The only solution was to bounce the single node.  Steps to repeat. 1) create a stream.  stream create twit --definition ""twittersearch --consumerKey=goodkey --consumerSecret=badsecret --query='Letterman'|log"" 2) deploy stream and get 403 3) destroy stream 4) create twittersearch stream with good keys 5) deploy stream 6) No twitter data is displayed",3
"Add ability to inject delimiter on pre-packaged jobs that deal with files.
nan",4
"Create documentation for Job listeners support
nan",1
"Update docs and samples now that deploy is false on job/stream creation
nan",4
"Eliminate stack trace on xd-container shutdown when active module running
After deploying stream (such as ""time | log""), the xd-container emits the following stacktrace(s) if the stream is in a deployed state when that xd-container process is halted via CTRL-C:    {code}  20:15:31,415  INFO main-EventThread module.ModuleDeployer:215 - removed SimpleModule [name=log, type=sink, group=s, index=1 @128936ff]  20:15:31,417 ERROR main-EventThread imps.CuratorFrameworkImpl:512 - Watcher exception  java.lang.IllegalStateException: org.springframework.context.annotation.AnnotationConfigApplicationContext@7bf4bc83 has been closed already    org.springframework.context.support.AbstractApplicationContext.assertBeanFactoryActive(AbstractApplicationContext.java:956)    org.springframework.context.support.AbstractApplicationContext.getBean(AbstractApplicationContext.java:978)    org.springframework.xd.module.core.SimpleModule.getComponent(SimpleModule.java:214)    org.springframework.xd.dirt.plugins.AbstractMessageBusBinderPlugin.unbindConsumerAndProducers(AbstractMessageBusBinderPlugin.java:140)    org.springframework.xd.dirt.plugins.stream.StreamPlugin.beforeShutdown(StreamPlugin.java:74)    org.springframework.xd.dirt.module.ModuleDeployer.beforeShutdown(ModuleDeployer.java:267)    org.springframework.xd.dirt.module.ModuleDeployer.destroyModule(ModuleDeployer.java:217)    org.springframework.xd.dirt.module.ModuleDeployer.handleUndeploy(ModuleDeployer.java:197)    org.springframework.xd.dirt.module.ModuleDeployer.undeploy(ModuleDeployer.java:169)    org.springframework.xd.dirt.server.ContainerRegistrar.undeployModule(ContainerRegistrar.java:252)    org.springframework.xd.dirt.server.ContainerRegistrar$StreamModuleWatcher.process(ContainerRegistrar.java:580)    org.apache.curator.framework.imps.NamespaceWatcher.process(NamespaceWatcher.java:56)    org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:522)    org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)  20:15:31,420  INFO main-EventThread server.ContainerRegistrar:250 - Undeploying module time-0: time type=source, deploymentProperties={count=1}  20:15:31,420  INFO main-EventThread module.ModuleDeployer:215 - removed SimpleModule [name=time, type=source, group=s, index=0 @51a42578]  20:15:31,420 ERROR main-EventThread imps.CuratorFrameworkImpl:512 - Watcher exception  java.lang.IllegalStateException: org.springframework.context.annotation.AnnotationConfigApplicationContext@6d223732 has been closed already    org.springframework.context.support.AbstractApplicationContext.assertBeanFactoryActive(AbstractApplicationContext.java:956)    org.springframework.context.support.AbstractApplicationContext.getBean(AbstractApplicationContext.java:978)    org.springframework.xd.module.core.SimpleModule.getComponent(SimpleModule.java:214)    org.springframework.xd.dirt.plugins.AbstractMessageBusBinderPlugin.unbindConsumerAndProducers(AbstractMessageBusBinderPlugin.java:144)    org.springframework.xd.dirt.plugins.stream.StreamPlugin.beforeShutdown(StreamPlugin.java:74)    org.springframework.xd.dirt.module.ModuleDeployer.beforeShutdown(ModuleDeployer.java:267)    org.springframework.xd.dirt.module.ModuleDeployer.destroyModule(ModuleDeployer.java:217)    org.springframework.xd.dirt.module.ModuleDeployer.handleUndeploy(ModuleDeployer.java:197)    org.springframework.xd.dirt.module.ModuleDeployer.undeploy(ModuleDeployer.java:169)    org.springframework.xd.dirt.server.ContainerRegistrar.undeployModule(ContainerRegistrar.java:252)    org.springframework.xd.dirt.server.ContainerRegistrar$StreamModuleWatcher.process(ContainerRegistrar.java:580)    org.apache.curator.framework.imps.NamespaceWatcher.process(NamespaceWatcher.java:56)    org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:522)    org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)  {code}  ",4
"stack trace on xd-admin restart when redploying streams
(the trace below occurs in the admin console but the Admin continues to handle subsequent deployment requests fine it seems)    To reproduce:    * start xd-admin and xd-container (just 1 of each)  * deploy 'time | log'  * kill both xd-admin and xd-container  * start xd-container by itself  * wait 10 seconds or so, then start xd-admin    result:  {code}  21:35:01,575 ERROR ContainersPathChildrenCache-0 cache.PathChildrenCache:550 -  org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /xd/deployments/modules/f89e5fdc-7dc1-49fb-93cc-d536ab853f12/s.source.time-0    org.apache.zookeeper.KeeperException.create(KeeperException.java:119)    org.apache.zookeeper.KeeperException.create(KeeperException.java:51)    org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:783)    org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:676)    org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:660)    org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)    org.apache.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:656)    org.apache.curator.framework.imps.CreateBuilderImpl.protectedPathInForeground(CreateBuilderImpl.java:441)    org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:431)    org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:411)    org.apache.curator.framework.imps.CreateBuilderImpl$4.forPath(CreateBuilderImpl.java:319)    org.apache.curator.framework.imps.CreateBuilderImpl$4.forPath(CreateBuilderImpl.java:255)    org.springframework.xd.dirt.server.ContainerListener.onChildAdded(ContainerListener.java:272)    org.springframework.xd.dirt.server.ContainerListener.childEvent(ContainerListener.java:152)    org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:494)    org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:488)    org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)    com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)    org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)    org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:485)    org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)    org.apache.curator.framework.recipes.cache.PathChildrenCache$11.run(PathChildrenCache.java:755)    java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)    java.util.concurrent.FutureTask.run(FutureTask.java:262)    java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)    java.util.concurrent.FutureTask.run(FutureTask.java:262)    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)    java.lang.Thread.run(Thread.java:744)  {code}  ",2
"Batch Basic Fails to launch job when rabbit is data transport
Deployed the batch basic as instructions prescribe.  Tests work for both singlenode and redis as a data transport.  However, while the job does deploy using rabbit  it does not launch.",3
"HdfsTextFileWriter incompatible with MaprFS
Use of FsShell in AbstractHdfsWriter.initializeCounterIfNecessary() causes MaprFS to throw an unimplemented operation exception.  Replacing the FsShell code with FileSystem.listStatus allows it to be used with MaprFs.",1
"Update instructions to how to setup admin to use RDBMS.
Need to update instructions to discuss the setup of the relational database requirement for the xd-admin.",2
"Need to be able setup ec2 env to test  job samples in spring-xd-samples repo
Currently in order to deploy a user created job we copy the job to the containers and admin server, and then bounce the servers.  The tests will need the ability to copy these jobs to the containers, & admin and then bounce the servers.   These sample jobs are located at https://github.com/spring-projects/spring-xd-samples",5
"Support deploying to multiple containers in EC2 acceptance tests
need the ability to support the --group option.",5
"Batch Notification Sample fails to execute
* payment_with_error.txt is not present in the project  * --makeUnique is not available for this command anymore  * Syntax for http post needs to be updated  * Need to add a job deploy command or --deploy  * Throws Exception when deployed (stacktrace attached).",3
"clean up dead entries in ZooKeeper /xd/deployments/modules
When starting and stopping xd containers there are entries left in the /xd/deployments/modules directory that will cause 'runtime modules' command to fail.    xd:>runtime modules   Command failed org.springframework.xd.rest.client.impl.SpringXDException: java.lang.RuntimeException: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /xd/deployments/modules/5201ac3f-e952-48a2-a807-4f0bb2dab82b/test.sink.hdfs-1/metadata    here the ""/xd/deployments/modules/5201ac3f-e952-48a2-a807-4f0bb2dab82b"" container is no longer running, but there is some data left over.  ",3
"Refactor duplicate code in Listeners
{{ContainerListener}}, {{StreamListener}}, and {{JobListener}} have duplicate code for deploying and undeploying modules. This needs to be consolidated. One possibility is for the deployment code to live in classes named {{StreamDeployer}} and {{JobDeployer}}.",10
"Proactively handle failed deployments
When a deployment fails on a container due to a misconfiguration, the container does not notify the admin. Instead the admin waits for the container to write out an ephemeral node to the definition path {{/xd/streams}} or {{/xd/jobs}} to indicate a successful deployment and if the path isn't written in 10 seconds the deployment is considered failed.    This ""timeout"" should be considered a heuristic failure, meaning that the container was not able to write out a response of success or failure. If the deployment fails, the container needs to indicate this by writing a node to ZK.",10
"Fix 'cannot find MessageBuilderFactory' warning
5:27:47,887 WARN DeploymentsPathChildrenCache-0 org.springframework.integration.context.IntegrationContextUtils:195 - No 'beanFactory' supplied; cannot find MessageBuilderFactory, using default.  a lot of those",1
"Fix Startup Messages
Started container : AdminServerApplication Documentation: https://github.com/SpringSource/spring-xd/wiki",1
"Remove --transport option except for single node
Since transport is now shared by Admin and Container, a command line arg is not appropriate since it allows the user to set them to different values which would break XD. The recommend way to configure transport is in servers.yml.  The command line arg is still valid for single node",2
"syslog source is not capturing log info.
Keep in mind.  This could be pbkac, on my part.  Please review and see if you can get it to work.",3
"JMS source can only connect to localhost
Can't connect to remote activemq instance.    Setup a jms-activemq.properties file with amq.url=tcp://:ec2-54-198-157-91.compute-1.amazonaws.com:61616.    Source always refers to defaults of tcp://localhost:61616.  Localhost works",3
"transform processor with script option is broken
Creating the following stream throws exception:    stream create s1 --definition ""http | transform --script=transform.groovy | log""  Command failed org.springframework.xd.rest.client.impl.SpringXDException: Error with option(s) for module transform of type processor:      valid: the 'script' and 'expression' options are mutually exclusive    The ExpressionOrScriptMixin's assertions to check if script and expression options are mutually exclusive `always` fails.",2
"Batch hashtag count throws exception when launched
1) Update Instructions to mention --hadoopDistro for both singlenode and shell.  Else demo will not work.  2) Pom needs to be updated to use 1.2.1 at the least.  3) I can see where hdfs is writing the results  4) throws NPE   Stacktrace is attached.",3
"Batch wordcount sample returns zero counts
if user already has /count/in on their hdfs the input file will not copy the sample file to hdfs.  need to make sure that a file is already present  if the hdfs  Documents have to be updated to mention that hadoopDistro needs to be added both xd-singlenode and xd-shell",3
"shell cp command fails
(from the wordcount sample):    {code}  xd:>! cp /tmp/nietzsche-chapter-1.txt /tmp/xd/input/wordCountFiles  You cannot specify option '' more than once in a single command  {code}",4
"Payload Conversion will need to migrated to M6 as soon as M6 is available
Currently uses m5 dependency. ",2
"Payload Conversion Sample throws exception.
After updating the dependency to use the snapshot (even with M5) the conversion throws an exception.  Stacktrace attached. ",2
"When using transform --script=foo.groovy shell displays error
When trying to create the stream for the gemfire example: stream create hashtags --definition ""tap:stream:tweets  > transform --script=tweetSummary.groovy | gemfire-server --keyExpression=payload['id']"" --deploy  The shell displays:  Command failed org.springframework.xd.rest.client.impl.SpringXDException: Error with option(s) for module transform of type processor:     valid: the 'script' and 'expression' options are mutually exclusive  I get the following error ",3
"Gauge & Rich Gauge fail to write results to redis for singlenode
Steps to reproduce:  stream create --name test --definition ""http --port=9090 | log""    stream create --name simplegauge --definition ""tap:stream:test > gauge""   http post --target http://localhost:9090 --data ""10""  redis-cli  get gauges.simplegauge    [the result]  redis 127.0.0.1:6379> get gauges.simplegauge  (nil)    Note:  It worked with admin/container but failed only on xd-singlenode.",3
"Rabbit Sink with explicit routingKey as 'string' SpEl literal expression fails
Following stream fails to work:    tream create s3 --definition ""http | rabbit --routingKey='mytest1'"" --deploy   Created and deployed new stream 's3'  xd:>http post --data ""testing""  > POST (text/plain;Charset=UTF-8) http://localhost:9000 testing  > 500 INTERNAL_SERVER_ERROR  > 500 INTERNAL_SERVER_ERROR    Error sending data 'testing' to 'http://localhost:9000'    The exception at the container log is:    07:24:57,245 ERROR pool-18-thread-4 http.NettyHttpInboundChannelAdapter:171 - Error sending message  org.springframework.messaging.MessageHandlingException: Expression evaluation failed: mytest1    org.springframework.integration.util.AbstractExpressionEvaluator.evaluateExpression(AbstractExpressionEvaluator.java:126)    org.springframework.integration.handler.ExpressionEvaluatingMessageProcessor.processMessage(ExpressionEvaluatingMessageProcessor.java:76)    org.springframework.integration.amqp.outbound.AmqpOutboundEndpoint.handleRequestMessage(AmqpOutboundEndpoint.java:196)    org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:170)    org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)    java.lang.reflect.Method.invoke(Method.java:606)    org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)    org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)    org.springframework.integration.monitor.SimpleMessageHandlerMetrics.handleMessage(SimpleMessageHandlerMetrics.java:106)    org.springframework.integration.monitor.SimpleMessageHandlerMetrics.invoke(SimpleMessageHandlerMetrics.java:86)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)    org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)    com.sun.proxy.$Proxy109.handleMessage(Unknown Source)    org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)    org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)    org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)    org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)    org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:255)    org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:223)    sun.reflect.GeneratedMethodAccessor100.invoke(Unknown Source)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)    java.lang.reflect.Method.invoke(Method.java:606)    org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)    org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)    org.springframework.integration.monitor.DirectChannelMetrics.monitorSend(DirectChannelMetrics.java:113)    org.springframework.integration.monitor.DirectChannelMetrics.doInvoke(DirectChannelMetrics.java:97)    org.springframework.integration.monitor.DirectChannelMetrics.invoke(DirectChannelMetrics.java:91)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)    org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)    com.sun.proxy.$Proxy54.send(Unknown Source)    org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:114)    org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:44)    org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:93)    org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendMessage(AbstractReplyProducingMessageHandler.java:260)    org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendReplyMessage(AbstractReplyProducingMessageHandler.java:241)    org.springframework.integration.handler.AbstractReplyProducingMessageHandler.produceReply(AbstractReplyProducingMessageHandler.java:205)    org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleResult(AbstractReplyProducingMessageHandler.java:199)    org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:177)    org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)    org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)    org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)    org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)    org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)    org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:255)    org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:223)    org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:114)    org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:44)    org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:93)    org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendMessage(AbstractReplyProducingMessageHandler.java:260)    org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendReplyMessage(AbstractReplyProducingMessageHandler.java:241)    org.springframework.integration.handler.AbstractReplyProducingMessageHandler.produceReply(AbstractReplyProducingMessageHandler.java:205)    org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleResult(AbstractReplyProducingMessageHandler.java:199)    org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:177)    org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)    org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)    org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)    org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)    org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)    org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:255)    org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:223)    sun.reflect.GeneratedMethodAccessor100.invoke(Unknown Source)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)    java.lang.reflect.Method.invoke(Method.java:606)    org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)    org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)    org.springframework.integration.monitor.DirectChannelMetrics.monitorSend(DirectChannelMetrics.java:113)    org.springframework.integration.monitor.DirectChannelMetrics.doInvoke(DirectChannelMetrics.java:97)    org.springframework.integration.monitor.DirectChannelMetrics.invoke(DirectChannelMetrics.java:91)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)    org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)    com.sun.proxy.$Proxy111.send(Unknown Source)    org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:114)    org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:44)    org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:93)    org.springframework.integration.endpoint.MessageProducerSupport.sendMessage(MessageProducerSupport.java:98)    org.springframework.integration.x.http.NettyHttpInboundChannelAdapter.access$300(NettyHttpInboundChannelAdapter.java:69)    org.springframework.integration.x.http.NettyHttpInboundChannelAdapter$Handler.messageReceived(NettyHttpInboundChannelAdapter.java:168)    org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)    org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)    org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)    org.jboss.netty.handler.execution.ChannelUpstreamEventRunnable.doRun(ChannelUpstreamEventRunnable.java:43)    org.jboss.netty.handler.execution.ChannelEventRunnable.run(ChannelEventRunnable.java:67)    org.jboss.netty.handler.execution.OrderedMemoryAwareThreadPoolExecutor$ChildExecutor.run(OrderedMemoryAwareThreadPoolExecutor.java:314)    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)    java.lang.Thread.run(Thread.java:744)  Caused by: org.springframework.expression.spel.SpelEvaluationException: EL1008E:(pos 0): Property or field 'mytest1' cannot be found on object of type 'org.springframework.messaging.support.GenericMessage' - maybe not public?    org.springframework.expression.spel.ast.PropertyOrFieldReference.readProperty(PropertyOrFieldReference.java:215)    org.springframework.expression.spel.ast.PropertyOrFieldReference.getValueInternal(PropertyOrFieldReference.java:85)    org.springframework.expression.spel.ast.PropertyOrFieldReference.getValueInternal(PropertyOrFieldReference.java:78)    org.springframework.expression.spel.ast.SpelNodeImpl.getTypedValue(SpelNodeImpl.java:114)    org.springframework.expression.spel.standard.SpelExpression.getValue(SpelExpression.java:111)    org.springframework.integration.util.AbstractExpressionEvaluator.evaluateExpression(AbstractExpressionEvaluator.java:159)    org.springframework.integration.util.AbstractExpressionEvaluator.evaluateExpression(AbstractExpressionEvaluator.java:119)  	... 91 more",2
"Document append support, else filepollhdfs writes empty file to hdfs
When testing in both singlenode and cluster (redis), XD throws exception (stacktrace attached).  The file is created on hdfs, but it is empty.    [Steps to recreate Using Hadoop12]  1) job create myjob --definition ""filepollhdfs --names=forename,surname,address"" --deploy   2) stream create csvStream --definition ""file --ref=true --dir=/tmp/dug --pattern=*.csv > queue:job:myjob"" --deploy  3) use excel to create a 3 column spreadsheet and save as csv.   4) Copy csv to /tmp/dug directory",3
"Document append configuration, else jdbchdfs writes empty file to hdfs
Similar to XD-1565  so I'd link these 2 together.    [Steps to reproduce on Hadoop12]  1) Created Table People with columns forename,surname and address (use the result from filejdbc)  2) job create myjob --definition ""jdbchdfs --sql='select col1,col2,col3 from some_table'""  3)job launch myjob  4) myjob is created on hdfs but with zero bytes  5) throws an exception, stack trace attached.",3
"Update XD-Ec2 deployer to use XD_TRANSPORT 
XD-EC2 must use environment variable XD_TRANSPORT instead of --transport to declare data-transport for XD.",3
"Update documentation related to transport and controlTransport
e.g. Need to update this section (maybe others):  https://github.com/spring-projects/spring-xd/wiki/Running-Distributed-Mode    Remove all mentions of Control Bus, and replace any mentions of the --transport cmd line arg with the xd.transport property in yml.  ",2
"Rename reactor-tcp module to reactor-ip since it also supports udp
the --transport option allows 'udp' as well as 'tcp/",2
"Deployed modules MBeans are not accessible via Jolokia
When a module is deployed, it doesn't use Jolokia auto configuration (which requires an embedded servlet container configuration). But, the module context isn't using a servlet context.    From SimpleModule:    application = new SpringApplicationBuilder().sources(PropertyPlaceholderAutoConfiguration.class).web(false);     and hence, the MBeans that are exposed by the deployed modules aren't accessible via Jolokia.  We definitely don't want SimpleModule to use web application context but we need to figure out if we can use the container's management port to expose the deployed modules MBeans via jolokia.  ",5
"JLine 1 is brought up (and shows in IDE) through ZK/curator
We don't want jline1 anymore.  This shows in IDE only (either run shell integration tests, or run the shell as Gunnar mentioned)",1
"Unable to delete composed module
After creating a composed module, I am unable to delete it.  [Steps to reproduce] xd:>module compose doo --definition ""filter --expression=payload.contains('doo') | file"" Successfully created module 'doo' with type sink xd:>module  module compose    module delete     module display    module info       module list        xd:>module delete --name doo --type sink java.lang.StringIndexOutOfBoundsException: Failed to convert 'doo' to type QualifiedModuleName for option 'name,' String index out of range: -1 xd:> ",3
"Update diagrams that show control transport
The first 2 images in the documentation section linked below should no longer show redis, rabbit, or local for the communication between Admins and Containers. Rather we need to show ZooKeeper.    https://github.com/spring-projects/spring-xd/wiki/Architecture",4
"JDBC Acceptance tests must jdbc props vs. configProps setting.
In this case we will use environment variables to set the JDBC sink settings.  Thus we will just remove code.",3
"Add UDP support to reactor-syslog source module
Currently the reactor-syslog source module only supports TCP.    Once we add UDP support, we can probably remove the existing syslog-tcp and syslog-udp modules.",8
"Remove unused .properties files in config and update docs
There are some properties files in the config directory that no longer are needed. We should clean that up and also remove/update any documentation references to these files",3
"Changing externalized module config properties at runtime
Once the container starts up and the module is deployed, the externalized module configuration properties could not be changed for the subsequent modules of same type.    Here is the scenario for a stream ""http | transform | log"", with XD_MODULE_CONFIG_LOCATION and XD_MODULE_CONFIG_NAME using their default values.   In, ${xd.config.home}/modules/modules.yml, I have:  processor:    transform:      expression: ""'Inside modules.yml'""    and, in, ${xd.config.home}/modules/processor/transform/transform.properties, I have:  expression: ""'First module: inside transform.properties'""    Now, I deploy this stream: ""http | transform | log"".    Lets say, I have another stream that uses the transform module, but this time I want to change the expression in ${xd.config.home}/modules/processor/transform/transform.properties to,  expression: ""'Second module'""    Now, when the stream containing this transform module gets deployed, it uses the same transform.properties that is used by the previously deployed transform module.    What I understand from this behavior is that, the EnvironmentAwareModuleOptionsMetadataResolver caches the module environments for a given module type and name. When the same module is deployed from a given stream again, it uses the stored module environment and doesn't refresh/load the property sources from the module config locations mentioned above. Though this is good in one way that same module environment is re-used, changing the externalized module config properties would have no affect after the first module of same type/name is deployed.    Though the EnvironmentAwareModuleOptionsMetadataResolver is used by both admin and container, this JIRA focuses more on the container side.  There is one valid point with the current behavior where the module environment is cached and won't change. But is this by design?",4
"Release 1.0.0.M6
nan",4
"Undeploy modules when container disconnected from ZK
Consider a module running in a container when it is disconnected from ZK:    {noformat}  12:30:13,021  WARN inbound.t.0-redis:queue-inbound-channel-adapter1 logger.t:150 - 2014-04-15 12:30:13  12:30:14,025  WARN inbound.t.0-redis:queue-inbound-channel-adapter1 logger.t:150 - 2014-04-15 12:30:14  12:30:15,029  WARN inbound.t.0-redis:queue-inbound-channel-adapter1 logger.t:150 - 2014-04-15 12:30:15  12:30:16,031  WARN inbound.t.0-redis:queue-inbound-channel-adapter1 logger.t:150 - 2014-04-15 12:30:16  12:30:32,590  WARN inbound.t.0-redis:queue-inbound-channel-adapter1 logger.t:150 - 2014-04-15 12:30:32  12:37:42,985  WARN inbound.t.0-redis:queue-inbound-channel-adapter1 logger.t:150 - 2014-04-15 12:37:42  12:37:43,398  INFO main-SendThread(fe80:0:0:0:0:0:0:1%1:2181) zookeeper.ClientCnxn:1096 - Client session timed out, have not heard from server in 430809ms for sessionid 0x145662be03e0002, closing socket connection and attempting reconnect  12:37:43,985  INFO main-SendThread(localhost:2181) zookeeper.ClientCnxn:975 - Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error)  12:37:43,986  INFO main-SendThread(localhost:2181) zookeeper.ClientCnxn:852 - Socket connection established to localhost/0:0:0:0:0:0:0:1:2181, initiating session  12:37:43,988  WARN inbound.t.0-redis:queue-inbound-channel-adapter1 logger.t:150 - 2014-04-15 12:37:43  12:37:43,989  INFO main-SendThread(localhost:2181) zookeeper.ClientCnxn:1094 - Unable to reconnect to ZooKeeper service, session 0x145662be03e0002 has expired, closing socket connection  {noformat}    Currently the module for the disconnected container continues to execute:    {noformat}  12:37:45,994  WARN inbound.t.0-redis:queue-inbound-channel-adapter1 logger.t:150 - 2014-04-15 12:37:45  12:37:46,997  WARN inbound.t.0-redis:queue-inbound-channel-adapter1 logger.t:150 - 2014-04-15 12:37:46  12:37:48,000  WARN inbound.t.0-redis:queue-inbound-channel-adapter1 logger.t:150 - 2014-04-15 12:37:47  12:37:48,094 ERROR main-EventThread imps.CuratorFrameworkImpl:512 - Watcher exception  org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /xd    org.apache.zookeeper.KeeperException.create(KeeperException.java:127)    org.apache.zookeeper.KeeperException.create(KeeperException.java:51)    org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:1155)    org.apache.curator.framework.imps.GetDataBuilderImpl$4.call(GetDataBuilderImpl.java:302)    org.apache.curator.framework.imps.GetDataBuilderImpl$4.call(GetDataBuilderImpl.java:291)    org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)    org.apache.curator.framework.imps.GetDataBuilderImpl.pathInForeground(GetDataBuilderImpl.java:287)    org.apache.curator.framework.imps.GetDataBuilderImpl.forPath(GetDataBuilderImpl.java:279)    org.apache.curator.framework.imps.GetDataBuilderImpl.forPath(GetDataBuilderImpl.java:41)    org.springframework.xd.dirt.server.ContainerRegistrar$StreamModuleWatcher.process(ContainerRegistrar.java:609)    org.apache.curator.framework.imps.NamespaceWatcher.process(NamespaceWatcher.java:56)    org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:522)    org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)  12:37:48,097  INFO main-EventThread state.ConnectionStateManager:194 - State change: SUSPENDED  12:37:48,097  INFO ConnectionStateManager-0 zookeeper.ZooKeeperConnection:262 - >>> Curator disconnected event: SUSPENDED  12:37:48,097  WARN ConnectionStateManager-0 server.ContainerRegistrar:325 - >>> disconnected container: 88ba115b-6190-497a-a67c-df1e295bf158  12:37:49,001  WARN inbound.t.0-redis:queue-inbound-channel-adapter1 logger.t:150 - 2014-04-15 12:37:49  12:37:50,004  WARN inbound.t.0-redis:queue-inbound-channel-adapter1 logger.t:150 - 2014-04-15 12:37:50  12:37:51,008  WARN inbound.t.0-redis:queue-inbound-channel-adapter1 logger.t:150 - 2014-04-15 12:37:51  12:37:52,012  WARN inbound.t.0-redis:queue-inbound-channel-adapter1 logger.t:150 - 2014-04-15 12:37:52  12:37:53,016  WARN inbound.t.0-redis:queue-inbound-channel-adapter1 logger.t:150 - 2014-04-15 12:37:53  12:37:54,021  WARN inbound.t.0-redis:queue-inbound-channel-adapter1 logger.t:150 - 2014-04-15 12:37:54  12:37:55,023  WARN inbound.t.0-redis:queue-inbound-channel-adapter1 logger.t:150 - 2014-04-15 12:37:55  ...  {noformat}    This container should not continue executing the module because the leader admin will likely select another container to execute this module. If and when this container reconnects to ZK, it can be (re)assigned modules for deployment.    This can be done via a simple undeployment; or we may even consider closing and reopening the application context.",4
"XD config home should use XD_CONFIG_LOCATION if this is set
If XD_CONFIG_LOCATION is set, then XD runtime's xd.config.home should use that. otherwise, they point to two different paths.",2
"Temporary race condition between deployment and ""runtime modules"" command
The ""runtime modules"" command can show a failure between the deployment command and the actual deployment on the container node, especially if there is a network hop. This clears up once the module is fully deployed.    {code}  xd:>stream create --name trois3 --definition ""time | jdbc"" --deploy   Created and deployed new stream 'trois3'  xd:>runtime modules   Command failed org.springframework.xd.rest.client.impl.SpringXDException: java.lang.RuntimeException: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /xd/deployments/modules/bc95653e-9da5-4738-beb2-f215e4003318/trois3.source.time-0/metadata    xd:>runtime modules     Module                Container Id                          Options    --------------------  ------------------------------------  ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------    trois3.source.time-0  bc95653e-9da5-4738-beb2-f215e4003318  {format=yyyy-MM-dd HH:mm:ss, fixedDelay=1}    wintu.sink.jdbc-1     bc95653e-9da5-4738-beb2-f215e4003318  {tableName=${xd.stream.name}, url=jdbc:hsqldb:hsql://carbon:9102/xdjob, columns=payload, driverClassName=org.hsqldb.jdbc.JDBCDriver, initializeDatabase=false, initializerScript=init_db.sql, username=sa}    trois3.sink.jdbc-1    d0ad8eda-be27-46ac-86be-e43b5d9921af  {tableName=${xd.stream.name}, url=jdbc:hsqldb:hsql://carbon:9102/xdjob, columns=payload, driverClassName=org.hsqldb.jdbc.JDBCDriver, initializeDatabase=false, initializerScript=init_db.sql, username=sa}    wintu.source.time-0   befa5f27-aac3-4d94-9171-77c07036ec75  {format=yyyy-MM-dd HH:mm:ss, fixedDelay=1}  {code}     ",3
"Add support for BSON
This has been encountered in a POC.  Could take the form of a processor (bson -> json) or better yet if possible, be added at the automatic type conversion level",4
"Sample app that can process and analyze network packets
Create an example application that demonstrates the processing / analysis from a stream of network packets.  A potential scenario could be the detection of ongoing cyber attacks by scanning for TLS packets that what to abuse a SSL vulnerability aka ""heart bleed"".  A library that could help with this is: http://jnetpcap.com/",1
"Tab completion does not work for stream definition following > 
>stream create ""tap:stream:foo >     does not suggest modules",8
"Stream should not be in deployed state following module failure. 
Run singlenode. Ensure twitterstream credentials are not valid. e.g.,  no consumerKey property. This is the default state.    >stream create tweets --definition ""twitterstream | log"" --deploy  Created and deployed stream 'tweets'    Meanwhile, Singlenode throws an exception, the stacktrace below     xd:>stream list    Stream Name  Stream Definition    Status    -----------  -------------------  --------    tweets       twitterstream | log  deployed    {code}  15:54:07,298 ERROR DeploymentsPathChildrenCache-0 cache.PathChildrenCache:550 -  java.lang.RuntimeException: org.springframework.beans.factory.BeanDefinitionStoreException: Invalid bean definition with name 'twitterTemplate' defined in URL [file:/Users/dturanski/spring-xd/spring-xd-1.0.0.M6/xd/modules/source/twitterstream/config/twitterstream.xml]: Could not resolve placeholder 'consumerKey' in string value ""${consumerKey}""; nested exception is java.lang.IllegalArgumentException: Could not resolve placeholder 'consumerKey' in string value ""${consumerKey}""    org.springframework.xd.dirt.server.ContainerRegistrar.deployStreamModule(ContainerRegistrar.java:448)    org.springframework.xd.dirt.server.ContainerRegistrar.onChildAdded(ContainerRegistrar.java:347)    org.springframework.xd.dirt.server.ContainerRegistrar.access$700(ContainerRegistrar.java:93)    org.springframework.xd.dirt.server.ContainerRegistrar$DeploymentListener.childEvent(ContainerRegistrar.java:678)    org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:494)    org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:488)    org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)    com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)    org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)    org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:485)    org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)    org.apache.curator.framework.recipes.cache.PathChildrenCache$11.run(PathChildrenCache.java:755)    java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)    java.util.concurrent.FutureTask.run(FutureTask.java:262)    java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)    java.util.concurrent.FutureTask.run(FutureTask.java:262)    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)    java.lang.Thread.run(Thread.java:744)  Caused by: org.springframework.beans.factory.BeanDefinitionStoreException: Invalid bean definition with name 'twitterTemplate' defined in URL [file:/Users/dturanski/spring-xd/spring-xd-1.0.0.M6/xd/modules/source/twitterstream/config/twitterstream.xml]: Could not resolve placeholder 'consumerKey' in string value ""${consumerKey}""; nested exception is java.lang.IllegalArgumentException: Could not resolve placeholder 'consumerKey' in string value ""${consumerKey}""  {/code}",3
"Provide module configuration templates for twitter sources
Provide module templates including required property keys but not values for  $XD_MODULE_CONFIG/source/twitter*/twitter*.properties. Also look for any other packaged modules that have required properties that should be statically configured and we cannot provide defaults.  The Source modules document should be more clear regarding the configuration of these properties.",2
"PropertySource leakage between runtime and modules
in EnvironmentAwareModuleOptionsMetadataResolver::loadPropertySources, the call to merge(parentEnv) was added to inherit the active profiles of the runtime.    Sadly, it added the parentEnv property sources by side effect.    Note that the jdbc module defaults rely on this bug",5
"Support Groovy bean definitions as XD extensions
Modify the PluginContextExtensionsInitializer to consume .groovy bean definitions as well as XML. ",3
"Move ephemeral nodes from /xd/streams to /xd/deployments/streams
To have a clear separation of definition vs runtime information, move the ephemeral nodes written by containers from {{/xd/streams/stream-name}} to {{/xd/deployments/streams/stream-name}}. Same for jobs.",2
"Flatten out ephemeral nodes 
Flatten out ephemeral nodes written by containers when deploying modules. For instance, instead of {{.../streams/moduleType/moduleLabel/container}} use {{.../streams/moduleType.moduleLabel.container}}.    This change allows us to derive state for a stream/job without having to traverse multiple layers of znodes. This is a big deal because:  * each level of children requires a network call  * Curator can only cache one level of children  ",5
"Make transport configuration extensible. 
Change message-bus.xml to read     <import resource=""classpath*:/META-INF/spring-xd/transports/${XD_TRANSPORT}-bus.xml""/>    So new transports may be configured in external jars",1
"Make transport serialization configurable
The refactoring done for M6 prevents overriding ""codec"" bean configured for MessageBus. Since MB is now in SharedServerContext, that context can only be altered by a custom OrderedContextInitializer, for example. There is currently no mechanism provided by the BootStrapContext for dynamically loading a user's OrderedContextInitializer. ",4
"Remove aliasHint flag usage when binding producer/consumer to MessageBus 
The MessageBus interface uses the aliasHint flag when binding consumer/producer on a point-to-point channel.     Actually, the aliasHint is only needed when computing Source/Sink channel names in case named channel names. Otherwise, indexed channel names will be used for the input/output channel name.     The only place where aliasHint is used in the message bus is on the LocalMessageBus where it provides a way to choose the channel provider (direct/queue channel) based on the alias hint. Otherwise, it is not needed in message bus bindproducer/consumer.    We need to simplify this.",3
"Rabbit Source Should Expose More Container Options
acknowlege-more, tx-size, prefetch-count, concurrency etc.",3
"Update to Spring AMQP 1.3.2
If the rabbit source receives a message it can't convert, a {{MessageConversionException}} is thrown and the message is rejected (and requeued), causing an endless loop.    Add an {{ErrorHandler}} to the inbound adapter to detect and convert {{MCE}} to {{AmqpRejectAndDontRequeueException}}.    Also consider adding a retry interceptor to do the same for exceptions in modules (when using local transport).",3
"Use MessageBus Binding to start() underlying endpoint
The messagebus implementations, upon registration of consumer and producer from/to messagebus the corresponding endpoints start. Instead of directly calling the start() on adapter/consumer we can call the corresponding Binding's start() which calls the underlying endpoint to start.   This is in-line with the way the corresponding endpoints are stopped (using Binding's stop()) during undeploy/destroy.",1
"Change SpringSource references in pom.xml to Spring/spring.io
This is currently in the M6 pom:      <organization>      <name>SpringSource</name>      <url>http://springsource.org</url>    </organization>  ",3
"Validate existence of batch job at the admin side
Since the batch job repository is not intended to be deleted, it is possible to have a batch job that already exists in the batch job repo even if the batch job definition is destroyed in XD. When a new job definition is created, we need to add a validation for the same job definition name against the batch job repository. Currently, we will only see a failure when the job is actually deployed into the container (when the batch job repository is updated during the deployment).",1
"Naming consistency for named channels
There seems to be some inconsistency with the naming strategy for the named channels.    For example:    If we create a job ""j1"", the job launching request queue name in the message broker would ""job:j1"". To send a launching request, we can either use ""queue:job:j1"" or ""job:j1"" (both seems to work).    If we create a stream with the named p2p channel ""foo"" we expect to use the syntax ""queue:foo"" and the message broker will have the queue name ""queue:foo""    The StreamConfigParser resolves the source/sink channel names for tap to deduce the module index from the channel component: ChannelNode.resolve(StreamLookupEnvironment env)    But, in case of the named channels that have prefix ""topic:"" or ""queue:"", their names are used as is the only exception in a case where (From StreamConfigParser's eatChannelReference(boolean tapAllowed)     {code:java}  // queue:XXX  // topic:XXX  if (firstToken.data.equalsIgnoreCase(""queue"")) {      channelType = ChannelType.QUEUE;  }  else if (firstToken.data.equalsIgnoreCase(""topic"")) {      channelType = ChannelType.TOPIC;  }  // TODO: DT not sure if this is the best way to handle  // StreamConfigParserTests.substreamsWithSourceChannels()  if (channelScopeComponents.size() >= 3) {      channelScopeComponents.remove(0);  }  {code}  The above code makes sure, ""queue:job:jobname"" still points ""job:jobname"".    We need some consistency when referring to the names of queues for the named channels above. Something like this:  ""queue:job:j1"" for job launching request queue   and ""queue:foo"" for the named p2p channel queue    or, a better strategy the has consistency across the named channels.",4
"JMS Source on EC2 only uses localhost for activemq broker
[Problem]  On a EC2 container jms-activemq.properties was configured to use a activemq broker on a different host, it still referred to localhost.    On my local mac, I was able to updated the jms-activemq.properties with an activemq on a different host and it worked.    [work-around]  While not recommended you can set the amq.url in the jms-activemq-infrastructure-context.xml.    [Steps to reproduce]  1) Deploy a single admin/container using xd-ec2.    2) create a jms-activemq.properties file in the spring-xd-1.0.0.BUILD-SNAPSHOT/xd/ where it refers to a broker on another machine (ec2-54-221-32-82.compute-1.amazonaws.com).    3) Create a stream with JMS as its source.",3
"XD-EC2 will have to support the --hadoopDistro command line for xd-container
nan",3
"Simplify/Refactor UI controllers
The UI controllers in spring-xd/spring-xd-ui/app/scripts/controllers.js definitions look overly complicated to get the modularization work.   We can possibly refactor and make it look clean; especially we will follow this as the example for subsequent controllers definitions.",3
"Parser fails on + after literal within an expression
This fails:  {code}  xd:>stream create s --definition ""http | transform --expression='hi'+payload | log""    Command failed org.springframework.xd.rest.client.impl.SpringXDException: XD115E:(pos 34): unexpected data in stream definition '+'  http | transform --expression='hi'+payload | log  {code}    But this works:  {code}  xd:>stream create s --definition ""http | transform --expression=payload+'hi' | log""    Created new stream 's'  {code}  ",2
"Job display command handling null date value for execution endtime
If the job display command is executed for JobExecution and StepExecution list, it may be possible that the Job/Step execution's endTime still null (because the status is still unknown and not completed). In this case, the display command throws assertion failure here:    Caused by: java.lang.IllegalArgumentException: The provided date must not be null.    org.springframework.util.Assert.notNull(Assert.java:112)    org.springframework.xd.shell.util.CommonUtils.getUtcTime(CommonUtils.java:144)    org.springframework.xd.shell.command.JobCommands.display(JobCommands.java:249)",1
"UI: The user can create a new job definition by selecting a job template and providing additional configuration properties
nan",6
"UI: The user should provide username/password to gain access to the UI
Secure Admin UI to challenge users to enter username and password to gain access.",3
"UI: The user can view progress information about a given step
nan",6
"UI: The user can stop a specific job execution
nan",3
"UI: The user can view detailed information about steps for a specific job execution based on the type of the job
This task will require additional Jiras in XD or also Batch. Currently we don't capture Step types.",4
"Fix JobCommandTests' verification of shell result table rows using specific index
Some of the tests in JobCommandTests use the verification of shell command results table row on a specific row (mostly first row) like this:    String id = jobExecutions.getRows().get(0).getValue(1);  		displayJobExecution(id);    It is possible that the list of table rows may have the intended row in different order. This poses inconsistent test failures. ",1
"UI: The user should be able to view the log file for a specific job execution
Will require additional server-side Jiras",4
"Add support for typed Batch Steps
This may require additional support (Jiras) for Spring Batch",8
"UI: For Hadoop Steps - provide a link to the MapReduce Job details in Hadoop. 
nan",6
"UI: The user should be able to view a graphical representation of the job
This may be broken up into multiple Jira issues. Provide a generic approach to render Batch jobs graphically.   Second, especially for Hadoop components - Provide step-type-dependent renderings of Batch components - see: XD-1622",12
"UI: The user should be able to view metrics about an executed job.
nan",4
"UI: Improve the filtering capabilities of jobs that are executing/have executed
* Show only the jobs that you have created vs. those of others. (Requires Security - e.g. XD-1616) - Probably a separate issue    * Show only jobs that are in a specific status/state,  running vs. other states.  * Show only jobs for the past x number of days.  * Show only jobs whose name matches a simple string, e.g. â€˜userAnalysisâ€™  ",4
"UI: When launching a job - Required parameters should be indicated and their names prepopulated
nan",6
"UI: The user can view the job properties that were specified when the job definition was created as well as job parameters when it was launched/executed
nan",6
"RabbitMessageBus should prefix all created queues with a prefix in order to support HA
To configure Rabbit HA a naming convention should be used to identify the queue that need to be mirrored.  ",3
"Packaging of lib directory for shell contains many jars that are not used
Between M5 and M6 the size of the shell/lib directory went up ~50 MB.  Investigate and remove jars from being packaged that are not used.",2
"Update hdfs sink docs
Options that are not covered:  --codec  --idleTimeout  --inUsePrefix  --inUseSuffix  --inputType  --overwrite    Options Renamed:  --filename is now --fileName",1
"Use unique queue names in shell tests
There seems to be some cross talk among the shell integration tests.   It looks like the same singlenode application might get shared among the test classes when they run in parallel.    Using unique queue names across the tests seem to fix the issue for now.",1
"Fix cross-talk among the shell integration tests
It looks like the singlenode application used by each of the shell integration test class is shared by other test classes as well. This causes some issues that are common to these tests. We need to avoid such scenarios.",2
"Update Spring Integration Version to 4.0.0.RELEASE
nan",1
"Documentation: Hovering over some of the examples corrupts the text
If you mouse over any of the examples in the documentation, the grey boxes, containing code, shell commands, etc., typically in the upper right hand corner a label for the type of code/example will appear.  E.g., 'Ruby', 'Javascript' ,etc.      1) The labels that appear seem to be random and incorrect.  Shell scripts show as 'Ruby' and 'Javascript'.    2) More importantly, on some of the examples the label appears in front of and part of the example, corrupting the example.  To see this hover your mouse over the two examples, grey boxes, here: http://docs.spring.io/spring-xd/docs/1.0.0.M6/reference/html/#_xd_shell_in_distributed_mode    There may be more but this is the ones I noticed.      -Derek",2
"servers.yaml's 'xd: -> transport: rabbit' overrides xd-singlenode's default of local transport
When working w/ SXD xd-singlenode, out of the box, it defaults to using all embedded components (transport, analytics, hsqldb, & zookeeper), which is easy and a great way to get going.  This is also great for development.    When I then started trying out the M6 distributed mode I set my transport to rabbit in servers.yaml (now that the --transport option is gone).  Rabbit is my preferred transport here.    I then went back to running the singlenode, for simplicity, and then got an exception saying that the singlenode couldn't contact RabbitMQ/AMQP (I was no longer running rabbit).  I then had to add the '--transport local' flag back to xd-singlenode.      Having the --transport option on xd-singlenode but not on xd-container is confusing.  Also I would expect xd-singlenode to default to local transport unless I specify another option in --transport.    -Derek",1
"Re-enable JSHint during grunt build
JSHint should be enabled in grunt build. There are few minor issues and needs to be fixed.   ",1
"Add a 'rank' expression to be used by the container matcher
following the HTCondor model for resource assignment, the use of a 'rank' expression that evaluates to an integer is used to order the containers that match the current 'criteria' expression.  This allows you to setup ranks such as 'prefer the machine with the most free memory' or 'prefer a machine from groupa'  (higher rank values match first).  From HTCondor Presentation Rank * The rank expression is evaluated into a number for every potential matching machine. * A machine with a higher number will be preferred over a machine with a lower number.  Rank Examples  * Prefer machines with more Mips: ** Rank = Mips  * Prefer more memory, but add 100 to the rank if the machine is Solaris 2.7: ** Rank = Memory + 100*(OpSys==â€œSOLARIS27)â€  * Prefer machines with a high ratio of memory to cpu performance: ** Rank = Memory/Mips  * Prefer machines that will checkpoint in Bologna: ** Rank = (CkptServer==â€œckpt.bo.infn.itâ€)   ",5
"Add 'xd-container' requirements and rank expressions
Just as a stream or a job may state its needs and preferences for container assignment using the 'criteria' or 'rank' expressions, the xd-containers themselves can specify needs and preferences using the same 'critera' and 'rank' expressions.  These would be sent to the xd-admin server and used in the ContainerMatcher, but is evaluated using the 'stream' or 'job' as the evaluation context.  Examples  * Requirements ** Require that this container only runs streams ** Never run jobs belonging to groupA  * Rank ** Prefer to run groupB's job.  ",10
"Add information that is updated in real-time for use in container matching
Information related to an xd-container process and/or machine that is not static, such as 'group', e.g. free memory, number of deployed streams, should be available for use as variables in the evaluation context of the criteria SpEL expression in the admin's container matcher.  A good candidate for the source of this information are system MBeans.  See http://docs.oracle.com/javase/7/docs/api/java/lang/management/package-summary.html   ",8
"Upon a container departure, redeployment of batch job fails on an existing container
When there are multiple containers (A, B and C) and a batch job is deployed into one of the containers A. When the container A goes down, the admin server tries re-deploy the job module that was deployed in container A into other matching container. But, when the re-deployment happens, it tries to update the distributed job locator as if a new job is being deployed and following exception is thrown:    17:13:38,811 ERROR DeploymentsPathChildrenCache-0 cache.PathChildrenCache:550 -   java.lang.RuntimeException: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'job': Post-processing of the FactoryBean's object failed; nested exception is org.springframework.xd.dirt.job.BatchJobAlreadyExistsException: Batch Job with the name myjob3 already exists    org.springframework.xd.dirt.server.ContainerRegistrar.deployJob(ContainerRegistrar.java:411)    org.springframework.xd.dirt.server.ContainerRegistrar.onChildAdded(ContainerRegistrar.java:355)    org.springframework.xd.dirt.server.ContainerRegistrar.access$8(ContainerRegistrar.java:349)    org.springframework.xd.dirt.server.ContainerRegistrar$DeploymentListener.childEvent(ContainerRegistrar.java:695)    org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:494)    org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:488)    org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)    com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:253)    org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)    org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:485)    org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)    org.apache.curator.framework.recipes.cache.PathChildrenCache$11.run(PathChildrenCache.java:755)    java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)    java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)    java.util.concurrent.FutureTask.run(FutureTask.java:166)    java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)    java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)    java.util.concurrent.FutureTask.run(FutureTask.java:166)    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)    java.lang.Thread.run(Thread.java:722)  Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'job': Post-processing of the FactoryBean's object failed; nested exception is org.springframework.xd.dirt.job.BatchJobAlreadyExistsException: Batch Job with the name myjob3 already exists    org.springframework.beans.factory.support.FactoryBeanRegistrySupport.doGetObjectFromFactoryBean(FactoryBeanRegistrySupport.java:167)    org.springframework.beans.factory.support.FactoryBeanRegistrySupport.getObjectFromFactoryBean(FactoryBeanRegistrySupport.java:103)    org.springframework.beans.factory.support.AbstractBeanFactory.getObjectForBeanInstance(AbstractBeanFactory.java:1514)    org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:252)    org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:195)    org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:699)    org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:760)    org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:482)    org.springframework.boot.SpringApplication.refresh(SpringApplication.java:648)    org.springframework.boot.SpringApplication.run(SpringApplication.java:311)    org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:130)    org.springframework.xd.module.core.SimpleModule.initialize(SimpleModule.java:241)    org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:186)    org.springframework.xd.dirt.module.ModuleDeployer.deployAndStore(ModuleDeployer.java:176)    org.springframework.xd.dirt.module.ModuleDeployer.deployAndStore(ModuleDeployer.java:166)    org.springframework.xd.dirt.server.ContainerRegistrar.deployModule(ContainerRegistrar.java:230)    org.springframework.xd.dirt.server.ContainerRegistrar.deployJob(ContainerRegistrar.java:399)  	... 20 more  Caused by: org.springframework.xd.dirt.job.BatchJobAlreadyExistsException: Batch Job with the name myjob3 already exists    org.springframework.xd.dirt.plugins.job.DistributedJobLocator.addJob(DistributedJobLocator.java:114)    org.springframework.xd.dirt.plugins.job.BatchJobRegistryBeanPostProcessor.postProcessAfterInitialization(BatchJobRegistryBeanPostProcessor.java:106)    org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyBeanPostProcessorsAfterInitialization(AbstractAutowireCapableBeanFactory.java:421)    org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.postProcessObjectFromFactoryBean(AbstractAutowireCapableBeanFactory.java:1698)    org.springframework.beans.factory.support.FactoryBeanRegistrySupport.doGetObjectFromFactoryBean(FactoryBeanRegistrySupport.java:164)  	... 36 more",2
"Fail fast admin server if admin's embedded tomcat couldn't start
During admin server startup, if it fails due to embedded tomcat failure, then the admin server instance still up and running. Since its tomcat isn't running it can not handle any REST client requests. In this scenario, we need fail fast the admin server process itself with better error message.",1
"Use new Table classes provided by Spring Shell
The classes:  * org.springframework.xd.shell.util.Table * org.springframework.xd.shell.util.TableHeader * org.springframework.xd.shell.util.TableRow  are now provided by *org.springframework.shell.support.table*.  Objectives:  * We should use those. (But wait for SHL-142 to be complete) * Improve all tests that use the table classes - Avoid accessing table data using numeric keys (row/column numbers) - Instead use ""business"" keys such as table header names etc.   	 ",4
"Rest: Improve the determination whether a Job Execution is Restartable
In *BatchJobExecutionsController$restartJobExecution()* we need to do a better check whether a Batch Job Execution is restartable.     This is also true when executing *BatchJobExecutionsController$list()*. The check performed under *new JobExecutionInfo(jobExecution, timeZone)* is not sufficient.    *Reason*:    Currently in the UI when I have failed Job Executions, I can restart those (good). However, if the next execution succeeds, the previously restartable jobs should NOT be marked as restartable anymore.     Right now you can restart those jobs, resulting in a:    {code}  Caused by: org.springframework.batch.core.repository.JobInstanceAlreadyCompleteException: A job instance already exists and is complete for parameters={random=0.5735953106895085, throwError=true}.  If you want to run this job again, change the parameters.    org.springframework.batch.core.repository.support.SimpleJobRepository.createJobExecution(SimpleJobRepository.java:126)    sun.reflect.GeneratedMethodAccessor211.invoke(Unknown Source)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)    java.lang.reflect.Method.invoke(Method.java:606)    org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)    org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)    org.springframework.transaction.interceptor.TransactionInterceptor$1.proceedWithInvocation(TransactionInterceptor.java:98)    org.springframework.transaction.interceptor.TransactionAspectSupport.invokeWithinTransaction(TransactionAspectSupport.java:262)    org.springframework.transaction.interceptor.TransactionInterceptor.invoke(TransactionInterceptor.java:95)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)    org.springframework.batch.core.repository.support.AbstractJobRepositoryFactoryBean$1.invoke(AbstractJobRepositoryFactoryBean.java:172)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)    org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)    com.sun.proxy.$Proxy40.createJobExecution(Unknown Source)    org.springframework.batch.core.launch.support.SimpleJobLauncher.run(SimpleJobLauncher.java:125)    sun.reflect.GeneratedMethodAccessor209.invoke(Unknown Source)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)    java.lang.reflect.Method.invoke(Method.java:606)    org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)    org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)    org.springframework.batch.core.configuration.annotation.SimpleBatchConfiguration$PassthruAdvice.invoke(SimpleBatchConfiguration.java:117)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)    org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)    com.sun.proxy.$Proxy42.run(Unknown Source)    org.springframework.batch.integration.launch.JobLaunchingMessageHandler.launch(JobLaunchingMessageHandler.java:50)    sun.reflect.GeneratedMethodAccessor208.invoke(Unknown Source)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)    java.lang.reflect.Method.invoke(Method.java:606)    org.springframework.expression.spel.support.ReflectiveMethodExecutor.execute(ReflectiveMethodExecutor.java:63)    org.springframework.expression.spel.ast.MethodReference.getValueInternal(MethodReference.java:95)    org.springframework.expression.spel.ast.MethodReference.access$000(MethodReference.java:44)    org.springframework.expression.spel.ast.MethodReference$MethodValueRef.getValue(MethodReference.java:258)    org.springframework.expression.spel.ast.CompoundExpression.getValueInternal(CompoundExpression.java:84)    org.springframework.expression.spel.ast.SpelNodeImpl.getTypedValue(SpelNodeImpl.java:114)    org.springframework.expression.spel.standard.SpelExpression.getValue(SpelExpression.java:111)    org.springframework.integration.util.AbstractExpressionEvaluator.evaluateExpression(AbstractExpressionEvaluator.java:159)    org.springframework.integration.util.MessagingMethodInvokerHelper.processInternal(MessagingMethodInvokerHelper.java:268)    org.springframework.integration.util.MessagingMethodInvokerHelper.process(MessagingMethodInvokerHelper.java:142)    org.springframework.integration.handler.MethodInvokingMessageProcessor.processMessage(MethodInvokingMessageProcessor.java:75)  	... 98 more  {code}        ",4
"Refactor Exception Handling and update JavaDocs for acceptance test
[Add JavaDocs to]  * StreamUtils  * HttpTest  * MqttTest  * JmsSource    [Exception Handling]  StreamUtils stream method should throw  IllegalStateException instead of a checked exception.  XDEc2Validation assertReceived, assertValid should throw IllegalStateException instead of a checked exception ",3
"CLI needs to be setup to use the updated acceptance test structure
This change has to be facilitated because of the XD-1456 and XD-1455 stories.",3
"Spring XD using Redis as data transport is failing to start in CI Acceptance Test.
nan",5
"Update HDFS sink to accept a partition strategy
Add configuration for the partition strategy to HDFS sink to support writing files into subdirectories based on a partition key provided in the header or field in the message of the stream data.    The writing using HDFS Store DataWriter should pass in the partition key value to be used for the write operation.    Partition configuration could be made available to the sink using a  --format parameter:    that could then be used in XML config like:  {code}  â€‚â€‚â€‚â€‚â€‚â€‚expression=""new java.text.SimpleDateFormat('${format}').format(${timestamp})  {code}  Similar to the time source.",8
"Update HDFS sink to use unique id (GUID) as part of file name
HDFS sink needs to have unique identifier for container id added as part of file name. Part of the file name in the directory will be the container id (GUID) - like base-path/logfile-GUID-1.txt  ",5
"Fix intermittent test failures at StreamDeploymentIntegrationTests
There are intermittent test failurs in the StreamDeploymentIntegrationTests (especially Rabbit tests). We can try using EventuallyMatcher and see if that fixes this.",1
"Add More Sophisticated Retry Configuration to the Rabbit MessageBus
XD-1019 added simple (stateless) retry to the message bus.    Use stateful retry and an {{AmqpRejectAndDontRequeueRecoverer}} enabling failed messages to be requeued on the broker until successful (perhaps because another instance can handle the message); also provides a mechanism to route failed messages to a dead-letter exchange.    Requires setting the message id header in bus-generated messages.    Also add profiles and properties for common retry/backoff policies.",8
"Change twittersearch default outputType to be application/json
The current output type is a Java object - this raises issues wrt to consumers in other JVM that to no have the spring social tweet object in the main container classpath.  See https://jira.spring.io/browse/XD-1370    Will also create another issue to update twittersearch to generate the raw twitterstream output vs. the structure of the spring social tweet object ",1
"twittersearch module to produce json data as-is from twitter
Do not convert the data into a spring social tweet object, pass along the json as-is from twitter search.",5
"The type StubDatasetOperations must implement the inherited abstract method DatasetOperations.getDatasetDescriptor(Class<T>)
StubDatasetOperations class needs to be either declared asbtract or implemente inherited methods from DatasetOperations",1
"Add Twitter search module tests acceptance tests
nan",4
"Add Twitter Stream tests acceptance tests
nan",4
"Add HDFS (apache Hadoop 2.2 distro) acceptance tests
nan",8
"Add Zookeeper distribution in the download zip
This will reduce one extra step for getting started using XD in distributed mode 'out of the box'",1
"Fix package tangles
Sonar build is currently failing.",2
"Tests are failing due to change in JMX endpoint data
Need to update the Jackson parser.",4
" Tap naming consistency for stream taps
Currently, when creating the taps for streams, the name of the pub/sub channel inside the message bus would be     ""tap:<name-of-the-stream>.<module-name>.<module-index>    For instance, the following stream with name ""test"":    http | transform --expression=payload.toLowerCase() | file    will have the exchanges as  'topic.tap:test.http.0', 'topic.tap:test.transform.1' when using rabbit message bus.    Though, the stream config parser takes care of translating what user would provide in the DSL (for example: tap:stream:test.transform.1 to use the message bus exchange topic.tap:test.transform.1), it would be better we have the consistency inside the message bus channel name as well.    Also, this would be in sync with how we name taps for jobs. (tap:job:*)",2
"Remove ""singlenode"" prefix from embeddedHsql propery in singlenode profile
The prefix ""singlenode"" in embeddedHsql option property defined in singlenode profile seems to be an overhead as it only exists in singlenode profile.    Also, we don't need a system property ""XD_SINGLENODE_EMBEDHSQL"" as config/servers.yml can be used to override the default (from application.yml)",1
"Update to snapshot builds of Spring Shell
nan",1
"Upgrade to Spring Shell 1.1 RC3
nan",1
"Add Steams page to show job triggers
The streams page needs to be added to the UI at least to show the job triggers that are created while scheduling XD jobs.",2
"Modularize angular app modules based on the functionality
When adding streams page to the UI (from XD-1667), it is necessary to modularize the angular app modules based on the functionality/components (job, stream, auth etc.,).     As we expand into more components and use cases in the UI, this definitely makes it easier to concentrate on specific modules based on the functionality.",5
"Update Spring Batch Admin dependency to release version
XD-1623 introduced the dependency to a SNAPSHOT version",1
"NPE when a container departs
When a container departs the cluster the admin will try to redeploy any modules that container was running. If the stream was *destroyed* and the container exited before it had the chance to clean up its deployments under {{/xd/deployments/modules}} (for example, with {{kill -9}}) the following NPE occurs:    {noformat}  java.lang.NullPointerException    org.springframework.xd.dirt.server.ContainerListener.loadStream(ContainerListener.java:347)    org.springframework.xd.dirt.server.ContainerListener.onChildLeft(ContainerListener.java:403)    org.springframework.xd.dirt.server.ContainerListener.childEvent(ContainerListener.java:158)    org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:494)    org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:488)    org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)    com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)    org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)    org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:485)    org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)    org.apache.curator.framework.recipes.cache.PathChildrenCache$11.run(PathChildrenCache.java:755)    java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)    java.util.concurrent.FutureTask.run(FutureTask.java:262)    java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)    java.util.concurrent.FutureTask.run(FutureTask.java:262)    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)    java.lang.Thread.run(Thread.java:724)  {noformat}    If the stream was *undeployed* the following stack appears:  {noformat}  15:13:06,002 ERROR ContainersPathChildrenCache-0 cache.PathChildrenCache:550 -   java.lang.RuntimeException: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /xd/deployments/streams/t0    org.springframework.xd.dirt.server.ContainerListener.onChildLeft(ContainerListener.java:468)    org.springframework.xd.dirt.server.ContainerListener.childEvent(ContainerListener.java:159)    org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:494)    org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:488)    org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)    com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)    org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)    org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:485)    org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)    org.apache.curator.framework.recipes.cache.PathChildrenCache$11.run(PathChildrenCache.java:755)    java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)    java.util.concurrent.FutureTask.run(FutureTask.java:266)    java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)    java.util.concurrent.FutureTask.run(FutureTask.java:266)    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)    java.lang.Thread.run(Thread.java:744)  Caused by: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /xd/deployments/streams/t0    org.apache.zookeeper.KeeperException.create(KeeperException.java:111)    org.apache.zookeeper.KeeperException.create(KeeperException.java:51)    org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:1155)    org.apache.curator.framework.imps.GetDataBuilderImpl$4.call(GetDataBuilderImpl.java:302)    org.apache.curator.framework.imps.GetDataBuilderImpl$4.call(GetDataBuilderImpl.java:291)    org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)    org.apache.curator.framework.imps.GetDataBuilderImpl.pathInForeground(GetDataBuilderImpl.java:287)    org.apache.curator.framework.imps.GetDataBuilderImpl.forPath(GetDataBuilderImpl.java:279)    org.apache.curator.framework.imps.GetDataBuilderImpl.forPath(GetDataBuilderImpl.java:41)    org.springframework.xd.dirt.server.ContainerListener.loadStream(ContainerListener.java:358)    org.springframework.xd.dirt.server.ContainerListener.onChildLeft(ContainerListener.java:417)  	... 16 more  {noformat}    In short, this logic makes the assumption that the stream is still present and deployed. It needs to take into account the fact that neither assumption can be made.",2
"Set fetch size when reading from database in pre-packaged jobs
When running a query against a large dataset, JDBC will attempt to load the entire result set into memory by default.  If this isn't desired (which would be the case in the prepackaged jobs), you can set the fetchSize on the JdbcCursorItemReader to set the number of rows to return with each fetch.  It is good practice to make this match the commit interval.      If the fetch size is not set with large datasets, the stack blows with an OutOfMemoryException.",4
"Add filepollhdfs Acceptance Tests
nan",5
"filepollhdfs documentation needs to be updated with all of the options available.
currently the documentation only shows the --name as an option.  Review the FilePollHdfsJobOptionsMetadata to find all the available options.",2
"Accessing non-existing module causes NullPointerException
This source exists:  {code}  http://localhost:9393/modules/source/time  {code}  But trying to access a non-existing source such as:  {code}  http://localhost:9393/modules/source/time2  {code}  Triggers in the UI:   {code}  [{""links"":[],""logref"":""NullPointerException"",""message"":""NullPointerException""}]  {code}  On the server-side:  {code}  6:03:45,387 ERROR http-nio-9393-exec-3 rest.RestControllerAdvice:199 - Caught exception while handling a request  java.lang.NullPointerException    org.springframework.xd.dirt.rest.DetailedModuleDefinitionResourceAssembler.toResource(DetailedModuleDefinitionResourceAssembler.java:49)    org.springframework.xd.dirt.rest.ModulesController.info(ModulesController.java:104)    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)    java.lang.reflect.Method.invoke(Method.java:483)    org.springframework.web.method.support.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:215)    org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:132)    org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:104)    org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandleMethod(RequestMappingHandlerAdapter.java:749)    org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:689)    org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:83)    org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:938)    org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:870)    org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:961)    org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:852)    javax.servlet.http.HttpServlet.service(HttpServlet.java:621)    org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:837)    javax.servlet.http.HttpServlet.service(HttpServlet.java:728)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:305)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)    org.springframework.boot.actuate.trace.WebRequestTraceFilter.doFilter(WebRequestTraceFilter.java:115)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)    org.springframework.boot.actuate.autoconfigure.EndpointWebMvcAutoConfiguration$ApplicationContextFilterConfiguration$1.doFilterInternal(EndpointWebMvcAutoConfiguration.java:137)    org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:108)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)    org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:77)    org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:108)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)    org.springframework.web.filter.HttpPutFormContentFilter.doFilterInternal(HttpPutFormContentFilter.java:88)    org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:108)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)    org.springframework.boot.actuate.autoconfigure.MetricFilterAutoConfiguration$MetricsFilter.doFilterInternal(MetricFilterAutoConfiguration.java:85)    org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:108)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)    org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:222)    org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:123)    org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:472)    org.apache.catalina.valves.RemoteIpValve.invoke(RemoteIpValve.java:680)    org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:171)    org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:99)    org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:118)    org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:407)    org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1004)    org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:589)    org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.run(NioEndpoint.java:1680)    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)    java.lang.Thread.run(Thread.java:744)  {code}    Accessing a non existing resource should probably result in a 404 status code.",3
"FilePollHdfs is not writing results to hdfs
XD Deployment   Description:		XD Cluster (1 Container)  Environment:		EC2  Type Of Test:		Manual Test  Test Failed On		filepollhdfs (only test that was run)  Build Used		Built May 7, 10:29 UTC    From the shell, attempted to create filepollhdfs however no results were written to hdfs (hadoop22).      The commands executed were the following:  job create myjob --definition ""filepollhdfs  --names=forename,surname,address"" --deploy  stream create mystream --definition ""file --dir=67fc27a6-224d-4c67-a02a-40730bcf8906 --pattern='*.out' > queue:job:myjob"" --deploy    No warnings nor exceptions were displayed till I changed the log4j.logger.org.springframework to INFO and restarted the container.   Then when I copied the sample file to the monitored directory the log reported:  21:30:07,605  INFO DeploymentsPathChildrenCache-0 module.ModuleDeployer:118 - deployed SimpleModule [name=file, type=source, group=mystream, index=0 @61612c7c]  Exception in thread ""inbound.job:myjob-redis:queue-inbound-channel-adapter1"" org.springframework.messaging.core.DestinationResolutionException: failed to look up MessageChannel with name 'errorChannel' in the BeanFactory (and there is no HeaderChannelRegistry present).    org.springframework.integration.support.channel.BeanFactoryChannelResolver.resolveDestination(BeanFactoryChannelResolver.java:108)    org.springframework.integration.support.channel.BeanFactoryChannelResolver.resolveDestination(BeanFactoryChannelResolver.java:44)    org.springframework.integration.channel.MessagePublishingErrorHandler.resolveErrorChannel(MessagePublishingErrorHandler.java:111)    org.springframework.integration.channel.MessagePublishingErrorHandler.handleError(MessagePublishingErrorHandler.java:78)    org.springframework.integration.util.ErrorHandlingTaskExecutor$1.run(ErrorHandlingTaskExecutor.java:55)    java.lang.Thread.run(Thread.java:724)  Caused by: org.springframework.beans.factory.NoSuchBeanDefinitionException: No bean named 'errorChannel' is defined    org.springframework.beans.factory.support.DefaultListableBeanFactory.getBeanDefinition(DefaultListableBeanFactory.java:641)    org.springframework.beans.factory.support.AbstractBeanFactory.getMergedLocalBeanDefinition(AbstractBeanFactory.java:1159)    org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:282)    org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)    org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:273)    org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)    org.springframework.integration.support.channel.BeanFactoryChannelResolver.resolveDestination(BeanFactoryChannelResolver.java:99)    When using the attached sample file, you need to rename the file to try2.out.",5
"FileJdbc Acceptance Test
nan",5
"Add ""log-full-message"" Property to the Log Sink
Allows looking at message headers without turning on debugging.",1
"XD does not reconnect to jobstore if connection is  lost, leaving Jobs in inconsistent state.
XD Deployment Description	XD Cluster (1 Container)  Environment	EC2  Type Of Test	Shell Command Line  Test Failed On	hdfs (only test that was run)  Build Used	Built May 7, 10:29 PST    [Overall issue]  XD Admin and container lost connectivity to the the jobstore (MySql on RDS) and did not reconnect.    Exception Displayed in log.  Caused by: com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: The last packet successfully received from the server was 54,321,927 milliseconds ago.  The last packet sent successfully to the server was 54,321,928 milliseconds ago. is longer than the server configured value of 'wait_timeout'. You should consider either expiring and/or testing connection validity before use in your application, increasing the server configured values for client timeouts, or using the Connector/J connection property 'autoReconnect=true' to avoid this problem.    sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)    sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)    sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)    java.lang.reflect.Constructor.newInstance(Constructor.java:526)    com.mysql.jdbc.Util.handleNewInstance(Util.java:411)    com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:1117)    com.mysql.jdbc.MysqlIO.send(MysqlIO.java:3871)    com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2484)    com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2664)    com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2788)    com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2738)    com.mysql.jdbc.StatementImpl.executeQuery(StatementImpl.java:1617)    org.springframework.jdbc.core.JdbcTemplate$1QueryStatementCallback.doInStatement(JdbcTemplate.java:452)    org.springframework.jdbc.core.JdbcTemplate.execute(JdbcTemplate.java:402)  	... 57 more    [Side Effects]  [Unable to create new Job with same name]  User executed a job list and then removed the jobs in the list.  When the user tried to create new jobs using the same names the application reported:  ""Command failed org.springframework.xd.rest.client.impl.SpringXDException: StatementCallback; SQL [SELECT JOB_NAME FROM JOB_REGISTRY_NAMES]; No operations allowed after connection closed.; nested exception is com.mysql.jdbc.exceptions.jdbc4.MySQLNonTransientConnectionException: No operations allowed after connection closed.""  The only way to resolve it completely was to:   1) shutdown the admin and the containers.    2) Clear the jobs from the Batch job tables by hand  3) restart XD admin and containers.  ",5
"Remove %L from Log4j PatternLayout
The xd-dirt log4j.properties includes the calling line number {{%L}} which is not recommended for production.    https://logging.apache.org/log4j/1.2/apidocs/org/apache/log4j/PatternLayout.html    {{WARNING Generating caller location information is extremely slow and should be avoided unless execution speed is not an issue.}}",1
"jdbchdfs Acceptance Test
nan",5
"Need to support XD_admin_host and xd_containers in test properties
If a user wishes to run the acceptance tests from their local machine against an XD instance on EC2  the only way to establish container and admin server is via the artifact ec2servers.csv. And this has to be copied to the spring-xd-integration-test subdirectory.   The user should be able to set the xd_admin_host and xd_containers in the application-<profile>.properties file. ",4
"Syslog Acceptance Tests
nan",8
"syslog-tcp throws exception when receiving syslog data
XD Deployment   Description	XD Cluster (1 Container)  Environment	EC2  Type Of Test	Manual test via shell  Test Failed On	syslog-tcp (only test that was run)  Build Used	Built May 7, 10:29 UTC    [Setting up the Environment]  * Used the wiki instructions to setup the syslog on the ec2 instance.   * Deploy the stream below:  stream create mystream --definition ""syslog-tcp | file --binary=true --mode=REPLACE"" --deploy   * On the EC2 Instance execute the line below:  logger -p local3.info -t TESTING ""Test Syslog Message""    [What occurred]  Stream fails to process inbound syslog information and throws the exception below:     Exception in thread ""inbound.mystream.0-redis:queue-inbound-channel-adapter17"" org.springframework.messaging.core.DestinationResolutionException: failed to look up MessageChannel with name 'errorChannel' in the BeanFactory (and there is no HeaderChannelRegistry present).    org.springframework.integration.support.channel.BeanFactoryChannelResolver.resolveDestination(BeanFactoryChannelResolver.java:108)    org.springframework.integration.support.channel.BeanFactoryChannelResolver.resolveDestination(BeanFactoryChannelResolver.java:44)    org.springframework.integration.channel.MessagePublishingErrorHandler.resolveErrorChannel(MessagePublishingErrorHandler.java:111)    org.springframework.integration.channel.MessagePublishingErrorHandler.handleError(MessagePublishingErrorHandler.java:78)    org.springframework.integration.util.ErrorHandlingTaskExecutor$1.run(ErrorHandlingTaskExecutor.java:55)    java.lang.Thread.run(Thread.java:724)  Caused by: org.springframework.beans.factory.NoSuchBeanDefinitionException: No bean named 'errorChannel' is defined    org.springframework.beans.factory.support.DefaultListableBeanFactory.getBeanDefinition(DefaultListableBeanFactory.java:641)    org.springframework.beans.factory.support.AbstractBeanFactory.getMergedLocalBeanDefinition(AbstractBeanFactory.java:1159)    org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:282)    org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)    org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:273)    org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)    org.springframework.integration.support.channel.BeanFactoryChannelResolver.resolveDestination(BeanFactoryChannelResolver.java:99)  	... 5 more",5
"Test integration with jboss queue message
I am new to Spring XD, I want to read the jboss queue message and then want to write it into text file by using stream create comment i.e ""stream create --name TEST-LOG  --definition ""jms | file"" --deploy"". I am trying to configure the ""spring-xd-1.0.0.M6\xd\modules\common\jms-jbossmq-infrastructure-context.xml"" to invoke the jboss queue and read the queue message. Can you help me to do the configuration and resolve my objective?",3
"Rabbit Source AcceptanceTest
nan",5
"Pluralization of admin nodes leadership selector group path (/xd/admin)
Currently, the admin nodes that participate in the leadership election are grouped under /xd/admin. Since, there are multiple lock nodes that correspond to all the admin servers that participate in leadership election, we can pluralize this node name to /xd/admins.",1
"Add UI screen shots to docs for new features in Alpha 
* StepExecutionContext  * StepExecutionProgress  * JobScheduler  * Stream page  * Job definition (XD1615)    ",4
"New libraries to experiment
We would like to experiment with open source libraries to further enrich Spring XDâ€™s service offerings and feature-set.  This epic remains the anchor point for following categories and the respective experimentation outcomes will be documented in the associated stories.  * Measure based alerts * Log analysis * Machine learning and graph computation",16
"Metrics - measure based alerts
We would like to experiment with â€œmetricsâ€, a Java library to extract insights of production code. This will help understand the behavior of critical components to eventually orchestrate workflows to proactively monitor and notify important contacts/groups as needed.  Website -> http://metrics.codahale.com/ GitHub -> https://github.com/dropwizard/metrics",8
"Riemann - measure based alerts
We would like to experiment with â€œRiemannâ€, a Clojure library to aggregate events from servers and applications through stream processing.   Features: * Tracking latency distribution * Email for exceptions * Memory and CPU statistics  This will help understand the behavior of critical components to eventually orchestrate workflows to proactively monitor and notify important contacts/groups as needed.   Webiste -> http://riemann.io/ GitHub -> https://github.com/aphyr/riemann",8
"ELK - log analysis
We would like to experiment with â€œELKâ€ stack. Itâ€™s the combination of Elasticsearch, Logstash and Kibana to extract end-to-end real-time insights from structured and unstructured data source. Possibly provide integration endpoints to some of the components.  Features: * Search and analyze in real-time * Scrub, parse and enrich data * Visualization  Website -> http://www.elasticsearch.org/overview/ GitHub -> http://www.elasticsearch.org/overview/elkdownloads/",8
"Stream-lib - analytics operators
We would like to experiment with â€œstream-libâ€, a stream summarizer and cardinality (counting distinct elements) estimator to further enrich Spring XD's analytics feature-set.  GitHub -> https://github.com/addthis/stream-lib",8
"GraphLab - enhance machine learning and computations
We would like to experiment with â€œGraphLabâ€, a graph based, high performance, distributed computation framework written in C++.   Features: * HDFS integration * Maching learning toolkits  Website -> http://graphlab.org/projects/index.html GitHub -> https://github.com/graphlab-code/graphlab  _Note: Could be part of unified offering of PHD_",8
"Provide security integration
Acceptance Criteria: - Users should be able to gain access using username/password - Both username/password are mandatory - Invalid user credentials should be displayed as error messages and the user will not be able to gain access",16
"Research how to secure Admin's REST endpoints
As a user, I'd like to have the option to provide security configurations so that I can access REST endpoints in a secured manner.     Ideally, all the listed [REST|https://github.com/spring-projects/spring-xd/wiki/REST-API#xd-resources] endpoints needs to be wrapped within a security layer.     *Scope of this spike:*    * Research Spring Security and Spring Boot and the OOTB features   * Design considerations and approach for XD  * Developer experience  ** How users will be configuring security credentials?  ** How DSL shell will be handled?  ** How Admin UI will be handled?",8
"Enable Job deployment properties for job deploy
Support the ability to provide deployment properties for ""job deploy"".",8
"hdfsjdbc Acceptance Test
nan",5
"Gemfire Source Acceptance Test
Deploy XD Sample Gemfire on Utility Machine. Deploy stream with gemfire as a source. Create a stream (stream2) with gemfire-server as a sink Send data to stream 2 and verify that the data has been received by gemfire source. Update CI tests to increase heap, to support gemfire tests..",8
"Rabbit Sink Acceptance Tests
nan",5
"Disable auto-formatting of JavaDoc
nan",1
"hdfs sink loads Codecs class during 'module info --name sink:hdfs' command
The hdfs sink metadata causes loading of  org.springframework.data.hadoop.store.codec.Codecs class during 'module info --name sink:hdfs' command since the type is a specific Spring Hadoop class    options.codec.description = compression codec alias name  options.codec.type = org.springframework.data.hadoop.store.codec.Codecs  options.codec.default =    Don't think we want to tie the sink module to specific Spring Hadoop classes during runtime of the admin, we can't be sure that admin has hadoop classes on classpath in all environments and there is no way of specifying the hadoop distro for admin.    Wouldn't it be better to have this option as a String to be passed in to the module's context that could then load the class",3
"Remove Hadoop from admin classpath
Not sure why the Hadoop classes are on the admin servers classpath. There is no way to select the distro, and the Hadoop classes shouldn't be needed except for module info for hdfs sink (see XD-1701)",1
"Add admin and container memory settings to servers.yml
We should add an easy way to configure the memory. Currently we only have the number of YARN containers configurable without diving into Spring YARN/Boot specific config options. {code} xd:     adminServers: 1     containers: 1 {code}  Proposing we do: {code} xd:     adminServers: 1     adminMemory: 512M     containers: 1     containerMemory: 512M {code}  ",3
"Create doc section about quotes handling
Document the different ""onion layers"" that come in play with regard to quoting and escaping (shell, xd-parser, SpEL expressions in some cases) and provide practical examples to common scenarios    ",5
"Add defaultYarnClasspath entry for phd20, cdh5 and hdp21
Each Hadoop distro uses different settings for ""yarn.application.classpath"" and we should provide some starting points for the distros we support running XD on YARN for.    We should add a commented out stub ""defaultYarnClasspath"" entry for phd20, cdh5 and hdp21 to replace the one for hadoop22 when someone deploys on these distros.  ",3
"Add tab completion for named channels (i.e., queue:xyz >)
Tab completion doesn't currently list/support ""queue"" as a source.  For example if typing the following stream: stream create b --definition ""queue:bar > transform --expression=payload+'-bar' | log"" --deploy  Tab completion doesn't recognize or suggest ""queue"" or anything after it until after the first bar ""|"".    The same applies to named channels, ""queue"", as a sink.",1
"The Dynamic Router example in the docs throws an exception with Rabbit Transport
The example in the M6 documentation for the Dynamic Router (here: http://docs.spring.io/spring-xd/docs/1.0.0.M6/reference/html/#dynamic-router) for the SpEL-Based Routing throws an exception when processing the message (from the HTTP post) saying ""No bean named 'queue:foo' is defined"", when using RabbitMQ as the transport.  I do not know a workaround.    Steps to reproduce:  1) Run RabbitMQ locally  2) Run xd-singlenode --transport rabbit  3) xd:>stream create f --definition ""queue:foo > transform --expression=payload+'-foo' | log"" --deploy    xd:>stream create b --definition ""queue:bar > transform --expression=payload+'-bar' | log"" --deploy    xd:>stream create r --definition ""http | router --expression=payload.contains('a')?'queue:foo':'queue:bar'"" --deploy    4) xd:>http post --data ""a""    5) This should give a stacktrace:  Caused by: org.springframework.beans.factory.NoSuchBeanDefinitionException: No bean named 'queue:foo' is defined    org.springframework.beans.factory.support.DefaultListableBeanFactory.getBeanDefinition(DefaultListableBeanFactory.java:641)    org.springframework.beans.factory.support.AbstractBeanFactory.getMergedLocalBeanDefinition(AbstractBeanFactory.java:1159)    org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:282)    org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)    org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:273)    org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)    org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:273)    org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)    org.springframework.integration.support.channel.BeanFactoryChannelResolver.resolveDestination(BeanFactoryChannelResolver.java:99)  	... 83 more  ",1
"Re-deployment of stream/job modules upon container departure doesn't choose appropriate container candidates
Upon container departure, the ContainerListener's onChildLeft() event triggers redeployment of stream/job modules that were deployed into the leaving container. During the redeployment, it happens that the container candidates from the DefaultContainerMatcher *sometimes* (based on the subset from distributeForRequestedCount(List<Container> candidates, int count))  includes the container which already have the module of the *same* stream/job definition deployed. This causes the re-deployment silently swallowing the NodeExistsException and the module being re-deployed doesn't actually get deployed.  ",4
"Handling JobExecution stop action if the JobExecution is COMPLETED
Currently, the flag ""stoppable"" on JobExecutionInfoResource is used to find if the jobExecution can be stopped.  Since this flag is set to true even if the JobExecution status is COMPLETED, the jobExecution can still say it can be stopped.",1
"ProcessorTest.testfailedSink needs to use http as its test source
Also check the JMX output to see that the filter rejected the entry.",5
"StreamUtil Cleanup
Update StreamUtils based on Code Review comments.",3
"Show visual representation of stream in admin UI
nan",5
"Add AngularJS Directive to format Stream Definition Strings
It would be neat if streams could be easily formatted. E.g:    * Make the definition name bold  * use different colors for parameter names and values    ",4
"Create documentation section for the shell
Create a new section in the docs regaring shell usage, in particular how to represent single and double quotes.    Include some discussion of basic commands to manipulate streams, jobs and list modules.  How to pass in a file that can be executed when the shell starts up.    Also point to spring-shell ref docs for extensibility in terms of adding custom commands.",3
"Document that modules can reference property values in servers.yml
Modules can use property values in servers.yml which is very handy to keep batch and hdfs functionality working without duplication of config values in servers.yml and modules.yml (or individual modules).   The configuration section should highlight the common cases where this occurs, batch, hdfs, rabbitmq/mqtt where using the server config values as defaults is useful and that they can still be overridden.  ",1
"Optimize JobService queries and batch domain object usages
Based on the discussion from here:  https://github.com/spring-projects/spring-xd/pull/849#issuecomment-43215168  we need a better strategy to handle some of the queries and updates to the batch domain objects.",5
"Twitter Search test uses case sensitive search when it should be case insensitive.
The TwitterSearch does a case insensitive search.  Tests need to do a insensitive check for the keywords in the search result.",3
"ZooKeeper Job deployments path state is not updated after successful deployment
After successful job deployment, the Job deployments path in ZK doesn't get updated with the data {""state"": ""deployed""}    Though this data is not used for deployed instance repository (org.springframework.xd.dirt.stream.zookeeper.ZooKeeperJobRepository) to check for the deployment status, it may be better to have this state updated like stream deployment path.",1
"Document OOTB available ${xd.???} keys
As a consequence of not fixing XD-1289, we should document keys of the form ${xd.stream.name} that are available to users    ${xd.[stream|job].name} and ${xd.container.???} come to mind, there may be others",4
"Add a test for ${xd.stream.name} inside the DSL definition of a stream
nan",2
"UI: Refactor Schedule and Launch Screen under Deployments
The current screen layout is problematic in cases where there are many deployments. Having a dedicated page for Launching or Scheduling jobs may be desirable.   Alternatively, a light-box-based approach may be possible but I personally don't favor that.",5
"'--type=' not supported by module delete as shown in documentation examples
In the Module Composition example here: http://docs.spring.io/spring-xd/docs/1.0.0.M6/reference/html/#composing-modules on of the examples is ""module delete --name foo --type sink"" which fails as the '--type' argument is not supported by the CLI.      There are 3 other references to the '--type' argument in the documentation which may not be supported by the CLI anymore. ",1
"CLI error when not specifying module type in module commands is cryptic an not helpful
All of the CLI module commands that require the module name (e.g., 'module display source:mqtt') require that you preface the name with the module type.  If you forget to do this, e.g., 'module display mqtt', you get a fairly cryptic exception which can confuse end users.  The exception is:  java.lang.StringIndexOutOfBoundsException: Failed to convert 'mqtt' to type QualifiedModuleName for option 'name,' String index out of range: -1",2
"Create tests for Stream/Job deployments path data verification
Based on the discussion here:  https://github.com/spring-projects/spring-xd/pull/852#issuecomment-43356579  we would like to have tests created for verifying the Stream/Job deployments path ""data"" ",1
"Change jacoco to 0.7 for jdk 8 builds
https://build.spring.io/browse/XD-JDK8-5  apply plugin: 'jacoco'  jacoco {     toolVersion = ""0.7.0.201403182114"" }",1
"Add Stream/Job destroy option at the UI
Add an option to destroy the stream/job definitions.  Also add confirm action that asks for user to confirm to proceed with destroy.",3
"Add Support for Bold/Strong Fonts 
Hitting this issue in Chrome:    http://stackoverflow.com/questions/22891611/google-font-varela-round-doesnt-support-font-weight-in-chrome    Looks like Chrome has some issues with making text bold if the font does not explicitly support it.  ",2
"Update dependencies in Spring XD Sample Repository
nan",4
"Zookeeper NoNode exception when deploying stream
Same problem on M6 and using BUILD-SNAPSHOT.    When deploying a stream that has a slow-starting component (that connects to Gemfire), the deployment fails with a ZK NoNode exception.    No log from the component seen, but in all honesty, the component could be waiting on a timeout.    org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /xd/deployments/modules/2af8624b-777c-4084-aa1a-9d675b53afe3/test1.sink.reactor-batching-client-1/metadata    org.apache.zookeeper.KeeperException.create(KeeperException.java:111)    org.apache.zookeeper.KeeperException.create(KeeperException.java:51)    org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:783)    org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:676)    org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:660)    org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)    org.apache.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:656)    org.apache.curator.framework.imps.CreateBuilderImpl.protectedPathInForeground(CreateBuilderImpl.java:441)    org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:431)    org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:44)    org.springframework.xd.dirt.server.ContainerRegistrar.onChildAdded(ContainerRegistrar.java:370)    org.springframework.xd.dirt.server.ContainerRegistrar.access$800(ContainerRegistrar.java:93)    org.springframework.xd.dirt.server.ContainerRegistrar$DeploymentListener.childEvent(ContainerRegistrar.java:706)    org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:494)    org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:488)    org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)    com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)    org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)    org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:485)    org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)    org.apache.curator.framework.recipes.cache.PathChildrenCache$11.run(PathChildrenCache.java:755)    java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)    java.util.concurrent.FutureTask.run(FutureTask.java:262)    java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)    java.util.concurrent.FutureTask.run(FutureTask.java:262)    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)    java.lang.Thread.run(Thread.java:745)",2
"Support for UUID suffix for hdfs file names in acceptance tests
nan",3
"Remove dependency on Sprint Boot in xd-dirt tests
nan",1
"Investigate fall through of server.yml values when running in YARN
We don't support using @Configuration for modules ATM.  The current code was committed during the same time as improvements to handling module configuration.  We should switch the reactor-ip.xml to include all bean definitions and remove referencing @Configuration classes or see how to add support for @Configuration.      Another short term hack is to put the prefix 'sink.reactor-ip' in all @Value used in NetServerInboundChannelAdapterConfiguration.",3
"Improve JMX checks for processors taking into account error channels
nan",2
"FileJdbcTest & JdbcHdfsTest failing
JdbcHdfsTest, FileJdbcTest works for singlenode but not for admin & Container on the same machine.",5
"Enhance Container object (org.springframework.xd.dirt.cluster.Container) for better matching strategies 
Creating this enhancement story based on the discussion here:  https://github.com/spring-projects/spring-xd/pull/867",8
"Add --verbose option to display all property values
nan",1
"Stream destroy fails to remove if the underlying modules have been removed.
In short if you attempt to destroy a stream that has had its modules removed, the destroy will fail.  1) So if I create my own processor:x and use the processor in a stream.  2) I then shutdown the admin and container.  3) Delete the processor:x from the $XD_HOME/modules directory  4) Restart the admin and container.  5) if you do a stream list the stream that used the processor.x is still present.  6) but you can not delete the stream because of the exception below.     [To Reproduce using Payload Conversion example]  1) Follow the instructions to install and use the myTupleProcessor module in a stream.  2) Now shutdown the admin and container  3) rm -rf $XD_HOME/modules/processor/myTupleProcessor.xml   4) rm -rf $XD_HOME/lib/payload-conversion.jar  5) Startup your admin and container  6) stream all destroy.  Then type 'y'<return>     6a) The shell will report ommand failed org.springframework.xd.rest.client.impl.SpringXDException: No content to map due to end-of-input at [Source: java.io.StringReader@5b8bd3d0; line: 1, column: 1]     6b) The Admin Server will report  11:59:43,543 ERROR http-nio-9393-exec-1 rest.RestControllerAdvice:199 - Caught exception while handling a request  org.springframework.xd.dirt.zookeeper.ZooKeeperAccessException: No content to map due to end-of-input   at [Source: java.io.StringReader@648849d5; line: 1, column: 1]    org.springframework.xd.dirt.zookeeper.ZooKeeperUtils.wrapThrowable(ZooKeeperUtils.java:47)    org.springframework.xd.dirt.zookeeper.ZooKeeperUtils.wrapThrowable(ZooKeeperUtils.java:31)    org.springframework.xd.dirt.zookeeper.ZooKeeperUtils.wrapAndThrowIgnoring(ZooKeeperUtils.java:65)    org.springframework.xd.dirt.module.store.ZooKeeperModuleDefinitionRepository.findByNameAndType(ZooKeeperModuleDefinitionRepository.java:95)    org.springframework.xd.dirt.stream.XDStreamParser.resolveModuleType(XDStreamParser.java:300)    org.springframework.xd.dirt.stream.XDStreamParser.determineType(XDStreamParser.java:196)    org.springframework.xd.dirt.stream.XDStreamParser.parse(XDStreamParser.java:152)    org.springframework.xd.dirt.stream.StreamDeployer.beforeDelete(StreamDeployer.java:116)    org.springframework.xd.dirt.stream.StreamDeployer.beforeDelete(StreamDeployer.java:43)    org.springframework.xd.dirt.stream.AbstractDeployer.delete(AbstractDeployer.java:246)    org.springframework.xd.dirt.stream.AbstractDeployer.deleteAll(AbstractDeployer.java:169)    org.springframework.xd.dirt.stream.AbstractInstancePersistingDeployer.deleteAll(AbstractInstancePersistingDeployer.java:100)    org.springframework.xd.dirt.rest.XDController.deleteAll(XDController.java:110)    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)    java.lang.reflect.Method.invoke(Method.java:606)    org.springframework.web.method.support.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:215)    org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:132)    org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:104)    org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandleMethod(RequestMappingHandlerAdapter.java:749)    org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:689)    org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:83)    org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:938)    org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:870)    org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:961)    org.springframework.web.servlet.FrameworkServlet.doDelete(FrameworkServlet.java:885)    javax.servlet.http.HttpServlet.service(HttpServlet.java:653)    org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:837)    javax.servlet.http.HttpServlet.service(HttpServlet.java:728)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:305)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)    org.springframework.boot.actuate.trace.WebRequestTraceFilter.doFilter(WebRequestTraceFilter.java:115)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)    org.springframework.boot.actuate.autoconfigure.EndpointWebMvcAutoConfiguration$ApplicationContextFilterConfiguration$1.doFilterInternal(EndpointWebMvcAutoConfiguration.java:137)    org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:108)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)    org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:77)    org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:108)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)    org.springframework.web.filter.HttpPutFormContentFilter.doFilterInternal(HttpPutFormContentFilter.java:88)    org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:108)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)    org.springframework.boot.actuate.autoconfigure.MetricFilterAutoConfiguration$MetricsFilter.doFilterInternal(MetricFilterAutoConfiguration.java:85)    org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:108)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)    org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:222)    org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:123)    org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:472)    org.apache.catalina.valves.RemoteIpValve.invoke(RemoteIpValve.java:680)    org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:171)    org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:99)    org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:118)    org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:407)    org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1004)    org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:589)    org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.run(NioEndpoint.java:1680)    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)    java.lang.Thread.run(Thread.java:724)  Caused by: com.fasterxml.jackson.databind.JsonMappingException: No content to map due to end-of-input   at [Source: java.io.StringReader@648849d5; line: 1, column: 1]    com.fasterxml.jackson.databind.JsonMappingException.from(JsonMappingException.java:164)    com.fasterxml.jackson.databind.ObjectMapper._initForReading(ObjectMapper.java:3036)    com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:2978)    com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:2098)    org.springframework.xd.dirt.module.store.ZooKeeperModuleDefinitionRepository.findByNameAndType(ZooKeeperModuleDefinitionRepository.java:82)  	... 61 more  ",5
"Container reconnection to ZK fails intermittently
As reported by Matt Stine:    After closing and reopening a laptop, the following stack trace appears in the container log:    {noformat}  00:47:28,226  INFO main-EventThread state.ConnectionStateManager:194 - State change: RECONNECTED  00:47:28,226  INFO ConnectionStateManager-0 zookeeper.ZooKeeperConnection:255 - >>> Curator connected event: RECONNECTED  00:47:28,322 ERROR ConnectionStateManager-0 listen.ListenerContainer:96 - Listener (org.springframework.xd.dirt.zookeeper.ZooKeeperConnection$DelegatingConnectionStateListener@6abf4158) threw an exception  java.lang.RuntimeException: java.lang.RuntimeException: org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /xd/containers/5a8deb7b-fd93-42a7-a393-2f15023e007a          at org.springframework.xd.dirt.server.ContainerRegistrar.registerWithZooKeeper(ContainerRegistrar.java:301)          at org.springframework.xd.dirt.server.ContainerRegistrar.access$100(ContainerRegistrar.java:93)          at org.springframework.xd.dirt.server.ContainerRegistrar$ContainerAttributesRegisteringZooKeeperConnectionListener.onConnect(ContainerRegistrar.java:316)          at org.springframework.xd.dirt.zookeeper.ZooKeeperConnection$DelegatingConnectionStateListener.stateChanged(ZooKeeperConnection.java:257)          at org.apache.curator.framework.state.ConnectionStateManager$2.apply(ConnectionStateManager.java:222)          at org.apache.curator.framework.state.ConnectionStateManager$2.apply(ConnectionStateManager.java:218)          at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)          at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)          at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)          at org.apache.curator.framework.state.ConnectionStateManager.processEvents(ConnectionStateManager.java:215)          at org.apache.curator.framework.state.ConnectionStateManager.access$000(ConnectionStateManager.java:42)          at org.apache.curator.framework.state.ConnectionStateManager$1.call(ConnectionStateManager.java:110)          at java.util.concurrent.FutureTask.run(FutureTask.java:262)          at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)          at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)          at java.lang.Thread.run(Thread.java:744)  Caused by: java.lang.RuntimeException: org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /xd/containers/5a8deb7b-fd93-42a7-a393-2f15023e007a          at org.springframework.xd.dirt.container.store.ZooKeeperContainerAttributesRepository.save(ZooKeeperContainerAttributesRepository.java:75)          at org.springframework.xd.dirt.container.store.ZooKeeperContainerAttributesRepository.save(ZooKeeperContainerAttributesRepository.java:42)          at org.springframework.xd.dirt.server.ContainerRegistrar.registerWithZooKeeper(ContainerRegistrar.java:295)          ... 15 more  Caused by: org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /xd/containers/5a8deb7b-fd93-42a7-a393-2f15023e007a          at org.apache.zookeeper.KeeperException.create(KeeperException.java:119)          at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)          at org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:783)          at org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:676)          at org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:660)          at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)          at org.apache.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:656)          at org.apache.curator.framework.imps.CreateBuilderImpl.protectedPathInForeground(CreateBuilderImpl.java:441)          at org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:431)          at org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:44)          at org.springframework.xd.dirt.container.store.ZooKeeperContainerAttributesRepository.save(ZooKeeperContainerAttributesRepository.java:69)          ... 17 more  {noformat}    This can occur if ZK does not remove the ephemeral node before the container creates a new one. This can be fixed in the following ways:    * Remove the existing ephemeral node if it already exists  * Register containers with a new UUID upon every new connection    For now I'll implement the first solution.",2
"ZooKeeper Admin server node data to have admin server host address
It would be useful to store admin server ip address in ZooKeeper leadership group node (/xd/admins) to identify admin server and it's admin port.  ",1
"Register StringToByteArrayMessageConverter
The converter was not configured, therefore String to byte[] for --outPutType application/octet-stream fails for a String payload.",1
"Remove toStringTransformer from tcp Source; Add Binary Support to the http Source
The TCP source unconditionally converts to String. This prevents binary transfers.    Remove the transformer; if the user wants a String; (s)he can use    {{tcp --outputType=text/plain;charset=UTF-8}} (assuming the byte stream has valid UTF-8 encoding).    Another option would be to add a {{--binary}} option, but since conversion can already handle it, it's probably better to use that.    On the other hand, a {{--binary}} option would enable backwards compatibility.    The http source also unconditionally converts to String.",1
"ClassNotFoundException: o.s.social.twitter.api.Tweet with Rabbit-transport
Using Rabbit as a transport, I get the below error when creating the following stream:  {code} stream create mytweets --definition ""twittersearch --query='spring' | log"" --deploy true {code}  In local mode, the stream executes just fine. Of course it works with *--outputType=application/json*. Nevertheless, I was not expecting that exception (see below)  Issue was also verified by [~grenfro].   {code} org.springframework.amqp.rabbit.listener.ListenerExecutionFailedException: Listener threw exception   org.springframework.amqp.rabbit.listener.AbstractMessageListenerContainer.wrapToListenerExecutionFailedExceptionIfNeeded(AbstractMessageListenerContainer.java:758)   org.springframework.amqp.rabbit.listener.AbstractMessageListenerContainer.doInvokeListener(AbstractMessageListenerContainer.java:653)   org.springframework.amqp.rabbit.listener.AbstractMessageListenerContainer.invokeListener(AbstractMessageListenerContainer.java:576)   org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer.access$001(SimpleMessageListenerContainer.java:75)   org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer$1.invokeListener(SimpleMessageListenerContainer.java:154)   sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)   sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)   sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)   java.lang.reflect.Method.invoke(Method.java:606)   org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)   org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)   org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)   org.springframework.retry.interceptor.RetryOperationsInterceptor$1.doWithRetry(RetryOperationsInterceptor.java:69)   org.springframework.retry.support.RetryTemplate.doExecute(RetryTemplate.java:255)   org.springframework.retry.support.RetryTemplate.execute(RetryTemplate.java:162)   org.springframework.retry.interceptor.RetryOperationsInterceptor.invoke(RetryOperationsInterceptor.java:87)   org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)   org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)   com.sun.proxy.$Proxy102.invokeListener(Unknown Source)   org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer.invokeListener(SimpleMessageListenerContainer.java:1111)   org.springframework.amqp.rabbit.listener.AbstractMessageListenerContainer.executeListener(AbstractMessageListenerContainer.java:559)   org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer.doReceiveAndExecute(SimpleMessageListenerContainer.java:904)   org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer.receiveAndExecute(SimpleMessageListenerContainer.java:888)   org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer.access$500(SimpleMessageListenerContainer.java:75)   org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer$AsyncMessageProcessingConsumer.run(SimpleMessageListenerContainer.java:989)   java.lang.Thread.run(Thread.java:745) Caused by: org.springframework.messaging.MessageHandlingException: error occurred in message handler [se.0.convert.bridge]   org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:84)   org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)   org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)   org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)   org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)   org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:255)   org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:223)   org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:114)   org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:44)   org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:93)   org.springframework.integration.endpoint.MessageProducerSupport.sendMessage(MessageProducerSupport.java:98)   org.springframework.integration.amqp.inbound.AmqpInboundChannelAdapter.access$400(AmqpInboundChannelAdapter.java:44)   org.springframework.integration.amqp.inbound.AmqpInboundChannelAdapter$1.onMessage(AmqpInboundChannelAdapter.java:90)   org.springframework.amqp.rabbit.listener.AbstractMessageListenerContainer.doInvokeListener(AbstractMessageListenerContainer.java:650) 	... 24 more Caused by: org.springframework.xd.dirt.integration.bus.serializer.SerializationException: unable to deserialize [null]. Class not found.   org.springframework.xd.dirt.integration.bus.MessageBusSupport.deserializePayload(MessageBusSupport.java:381)   org.springframework.xd.dirt.integration.bus.MessageBusSupport.deserializePayload(MessageBusSupport.java:363)   org.springframework.xd.dirt.integration.bus.MessageBusSupport.deserializePayloadIfNecessary(MessageBusSupport.java:346)   org.springframework.xd.dirt.integration.rabbit.RabbitMessageBus.access$500(RabbitMessageBus.java:70)   org.springframework.xd.dirt.integration.rabbit.RabbitMessageBus$ReceivingHandler.handleRequestMessage(RabbitMessageBus.java:448)   org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:170)   org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78) 	... 37 more Caused by: java.lang.ClassNotFoundException: org.springframework.social.twitter.api.Tweet   java.net.URLClassLoader$1.run(URLClassLoader.java:366)   java.net.URLClassLoader$1.run(URLClassLoader.java:355)   java.security.AccessController.doPrivileged(Native Method)   java.net.URLClassLoader.findClass(URLClassLoader.java:354)   java.lang.ClassLoader.loadClass(ClassLoader.java:425)   sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)   java.lang.ClassLoader.loadClass(ClassLoader.java:358)   java.lang.Class.forName0(Native Method)   java.lang.Class.forName(Class.java:190)   org.springframework.xd.dirt.integration.bus.MessageBusSupport.deserializePayload(MessageBusSupport.java:375) 	... 43 more {code}",4
"Update CI server to run tests that depend on rabbit/redis and hadoop
nan",2
"Support for hadoop name node HA configuration
Hadoop supports namenode HA with two name nodes running, one being active and other in standby. If the active name node fails the standby name node has all the data readily available and can start serving requests. In this configuration name node url is no longer a host:port url but a logical name that translates to any active name node at runtime.     This is to ensure spring xd stream can handle a name node failure, for instance when writing a hdfs sink, seamlessly",5
"Add MVC-aware HTTP source module
As a developer, I'd like to have an OOTB MVC-aware HTTP module (with embedded tomcat), so I can use this module to leverage spring-mvc and spring-security features, instead of rewriting them within the existing HTTP source module.   * Adds richer support for content-type in the HTTP Source module. See [~jbrisbin] comments: https://github.com/spring-projects/spring-xd/pull/879.  * Adds full header mapping in the source (see comments)  * See SO request: http://stackoverflow.com/questions/29353471/spring-xd-as-a-rest-endpoint",8
"Module deployment order is not guaranteed
We are getting test failures such as    https://build.spring.io/browse/XD-MASTER-1381    quite often in the CI environment recently.  I suspect the ordering of checks in AbstractSingleNodeStreamDeploymentIntegrationTests    			// Deploys in reverse order  			assertModuleRequest(streamName, ""log"", false);  			assertModuleRequest(streamName, ""filter"", false);  			assertModuleRequest(streamName, ""transform"", false);  			assertModuleRequest(streamName, ""http"", false);  			// Undeploys in stream order  			assertModuleRequest(streamName, ""http"", true);  			assertModuleRequest(streamName, ""transform"", true);  			assertModuleRequest(streamName, ""filter"", true);  			assertModuleRequest(streamName, ""log"", true);    isn't happening    perhaps  changing nextUndeployEvent poll time from 5 seconds to higher is appropriate (no idea why CI environment seems so slow)  ",4
"Update to Spring Integration 4.0.1
Add messages store optimization to the `hdfs-dataset`",1
"Investigate skipped tests in build, enable or remove.
We have 13 skipped tests now...",8
"Exception handling at Module info command
When not prefixing with appropriate module type, module info command throws StringIndexOutOfBoundsException:      xd:>module info file  java.lang.StringIndexOutOfBoundsException: Failed to convert 'file' to type QualifiedModuleName for option 'name,'  String index out of range: -1",2
"Modules that use tomcat connection pool need to expose configurations
filejdbc, hdfsjdbc, jdbchdfs & jdbc modules each support a tomcat connection  pool.  At this time none of the configurations allowed by the tomcat connection pool are available unless the user adds them to the appropriate module xml file.  We need to allow the user to configure them via yml, property file and environment variables.  ",8
"multiple streams with same port for HTTP Source 
Using a singleXD VM  I can create multiple streams with the same http port.    Stream Name  Stream Definition  Status   -----------  -----------------  ------  xd:>stream create --definition ""http --port=8081|file --dir=/tmp/test3"" --name test1 --deploy Created and deployed new stream 'test1' xd:>stream create --definition ""http --port=8081|file --dir=/tmp/test3"" --name test1 --deploy Created and deployed new stream 'test1' xd:>stream create --definition ""http --port=8081|file --dir=/tmp/test3"" --name test2 --deploy Created and deployed new stream 'test2' xd:>stream create --definition ""http --port=8081|file --dir=/tmp/test3"" --name test3 --deploy Created and deployed new stream 'test3' xd:>stream list   Stream Name  Stream Definition                       Status   -----------  --------------------------------------  --------   test1        http --port=8081|file --dir=/tmp/test3  deployed   test2        http --port=8081|file --dir=/tmp/test3  deployed   test3        http --port=8081|file --dir=/tmp/test3  deployed  ISSUES 1) No error is returned for the duplicate use of the port. In M5 an error was returned. 2) The status shows as ""deployed"" for all three.  3) Even though there are three streams. Only the first stream is active. I can post to the first stream that was declared and it will successfully post to the  directory. If  I post to the remaining streams, I don't get an error and I no data is written to the flle. 4) If I remove the working stream, the remaining stream still don't work. ",4
"Change default date formats to be 'yyyy-MM-dd'
We have some places where we us a default data format specified as 'yyyy/MM/dd'. In Spring for Apache Hadoop we use 'yyyy-MM-dd' for partitioning path expressions. This seems more in line with ISO standard date format. For consistency we should have both SHDP and XD use the same default format.",3
"Assess if GemfireJsonServer & gemfireServer sinks should close the client cache
* OS - Mac  * XD Deployment Type - Singlenode  * SHA - bb4dd58  * Required Software - XD Gemfire Sample Server    [Description]  After creating and destroying 3 streams with gemfireJsonServer sink the 4th will fail with this error:    * 44707 refused connection: The number of clients, 4, exceeded the limit of 3 allowed by the default evaluation license.    [Steps to reproduce]  * From your shell execute the following 4 times:  ** stream create --name stocks --definition ""http --port=9090 | gemfire-json-server --regionName=Stocks --host=ec2-54-221-32-82.compute-1.amazonaws.com --port=40404 --keyExpression=payload.getField('symbol')"" --deploy  ** stream destroy stocks",4
"gemfire source does not offer --host nor --port options
nan",2
"Update spring-data-hadoop version to 2.0.0.RC4
Update spring-data-hadoop version to 2.0.0.RC4 and make necessary changes to the YARN configuration.",3
"Resolve runtime module option properties using module metadata
Since the module metadata properties are resolved at runtime (when the module gets deployed), we can resolve the module options values that are already resolved in there.    For example, currently the ""runtime modules"" command for ""log"" module would show this:    runtime modules    [7m[27;32m  Module            Container Id                          Options    ----------------  ------------------------------------  --------------------------------------------------------    s1.source.http-0  633f0fb1-5396-4bc0-8f1e-c9d5104e0ea7  {port=9000}    s1.sink.log-1     633f0fb1-5396-4bc0-8f1e-c9d5104e0ea7  {name=${xd.stream.name}, expression=payload, level=INFO}    In this case, we can resolve the module option ""name"" from the module metadata.      ",2
"JMS Source (ActiveMQ) failing to use jmsUrl environment variable
Deployed on: SingleNode Ec2, SingleNode Mac  SHA: 942c7868e3e0d0cf7730b536170438a0291f5cab    [Description]  JMS Source (Activemq) tried to access a broker on localhost.    The current deployment uses the following to set the JMS Broker:  * export amq_url=tcp://ec2-54-221-32-82.compute-1.amazonaws.com:61616    [Analysis]  After reviewing the configuration of the jms-activemq-infrastructure-context.xml, it was noted that the brokerUrl environment variable has been changed from amq.url to amqUrl.  While the jms-activemq.properties has not been changed (still amq.url).    After setting the following, the test still failed:  * export amqUrl=tcp://ec2-54-221-32-82.compute-1.amazonaws.com:61616    After going into the jms-activemq-infrastructure-context.xml and replacing the amqUrl with amq.url, the jms source (activemq) returned to normal operation.      [Incident]  Acceptance tests reported a failure on Saturday Morning's build that the JMS Source failed.",2
"Status on Shell command prompt is inconsistent
Deployment: xd-shell local, xd-singlenode (ec2)  SHA: 942c7868e3e0d0cf7730b536170438a0291f5cab    [Description]  <Case 1>  Once successfully connected to a server, if you connect to a server that is not present.  The prompt still shows XD when it should show server-unknown.  Can be reproduced consistently.  Conversely:  <Case 2>  Attempted to connect to a xd-singlenode on ec2 using a local xd-shell.    The xd-singlenode was not running.  After bringing up the xd-singlenode, I was able to connect however the status did not change from ""server-unknown""  *This behavior, can not be consistently reproduced, but have seen it happen on multiple accounts.*     [Steps to reproduce]  <Case 1>  1. Bring up shell while xd-singlenode is not running.  2. Bring up xd-singlenode  3. Connect to xd-singlenode  * xd:>admin config server http://localhost:9393    4. Connect to a fake address  * xd:>admin config server http://foo.bar:9393    <Case 2>  1.  Attempt to connect to remote server that is not available  * xd:>admin config server http://ec2-54-237-186-186.compute-1.amazonaws.com:9393  * Unable to contact XD Admin Server at 'http://ec2-54-237-186-186.compute-1.amazonaws.com:9393'.    2. Bring up xd-singlenode on remote  * server-unknown:>admin config server http://ec2-54-237-186-186.compute-1.amazonaws.com:9393  * Successfully targeted http://ec2-54-237-186-186.compute-1.amazonaws.com:9393    3. Still see the incorrect prompt.  server-unknown:>  server-unknown:>",5
"Support in-memory transport for co-located modules
We are looking to speed up the message passing from source to sink  and wondering if we could use a in-memory transport whenever we know that source and sink modules are co-located on the same container. Currently we do not see a straight forward way of doing it    Option 1 : Create a composite module and let users deploy a composite module by itself or in other words deploy a stream with one module    Option 2 : Let users define a transport as in-memory when defining a stream. This could be used along with the deployment manifest feature enforcing co-location of a source and sink module, with in-memory transport    cc @adenissov  ",8
"Documentation for data partitioning, and all Rabbit Bus properties
nan",4
"Update data partitioning functionality to use murmur hash function
https://github.com/addthis/stream-lib/blob/master/src/main/java/com/clearspring/analytics/hash/MurmurHash.java",2
"Update to Spring Batch 3.0 RELEASE
nan",2
"Documentation for enhanced HDFS sink with paths based off date/time/message content
nan",2
"Update documentation to list supported Hadoop distributions
After spring hadoop 2.0 RC4 update.",1
"Failing tcp to file in script tests
build	22-May-2014 08:45:04	Creating stream tcptofile with definition 'tcp+--port%3D21234+--socketTimeout%3D2000+%7C+file+--dir%3D%2Ftmp%2Fxdtest%2Fbasic' ...  build	22-May-2014 08:45:04	{""name"":""tcptofile"",""deployed"":null,""definition"":""tcp --port=21234 --socketTimeout=2000 | file --dir=/tmp/xdtest/basic"",""links"":[{""rel"":""self"",""href"":""http://127.0.0.1:9393/streams/tcptofile""}]}  build	22-May-2014 08:45:04	  build	22-May-2014 08:45:11	Destroying stream tcptofile ...  build	22-May-2014 08:45:11	  build	22-May-2014 08:45:11	  build	22-May-2014 08:45:11	Expected blahblah does not match actual value (98,108,97,104,98,108,97,104)  simple	22-May-2014 08:45:11	Failing task since return code of [/bin/sh /tmp/XD-SCRIPTS-RS-513-ScriptBuildTask-7280766559152712153.sh] was 1 while expected 0  simple	22-May-2014 08:45:11	Finished task 'Run basic_stream_tests'    See https://build.spring.io/download/XD-SCRIPTS-RS/build_logs/XD-SCRIPTS-RS-513.log",2
"JobExecution restart action should depend on job deployment status
At the JobExecution page, if the job execution is failed and restartable, then we should enable the ""restart"" action only if the job is deployed.    Please see https://github.com/spring-projects/spring-xd/pull/884 for the discussion related to this.",3
"User should be able to specify deploy properties for Jobs
When clicking deploy from the job definitions page, user should be able to specify the deployment manifest (module count, module criteria etc.,)",3
"User should be able to provide job deployment properties
At the job definitions page, user should be able to provide the job deployment manifest (module count, criteria etc.,)",3
"Handle NPE while deploying stream module at the Container
When trying to deploy a stream module, the ContainerRegistrar throws NPE if the deployment loader couldn't load a non-null stream based on the stream name.    07:10:29,902 ERROR DeploymentsPathChildrenCache-0 server.ContainerRegistrar:450 - Exception deploying module  java.lang.NullPointerException    org.springframework.xd.dirt.server.ContainerRegistrar.deployStreamModule(ContainerRegistrar.java:549)    org.springframework.xd.dirt.server.ContainerRegistrar.onChildAdded(ContainerRegistrar.java:436)    org.springframework.xd.dirt.server.ContainerRegistrar.access$800(ContainerRegistrar.java:96)    org.springframework.xd.dirt.server.ContainerRegistrar$DeploymentListener.childEvent(ContainerRegistrar.java:803)    org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:494)    org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:488)    org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)    com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)    org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)    org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:485)    org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)    org.apache.curator.framework.recipes.cache.PathChildrenCache$11.run(PathChildrenCache.java:755)    java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)    java.util.concurrent.FutureTask.run(FutureTask.java:262)    java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)    java.util.concurrent.FutureTask.run(FutureTask.java:262)    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)    java.lang.Thread.run(Thread.java:744)",1
"Update twitterSearchTest to handle the latest release of twitterSearch
The changes to twitterSearch means that it will send multiple messages during the duration of the test.  To support these changes:  1) Remove assertReceived.  Since the number of messages is indeterminate  2) Change file sink that captures the results to append mode.  Because each message will overwrite the previous messages result.",5
"Do not allow a stream definition to contain ambiguous module references
A stream definition such as http | transform | transform | file     will limit functionality such as taps since you can't reference which specific module to apply the tap.     Should be proactive in parsing the DSL and force the use of a label to disambiguate.  ",4
"Upgrade Curator to 2.5.0
nan",1
"UI Automatically close notification messages
* Automatically close notification messages * Polish UI",3
"HdfsJdbc Acceptance Test
  ",8
"Need to support the ability to test Gemfire Locators
Need to be able to setup a gemfire server that has locators enabled such that we can enable the gemfire modules locator features.  run tests for the use_server (default) and tests for locator.  They are enabled by activating the use-locator profile for the container(s) . For example: if you are running singlenode the profile would be: export spring_profiles_active=singlenode,use-locator",16
"Restore deployment properties for orphaned modules
As part of XD-1338 we modified how module deployment works. Now module deployment requests include deployment properties as the data for the ZooKeeper node. This allows us to reuse those properties when a container exit the cluster and the module is redeployed to another container.    However if there are no other containers to handle the deployment, the module deployment node is erased, along with the properties. This mean no module will ever handle the partition that module was responsible for.    This condition needs to be handled so that partitioned streams continue to function in cases where the cluster temporarily doesn't have enough containers to support the stream.",20
"Check job ""restartable"" flag for JobExecution restart action
job create bogus --definition ""jdbchdfs --sql='select * from bogus' --restartable=false""  job deploy bogus  job launch bogus    http://localhost:9393/admin-ui/#/jobs/executions    click ""Restart Job Execution"" on the failed job execution    get message ""Job was relaunched""    container log has:    12:36:27,231 ERROR task-scheduler-10 handler.LoggingHandler:145 - org.springframework.messaging.MessageHandlingException: org.springframework.batch.core.repository.JobRestartException: JobInstance already exists and is not restartable",2
"Exception thrown when running undeploy and redeploy a stream tests
Environment: Running on local Mac Instance: XD Deployment Type: XD-SingleNode  SHA: 66c28e3  While running a test that deployed and undeployed a stream over 500 times the following exception is thrown: org.apache.zookeeper.KeeperException$NotEmptyException: KeeperErrorCode = Directory not empty for /xd/deployments/modules/54ef010f-3d5d-4f86-a50f-44453e48633d/streamfoo.source.http-0  [Steps to reproduce] * Created the following Job and Stream using xd-shell: ** stream create streamfoo --definition ""http --port=9000 | hdfs --directory=/xd/streamfoo --fileName=streamfoo ""  ** job create foo --definition ""hdfsjdbc --resources=/xd/streamfoo/*.txt --names=data --tableName=streamfoo --initializeDatabase=true "" --deploy * Repeated the following 500 times using xd-shell --cmdfile ** Ran a script that would deploy streamfoo ** Ran a script that would execute the following 25 times *** http post --data ""hello world0 "" ** Ran a script to undeploy streamfoo ** Ran a script that would launch the job  [Artifacts] The logfile of the singlenode is attached.",10
"Upgrade ZK installation on EC2 to 3.4.6
* Environment:   ** Admin/Container on separate EC2 instances with rabbit transport.  *** Redis, Rabbit & Zookeeper deployed on admin instance  ** Admin/Container on separate EC2 instances with redis transport.  *** Redis, Rabbit & Zookeeper deployed on admin instance  * XD Deployment Type: XD-SingleNode   * Commit: https://github.com/spring-projects/spring-xd/commit/8fba31d21e96a371dacf26b40eeb542c3564b2e3    Both Redis and Rabbit clusters failed acceptance tests.  I've attached the portion of the admin log that was available.  ",10
"Add HdfsMongoDb Acceptance Test.
Create Acceptance Test  Add Mongo to Ec2 Acceptance Test Environment.",10
"Exception thrown when all containers are shut down
If all containers are shut down and there's a stream deployed this exception is thrown on the admin:  {noformat} java.util.NoSuchElementException   java.util.ArrayList$Itr.next(ArrayList.java:839)   org.springframework.xd.dirt.server.ModuleDeploymentWriter.writeDeployment(ModuleDeploymentWriter.java:260)   org.springframework.xd.dirt.server.ContainerListener.redeployStreamModule(ContainerListener.java:432)   org.springframework.xd.dirt.server.ContainerListener.onChildLeft(ContainerListener.java:403)   org.springframework.xd.dirt.server.ContainerListener.childEvent(ContainerListener.java:159)   org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:509)   org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:503)   org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)   com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)   org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)   org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:500)   org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)   org.apache.curator.framework.recipes.cache.PathChildrenCache$10.run(PathChildrenCache.java:762)   java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)   java.util.concurrent.FutureTask.run(FutureTask.java:266)   java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)   java.util.concurrent.FutureTask.run(FutureTask.java:266)   java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)   java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)   java.lang.Thread.run(Thread.java:744) {noformat}  This is a regression caused by the earlier code refactoring for stream deployment.",2
"Make failed deployment cleanup more robust
When a deployment fails, the supervisor will clean up failed deployment attempts. If the deployment path is removed while the supervisor is waiting (for instance if the target container departs the cluster) then a NoNodeException will be thrown:  {code} 17:22:59,702  WARN ContainersPathChildrenCache-0 server.ModuleDeploymentWriter:361 - Error while cleaning up failed deployment /deployments/modules/40494bdb-0d8c-4b5d-a895-bf94432d9d3b/s.sink.log-1 org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /xd/deployments/modules/40494bdb-0d8c-4b5d-a895-bf94432d9d3b/s.sink.log-1   org.apache.zookeeper.KeeperException.create(KeeperException.java:111)   org.apache.zookeeper.KeeperException.create(KeeperException.java:51)   org.apache.zookeeper.ZooKeeper.delete(ZooKeeper.java:873)   org.apache.curator.framework.imps.DeleteBuilderImpl$5.call(DeleteBuilderImpl.java:239)   org.apache.curator.framework.imps.DeleteBuilderImpl$5.call(DeleteBuilderImpl.java:234)   org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)   org.apache.curator.framework.imps.DeleteBuilderImpl.pathInForeground(DeleteBuilderImpl.java:230)   org.apache.curator.framework.imps.DeleteBuilderImpl.forPath(DeleteBuilderImpl.java:215)   org.apache.curator.framework.imps.DeleteBuilderImpl.forPath(DeleteBuilderImpl.java:42)   org.springframework.xd.dirt.server.ModuleDeploymentWriter.processResults(ModuleDeploymentWriter.java:355)   org.springframework.xd.dirt.server.ModuleDeploymentWriter.writeDeployment(ModuleDeploymentWriter.java:325)   org.springframework.xd.dirt.server.ModuleDeploymentWriter.writeDeployment(ModuleDeploymentWriter.java:247)   org.springframework.xd.dirt.server.ContainerListener.redeployStreamModule(ContainerListener.java:432)   org.springframework.xd.dirt.server.ContainerListener.onChildLeft(ContainerListener.java:403)   org.springframework.xd.dirt.server.ContainerListener.childEvent(ContainerListener.java:159)   org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:509)   org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:503)   org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)   com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)   org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)   org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:500)   org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)   org.apache.curator.framework.recipes.cache.PathChildrenCache$10.run(PathChildrenCache.java:762)   java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)   java.util.concurrent.FutureTask.run(FutureTask.java:262)   java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)   java.util.concurrent.FutureTask.run(FutureTask.java:262)   java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)   java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)   java.lang.Thread.run(Thread.java:744) 17:22:59,718 ERROR ContainersPathChildrenCache-0 cache.PathChildrenCache:557 -  java.lang.IllegalStateException: Container 40494bdb-0d8c-4b5d-a895-bf94432d9d3b experienced the following error deploying module log-1 of type sink: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode   org.springframework.xd.dirt.server.ModuleDeploymentWriter.validateResults(ModuleDeploymentWriter.java:523)   org.springframework.xd.dirt.server.ModuleDeploymentWriter.validateResult(ModuleDeploymentWriter.java:474)   org.springframework.xd.dirt.server.ContainerListener.redeployStreamModule(ContainerListener.java:436)   org.springframework.xd.dirt.server.ContainerListener.onChildLeft(ContainerListener.java:403)   org.springframework.xd.dirt.server.ContainerListener.childEvent(ContainerListener.java:159)   org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:509)   org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:503)   org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)   com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)   org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)   org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:500)   org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)   org.apache.curator.framework.recipes.cache.PathChildrenCache$10.run(PathChildrenCache.java:762)   java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)   java.util.concurrent.FutureTask.run(FutureTask.java:262)   java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)   java.util.concurrent.FutureTask.run(FutureTask.java:262)   java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)   java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)   java.lang.Thread.run(Thread.java:744) {code}  Additionally, if the supervisor tries to remove a failed deployment after the container has written to it, the following appears in the log:  {code} WARN ContainersPathChildrenCache-0 server.ModuleDeploymentWriter:361 - Error while cleaning up failed deployment /deployments/modules/18c7b4d7-991c-487b-a6e3-006b2dbf87fa/s.source.http-0 org.apache.zookeeper.KeeperException$NotEmptyException: KeeperErrorCode = Directory not empty for /xd/deployments/modules/18c7b4d7-991c-487b-a6e3-006b2dbf87fa/s.source.http-0 {code}  The supervisor has to force the removal of the node, including children.",2
"REST endpoint/command interface for runtime module deployment properties
We need a way to access the deployment properties for the deployed modules.    For example: 'runtime module foo.sink.bar-2'",5
"Create startup script to start one admin & multiple containers
We could have a XD startup script something like this:  'xd-distributed --admins=1 --containers=3'  We could possibly provide a configuration option to specify the admin port so that multiple admin servers can be started.",3
"Support Partitioning/Bus Properties in the RedisMessageBus
PR: https://github.com/spring-projects/spring-xd/pull/926",5
"Detect Invalid Deployment Properties in the Bus
Detect properties the bus doesn't support.",3
"Detect Module Properties for Non-existent Modules
stream create foo --definition=""bar | baz""    stream deploy foo --properties=module.qux.fiz",4
"New job that executes SQL script using JDBC
Create OOTB batch job that executes SQL script using JDBC - can be used for Hive2 jobs or HAWQ jobs etc.  Christian Tzolov provided the following baed on Dave Syers JdbcTasklte (https://src.springframework.org/svn/spring-batch-admin/sandbox/cloud-sample/src/main/java/org/springframework/batch/admin/sample/job/JdbcTasklet.java):  Attached is a simple job module that can run sql commands on Hawq or other DB over jdbc.  Just unzip it in <springxd>/xd/modules/job folder and create something like this: xd> job create analyticsJob  --definition ""     jdbc --driverClassName=org.postgresql.Driver             --url=jdbc:postgresql://<HAWQ master host>:5432/gpadmin             --username=gpadmin --password=''              --sql='CREATE TABLE fonecta_demo.analytics AS                              SELECT segmenttiluokka, count(*) as cnt FROM fonecta_demo.segmenttiluokka                                    GROUP BY segmenttiluokka;'"" --deploy ",5
"New job that executes a job on YARN
Create OOTB batch job that executes a job on YARN  could be:  job create yarnJob --definition ""yarnjob --containerCount=4 --applicationDir=/apps/mystuff --archiveFile=yarn-job-0.1.0.jar --arguments=#{payload[value]}"" ",5
"New job that executes a Spark job
Create OOTB batch job that executes a job on Spark as a tasklet    could be something along this:    job create yarnJob --definition ""sparkjob --master=spark://localhost:7077 --class=SimpleApp""  ",5
"Array class names cannot be parsed to MimeType
MessageBusSupport creates an 'original content type' message header to support serialization for remote transports. The form of the header application/x-java-object;type=<classname>.  For java array types, the ""["" prefix causes an error when converting this value to a MimeType.    This can be avoided by quoting the classname. However, a further complication is if the array element is an object. In this case the classname is '[L<classname>;'. The trailing colon causes a parse exception even in a quoted string.  A simple fix is to check for the trailing colon, remove it and add it back if MimeType.getParameter(""type"").contains(""[L"").   See http://docs.oracle.com/javase/7/docs/api/java/lang/Class.html#getName for more info.  Preliminary testing indicates primitive array and multi-dimensional arrays will work fine with quoting, but tests should be added for these cases.  ",1
"Jobs  are not completely removed from Jobstore 
Deploy Type: Admin/Container on EC2 (Rabbit Transport) SHA: 8fba31d  [Steps to reproduce] 1) Using Rabbit 3.3 above create a user that does not have privileges to write to / * sudo rabbitmqctl add_user joe password * (omit this step) sudo rabbitmqctl set_permissions -p / joe "".*"" "".*"" "".*"" 2) Set the rabbitmq user name and password * export spring_rabbitmq_username=acctest * export spring_rabbitmq_password=acctest23 3) Start container. 4) Create stream or job ""foo"" 5) Error will occur 6) delete stream or job ""foo"" 7)  stop container 8) set privilege * sudo rabbitmqctl set_permissions -p / joe "".*"" "".*"" "".*"" 9) Start container 10) create stream or job ""foo"" 11)  System will report that foo exists.",10
"Module count value at module deployments path
In case of module count > 1, the module deployments path for each deployed module always has: {""count"":""1""}    For a scenario:  The stream test1: ""http | log""  with the deployment manifest:  module.log.count=3,module.log.criteria=groups.contains('test')    get /xd/deployments/streams/test1â€¨module.log.count=3,module.log.criteria=groups.contains('test')  get /xd/deployments/modules/9ecaf59a-a1f5-4ed9-984d-f5dff8cc9b57/test1.sink.log-1â€¨{""count"":""1""}  get /xd/deployments/modules/1bbdb2dd-97ed-48a2-a3cd-3633c3e82f52/test1.sink.log-1â€¨{""count"":""1""}",5
"XD-EC2 Needs to support XD_CONTAINER_GROUPS for created containers
XD-EC2 applies the environment variables to all container instances that are created.  This behavior has to be altered such that a environment variable can be applied to to a specific container instance.      For example if we create a 3 node cluster Admin, Container1, Container2 & Container3.  For Example:  * XD1_XD_CONTAINER_GROUPS=GROUP1  * XD2_XD_CONTAINER_GROUPS=GROUP2    * In this example XD1_XD_CONTAINER_GROUPS=GROUP1 would apply XD_CONTAINER_GROUPS=GROUP1 to container1's environment.    * XD2_XD_CONTAINER_GROUPS=GROUP2 would apply XD_CONTAINER_GROUPS=GROUP2 to container2's environment.    * While container3 would not receive a specific environment setting for XD_CONTAINER_GROUPS.  ",8
"Create Acceptance tests for Container Groups
* Create infrastructure to retrieve container data. * Create infrastructure to retrieve  Stream data and the associated container * Create tests that verify default behavior without group  * Create tests that verify behavior with sink belonging to specific group * Create tests that verify behavior with processor belonging to specific group  * Also generate tests for the scenarios above where the count >1",15
"Create System Tests for Partitioning
nan",5
"Rabbit Tests need to use another another account besides guest.
Need to update the Rabbit & MQTT test to use the Acctest account instead of guest.  As of Rabbit 3.3, the guest account can only accept connections from localhost.  For now I've setup a loop back so that guest can be accessed from other manchines.  ",8
"Twittersearch: ArrayIndexOutOfBoundsException
{noformat}  0:44:30,715  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.  java.lang.ArrayIndexOutOfBoundsException: 255    groovy.json.internal.CharBuf.doAddJsonEscapedString(CharBuf.java:525)    groovy.json.internal.CharBuf.addJsonEscapedString(CharBuf.java:394)    groovy.json.internal.CharBuf.addJsonEscapedString(CharBuf.java:357)    groovy.json.JsonOutput.writeCharSequence(JsonOutput.java:309)    groovy.json.JsonOutput.writeObject(JsonOutput.java:260)    groovy.json.JsonOutput.writeMap(JsonOutput.java:425)    groovy.json.JsonOutput.writeObject(JsonOutput.java:270)    groovy.json.JsonOutput.writeIterator(JsonOutput.java:442)    groovy.json.JsonOutput.writeObject(JsonOutput.java:272)    groovy.json.JsonOutput.writeMap(JsonOutput.java:425)    groovy.json.JsonOutput.writeObject(JsonOutput.java:270)    groovy.json.JsonOutput.writeMap(JsonOutput.java:425)    groovy.json.JsonOutput.toJson(JsonOutput.java:204)    org.springframework.integration.x.twitter.TwitterSearchChannelAdapter.doSendLine(TwitterSearchChannelAdapter.java:137)    org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter$StreamReadingTask$2.extractData(AbstractTwitterInboundChannelAdapter.java:200)    org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter$StreamReadingTask$2.extractData(AbstractTwitterInboundChannelAdapter.java:186)    org.springframework.web.client.RestTemplate.doExecute(RestTemplate.java:549)    org.springframework.web.client.RestTemplate.execute(RestTemplate.java:517)    org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter$StreamReadingTask.readStream(AbstractTwitterInboundChannelAdapter.java:180)    org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter$StreamReadingTask.run(AbstractTwitterInboundChannelAdapter.java:158)    org.springframework.scheduling.support.DelegatingErrorHandlingRunnable.run(DelegatingErrorHandlingRunnable.java:54)    java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)    java.util.concurrent.FutureTask.run(FutureTask.java:262)    java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)    java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)    java.lang.Thread.run(Thread.java:744)  20:44:30,718  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting  20:44:31,136  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.  java.lang.ArrayIndexOutOfBoundsException: 255    groovy.json.internal.CharBuf.doAddJsonEscapedString(CharBuf.java:525)    groovy.json.internal.CharBuf.addJsonEscapedString(CharBuf.java:394)    groovy.json.internal.CharBuf.addJsonEscapedString(CharBuf.java:357)    groovy.json.JsonOutput.writeCharSequence(JsonOutput.java:309)    groovy.json.JsonOutput.writeObject(JsonOutput.java:260)    groovy.json.JsonOutput.writeMap(JsonOutput.java:425)    groovy.json.JsonOutput.writeObject(JsonOutput.java:270)    groovy.json.JsonOutput.writeIterator(JsonOutput.java:442)    groovy.json.JsonOutput.writeObject(JsonOutput.java:272)    groovy.json.JsonOutput.writeMap(JsonOutput.java:425)    groovy.json.JsonOutput.writeObject(JsonOutput.java:270)    groovy.json.JsonOutput.writeMap(JsonOutput.java:425)    groovy.json.JsonOutput.toJson(JsonOutput.java:204)    org.springframework.integration.x.twitter.TwitterSearchChannelAdapter.doSendLine(TwitterSearchChannelAdapter.java:137)    org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter$StreamReadingTask$2.extractData(AbstractTwitterInboundChannelAdapter.java:200)    org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter$StreamReadingTask$2.extractData(AbstractTwitterInboundChannelAdapter.java:186)    org.springframework.web.client.RestTemplate.doExecute(RestTemplate.java:549)    org.springframework.web.client.RestTemplate.execute(RestTemplate.java:517)    org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter$StreamReadingTask.readStream(AbstractTwitterInboundChannelAdapter.java:180)    org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter$StreamReadingTask.run(AbstractTwitterInboundChannelAdapter.java:158)    org.springframework.scheduling.support.DelegatingErrorHandlingRunnable.run(DelegatingErrorHandlingRunnable.java:54)    java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)    java.util.concurrent.FutureTask.run(FutureTask.java:262)    java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)    java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)    java.lang.Thread.run(Thread.java:744)  20:44:31,137  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting  20:44:31,525  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.  java.lang.ArrayIndexOutOfBoundsException: 255    groovy.json.internal.CharBuf.doAddJsonEscapedString(CharBuf.java:525)    groovy.json.internal.CharBuf.addJsonEscapedString(CharBuf.java:394)    groovy.json.internal.CharBuf.addJsonEscapedString(CharBuf.java:357)    groovy.json.JsonOutput.writeCharSequence(JsonOutput.java:309)    groovy.json.JsonOutput.writeObject(JsonOutput.java:260)    groovy.json.JsonOutput.writeMap(JsonOutput.java:425)    groovy.json.JsonOutput.writeObject(JsonOutput.java:270)    groovy.json.JsonOutput.writeIterator(JsonOutput.java:442)    groovy.json.JsonOutput.writeObject(JsonOutput.java:272)    groovy.json.JsonOutput.writeMap(JsonOutput.java:425)    groovy.json.JsonOutput.writeObject(JsonOutput.java:270)    groovy.json.JsonOutput.writeMap(JsonOutput.java:425)    groovy.json.JsonOutput.toJson(JsonOutput.java:204)    org.springframework.integration.x.twitter.TwitterSearchChannelAdapter.doSendLine(TwitterSearchChannelAdapter.java:137)    org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter$StreamReadingTask$2.extractData(AbstractTwitterInboundChannelAdapter.java:200)    org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter$StreamReadingTask$2.extractData(AbstractTwitterInboundChannelAdapter.java:186)    org.springframework.web.client.RestTemplate.doExecute(RestTemplate.java:549)    org.springframework.web.client.RestTemplate.execute(RestTemplate.java:517)    org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter$StreamReadingTask.readStream(AbstractTwitterInboundChannelAdapter.java:180)    org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter$StreamReadingTask.run(AbstractTwitterInboundChannelAdapter.java:158)    org.springframework.scheduling.support.DelegatingErrorHandlingRunnable.run(DelegatingErrorHandlingRunnable.java:54)    java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)    java.util.concurrent.FutureTask.run(FutureTask.java:262)    java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)    java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)    java.lang.Thread.run(Thread.java:744)  20:44:31,526  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting  20:44:31,948  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.  java.lang.ArrayIndexOutOfBoundsException: 255    groovy.json.internal.CharBuf.doAddJsonEscapedString(CharBuf.java:525)    groovy.json.internal.CharBuf.addJsonEscapedString(CharBuf.java:394)    groovy.json.internal.CharBuf.addJsonEscapedString(CharBuf.java:357)    groovy.json.JsonOutput.writeCharSequence(JsonOutput.java:309)    groovy.json.JsonOutput.writeObject(JsonOutput.java:260)    groovy.json.JsonOutput.writeMap(JsonOutput.java:425)    groovy.json.JsonOutput.writeObject(JsonOutput.java:270)    groovy.json.JsonOutput.writeIterator(JsonOutput.java:442)    groovy.json.JsonOutput.writeObject(JsonOutput.java:272)    groovy.json.JsonOutput.writeMap(JsonOutput.java:425)    groovy.json.JsonOutput.writeObject(JsonOutput.java:270)    groovy.json.JsonOutput.writeMap(JsonOutput.java:425)    groovy.json.JsonOutput.toJson(JsonOutput.java:204)    org.springframework.integration.x.twitter.TwitterSearchChannelAdapter.doSendLine(TwitterSearchChannelAdapter.java:137)    org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter$StreamReadingTask$2.extractData(AbstractTwitterInboundChannelAdapter.java:200)    org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter$StreamReadingTask$2.extractData(AbstractTwitterInboundChannelAdapter.java:186)    org.springframework.web.client.RestTemplate.doExecute(RestTemplate.java:549)    org.springframework.web.client.RestTemplate.execute(RestTemplate.java:517)    org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter$StreamReadingTask.readStream(AbstractTwitterInboundChannelAdapter.java:180)    org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter$StreamReadingTask.run(AbstractTwitterInboundChannelAdapter.java:158)    org.springframework.scheduling.support.DelegatingErrorHandlingRunnable.run(DelegatingErrorHandlingRunnable.java:54)    java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)    java.util.concurrent.FutureTask.run(FutureTask.java:262)    java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)    java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)    java.lang.Thread.run(Thread.java:744)  20:44:31,949  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting  20:44:32,345  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.  java.lang.ArrayIndexOutOfBoundsException  20:44:32,346  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting  20:44:32,727  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.  java.lang.ArrayIndexOutOfBoundsException  20:44:32,727  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting  20:44:33,103  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.  java.lang.ArrayIndexOutOfBoundsException  20:44:33,104  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting  20:44:33,548  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.  java.lang.ArrayIndexOutOfBoundsException  20:44:33,548  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting  20:44:33,935  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.  java.lang.ArrayIndexOutOfBoundsException  20:44:33,935  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting  20:44:34,318  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.  java.lang.ArrayIndexOutOfBoundsException  20:44:34,318  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting  20:44:34,696  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.  java.lang.ArrayIndexOutOfBoundsException  20:44:34,696  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting  20:44:35,060  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.  java.lang.ArrayIndexOutOfBoundsException  20:44:35,061  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting  20:44:35,445  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.  java.lang.ArrayIndexOutOfBoundsException  20:44:35,445  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting  20:44:35,825  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.  java.lang.ArrayIndexOutOfBoundsException  20:44:35,825  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting  20:44:36,221  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.  java.lang.ArrayIndexOutOfBoundsException  20:44:36,221  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting  20:44:36,602  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.  java.lang.ArrayIndexOutOfBoundsException  20:44:36,602  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting  20:44:37,006  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.  java.lang.ArrayIndexOutOfBoundsException  20:44:37,006  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting  20:44:37,396  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.  java.lang.ArrayIndexOutOfBoundsException  20:44:37,396  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting  20:44:37,790  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.  java.lang.ArrayIndexOutOfBoundsException  20:44:37,790  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting  20:44:38,179  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.  java.lang.ArrayIndexOutOfBoundsException  20:44:38,179  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting  20:44:38,559  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.  java.lang.ArrayIndexOutOfBoundsException  20:44:38,559  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting  20:44:38,967  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.  java.lang.ArrayIndexOutOfBoundsException  20:44:38,967  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting  20:44:39,365  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.  java.lang.ArrayIndexOutOfBoundsException  20:44:39,365  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting  20:44:39,747  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.  java.lang.ArrayIndexOutOfBoundsException  20:44:39,747  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting  20:44:40,179  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.  java.lang.ArrayIndexOutOfBoundsException  20:44:40,179  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting  20:44:40,596  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.  java.lang.ArrayIndexOutOfBoundsException  20:44:40,597  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting  20:44:40,978  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.  java.lang.ArrayIndexOutOfBoundsException  20:44:40,979  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting  20:44:41,342  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.  java.lang.ArrayIndexOutOfBoundsException  20:44:41,342  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting  20:44:41,732  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.  java.lang.ArrayIndexOutOfBoundsException  20:44:41,732  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting  20:44:42,125  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.  java.lang.ArrayIndexOutOfBoundsException  20:44:42,126  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting  20:44:42,511  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.  java.lang.ArrayIndexOutOfBoundsException  20:44:42,512  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting  20:44:42,918  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.  java.lang.ArrayIndexOutOfBoundsException  20:44:42,918  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting  20:44:43,309  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.  java.lang.ArrayIndexOutOfBoundsException  20:44:43,309  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting  20:44:43,689  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.  java.lang.ArrayIndexOutOfBoundsException  20:44:43,689  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting  20:44:44,071  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.  ...",2
"Encapsulate retrieval of module deployment metadata
See comment here: https://github.com/spring-projects/spring-xd/pull/912/files#r13331190 ",2
"Add @XmlRootElement to all REST resources
Some REST resources lack an @XmlRootElement annotation.  This causes a JAXB marshalling error when trying to access the API with an Accept header of xml (which is the default in most browsers)  This is a preliminary to XD-1800 (which is much more involved), only to fix ugly exception",1
"Investigate and create ModuleId class for ModuleMetadata
The id field in ModuleMetadata is using module id that is derived from module parameters which varies between stream and job modules.  We can come up with ModuleId class that applies for both stream/job modules and can be used in ModuleMetadata.",2
"Remove unused parser code related to ""substreams"" & co
The XD parser had initial support for substreams, which have been subsumed by composed modules.  That legacy code is unused and not needed",5
"Support the ability to create module definitions in Groovy
XML is currently required for module definitions. XD should also support Java @Config and Groovy bean definitions and potentially, SI DSLs. ",8
"Add Eclipse target to EC2 build.gradle.
So that the code format matches that of the XD Project.",5
"Paging support for ModuleMetadata/Container repositories
Add paging support for the appropriate accessor methods in ModuleMetadata/Container repositories",3
"Add global Http Interceptor in order to centralize error logging
Theoretically I would have liked to centralize logging of Http/Resource calls global more substantially - but see this limitation:  https://github.com/angular/angular.js/issues/4013",4
"Improve E2E Test Coverage
nan",3
"Assess XD Fails to connect to remote Redis Instance
Deployment: Admin/Container Redis as data transport  SHA: 45e1beb    [Description]  In the case that the Redis is not running locally XD cannot connect to the Redis instance even though the environment variable spring_redis_host has been set.      [Steps to reproduce]  * Shutdown local instance of Redis.  * For both the admin and container execute the command prior to running the instances:  ** export spring_redis_host=YourRedisHost  * Start admin and container instances  * deploy a simple stream   ** You will see the following error:  13:56:59,647 ERROR task-scheduler-9 handler.LoggingHandler:145 - org.springframework.messaging.MessageHandlingException: error occurred in message handler [org.springframework.integration.redis.outbound.RedisQueueOutboundChannelAdapter@6a1f1d12]    org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:84)    org.springframework.xd.dirt.integration.redis.RedisMessageBus$SendingHandler.handleMessageInternal(RedisMessageBus.java:235)    org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)    org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)    org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)    org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)    org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)    org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:255)    org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:223)    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)    java.lang.reflect.Method.invoke(Method.java:606)    org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)    org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)    org.springframework.integration.monitor.DirectChannelMetrics.monitorSend(DirectChannelMetrics.java:113)    org.springframework.integration.monitor.DirectChannelMetrics.doInvoke(DirectChannelMetrics.java:97)    org.springframework.integration.monitor.DirectChannelMetrics.invoke(DirectChannelMetrics.java:91)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)    org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)    com.sun.proxy.$Proxy57.send(Unknown Source)    org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:114)    org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:44)    org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:93)    org.springframework.integration.endpoint.SourcePollingChannelAdapter.handleMessage(SourcePollingChannelAdapter.java:110)    org.springframework.integration.endpoint.AbstractPollingEndpoint.doPoll(AbstractPollingEndpoint.java:205)    org.springframework.integration.endpoint.AbstractPollingEndpoint.access$000(AbstractPollingEndpoint.java:55)    org.springframework.integration.endpoint.AbstractPollingEndpoint$1.call(AbstractPollingEndpoint.java:149)    org.springframework.integration.endpoint.AbstractPollingEndpoint$1.call(AbstractPollingEndpoint.java:146)    org.springframework.integration.endpoint.AbstractPollingEndpoint$Poller$1.run(AbstractPollingEndpoint.java:284)    org.springframework.integration.util.ErrorHandlingTaskExecutor$1.run(ErrorHandlingTaskExecutor.java:52)    org.springframework.core.task.SyncTaskExecutor.execute(SyncTaskExecutor.java:50)    org.springframework.integration.util.ErrorHandlingTaskExecutor.execute(ErrorHandlingTaskExecutor.java:49)    org.springframework.integration.endpoint.AbstractPollingEndpoint$Poller.run(AbstractPollingEndpoint.java:278)    org.springframework.scheduling.support.DelegatingErrorHandlingRunnable.run(DelegatingErrorHandlingRunnable.java:54)    org.springframework.scheduling.concurrent.ReschedulingRunnable.run(ReschedulingRunnable.java:81)    java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)    java.util.concurrent.FutureTask.run(FutureTask.java:262)    java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)    java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)    java.lang.Thread.run(Thread.java:745)  Caused by: org.springframework.data.redis.RedisConnectionFailureException: Cannot get Jedis connection; nested exception is redis.clients.jedis.exceptions.JedisConnectionException: Could not get a resource from the pool    org.springframework.data.redis.connection.jedis.JedisConnectionFactory.fetchJedisConnector(JedisConnectionFactory.java:97)    org.springframework.data.redis.connection.jedis.JedisConnectionFactory.getConnection(JedisConnectionFactory.java:143)    org.springframework.data.redis.connection.jedis.JedisConnectionFactory.getConnection(JedisConnectionFactory.java:41)    org.springframework.data.redis.core.RedisConnectionUtils.doGetConnection(RedisConnectionUtils.java:85)    org.springframework.data.redis.core.RedisConnectionUtils.getConnection(RedisConnectionUtils.java:55)    org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:169)    org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:149)    org.springframework.data.redis.core.AbstractOperations.execute(AbstractOperations.java:84)    org.springframework.data.redis.core.DefaultListOperations.leftPush(DefaultListOperations.java:68)    org.springframework.data.redis.core.DefaultBoundListOperations.leftPush(DefaultBoundListOperations.java:60)    org.springframework.integration.redis.outbound.RedisQueueOutboundChannelAdapter.handleMessageInternal(RedisQueueOutboundChannelAdapter.java:109)    org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)  	... 43 more  Caused by: redis.clients.jedis.exceptions.JedisConnectionException: Could not get a resource from the pool    redis.clients.util.Pool.getResource(Pool.java:42)    org.springframework.data.redis.connection.jedis.JedisConnectionFactory.fetchJedisConnector(JedisConnectionFactory.java:90)  	... 54 more  Caused by: redis.clients.jedis.exceptions.JedisConnectionException: java.net.ConnectException: Connection refused    redis.clients.jedis.Connection.connect(Connection.java:142)    redis.clients.jedis.BinaryClient.connect(BinaryClient.java:75)    redis.clients.jedis.BinaryJedis.connect(BinaryJedis.java:1724)    redis.clients.jedis.JedisFactory.makeObject(JedisFactory.java:65)    org.apache.commons.pool2.impl.GenericObjectPool.create(GenericObjectPool.java:819)    org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:429)    org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:360)    redis.clients.util.Pool.getResource(Pool.java:40)  	... 55 more  Caused by: java.net.ConnectException: Connection refused    java.net.PlainSocketImpl.socketConnect(Native Method)    java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)    java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)    java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)    java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)    java.net.Socket.connect(Socket.java:579)    redis.clients.jedis.Connection.connect(Connection.java:137)  	... 62 more",5
"Add ResourceModuleRegistry with custom modules location
If someone wants to have a dedicated location (module registry) for the custom modules and can be accessed from any Spring supported resource URL location, then we need to support.    Currently, we use Delegating ModuleRegistry which uses ResourceModuleRegistry implementations that look for location `xd.module.home` and `classpath:/modules/`.     Maybe we can add an additional ResourceModuleRegistry with `xd.custom.module.home` and use it for custom modules.",1
"Support Bus Producer Properties for Dynamic Producers
Pass module properties from stream plugin to {{MessageBusAwareChannelResolver}}.    Disallow partitioning properties.",2
"Upgrade to Spring Boot 1.1 SNAPSHOT
nan",1
"Acceptance Tests for Labels and taps
nan",8
"UI - Setup Sauce Labs Integration
We should have a facility to easily test the E2E Protractor tests against a variety of common browsers including IE. Sauce Labs seems to be the service to use.",4
"Generate asciidoc doc from module options
Generate asciidoc fragments for each module's options, this way it is always up to date.",3
"ContainerListener to redeploy modules based on stream order.
When redeploying in the case of a container failure the modules are now redeployed in a random order.  The list of modules in the failed container needs to be sorted based on its position in a given stream and then redeployed.",5
"Provide DSL completion after a ""some:channel >"" prefix
nan",8
"Investigate increased size of XD distribution.
nan",2
"Combine Distributed job locator related schema changes into one table
Currently, there are JOB_REGISTRY_NAMES, JOB_REGISTRY_RESTARTABLES and JOB_REGISTRY_INCREMENTABLES tables and we can possibly combine them into one table and have a better schema for this.",5
"Investigate need for UI Pagination
This issue could be more involved. Proper pagination may not be implemented correctly by the REST controller (making the respective service call).    This would also necessitate some form of improved state management for the UI. E.g.    * User is on page 5 of the listing of Job Executions  * User views details  * User presses the back-button (on the screen)  * The the listing of Job Executions *should* be still on page 5    ",8
"Add Support for addresses Property on RabbitMQ Source
Support receiving messages from an HA cluster.",1
"Remove all javadoc warnings
nan",5
"UI Suport ""Mandatory"" Module Parameters
Spring XD should support mandatory module properties. In the UI when creating a definition from an existing Module, mandatory definition properties should be visually highlighted and enforced.  I don't think the XD backend supports this though. ",6
"Modules utilizing Jdbc Data Source need to offer connection pool configurations externally.
Currently JdbcSink, HdfsJdbc&  FileJdbc offer only driverClassName, url, user name & password.  They need to offer a full range of configurations offered by the Tomcat Jdbc Connection pool.",2
"Update TwitterSearchTest to use #katyperry
TwitterSearchTest is the only test that is dependent on an external system for its success.  As such there are times that the service is running slower than the test expects, thus the test fails un-necessarily.   Once XD-1814 is merged we can utilize the ""waitForFile"" feature to wait for the result file from the stream to be written.  But the wait time for twitter will be extended to 1 min.   AAAAND make the the search string configurable.  Some tests fail because the #springio is not consistently present.",4
"Mongo  Sink Acceptance Tests
nan",8
"Support completion proposals of processors after a named channel
Due to the way the heuristics for module type guessing work, we can't currently support completions of the like: ""queue:foo > s<TAB>"" that would yield valid processor names  We need to add non-determinism (list of types instead of single type) to the type guessing heuristic",6
"Mask Database Passwords in REST Controllers and Admin UI
When deploying a batch job, the UI displays the database password found in the server.yml in plain text to the user.  At the very least, this should be displayed in a password field so it's masked out and have it masked out in the resulting definition at the bottom of the page.  Ideally, we wouldn't provide the password on that page at all and only accept overriding options (if the user wants a password other than the configured one, enter itâ€¦otherwise, we'll use what we have).    I'm finding that this occurs in other places as well.  A full pass though of the UI should be done to mask out passwords (or eliminate their display all together).",2
"Fix JMS Property Names
The JMS Source/Sink has a pluggable provider (default {{activemq}}) but the URL property {{amqUrl}} implies activeMQ - the property name should be generic (found while testing XD-1149).",1
"Add single threaded executor service to DeploymentSupervisor
This will  eliminate any race conditions between deployments and containers  joining/leaving the cluster.",2
"Job execution display to show job deployment/definition status
With XD-1311, the job execution list shows the definition/deployment status of the associated job. We need to show the same information for a given job execution.",2
"Remove unnecessary usage of module.getIndex()
Following the merge of https://github.com/spring-projects/spring-xd/commit/03cf962845499610ad021d9e6689bccbf5e13cef , investigate calling sites of module.getIndex() or any similar API and remove it if needed",6
"Resolve compile warnings
For example:  {noformat}  :spring-xd-dirt:compileTestJava                                                                          warning: [options] bootstrap class path not set in conjunction with -source 1.7  D:\SpringIO\spring-xd\spring-xd-dirt\src\test\java\org\springframework\xd\dirt\integration\bus\LocalMessageBusTests.java:95: warning: [overrides] Class Foo overrides equals, but neither it nor any superclass overrides hashCode method          static class Foo {                                       ^                                  D:\SpringIO\spring-xd\spring-xd-dirt\src\test\java\org\springframework\xd\dirt\integration\bus\serializer\kryo\CompositeCodecTests.java:77: warning: [overrides] Class SomeClassWithNoDefaultConstructors overrides equals, but neither it nor any superclass override  s hashCode method          static class SomeClassWithNoDefaultConstructors {                 ^                                  D:\SpringIO\spring-xd\spring-xd-dirt\src\test\java\org\springframework\xd\dirt\integration\bus\serializer\kryo\KryoCodecTests.java:77: warning: [overrides] Class SomeClassWithNoDefaultConstructors overrides equals, but neither it nor any superclass overrides has  hCode method          static class SomeClassWithNoDefaultConstructors {                 ^                                  D:\SpringIO\spring-xd\spring-xd-dirt\src\test\java\org\springframework\xd\dirt\integration\redis\KryoMessageSerializerTests.java:64: warning: [overrides] Class Foo overrides equals, but neither it nor any superclass overrides hashCode method          public static class Foo {                                       ^                           D:\SpringIO\spring-xd\spring-xd-dirt\src\test\java\org\springframework\xd\dirt\stream\TypeConvertingStreamTests.java:181: warning: [rawtypes] found raw type: Map                                  Map map = (Map) message.getPayload();                                  ^                   missing type arguments for generic class Map<K,V>    where K,V are type-variables:                       K extends Object declared in interface Map        V extends Object declared in interface Map    6 warnings   {noformat}",1
"FileSourceTest needs to apply label to source and sink
* Currently Acceptance FileSource Acceptance Tests are failing  ** This is because the sink that tests the result for the file source test is a filesink.  Both use the ""file"" token.  Thus causing a failure  * SimpleFileSource and SimpleFileSink needs to support a label method.  * Update testFileSource to use the labels.",3
"Do not allow the use of named channels in composed modules
This needs closer inspection, but here are some things that currently do not work, either at the parser level, or at actual deployment time:    {noformat}  xd:>module compose foo --definition ""queue:bar > filter""  Command failed org.springframework.xd.rest.client.impl.SpringXDException: Could not find module with name 'filter' and type 'sink'    xd:>module compose foolog --definition ""queue:foo > log""â€¨  Successfully created module 'foolog' with type sink  ==> should fail (not a module, but a full stream)    xd:>module compose foo --definition ""queue:bar > filter | transform""  Successfully created module 'foo' with type processor  ==> should be source  {noformat}",8
"Document and review REST API
REST API needs to be finalized and documented for the GA release. The API to be reviewed by REST experts ",8
"Remove Dynamically Created Producers
When producers are created/bound dynamically (e.g. from the router sink), they are not unbound when the module is undeployed.  There is currently no metadata in the binding to provide that functionality.  It is not critical because the producers are just sitting there (and may be reused if the module is redeployed, but it is a leak and should be addressed at some time.",5
"LocalMessageBus Does Not Destroy Channels
When a {{LocalMessageBus}} bridges producer and consumers, it creates an internal channel and registers it with the application context. This can be a {{DirectChannel}}, a {{QueueChannel}} (for named {{queue:*}} channels) or a {{PublishSubscribeChannel}} (for named {{topic:*}} channels).  These channels remain in the context when both the producer and consumer are undeployed. It's rather benign in that the channels will be reused if the modules are redeployed, but it is a leak and should be addressed at some point. A {{QueueChannel}} should remain in the context if its queue is not empty.",2
"Write out initial stream deployment state
Currently when a stream is deployed by {{DeploymentSupervisor}} the event thread blocks until all deployment requests have been answered or timed out. If there were any deployment errors we log a stack trace.    Instead (or in addition to) we need to write out the results of the deployment request. My initial thought is that it would go under:  {panel}  {{/xd/deployments/streams/<name>/state}}  {panel}  The data for {{state}} will be a JSON map with fields {{state}} and an optional {{errorDescription}}.",6
"Interface to capture required data for state calculation
When the state of all the individual modules for a stream are collected, these states need to be examined in order to determine what the overall stream state is. These should be fed into an interface that can potentially be pluggable to handle all of the edge cases/scenarios that may arise.    One possibility is:  {noformat}  public interface DeploymentUnitStateCalculator {    	DeploymentUnit.State calculate(DeploymentUnit deploymentUnit,  			ModuleDeploymentPropertiesProvider provider,  			Collection<ModuleState> moduleStates);  }  {noformat}",4
"Add watch to stream deployment paths
{{DeploymentSupervisor}} will be responsible for maintaining the state data for each stream. When a stream is deployed, a watch should be created so that the supervisor can recalculate the state of a stream as modules are added/removed.",10
"Improve DeploymentVerifier when stream state is complete
As part of XD-1591, {{DeploymentVerifier}} was modified to take the node structure into account. As indicated in the review below, the implementation does not take module properties (such as count) into account:    https://github.com/spring-projects/spring-xd/pull/939/files#r13730134    This means the implementation is incorrect. For now this won't affect us since all tests at the moment are single node. However this can be drastically improved (and simplified) once XD-1270 is completed. At that point we'll be able to simply read a single ZK node to determine if/when a deployment succeeded. ",3
"Method for obtaining stream/job state
See the [design document|https://docs.google.com/a/gopivotal.com/document/d/1kWtoH_xEF1wMklzQ8AZaiuhBZWIlpCDi8G9_hAP8Fgc/edit#heading=h.2rk74f16ow4i] for more details.    This class (or perhaps a method on a repository?) will need to query ZooKeeper to obtain the state of a stream/job in order to pass it along to the REST controller.",3
"Modify REST controller to obtain stream/job state
See the [design document|https://docs.google.com/a/gopivotal.com/document/d/1kWtoH_xEF1wMklzQ8AZaiuhBZWIlpCDi8G9_hAP8Fgc/edit#heading=h.2rk74f16ow4i] for more details.    The REST controller needs to be modified to obtain stream/job state once it is available in ZooKeeper. This depends on XD-1847.",5
"Consolidate REST endpoints for batch resources under /jobs
currently we have /batch and /jobs. Everything should move to /jobs. See https://github.com/spring-projects/spring-xd/wiki/REST-API for details.",4
"IllegalStateException when deploying orphaned stream modules upon a matching container arrival
Upon a matching container arrival, if there are orphaned stream modules to be deployed, then following exception is thrown:    java.lang.IllegalStateException: Container missing      at org.springframework.util.Assert.state(Assert.java:385)      at org.springframework.xd.dirt.core.StreamDeploymentsPath.hasDeploymentInfo(StreamDeploymentsPath.java:275)      at org.springframework.xd.dirt.core.StreamDeploymentsPath.build(StreamDeploymentsPath.java:233)      at org.springframework.xd.dirt.server.ContainerListener.getContainersForStreamModule(ContainerListener.java:337)      at org.springframework.xd.dirt.server.ContainerListener.redeployStreams(ContainerListener.java:278)      at org.springframework.xd.dirt.server.ContainerListener.onChildAdded(ContainerListener.java:186)      at org.springframework.xd.dirt.server.ContainerListener.childEvent(ContainerListener.java:155)",3
"Introduce cache to ZooKeeperContainerRepository
Add Cache implementation for ZooKeeperContainerRepository",5
"Use HATEOAS Traverson for client side consumption
Now that Spring HATEOAS has some support for client side consumption, we can consider using its Traverson object to replace code such as  {noformat} 		resources.put(""streams/definitions"", URI.create(xdRuntime.getLink(""streams"").getHref() + ""/definitions"")); 		resources.put(""streams/deployments"", URI.create(xdRuntime.getLink(""streams"").getHref() + ""/deployments"")); 		resources.put(""jobs"", URI.create(xdRuntime.getLink(""jobs"").getHref()));  {noformat}  in SpringXDTemplate, etc.",8
"Tap Fixture refactoring
The Tap fixture does not need to inherit from AbstractModuleFixture  Replace moduleName method with moduleToTap.    The current tap syntax is: tap:stream:<streamname>.<modulelabel>  and not  tap:stream:<streamname>.<modulelabel>.<modulename> as currently implemented by the label fixture.  ",3
"Remove Hadoop v1 support
Going forward it seems that providing Hadoop v1 will be of lesser importance and we might as well drop it now. SHDP 2.1 will also drop any v1 support.    Remove support for:  - hadoop12 - Apache Hadoop 1.2.1  - cdh4 - Cloudera CDH 4.6.0  - hdp13 - Hortonworks Data Platform 1.3    Keep:  - hadoop22 - Apache Hadoop 2.2.0 (default)  - phd1 - Pivotal HD 1.1  - phd20 - Pivotal HD 2.0  - cdh5 - Cloudera CDH 5.0.0  - hdp21 - Hortonworks Data Platform 2.1    This should make configuration and documentation easier too. Not to mention testing.    This affects startup scripts and the shell plus the build script.",5
"Update to use SHDP 2.0.0.RELEASE
nan",1
"Add option to specify fsUri to hdfs sinks
We should have an --fsUri parameter for hdfs and hdfs-dataset sinks so we can write to different file systems (hdfs, webhdfs)",5
"Can't use webhdfs with hdfs sink
When using spring.hadoop.fsUri set to webhdfs://localhost/ I'm getting an error:    java.lang.NoClassDefFoundError: javax/ws/rs/core/MediaType    including the following in xd/lib seems to fix this:  - jersey-core-1.9.jar  - jersey-server-1.9.jar  ",3
"Provide error location when tapping inexistent stream/module
See impacted code at https://github.com/spring-projects/spring-xd/pull/951",3
"Combine JobDefinition info and BatchJobInfo endpoints
Currently, /jobs/definitions and /batch/jobs offer similar info related to the job configuration info. The former comes from ZKJobDefinitionRepository while the latter comes from Batch Job Repository.  We need to combine this together so that it is not confusing to the end user.",4
"Fix XD config initializer for ZK connection string
Spring Boot 1.1.1 has the following change:    https://github.com/spring-projects/spring-boot/commit/b75578d99c8d435e1f8bf18d0dbb3a2ddf56fdc4    where, an external property source precedence would get re-ordered after the application configuration properties. This change affects Spring XD config initializer which expects an external ""zk-properties"" property source always preceding over the application configuration properties.  ",3
"Organize modules consistently using a dir per module
Even if not strictly necessary for all modules, use the one-dir-per-module scheme for all of them.  Care should be taken in case there are <import>",2
"Create way to deploy custom modules for XD on YARN
Need a way for end-user to package and add custom modules/scripts when deploying XD on YARN. Currently we have a zip file containing all code including modules. It's not convenient to un-zip/re-zip this archive to add custom modules/scripts.    See - https://github.com/spring-projects/spring-xd/issues/931",5
"Add paging support for UI list views
As a user, I'd like to have _paging_ support so that I can scroll through the list of streams, jobs and containers.     Currently the following error is thrown when we cross >20 rows:    http://localhost:9393/jobs/definitions.json    JSON Response:  {code:xml}  [  	{  		links: [ ],  		logref: ""IllegalStateException"",  		message: ""Not all instances were looked at""  	}  ]  {code}    Stack trace:  {code}  15:51:21,931 ERROR http-nio-9393-exec-9 rest.RestControllerAdvice - Caught exception while handling a request  java.lang.IllegalStateException: Not all instances were looked at    org.springframework.util.Assert.state(Assert.java:385)  {code}",5
"Increase performance of query to determine Job restartability
In BatchJobExecutionsController's list all job executions, for each given job execution, it needs to be evaluated against all the job executions of a given job instance to see if the job execution is restarted.  The rule is: for a given JobInstance, there could be only one job execution that can be in ""COMPLETED"" state. If the job itself is restartable and if any of the job executions for this job instance are ""FAILED"" or ""STOPPED"" then, that job execution can be restarted (based on the client request).  Hence, if the job execution is complete, then it will set the restartable flag to false for all the job executions on a given job instance.",3
"Remove ability to do index-based tapping
nan",1
"Change internal tapping namings to use labels
Currently, the tapping infrastructure uses <modulename>.<index> as part of  the internal name of the channel.  Now that module labels are uniques per stream, we may change to just <modulelabel>  The strategy used to derive the channel name should also be extracted, so that a change to a single piece of code is necessary in the future (currently, there is likely duplication in ChannelNode.resolve() and -I assume- the bus)",6
"Split DIRT project into common/admin/container/standalone
This is a generalization of XD-1702",6
"Provide option for sources/sinks to configure mapped headers to/from Messages
See the discussion: https://gopivotal-com.socialcast.com/messages/20771872",1
"Rabbit Sink & Source --host and --port are not updating module host/port.
Acceptance Tests failed on the Rabbit Source and Sink Tests.  The test started failing when XD-1824 was introduced (Support RabbitMQ Cluster in source/sink).  This story added addresses to support rabbit cluster failover.    Currently if a user set --host --port to a remote Rabbit instance, XD will use the default host=localhost and port=5672.  However using --addresses does work.  ",5
"Create documentation for Batch DB migration
Update documentation related to database migration with the changes from XD-1822",1
"Tap lifecycle connection listener should close the tap path children cache upon ZK disconnect
Currently, TapLifecycleConnectionListener (which implements ZooKeeperConnectionListener) clears(but not closes) the taps (PathChildrenCache) upon ZK `onDisconnect` child event.  Since, `onConnect` child event re-creates the tap PathChildrenCache, the previously created PathChildrenCache is still hanging in there.  I would be better to close the cache upon disconnect.",1
"Investigate JobExecutions page list performance
Investigate ""Job Executions"" list page load timing based on number of job executions to load.     The investigation can be of the following steps:    1) Return all the size restrictions to retrieve the number of job executions.  2) Setup 5, 10, 100, 500, 1000 number of job executions and measure the page load timings.    Based on this, we can address the paging support mentioned in XD-1864.",4
"Document json-to-tuple, object-to-json and http-client
Note that the documentation for the module options (in particular for http-client) should be autogenerated using the following syntax (see others):  {noformat}  //^processor.http-client  //$processor.http-client  {noformat}    ",4
"Create test that uses #jsonPath with the filter module
The script tests does the following.    {code}  # Filter for good and bad  create_stream 'httpfilter' ""http | good: filter --expression=#jsonPath(payload,'\$.entities.hashtags[*].text').contains('good') \  | aftergood: filter --expression=true \  | bad: filter --expression=#jsonPath(payload,'\$.entities.hashtags[*].text').contains('bad') \  | goodandbad: splitter --expression=#jsonPath(payload,'$.id') \  | file --dir=$TEST_DIR"" 'true'  {code}",2
"Create gemfire test
Port https://github.com/spring-projects/spring-xd/blob/master/src/test/scripts/gemfire_stream_tests    Need to consider how to start the server, maybe use the jvm fork utilities?  Look into spring-data-gemfire as well.",5
"Create low volume http stress test
The test    https://github.com/spring-projects/spring-xd/blob/master/src/test/scripts/httpbash    is very simple, it doesn't even check the results.  A small change to    https://github.com/spring-projects/spring-xd/blob/master/spring-xd-test-fixtures/src/main/java/org/springframework/xd/test/generator/SimpleHttpGenerator.java    so that number of messages to post is specified would be part of this work.",2
"Create test with jdbc sink and initializeDb=false
See    https://github.com/spring-projects/spring-xd/blob/master/src/test/scripts/jdbc_tests#L96    and    https://github.com/spring-projects/spring-xd/blob/master/src/test/scripts/jdbc_tests#L64    The second assert has initializeDb=false and so there are double the number of rows running the job a second time.",2
"Integration test for field-value-counter and aggregate-counter
https://github.com/spring-projects/spring-xd/blob/master/src/test/scripts/tweet_tests  use field-value-counter and aggregate-counter.    Should do a simplified version of this so that we can assert values of the field-value-counter and aggregate-counter.",3
"HadoopDistroOptionHandler fails when XD_HOME ends with a ""/""
Also, the approach may not work as expected on windows.",3
"Add remote partitioning to filejdbc job
h2. Narrative As a developer, I need to be able to process the importing of files in parallel via the filejdbc batch job.  h2. Acceptance Criteria # Be able to provide a list of files to the job and have them be read in parallel based on the number of slaves deployed. # Use {{MultiResourcePartitioner}} to create on partition per incoming file.",16
"Update dependencies to latest Spring IO platform versions
https://github.com/spring-projects/gradle-plugins/tree/master/spring-io-plugin is the starting point to introduce the appropriate plugin to check for the correct dependencies.    ",5
"Module option validation not happening anymore
It seems that no option validation (being Spring or jsr 303) is happening anymore at stream creation time.    eg  {noformat}  stream create foo --definition ""http --port=bar | log""  {noformat}",2
"Provide ability to disable tab completion for specific module options
Not all module options are born equal. Some are more important/useful than others, and having the more ""expert"" ones show up e.g. in TAB completion is very noisy (esp. given how JLine2 currently presents the whole stream definition typed so far when doing completion, as opposed to just the last bit)",3
"Update Netty to 4
Spring IO Compatibility",1
"Add remote partitioning to hdfsjdbc job
h2. Narrative As a developer, I need to be able to process the importing of files in parallel via the hdfsjdbc batch job.  h2. Acceptance Criteria # Be able to provide a list of files to the job and have them be read in parallel based on the number of slaves deployed. # Use {{MultiResourcePartitioner}} to create on partition per incoming file.",8
"Use 2 tabs for hidden options in shell
nan",1
"UI Needs to handle the finer-grained deployment statuses
Addressed for the REST API by XD-1848    The status of a stream, as returned by the REST API (and thus the shell also), may now contain any of the following states:    * deploying (deployment has been initiated)  * deployed (fully deployed based on each of the stream's modules' count properties)  * incomplete (at least 1 of each module, but 1 or more of them not at requested capacity)  * failed (1 or more of the modules does not have even a single instance deployed)  * undeployed (intentionally undeployed, or created but not yet deployed)  ",4
"Create multi-container, single host, testing framework
Use external JVM launch support provided by the Oracle Tools framework (https://java.net/projects/oracletools).",8
"Add configuration files for hornetmq jms provider.
See https://jira.spring.io/browse/XD-1684    Requires update to gradle 2.1",1
"Update to Spring Platform 1.0.1
nan",2
"Update to Spring Batch 3.0.1 snapshots
nan",1
"Create documentation section on best practices
This should include some guidance on setting rabbit message bus paramters relating to prefetch and concurrency.    It should also discuss the 'bypass' functionality - or reference another section that covers it.    We should probably include how to scale out http sources, e.g. the need to use a load balancer.",4
"Use Boot plugin and IO Platform for versions where possible
nan",4
"Investigate why netty 3.7 is in xd/lib and not 3.6.6
nan",3
"Spring XD - Handling sink failures
If a sink fails for whatever reason, will it be possible to handle it? Say by sending the payload to an error queue for later processing when a JDBC or Mongo sink fails due to a database connectivity loss? Or the modules are designed by certain principles / contracts not to be meant to handle such failures? ",3
"dist task failure - unable to access http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd
The build task ""gradlew dist"" fails with the following error.  00:20:16.976 [ERROR] [org.gradle.BuildExceptionReporter] 	... 52 more 00:20:16.976 [ERROR] [org.gradle.BuildExceptionReporter] Caused by: javax.xml.transform.TransformerException: Failure reading /Users/ixr303/spring-xd/build/reference-work/index.xml 00:20:16.976 [ERROR] [org.gradle.BuildExceptionReporter]   com.icl.saxon.om.Builder.build(Builder.java:267) 00:20:16.976 [ERROR] [org.gradle.BuildExceptionReporter]   com.icl.saxon.Controller.transform(Controller.java:936) 00:20:16.976 [ERROR] [org.gradle.BuildExceptionReporter]   com.icl.saxon.Controller$transform.call(Unknown Source) 00:20:16.976 [ERROR] [org.gradle.BuildExceptionReporter]   AbstractDocbookReferenceTask.transform(DocbookReferencePlugin.groovy:146) 00:20:16.976 [ERROR] [org.gradle.BuildExceptionReporter]   org.gradle.internal.reflect.JavaMethod.invoke(JavaMethod.java:63) 00:20:16.976 [ERROR] [org.gradle.BuildExceptionReporter] 	... 59 more 00:20:16.976 [ERROR] [org.gradle.BuildExceptionReporter] Caused by: java.io.IOException: Server returned HTTP response code: 500 for URL: http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd 00:20:16.977 [ERROR] [org.gradle.BuildExceptionReporter]   org.apache.xerces.impl.XMLEntityManager.setupCurrentEntity(Unknown Source) 00:20:16.977 [ERROR] [org.gradle.BuildExceptionReporter]   org.apache.xerces.impl.XMLEntityManager.startEntity(Unknown Source) 00:20:16.977 [ERROR] [org.gradle.BuildExceptionReporter]   org.apache.xerces.impl.XMLEntityManager.startDTDEntity(Unknown Source) 00:20:16.977 [ERROR] [org.gradle.BuildExceptionReporter]   org.apache.xerces.impl.XMLDTDScannerImpl.setInputSource(Unknown Source) 00:20:16.977 [ERROR] [org.gradle.BuildExceptionReporter]   org.apache.xerces.impl.XMLDocumentScannerImpl$DTDDispatcher.dispatch(Unknown Source) 00:20:16.977 [ERROR] [org.gradle.BuildExceptionReporter]   org.apache.xerces.impl.XMLDocumentFragmentScannerImpl.scanDocument(Unknown Source) 00:20:16.977 [ERROR] [org.gradle.BuildExceptionReporter]   org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source) 00:20:16.977 [ERROR] [org.gradle.BuildExceptionReporter]   org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source) 00:20:16.977 [ERROR] [org.gradle.BuildExceptionReporter]   org.apache.xerces.parsers.XMLParser.parse(Unknown Source) 00:20:16.977 [ERROR] [org.gradle.BuildExceptionReporter]   org.apache.xerces.parsers.AbstractSAXParser.parse(Unknown Source) 00:20:16.978 [ERROR] [org.gradle.BuildExceptionReporter]   org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser.parse(Unknown Source) 00:20:16.978 [ERROR] [org.gradle.BuildExceptionReporter]   com.icl.saxon.om.Builder.build(Builder.java:265)   The URL http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd returns 500 error most of the times.   Tried to circumvent the error by manually getting the did file locally and running again causes unintended relative path issues.  Help is very much appreciated. ",1
"IllegalStateException on single node shutdown
Upon shutdown via ^C, an IllegalStateException stack trace appears in the server logs. While harmless, the traces are annoying and should be prevented.",2
"Batch job is marked as undeployed once computer comes back from hibernation
I have a deployed Batch Job (Single Node Server, running inside STS). My machine goes to sleep. Once I bring it back up I see the following log:     {code}  12:57:35,854 ERROR LeaderSelector-5 leader.LeaderSelector - The leader threw an exception  java.lang.IllegalArgumentException: Label is required    org.springframework.util.Assert.hasText(Assert.java:162)    org.springframework.xd.module.ModuleDescriptor$Key.<init>(ModuleDescriptor.java:616)    org.springframework.xd.dirt.server.JobDeploymentListener.recalculateJobStates(JobDeploymentListener.java:218)    org.springframework.xd.dirt.server.DeploymentSupervisor$LeaderListener.takeLeadership(DeploymentSupervisor.java:354)    org.apache.curator.framework.recipes.leader.LeaderSelector$WrappedListener.takeLeadership(LeaderSelector.java:536)    org.apache.curator.framework.recipes.leader.LeaderSelector.doWork(LeaderSelector.java:398)    org.apache.curator.framework.recipes.leader.LeaderSelector.doWorkLoop(LeaderSelector.java:443)    org.apache.curator.framework.recipes.leader.LeaderSelector.access$100(LeaderSelector.java:63)    org.apache.curator.framework.recipes.leader.LeaderSelector$2.call(LeaderSelector.java:244)    org.apache.curator.framework.recipes.leader.LeaderSelector$2.call(LeaderSelector.java:238)    java.util.concurrent.FutureTask.run(FutureTask.java:266)    java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)    java.util.concurrent.FutureTask.run(FutureTask.java:266)    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)    java.lang.Thread.run(Thread.java:744)  {code}    In the UI the job is marked as *undeployed* - however, when I click *deploy* I get an error: *The job named 'bbb' is already deployed*.    ",4
"Job undeploy operation throws exception
Job `undeploy` operation throws the following stacktrace:    ```  http-nio-9393-exec-5 zookeeper.ZooKeeperJobRepository - Exception while transitioning job 'j' state to undeploying  org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /xd/deployments/jobs/j/status    org.apache.zookeeper.KeeperException.create(KeeperException.java:111)    org.apache.zookeeper.KeeperException.create(KeeperException.java:51)    org.apache.zookeeper.ZooKeeper.setData(ZooKeeper.java:1266)    org.apache.curator.framework.imps.SetDataBuilderImpl$4.call(SetDataBuilderImpl.java:260)    org.apache.curator.framework.imps.SetDataBuilderImpl$4.call(SetDataBuilderImpl.java:256)    org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)    org.apache.curator.framework.imps.SetDataBuilderImpl.pathInForeground(SetDataBuilderImpl.java:252)    org.apache.curator.framework.imps.SetDataBuilderImpl.forPath(SetDataBuilderImpl.java:239)    org.apache.curator.framework.imps.SetDataBuilderImpl.forPath(SetDataBuilderImpl.java:39)    org.springframework.xd.dirt.stream.zookeeper.ZooKeeperJobRepository.delete(ZooKeeperJobRepository.java:177)    org.springframework.xd.dirt.stream.zookeeper.ZooKeeperJobRepository.delete(ZooKeeperJobRepository.java:199)    org.springframework.xd.dirt.stream.zookeeper.ZooKeeperJobRepository.delete(ZooKeeperJobRepository.java:1)    org.springframework.xd.dirt.stream.AbstractInstancePersistingDeployer.undeploy(AbstractInstancePersistingDeployer.java:68)    org.springframework.xd.dirt.rest.XDController.undeploy(XDController.java:125)    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  ```",2
"Avoid using jQuery inside Admin UI
There are couple of places where jquery is being used in the admin UI and those are for some DOM manipulations.  I believe these can certainly be replaced with the use of angular directive or custom functions and thereby we can remove jQuery dependency in the app.",3
"Leverage gradle plugin for provided/optional deps
see https://github.com/spring-projects/gradle-plugins/blob/master/propdeps-plugin/README.md    This would allow us to reduce the distribution size further maybe",4
"Create AngularJS directive to render deployment statuses
In order to minimize code duplication - Create an AngularJS directive to render deployment statuses. We could even color code the various statuses.  We should also create a 2nd directive for the status-help-popover. We should consider to possibly use the Angular Bootstrap UI popover support.",4
"DefaultContainerMatcher - Improve Logging and mention affected Module
When deploying a definition with a container match criteria specified, and no container could be selected - the logging is ambiguous and should mention the affected module:    {code}  11:58:24,089  WARN DeploymentSupervisorCacheListener-0 cluster.DefaultContainerMatcher - No currently available containers match criteria 'somecriteria'  {code}",1
"Handle Status Changes in Client (Dynamically update UI)
As a minimum we need some common polling strategy on the client side to detect status changes of job + streams etc. (E.g. during deployment of streams/jobs)    Ideally, I would like to have this addressed on the server-side as well. It would be nice if we could propagate events between, containers and admin-server that would inform about any changes in the system. We could then use those to notify connected UI clients.",3
"Handle 'deploying' state at the Admin UI
When the job is in ""deploying"" state, until we decide whether the job is actually ""deployed"" or ""failed""/""incomplete"", there is no way to know if it is fine to launch/schedule (though the launching requests are going to go to the job launch request queue).   We could either disable both ""deploy""/""undeploy"" until the state changes from ""deploying""?",3
"Remove Retry from TCP Sink
Now that the bus supports retry it is no longer necessary to have the retry advice in the TCP Sink.",1
"Update to Spring Shell 1.0 RC4
nan",2
"Improve getting started docs for installation
https://github.com/spring-guides/gs-spring-xd/issues/1",1
"Rabbitmq source is not ingested the data into jdbc sink
I am using Spring XD to ingest the data into Pivotal HD.My source is log files which is coming from logstash through Rabbitmq. I could able to ingest the log files in HDFS (by using Rabbitmq source and HDFS sink)  However when i try to ingest the data directly into Hawq by using JDBC sink,it's not working. Shall we directly load Rabbitmq source into any databases like Hawq?      stream create --name pivotalqueue --definition ""rabbit --host=<my host name>   | jdbc   --columns='colum list'""      ---Not working    I configured jdbc in jdbc.properties. There was no issue with jdbc configuration(because i tested this with simple tail source it's working and load the data into HAWQ.  stream create --name pivotalqueue --definition ""tail --name=/tmp/xd/output/test.out   | jdbc  --columns='columns list'""  )  ",3
"Enable FtpHdfsTest
FtpHdfsTest was added in https://github.com/spring-projects/spring-xd/pull/1005 but excluded since it will require some changes to run on ec2, including setting up an ftpServer on ec2 with appropriate security, etc.",4
"Stream/Job deployment state is always ""incomplete"" in case of module count zero
In case of the deployment property module count for a specific module in a stream/job is zero, the stream/job deployment status is calculated as ""zero"" even though the modules are deployed to all matching containers.  Currently, the DefaultStateCalculator's 'calculate' method doesn't check if the expected count is zero.  Also, we would need all the matching containers to determine the same module is deployed to all the matching containers (in case of module count = 0)",4
"Add Hadoop 2.4.x as an option
Hadoop 2.4.1 is now a stable release and we should add support for running against it",3
"Exception for sample hdfs sample
Hi    If I create a hdfs stream as proposed in the documentation I get some errors.    Example:  stream create --name xxx --definition ""http --port=8000 | hdfs --rollover=10""   stream deploy --name xxx    The exception:  Caused by: java.io.FileNotFoundException: ...spring-xd-1.0.0.M7/config/hadoop.properties (No such file or directory)    java.io.FileInputStream.open(Native Method)    There is a folder /spring-xd-1.0.0.M7/xd/config but no folder /spring-xd-1.0.0.M7/config  when I copied the file from /xd/config to /config it worked fine.",1
"Update TypeConversion Page
Need to update the examples in the TypeConversion doc, re spring social Tweet which is no longer used.",1
"Add a JMS Sink
XD currently has rabbit source and sink but only a source for JMS.    Add a JMS sink - the provider infrastructure should be configured in a similar manner to the source ({{--provider}}).    Other properties needed:   {code}  destinationName  destinationExpression  sessionTransacted  deliveryPersistent*  pubSubDomain  priority*  timeToLive*  {code}    * if any of these properties are set we need to coerce the {{explicit-qos-enabled}} to be {{true}}  ",3
"Update https://github.com/spring-guides/gs-spring-xd/ for new Release
nan",1
"TriggerSourceOptionsMetadata - Change DateFormat to be ISO 8601 compliant (with TimeZone)
To bring in line with the rest of default date-formats, change the date format to yyyy-MM-dd HH:mm:ss in TriggerSourceOptionsMetadata",2
"Make Cron-based Triggers TimeZone aware 
Currently specified Cron Expressions are executed in the Container's default TimeZone. In *Trigger.xml* we specify:  {code} 	<beans profile=""use-cron""> 		<int:inbound-channel-adapter channel=""output"" 			auto-startup=""false"" expression=""'${payload}'""> 			<int:poller cron=""${cron}"" /> 		</int:inbound-channel-adapter> 	</beans> {code}  This translates in *org.springframework.integration.config.xml.PollerParser* to  {code} BeanDefinitionBuilder builder = BeanDefinitionBuilder.genericBeanDefinition(CronTrigger.class); builder.addConstructorArgValue(cronAttribute) {code}  Which will call *org.springframework.scheduling.support.CronTrigger*:  {code} 	/** 	 * Build a {@link CronTrigger} from the pattern provided in the default time zone. 	 * @param cronExpression a space-separated list of time fields, 	 * following cron expression conventions 	 */ 	public CronTrigger(String cronExpression) { 		this.sequenceGenerator = new CronSequenceGenerator(cronExpression); 	} {code}  ""Build a {@link CronTrigger} from the pattern provided in the *default time zone*.""  We need to pass-in a timezone. *Should cron expressions as part of an XD *Definition* have a TimeZone parameter?* When creating the stream via the UI or the Shell the timezone can be inferred (if not specified) but should be mandatory for the REST API, meaning being passed in as a mandatory parameter (OR alternatively, if not passed in we assume the Cron expression is specified for UTC).  That way we could ensure that a (Stream/Job) Definition is globally valid.",8
"XD Shell Should be lenient in regards to specified dates/times for triggers 
Ideally we could try out best if there was a date that didn't have the TZD format on it: e.g. figure what the local time zone is (AdminUI or Shell) and figure out if we are ahead/behind UTC.  So * ""YYYY-MM-DDThh:mm""  would say 'try to use local time zone' * ""YYYY-MM-DDThh:mmZ"" would be UTC * ""YYYY-MM-DDThh:mm+4:00"" would be '4 hrs ahead of UTC  However, we need to ultimately submit an ISO compliant time to the XD server.  The UI/shell should try to be as accommodating as possible...e.g postel's law: http://en.wikipedia.org/wiki/Robustness_principle  Be conservative in what you do, be liberal in what you accept from others.  ",8
"Create RuntimeModuleDescriptor that represents runtime module instance
See this for more info: https://github.com/spring-projects/spring-xd/pull/1021/files#r14611854",1
"Rename ModuleDeployer
For more info, please see here:  https://github.com/spring-projects/spring-xd/pull/1021/files#r14617723",1
"Create REST endpoint for validation of a job/stream definition
Have a REST endpoint that would run validation of a definition (without actually attempting the creation, let alone deployment) and return a structured representation of validation errors.  This to benefit the web ui",5
"Find logging configuration relative to environment
Previously, the scripts all looked for the logging configuration in $XD_HOME/config (or %XD_HOME%/config). This caused issues because it meant that if you moved all of the configuration and overrode $XD_CONFIG_LOCATION (or %XD_CONFIG_LOCATION%), the logging configuration changes would not be found in the new location. This change updates the scripts to look for logging configuration in $XD_CONFIG_LOCATION (or %XD_CONFIG_LOCATION%).",2
"Provide JMS as a supported MessageBus implementation
nan",7
"Jolokia endpoints returning 404
http://localhost:9393/management/jolokia/search/xd.*:type=*,*    for singlenode in M7 returned a value..... on master it returns 404 error...    ",4
"JDK 1.8 compile warning for ContainerConfiguration
The following warning appears when compiling with JDK 8:      {panel}  /Users/pperalta/src/github/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/server/SingleNodeApplication.java:67: warning: auxiliary class ContainerConfiguration in /Users/pperalta/src/github/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/server/ContainerServerApplication.java should not be accessed from outside its own source file  						.child(ContainerConfiguration.class)}}  {panel}    Can this be turned into a static inner class?",1
"Verify platform compatibility versions with the XD dependencies
We need to make sure there is no conflicting/missing dependency with build.gradle using spring IO platform dependencies.    https://jira.spring.io/browse/XD-1929 is one such scenario where jolokia dependency went missing.",1
"Support user impersonation in HDFS sink
Should be able to specify which user,group will own the files that are written in HDFS.",4
"Update http source to use netty 4
Platform version of netty is 4.0.18.Final.  Current http source is using 3.7.  Packages/classes have changed in netty4",4
"Update Spring Integration Splunk Extension to 1.1 GA
nan",1
"Batch jobs executions by jobname causes stackoverflow
Whether it's after applying https://github.com/spring-projects/spring-xd/pull/1034/ or not, this causes the following problem:    {noformat}  ....    com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:644)    com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:152)    com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:541)    com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:644)    com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:152)    com.fasterxml.jackson.databind.ser.impl.IndexedListSerializer.serializeContents(IndexedListSerializer.java:100)    com.fasterxml.jackson.databind.ser.impl.IndexedListSerializer.serializeContents(IndexedListSerializer.java:21)    com.fasterxml.jackson.databind.ser.std.AsArraySerializerBase.serialize(AsArraySerializerBase.java:183)    com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:541)    com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:644)    com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:152)    com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:541)    com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:644)    com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:152)    com.fasterxml.jackson.databind.ser.impl.IndexedListSerializer.serializeContents(IndexedListSerializer.java:100)    com.fasterxml.jackson.databind.ser.impl.IndexedListSerializer.serializeContents(IndexedListSerializer.java:21)  Caused by: java.lang.StackOverflowError    java.lang.ClassLoader.defineClass1(Native Method)    java.lang.ClassLoader.defineClass(ClassLoader.java:800)    java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)    java.net.URLClassLoader.defineClass(URLClassLoader.java:449)    java.net.URLClassLoader.access$100(URLClassLoader.java:71)    java.net.URLClassLoader$1.run(URLClassLoader.java:361)    java.net.URLClassLoader$1.run(URLClassLoader.java:355)    java.security.AccessController.doPrivileged(Native Method)    java.net.URLClassLoader.findClass(URLClassLoader.java:354)    java.lang.ClassLoader.loadClass(ClassLoader.java:425)    sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)    java.lang.ClassLoader.loadClass(ClassLoader.java:358)    com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:660)  	... 1011 more  {noformat}",4
"Remove jars from .zip packaging whose license prevents distribution
The work here is doing the research....    mysql client jar should be removed as it is GPL - http://dev.mysql.com/downloads/connector/j/5.0.html  (GPL)    postgresql is BSD so that is ok (another issue will handle license file inclusion.",3
"Add licence files in distribution for 3rd party dependencies.
postgresql is BSD - http://jdbc.postgresql.org/about/license.html",4
"Shell completion crashes
The XD shell completion crashes on:  job launch --name <TAB> gives  {noformat}  xd:>job launch --name Exception in thread ""Spring Shell"" java.lang.IllegalStateException: Could not determine kind: tab-completion-count-1 existing-job disable-string-converter    org.springframework.xd.shell.converter.CompletionConverter.determineKind(CompletionConverter.java:109)    org.springframework.xd.shell.converter.CompletionConverter.getAllPossibleValues(CompletionConverter.java:69)    org.springframework.shell.core.SimpleParser.completeAdvanced(SimpleParser.java:857)    org.springframework.shell.core.ParserCompleter.complete(ParserCompleter.java:47)    jline.console.ConsoleReader.complete(ConsoleReader.java:3077)    jline.console.ConsoleReader.readLine(ConsoleReader.java:2501)    jline.console.ConsoleReader.readLine(ConsoleReader.java:2162)    jline.console.ConsoleReader.readLine(ConsoleReader.java:2150)    org.springframework.shell.core.JLineShell.promptLoop(JLineShell.java:522)    org.springframework.shell.core.JLineShell.run(JLineShell.java:179)    java.lang.Thread.run(Thread.java:744)  {noformat}    Moreover, seems completion generally crashes when the server is not up (which was taken care of previously if I'm not mistaken):    xd:>job destroy --name <TAB>  {noformat}  Exception in thread ""Spring Shell"" org.springframework.web.client.ResourceAccessException: I/O error on GET request for ""http://localhost:9393/jobs/definitions?size=10000&deployments=true"":Connection refused; nested exception is java.net.ConnectException: Connection refused    org.springframework.web.client.RestTemplate.doExecute(RestTemplate.java:561)    org.springframework.web.client.RestTemplate.execute(RestTemplate.java:506)    org.springframework.web.client.RestTemplate.getForObject(RestTemplate.java:243)    org.springframework.xd.rest.client.impl.JobTemplate.list(JobTemplate.java:121)    org.springframework.xd.rest.client.impl.JobTemplate.list(JobTemplate.java:40)    org.springframework.xd.shell.converter.ExistingXDEntityConverter.getAllPossibleValues(ExistingXDEntityConverter.java:72)    org.springframework.shell.core.SimpleParser.completeAdvanced(SimpleParser.java:857)    org.springframework.shell.core.ParserCompleter.complete(ParserCompleter.java:47)    jline.console.ConsoleReader.complete(ConsoleReader.java:3077)    jline.console.ConsoleReader.readLine(ConsoleReader.java:2501)    jline.console.ConsoleReader.readLine(ConsoleReader.java:2162)    jline.console.ConsoleReader.readLine(ConsoleReader.java:2150)    org.springframework.shell.core.JLineShell.promptLoop(JLineShell.java:522)    org.springframework.shell.core.JLineShell.run(JLineShell.java:179)    java.lang.Thread.run(Thread.java:744)  Caused by: java.net.ConnectException: Connection refused    java.net.PlainSocketImpl.socketConnect(Native Method)    java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)    java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)    java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)    java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)    java.net.Socket.connect(Socket.java:579)    java.net.Socket.connect(Socket.java:528)    sun.net.NetworkClient.doConnect(NetworkClient.java:180)    sun.net.www.http.HttpClient.openServer(HttpClient.java:432)    sun.net.www.http.HttpClient.openServer(HttpClient.java:527)    sun.net.www.http.HttpClient.<init>(HttpClient.java:211)    sun.net.www.http.HttpClient.New(HttpClient.java:308)    sun.net.www.http.HttpClient.New(HttpClient.java:326)    sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:996)    sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:932)    sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:850)    org.springframework.http.client.SimpleBufferingClientHttpRequest.executeInternal(SimpleBufferingClientHttpRequest.java:78)    org.springframework.http.client.AbstractBufferingClientHttpRequest.executeInternal(AbstractBufferingClientHttpRequest.java:48)    org.springframework.http.client.AbstractClientHttpRequest.execute(AbstractClientHttpRequest.java:52)  {noformat}",2
"Update to Spring Batch Admin 1.3.0.RC1
nan",1
"Clean up duplicated dependencies from XD on YARN installation
Remove unnecessary/duplicated jars from the lib directory in spring-xd-yarn zip distribution",3
"No main manifest attribute in xd-yarn-client jar
Error deploying to YARN -     $ ./spring-xd-1.0.0.BUILD-SNAPSHOT-yarn/bin/xd-yarn push -p spring-xd-1.0.0.BUILD-SNAPSHOT-yarn  no main manifest attribute, in spring-xd-1.0.0.BUILD-SNAPSHOT-yarn/lib/spring-xd-yarn-client-1.0.0.BUILD-SNAPSHOT.jar    probably related to boot changes",3
"Use guava 15.0 for spring-xd-integration-test
jclouds is not compatible with versions of guava higher than 15.",3
"Update to Spring Batch Admin 1.3.0.GA
nan",1
"Error deploying stream when admin running and container arrives after stream deployment request
Steps to reproduce:    1. start xd-admin    2. start shell and create and deploy stream (""time | hdfs"")    3. start container    I got:    [2014-07-10 09:10:29.019] boot - 19923â€‚â€‚INFO [DeploymentSupervisorCacheListener-0] --- InitialDeploymentListener: Path cache event: /deployments/streams/test, type: CHILD_ADDED  [2014-07-10 09:10:29.137] boot - 19923â€‚â€‚INFO [Deployer] --- StreamDeploymentListener: Deploying stream Stream{name='test'}  [2014-07-10 09:10:29.146] boot - 19923â€‚â€‚WARN [Deployer] --- StreamDeploymentListener: No containers available for deployment of stream test  [2014-07-10 09:10:29.146] boot - 19923â€‚â€‚INFO [Deployer] --- StreamDeploymentListener: Stream Stream{name='test'} deployment attempt complete  [2014-07-10 09:11:08.003] boot - 19923â€‚â€‚INFO [DeploymentSupervisorCacheListener-0] --- ContainerListener: Path cache event: /containers/007c2bcc-13f4-466e-95d3-bd926bb456ea, type: CHILD_ADDED  [2014-07-10 09:11:08.006] boot - 19923â€‚â€‚INFO [DeploymentSupervisorCacheListener-0] --- ArrivingContainerModuleRedeployer: Container arrived: 007c2bcc-13f4-466e-95d3-bd926bb456ea  [2014-07-10 09:11:08.176] boot - 19923 ERROR [DeploymentSupervisorCacheListener-0] --- PathChildrenCache:   org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /xd/deployments/streams/test/modules  â€‚â€‚at org.apache.zookeeper.KeeperException.create(KeeperException.java:111)  â€‚â€‚at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)  â€‚â€‚at org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1590)  â€‚â€‚at org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:214)  â€‚â€‚at org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:203)  â€‚â€‚at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)  â€‚â€‚at org.apache.curator.framework.imps.GetChildrenBuilderImpl.pathInForeground(GetChildrenBuilderImpl.java:199)  â€‚â€‚at org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:191)  â€‚â€‚at org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:38)  â€‚â€‚at org.springframework.xd.dirt.server.ArrivingContainerModuleRedeployer.deployUnallocatedStreamModules(ArrivingContainerModuleRedeployer.java:133)  â€‚â€‚at org.springframework.xd.dirt.server.ArrivingContainerModuleRedeployer.deployModules(ArrivingContainerModuleRedeployer.java:106)  â€‚â€‚at org.springframework.xd.dirt.server.ContainerListener.childEvent(ContainerListener.java:99)  â€‚â€‚at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:509)  â€‚â€‚at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:503)  â€‚â€‚at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)  â€‚â€‚at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)  â€‚â€‚at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)  â€‚â€‚at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:500)  â€‚â€‚at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)  â€‚â€‚at org.apache.curator.framework.recipes.cache.PathChildrenCache$10.run(PathChildrenCache.java:762)  â€‚â€‚at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)  â€‚â€‚at java.util.concurrent.FutureTask.run(FutureTask.java:262)  â€‚â€‚at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)  â€‚â€‚at java.util.concurrent.FutureTask.run(FutureTask.java:262)  â€‚â€‚at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)  â€‚â€‚at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)  â€‚â€‚at java.lang.Thread.run(Thread.java:744)  ",3
"XD_CONFIG_LOCATION doesn't seem to be set for log4j config files 
Starting xd-singlenode and then ctrl-c to shut down produces WARN message that should be suppressed according to log4j config  Setting XD_CONFIG_LOCATION explicitly works for suppressing the message.  The message I see:  [2014-07-10 09:39:59.786] boot - 58034  WARN [Thread-2] --- MBeanRegistry: Failed to unregister MBean InMemoryDataTree [2014-07-10 09:39:59.786] boot - 58034  WARN [Thread-2] --- MBeanRegistry: Error during unregister javax.management.InstanceNotFoundException: org.apache.ZooKeeperService:name0=StandaloneServer_port-1,name1=InMemoryDataTree   com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getMBean(DefaultMBeanServerInterceptor.java:1095)   com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.exclusiveUnregisterMBean(DefaultMBeanServerInterceptor.java:427)   com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.unregisterMBean(DefaultMBeanServerInterceptor.java:415)   com.sun.jmx.mbeanserver.JmxMBeanServer.unregisterMBean(JmxMBeanServer.java:546)   org.apache.zookeeper.jmx.MBeanRegistry.unregister(MBeanRegistry.java:115)   org.apache.zookeeper.jmx.MBeanRegistry.unregister(MBeanRegistry.java:132)   org.apache.zookeeper.server.ZooKeeperServer.unregisterJMX(ZooKeeperServer.java:465)   org.apache.zookeeper.server.ZooKeeperServer.shutdown(ZooKeeperServer.java:458)   org.apache.zookeeper.server.NIOServerCnxnFactory.shutdown(NIOServerCnxnFactory.java:271)   org.apache.zookeeper.server.ZooKeeperServerMain.shutdown(ZooKeeperServerMain.java:132)   sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)   sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)   sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)   java.lang.reflect.Method.invoke(Method.java:606)   org.springframework.xd.dirt.zookeeper.EmbeddedZooKeeper.stop(EmbeddedZooKeeper.java:176)   org.springframework.xd.dirt.zookeeper.EmbeddedZooKeeper.stop(EmbeddedZooKeeper.java:204)   org.springframework.context.support.DefaultLifecycleProcessor.doStop(DefaultLifecycleProcessor.java:229)   org.springframework.context.support.DefaultLifecycleProcessor.access$300(DefaultLifecycleProcessor.java:51)   org.springframework.context.support.DefaultLifecycleProcessor$LifecycleGroup.stop(DefaultLifecycleProcessor.java:363)   org.springframework.context.support.DefaultLifecycleProcessor.stopBeans(DefaultLifecycleProcessor.java:202)   org.springframework.context.support.DefaultLifecycleProcessor.onClose(DefaultLifecycleProcessor.java:118)   org.springframework.context.support.AbstractApplicationContext.doClose(AbstractApplicationContext.java:888)   org.springframework.context.support.AbstractApplicationContext.close(AbstractApplicationContext.java:841)   org.springframework.boot.builder.ParentContextCloserApplicationListener$ContextCloserListener.onApplicationEvent(ParentContextCloserApplicationListener.java:100)   org.springframework.boot.builder.ParentContextCloserApplicationListener$ContextCloserListener.onApplicationEvent(ParentContextCloserApplicationListener.java:84)   org.springframework.context.event.SimpleApplicationEventMulticaster.multicastEvent(SimpleApplicationEventMulticaster.java:98)   org.springframework.context.support.AbstractApplicationContext.publishEvent(AbstractApplicationContext.java:333)   org.springframework.context.support.AbstractApplicationContext.doClose(AbstractApplicationContext.java:880)   org.springframework.context.support.AbstractApplicationContext$1.run(AbstractApplicationContext.java:809) ",3
"Add stream state tests
Test to verify stream state is correct after starting/stopping containers.",3
"Fix support for @CliAvailabilityIndicator
See PR https://github.com/spring-projects/spring-xd/pull/1043/",3
"Build should use Spring Boot plugin version 1.1.4 
The platform uses Boot version 1.1.4 so the plugin version used in build.gradle should match that.",1
"Ensure DSM matrix is diagonal
nan",2
"Single step partition support on filejdbc module uses module's datasource
The filejdbc module's single step partition support configures to use jdbc module's datasource rather than XD's batch datasource.    ```  org.springframework.messaging.MessageHandlingException: org.springframework.jdbc.UncategorizedSQLException: PreparedStatementCallback; uncategorized SQLException for SQL [SELECT JOB_EXECUTION_ID, START_TIME, END_TIME, STATUS, EXIT_CODE, EXIT_MESSAGE, CREATE_TIME, LAST_UPDATED, VERSION, JOB_CONFIGURATION_LOCATION from BATCH_JOB_EXECUTION where JOB_EXECUTION_ID = ?]; SQL state [null]; error code [0]; [SQLITE_ERROR] SQL error or missing database (no such table: BATCH_JOB_EXECUTION); nested exception is java.sql.SQLException: [SQLITE_ERROR] SQL error or missing database (no such table: BATCH_JOB_EXECUTION)    org.springframework.integration.handler.MethodInvokingMessageProcessor.processMessage(MethodInvokingMessageProcessor.java:78)    org.springframework.integration.handler.ServiceActivatingHandler.handleRequestMessage(ServiceActivatingHandler.java:71)    org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:170)    org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)    java.lang.reflect.Method.invoke(Method.java:606)    org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)    org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)    org.springframework.integration.monitor.SimpleMessageHandlerMetrics.handleMessage(SimpleMessageHandlerMetrics.java:106)    org.springframework.integration.monitor.SimpleMessageHandlerMetrics.invoke(SimpleMessageHandlerMetrics.java:86)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)    org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)    com.sun.proxy.$Proxy117.handleMessage(Unknown Source)    org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)    org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)    org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)    org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)    org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:255)    org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:223)    sun.reflect.GeneratedMethodAccessor107.invoke(Unknown Source)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)    java.lang.reflect.Method.invoke(Method.java:606)    org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)    org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)    org.springframework.integration.monitor.DirectChannelMetrics.monitorSend(DirectChannelMetrics.java:113)    org.springframework.integration.monitor.DirectChannelMetrics.doInvoke(DirectChannelMetrics.java:97)    org.springframework.integration.monitor.DirectChannelMetrics.invoke(DirectChannelMetrics.java:91)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)    org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)    com.sun.proxy.$Proxy115.send(Unknown Source)    org.springframework.xd.dirt.integration.bus.LocalMessageBus$3.handleMessage(LocalMessageBus.java:188)    org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)    org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)    org.springframework.integration.dispatcher.UnicastingDispatcher.access$000(UnicastingDispatcher.java:48)    org.springframework.integration.dispatcher.UnicastingDispatcher$1.run(UnicastingDispatcher.java:92)    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)    java.lang.Thread.run(Thread.java:744)  Caused by: org.springframework.jdbc.UncategorizedSQLException: PreparedStatementCallback; uncategorized SQLException for SQL [SELECT JOB_EXECUTION_ID, START_TIME, END_TIME, STATUS, EXIT_CODE, EXIT_MESSAGE, CREATE_TIME, LAST_UPDATED, VERSION, JOB_CONFIGURATION_LOCATION from BATCH_JOB_EXECUTION where JOB_EXECUTION_ID = ?]; SQL state [null]; error code [0]; [SQLITE_ERROR] SQL error or missing database (no such table: BATCH_JOB_EXECUTION); nested exception is java.sql.SQLException: [SQLITE_ERROR] SQL error or missing database (no such table: BATCH_JOB_EXECUTION)    org.springframework.jdbc.support.AbstractFallbackSQLExceptionTranslator.translate(AbstractFallbackSQLExceptionTranslator.java:84)    org.springframework.jdbc.support.AbstractFallbackSQLExceptionTranslator.translate(AbstractFallbackSQLExceptionTranslator.java:81)    org.springframework.jdbc.support.AbstractFallbackSQLExceptionTranslator.translate(AbstractFallbackSQLExceptionTranslator.java:81)    org.springframework.jdbc.core.JdbcTemplate.execute(JdbcTemplate.java:660)    org.springframework.jdbc.core.JdbcTemplate.query(JdbcTemplate.java:695)    org.springframework.jdbc.core.JdbcTemplate.query(JdbcTemplate.java:727)    org.springframework.jdbc.core.JdbcTemplate.query(JdbcTemplate.java:737)    org.springframework.jdbc.core.JdbcTemplate.queryForObject(JdbcTemplate.java:811)    org.springframework.batch.core.repository.dao.JdbcJobExecutionDao.getJobExecution(JdbcJobExecutionDao.java:267)    org.springframework.batch.core.explore.support.SimpleJobExplorer.getStepExecution(SimpleJobExplorer.java:142)    org.springframework.batch.integration.partition.StepExecutionRequestHandler.handle(StepExecutionRequestHandler.java:52)    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)    java.lang.reflect.Method.invoke(Method.java:606)    org.springframework.expression.spel.support.ReflectiveMethodExecutor.execute(ReflectiveMethodExecutor.java:63)    org.springframework.expression.spel.ast.MethodReference.getValueInternal(MethodReference.java:122)    org.springframework.expression.spel.ast.MethodReference.access$000(MethodReference.java:44)    org.springframework.expression.spel.ast.MethodReference$MethodValueRef.getValue(MethodReference.java:258)    org.springframework.expression.spel.ast.CompoundExpression.getValueInternal(CompoundExpression.java:84)    org.springframework.expression.spel.ast.SpelNodeImpl.getTypedValue(SpelNodeImpl.java:114)    org.springframework.expression.spel.standard.SpelExpression.getValue(SpelExpression.java:111)    org.springframework.integration.util.AbstractExpressionEvaluator.evaluateExpression(AbstractExpressionEvaluator.java:159)    org.springframework.integration.util.MessagingMethodInvokerHelper.processInternal(MessagingMethodInvokerHelper.java:268)    org.springframework.integration.util.MessagingMethodInvokerHelper.process(MessagingMethodInvokerHelper.java:142)    org.springframework.integration.handler.MethodInvokingMessageProcessor.processMessage(MethodInvokingMessageProcessor.java:75)  	... 41 more  Caused by: java.sql.SQLException: [SQLITE_ERROR] SQL error or missing database (no such table: BATCH_JOB_EXECUTION)    org.sqlite.DB.newSQLException(DB.java:383)    org.sqlite.DB.newSQLException(DB.java:387)    org.sqlite.DB.throwex(DB.java:374)    org.sqlite.NestedDB.prepare(NestedDB.java:134)    org.sqlite.DB.prepare(DB.java:123)    org.sqlite.PrepStmt.<init>(PrepStmt.java:42)    org.sqlite.Conn.prepareStatement(Conn.java:404)    org.sqlite.Conn.prepareStatement(Conn.java:399)    org.sqlite.Conn.prepareStatement(Conn.java:383)    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)    java.lang.reflect.Method.invoke(Method.java:606)    org.apache.tomcat.jdbc.pool.ProxyConnection.invoke(ProxyConnection.java:126)    org.apache.tomcat.jdbc.pool.JdbcInterceptor.invoke(JdbcInterceptor.java:109)    org.apache.tomcat.jdbc.pool.DisposableConnectionFacade.invoke(DisposableConnectionFacade.java:80)    com.sun.proxy.$Proxy109.prepareStatement(Unknown Source)    org.springframework.jdbc.core.JdbcTemplate$SimplePreparedStatementCreator.createPreparedStatement(JdbcTemplate.java:1557)    org.springframework.jdbc.core.JdbcTemplate.execute(JdbcTemplate.java:638)  	... 63 more  12:23:37,941  INFO main-EventThread server.ContainerRegistrar:254 - Undeploying module [ModuleDescriptor@d192973 moduleName = 'filejdbc', moduleLabel = 'filejdbc', group = 'csvjdbcjob0', sourceChannelName = [null], sinkChannelName = [null], sinkChannelName = [null], index = 0, type = job, parameters = map['resources' -> 'file:///tmp/xdtest/jdbc/delete_after_use.csv', 'initializeDatabase' -> 'true', 'names' -> 'col1,col2,col3', 'deleteFiles' -> 'true', 'driverClassName' -> 'org.sqlite.JDBC', 'url' -> 'jdbc:sqlite:/tmp/xdtest/jdbc/jdbc.db'], children = list[[empty]]]  12:23:37,941  INFO main-EventThread module.ModuleDeployer:158 - removed SimpleModule [name=filejdbc, type=job, group=csvjdbcjob0, index=0 @73cc35b5]  12:23:37,944 ERROR task-scheduler-1 step.AbstractStep:225 - Encountered an error executing step step1-master in job csvjdbcjob0  org.springframework.integration.MessageTimeoutException: Timeout occurred before all partitions returned    org.springframework.batch.integration.partition.MessageChannelPartitionHandler.handle(MessageChannelPartitionHandler.java:141)    org.springframework.batch.core.partition.support.PartitionStep.doExecute(PartitionStep.java:106)    org.springframework.batch.core.step.AbstractStep.execute(AbstractStep.java:198)    org.springframework.batch.core.job.SimpleStepHandler.handleStep(SimpleStepHandler.java:148)    org.springframework.batch.core.job.flow.JobFlowExecutor.executeStep(JobFlowExecutor.java:64)    org.springframework.batch.core.job.flow.support.state.StepState.handle(StepState.java:67)    org.springframework.batch.core.job.flow.support.SimpleFlow.resume(SimpleFlow.java:162)    org.springframework.batch.core.job.flow.support.SimpleFlow.start(SimpleFlow.java:141)    org.springframework.batch.core.job.flow.FlowJob.doExecute(FlowJob.java:134)    org.springframework.batch.core.job.AbstractJob.execute(AbstractJob.java:304)    org.springframework.batch.core.launch.support.SimpleJobLauncher$1.run(SimpleJobLauncher.java:135)    org.springframework.core.task.SyncTaskExecutor.execute(SyncTaskExecutor.java:50)    org.springframework.batch.core.launch.support.SimpleJobLauncher.run(SimpleJobLauncher.java:128)    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)    java.lang.reflect.Method.invoke(Method.java:606)    org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)    org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)    org.springframework.batch.core.configuration.annotation.SimpleBatchConfiguration$PassthruAdvice.invoke(SimpleBatchConfiguration.java:127)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)    org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)    com.sun.proxy.$Proxy44.run(Unknown Source)    org.springframework.batch.integration.launch.JobLaunchingMessageHandler.launch(JobLaunchingMessageHandler.java:50)    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)    java.lang.reflect.Method.invoke(Method.java:606)    org.springframework.expression.spel.support.ReflectiveMethodExecutor.execute(ReflectiveMethodExecutor.java:63)    org.springframework.expression.spel.ast.MethodReference.getValueInternal(MethodReference.java:122)    org.springframework.expression.spel.ast.MethodReference.access$000(MethodReference.java:44)    org.springframework.expression.spel.ast.MethodReference$MethodValueRef.getValue(MethodReference.java:258)    org.springframework.expression.spel.ast.CompoundExpression.getValueInternal(CompoundExpression.java:84)    org.springframework.expression.spel.ast.SpelNodeImpl.getTypedValue(SpelNodeImpl.java:114)    org.springframework.expression.spel.standard.SpelExpression.getValue(SpelExpression.java:111)    org.springframework.integration.util.AbstractExpressionEvaluator.evaluateExpression(AbstractExpressionEvaluator.java:159)    org.springframework.integration.util.MessagingMethodInvokerHelper.processInternal(MessagingMethodInvokerHelper.java:268)    org.springframework.integration.util.MessagingMethodInvokerHelper.process(MessagingMethodInvokerHelper.java:142)    org.springframework.integration.handler.MethodInvokingMessageProcessor.processMessage(MethodInvokingMessageProcessor.java:75)    org.springframework.integration.handler.ServiceActivatingHandler.handleRequestMessage(ServiceActivatingHandler.java:71)    org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:170)    org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)    java.lang.reflect.Method.invoke(Method.java:606)    org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)    org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)    org.springframework.integration.monitor.SimpleMessageHandlerMetrics.handleMessage(SimpleMessageHandlerMetrics.java:106)    org.springframework.integration.monitor.SimpleMessageHandlerMetrics.invoke(SimpleMessageHandlerMetrics.java:86)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)    org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)    com.sun.proxy.$Proxy117.handleMessage(Unknown Source)    org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)    org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)    org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)    org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)    org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:255)    org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:223)    sun.reflect.GeneratedMethodAccessor107.invoke(Unknown Source)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)    java.lang.reflect.Method.invoke(Method.java:606)    org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)    org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)    org.springframework.integration.monitor.DirectChannelMetrics.monitorSend(DirectChannelMetrics.java:113)    org.springframework.integration.monitor.DirectChannelMetrics.doInvoke(DirectChannelMetrics.java:97)    org.springframework.integration.monitor.DirectChannelMetrics.invoke(DirectChannelMetrics.java:91)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)    org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)    com.sun.proxy.$Proxy115.send(Unknown Source)    org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:109)    org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:44)    org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:94)    org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendMessage(AbstractReplyProducingMessageHandler.java:260)    org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendReplyMessage(AbstractReplyProducingMessageHandler.java:241)    org.springframework.integration.handler.AbstractReplyProducingMessageHandler.produceReply(AbstractReplyProducingMessageHandler.java:205)    org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleResult(AbstractReplyProducingMessageHandler.java:199)    org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:177)    org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)    java.lang.reflect.Method.invoke(Method.java:606)    org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)    org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)    org.springframework.integration.monitor.SimpleMessageHandlerMetrics.handleMessage(SimpleMessageHandlerMetrics.java:106)    org.springframework.integration.monitor.SimpleMessageHandlerMetrics.invoke(SimpleMessageHandlerMetrics.java:86)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)    org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)    com.sun.proxy.$Proxy117.handleMessage(Unknown Source)    org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)    org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)    org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:...",1
"Anchor the footer at the bottom of page
nan",1
"Automate execution of gradle pushGeneratedDocs
Should be part of the daily build. One ""easy"" way to do it would be to use the ""hardcoded"" authentication scheme as described here (bamboo should mask a property whose name contains password) We may want to create a dedicated github user though",2
"Stacktrace on container with deployed modules is shutdown
When the container that has deployed module is shutdown, following stacktrace is thrown:  10:10:27,560  INFO main-EventThread server.ContainerRegistrar:254 - Undeploying module [ModuleDescriptor@3a615460 moduleName = 'job', moduleLabel = 'job', group = 'j4', sourceChannelName = [null], sinkChannelName = [null], sinkChannelName = [null], index = 0, type = job, parameters = map[[empty]], children = list[[empty]]] 10:10:27,560  INFO main-EventThread module.ModuleDeployer:158 - removed SimpleModule [name=job, type=job, group=j4, index=0 @7df1aff2] 10:10:27,561 ERROR main-EventThread imps.CuratorFrameworkImpl:555 - Watcher exception java.lang.IllegalStateException: org.springframework.context.annotation.AnnotationConfigApplicationContext@422fd7b7 has been closed already   org.springframework.context.support.AbstractApplicationContext.assertBeanFactoryActive(AbstractApplicationContext.java:956)   org.springframework.context.support.AbstractApplicationContext.getBean(AbstractApplicationContext.java:978)   org.springframework.xd.module.core.SimpleModule.getComponent(SimpleModule.java:164)   org.springframework.xd.dirt.plugins.AbstractMessageBusBinderPlugin.unbindConsumerAndProducers(AbstractMessageBusBinderPlugin.java:219)   org.springframework.xd.dirt.plugins.job.JobPlugin.removeModule(JobPlugin.java:70)   org.springframework.xd.dirt.module.ModuleDeployer.removeModule(ModuleDeployer.java:204)   org.springframework.xd.dirt.module.ModuleDeployer.destroyModule(ModuleDeployer.java:162)   org.springframework.xd.dirt.module.ModuleDeployer.handleUndeploy(ModuleDeployer.java:140)   org.springframework.xd.dirt.module.ModuleDeployer.undeploy(ModuleDeployer.java:112)   org.springframework.xd.dirt.server.ContainerRegistrar.undeployModule(ContainerRegistrar.java:256)   org.springframework.xd.dirt.server.ContainerRegistrar$JobModuleWatcher.process(ContainerRegistrar.java:753)   org.apache.curator.framework.imps.NamespaceWatcher.process(NamespaceWatcher.java:67)   org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:522)   org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) 10:10:27,561  INFO main-EventThread zookeeper.ClientCnxn:512 - EventThread shut down 10:10:27,564  INFO Thread-2 jmx.EndpointMBeanExporter:433 - Unregistering JMX-exposed beans on shutdown",2
"Investigate deployed module context close upon container shutdown
Upon the container shutdown, the deployed modules' contexts get closed before the corresponding `Stream/JobModuleWatcher` does the undeployment of the stream/job modules.",3
"Remove copyright/licence info in UI screens
nan",1
"filepollhdfs --deleteFiles=true has no effect, files are not deleted
Setting --deleteFiles=true has no effect any longer. This also causes the Script Integration Tests to fail.  Suspect this is related to the change here https://github.com/spring-projects/spring-xd/commit/6dbac167758ce23b9a4dbf07169b2d26d1eddef1 ",3
"Remove footer from admin UI
Please see the discussion here:  https://github.com/spring-projects/spring-xd/pull/1052#issuecomment-48761686",1
"filejdbc job broken in distributed mode
The filejdbc job is broken in distributed mode (redis and rabbit)    To reproduce:    export XD_TRANSPORT=rabbit    start xd-admin  start xd-container    start shell and create this job:    {code}  >job create mydata --definition ""filejdbc --names=col1,col2,col3 --resources=file:///home/trisberg/Test/input/*.csv --initializeDatabase=true"" --deploy  >job launch mydata  {code}    results in JOB starting but never completing:    {code}  >job execution list    Id  Job Name  Start Time                            Step Execution Count  Execution Status  Deployment Status  Definition Status    --  --------  ------------------------------------  --------------------  ----------------  -----------------  -----------------      0  mydata      2014-07-11 15:44:33 America/New_York  0                     STARTED           Deployed           Exists  {code}    Steps:    {code}  Step Id	Step Name	Reads	Writes	Commits	Rollbacks	Duration	Status	Details  0	step1-master	0	0	0	0	-1405349644032 ms	EXECUTING	  1	step1-master:partition0	292	292	3	0	302 ms	COMPLETED	  2	step1-master:partition1	292	292	3	0	203 ms	COMPLETED	  3	step1-master:partition2	292	292	3	0	193 ms	COMPLETED	  {code}    When using Redis, I also get this stacktrace in container:    {code}  15:40:51,220  INFO DeploymentsPathChildrenCache-0 boot.SpringApplication - Started application in 1.965 seconds (JVM running for 66.949)  15:40:51,220  INFO DeploymentsPathChildrenCache-0 core.SimpleModule - initialized module: SimpleModule [name=filejdbc, type=job, group=job1, index=0 @64a28a58]  15:40:51,233  INFO DeploymentsPathChildrenCache-0 redis.RedisMessageBus - binding requestor: job1.0  15:40:51,236  INFO DeploymentsPathChildrenCache-0 redis.RedisMessageBus - binding replier: job1.0  15:40:51,243  INFO DeploymentsPathChildrenCache-0 module.ModuleDeployer - deployed SimpleModule [name=filejdbc, type=job, group=job1, index=0 @64a28a58]  15:40:57,110 ERROR inbound.job1.0-redis:queue-inbound-channel-adapter1 redis.RedisMessageBus$1 - Failed to deliver message; retries exhausted; message sent to queue 'ERRORS:name'  org.springframework.messaging.MessageHandlingException: error occurred in message handler [org.springframework.integration.aggregator.AggregatingMessageHandler#0]    org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:84)    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)    java.lang.reflect.Method.invoke(Method.java:606)    org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)    org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)    org.springframework.integration.monitor.SimpleMessageHandlerMetrics.handleMessage(SimpleMessageHandlerMetrics.java:106)    org.springframework.integration.monitor.SimpleMessageHandlerMetrics.invoke(SimpleMessageHandlerMetrics.java:86)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)    org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)    com.sun.proxy.$Proxy84.handleMessage(Unknown Source)    org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)    org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)    org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)    org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)    org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:255)    org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:223)    sun.reflect.GeneratedMethodAccessor89.invoke(Unknown Source)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)    java.lang.reflect.Method.invoke(Method.java:606)    org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)    org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)    org.springframework.integration.monitor.DirectChannelMetrics.monitorSend(DirectChannelMetrics.java:113)    org.springframework.integration.monitor.DirectChannelMetrics.doInvoke(DirectChannelMetrics.java:97)    org.springframework.integration.monitor.DirectChannelMetrics.invoke(DirectChannelMetrics.java:91)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)    org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)    com.sun.proxy.$Proxy78.send(Unknown Source)    org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:109)    org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:44)    org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:94)    org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendMessage(AbstractReplyProducingMessageHandler.java:260)    org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendReplyMessage(AbstractReplyProducingMessageHandler.java:241)    org.springframework.integration.handler.AbstractReplyProducingMessageHandler.produceReply(AbstractReplyProducingMessageHandler.java:205)    org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleResult(AbstractReplyProducingMessageHandler.java:199)    org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:177)    org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)    org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)    org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)    org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)    org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)    org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:255)    org.springframework.xd.dirt.integration.redis.RedisMessageBus$1$1.doWithRetry(RedisMessageBus.java:251)    org.springframework.xd.dirt.integration.redis.RedisMessageBus$1$1.doWithRetry(RedisMessageBus.java:247)    org.springframework.retry.support.RetryTemplate.doExecute(RetryTemplate.java:263)    org.springframework.retry.support.RetryTemplate.execute(RetryTemplate.java:168)    org.springframework.xd.dirt.integration.redis.RedisMessageBus$1.doSend(RedisMessageBus.java:247)    org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:255)    org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:223)    org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:109)    org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:44)    org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:94)    org.springframework.integration.endpoint.MessageProducerSupport.sendMessage(MessageProducerSupport.java:98)    org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.popMessageAndSend(RedisQueueMessageDrivenEndpoint.java:211)    org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.access$300(RedisQueueMessageDrivenEndpoint.java:50)    org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint$ListenerTask.run(RedisQueueMessageDrivenEndpoint.java:290)    org.springframework.integration.util.ErrorHandlingTaskExecutor$1.run(ErrorHandlingTaskExecutor.java:52)    java.lang.Thread.run(Thread.java:744)  Caused by: java.lang.IllegalStateException: Null correlation not allowed.  Maybe the CorrelationStrategy is failing?    org.springframework.util.Assert.state(Assert.java:385)    org.springframework.integration.aggregator.AbstractCorrelatingMessageHandler.handleMessageInternal(AbstractCorrelatingMessageHandler.java:383)    org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)  	... 60 more  15:41:00,129 ERROR inbound.job1.0-redis:queue-inbound-channel-adapter1 redis.RedisMessageBus$1 - Failed to deliver message; retries exhausted; message sent to queue 'ERRORS:name'  org.springframework.messaging.MessageHandlingException: error occurred in message handler [org.springframework.integration.aggregator.AggregatingMessageHandler#0]    org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:84)    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)    java.lang.reflect.Method.invoke(Method.java:606)    org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)    org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)    org.springframework.integration.monitor.SimpleMessageHandlerMetrics.handleMessage(SimpleMessageHandlerMetrics.java:106)    org.springframework.integration.monitor.SimpleMessageHandlerMetrics.invoke(SimpleMessageHandlerMetrics.java:86)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)    org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)    com.sun.proxy.$Proxy84.handleMessage(Unknown Source)    org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)    org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)    org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)    org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)    org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:255)    org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:223)    sun.reflect.GeneratedMethodAccessor89.invoke(Unknown Source)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)    java.lang.reflect.Method.invoke(Method.java:606)    org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)    org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)    org.springframework.integration.monitor.DirectChannelMetrics.monitorSend(DirectChannelMetrics.java:113)    org.springframework.integration.monitor.DirectChannelMetrics.doInvoke(DirectChannelMetrics.java:97)    org.springframework.integration.monitor.DirectChannelMetrics.invoke(DirectChannelMetrics.java:91)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)    org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)    com.sun.proxy.$Proxy78.send(Unknown Source)    org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:109)    org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:44)    org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:94)    org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendMessage(AbstractReplyProducingMessageHandler.java:260)    org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendReplyMessage(AbstractReplyProducingMessageHandler.java:241)    org.springframework.integration.handler.AbstractReplyProducingMessageHandler.produceReply(AbstractReplyProducingMessageHandler.java:205)    org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleResult(AbstractReplyProducingMessageHandler.java:199)    org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:177)    org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)    org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)    org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)    org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)    org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)    org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:255)    org.springframework.xd.dirt.integration.redis.RedisMessageBus$1$1.doWithRetry(RedisMessageBus.java:251)    org.springframework.xd.dirt.integration.redis.RedisMessageBus$1$1.doWithRetry(RedisMessageBus.java:247)    org.springframework.retry.support.RetryTemplate.doExecute(RetryTemplate.java:263)    org.springframework.retry.support.RetryTemplate.execute(RetryTemplate.java:168)    org.springframework.xd.dirt.integration.redis.RedisMessageBus$1.doSend(RedisMessageBus.java:247)    org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:255)    org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:223)    org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:109)    org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:44)    org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:94)    org.springframework.integration.endpoint.MessageProducerSupport.sendMessage(MessageProducerSupport.java:98)    org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.popMessageAndSend(RedisQueueMessageDrivenEndpoint.java:211)    org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.access$300(RedisQueueMessageDrivenEndpoint.java:50)    org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint$ListenerTask.run(RedisQueueMessageDrivenEndpoint.java:290)    org.springframework.integration.util.ErrorHandlingTaskExecutor$1.run(ErrorHandlingTaskExecutor.java:52)    java.lang.Thread.run(Thread.java:744)  Caused by: java.lang.IllegalStateException: Null correlation not allowed.  Maybe the CorrelationStrategy is failing?    org.springframework.util.Assert.state(Assert.java:385)    org.springframework.integration.aggregator.AbstractCorrelatingMessageHandler.handleMessageInternal(AbstractCorrelatingMessageHandler.java:383)    org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)  	... 60 more  {code}  ",5
"NoNodeException after bouncing admin server
Steps to reproduce:    h6. 1. Clear out ZK  {code}  [zk: localhost:2181(CONNECTED) 0] rmr /xd  {code}    h6. 2. Start admin    h6. 3. Deploy stream  {code}  xd:>stream create --name tt --definition ""time|log"" --deploy   {code}    Admin log:  {code}  16:38:10,537  INFO Deployer server.StreamDeploymentListener - Deploying stream Stream{name='tt'}  16:38:10,545  WARN Deployer server.StreamDeploymentListener - No containers available for deployment of module 'log' for stream 'tt'  16:38:10,547  WARN Deployer server.StreamDeploymentListener - No containers available for deployment of module 'time' for stream 'tt'  16:38:10,547  INFO Deployer server.StreamDeploymentListener - Deployment status for stream 'tt': DeploymentStatus{state=failed}  16:38:10,550  INFO Deployer server.StreamDeploymentListener - Stream Stream{name='tt'} deployment attempt complete  {code}    h6. 4. Shut down and restart admin. The following is logged:  {code}  org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /xd/deployments/streams/tt/modules    org.apache.zookeeper.KeeperException.create(KeeperException.java:111)    org.apache.zookeeper.KeeperException.create(KeeperException.java:51)    org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1590)    org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:214)    org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:203)    org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)    org.apache.curator.framework.imps.GetChildrenBuilderImpl.pathInForeground(GetChildrenBuilderImpl.java:199)    org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:191)    org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:38)    org.springframework.xd.dirt.server.StreamDeploymentListener.recalculateStreamStates(StreamDeploymentListener.java:207)    org.springframework.xd.dirt.server.DeploymentSupervisor$LeaderListener.takeLeadership(DeploymentSupervisor.java:352)    org.apache.curator.framework.recipes.leader.LeaderSelector$WrappedListener.takeLeadership(LeaderSelector.java:536)    org.apache.curator.framework.recipes.leader.LeaderSelector.doWork(LeaderSelector.java:398)    org.apache.curator.framework.recipes.leader.LeaderSelector.doWorkLoop(LeaderSelector.java:443)    org.apache.curator.framework.recipes.leader.LeaderSelector.access$100(LeaderSelector.java:63)    org.apache.curator.framework.recipes.leader.LeaderSelector$2.call(LeaderSelector.java:244)    org.apache.curator.framework.recipes.leader.LeaderSelector$2.call(LeaderSelector.java:238)    java.util.concurrent.FutureTask.run(FutureTask.java:266)    java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)    java.util.concurrent.FutureTask.run(FutureTask.java:266)    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)    java.lang.Thread.run(Thread.java:744)    {code}  ",1
"Prevent deploying modules of same type on a given stream/job when new leadership election happens
When the leadership election happens, the new deployment supervisor's container listener tries to deploy unallocated modules (via ArrivingContainerModuleRedeployer) into existing container that has the modules of the same type on a given stream/job already deployed.  Currently, on a given stream/job we don't allow more than one deployment of the same module type and there by avoiding any conflicting properties for the given module type. ",2
"Module info for jdbc sink and jobs are unreadable
The 'module info' command renders text that is pretty much unreadable on a reasonably sized screen. See attached screen shot. Also all the jdbc pool settings are mixed in with module settings making for a confusing list of options. What the heck does 'fairQueue' have to do with filejdbc jobs?",3
"Acceptance Tests fail to map some EC2 internal IPs to External IPs
The acceptance tests interrogate the XD-Admin for the containers that are available.  When on EC2 the admin only returns the internal EC2 addresses without the associated suffix of .ec2.internal or .compute-1.internal.     [Defect] The acceptance tests only handled the most common suffix of .ec2.internal.  Thus some CI Acceptance tests will fail because, because the container's IPs were not properly mapped.  Thus the acceptance tests should map internal to external IP without regard to the suffixes EC2 issues.  FYI EC2 issues addresses in 2 different formats: ip-XXX-XXX-XXX-XXX.ec2.internal or domU-XX-XX-XX-XX-XX-XX.compute-1.internal.  The code only able to handle ip-XXX-XXX-XXX-XXX.ec2.internal.  ",3
"Upgrade Curator to 2.6.0
Curator 2.6.0 was released on July 11:  https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12314425&version=12327098",1
"Servers not finding logging file
12:30:43,930  WARN main logging.LoggingApplicationListener - Logging environment value 'file:/data/projects/spring-xd/build/dist/spring-xd/xd/config///xd-container-logger.properties' cannot be opened and will be ignored    There are extra slashes in there.... probably due to some recent changes related to xd/config location in the scripts.",2
"StepExecutionInfo can not be retrieved in distributed mode
When constructing StepExecutionInfo, the TaskletType class could not be loaded as the spring-data-hadoop-batch jar is missing from admin classpath in distributed mode.    Following exception is thrown:    SEVERE: Servlet.service() for servlet [dispatcherServlet] in context with path [] threw exception [Handler processing failed; nested exception is java.lang.NoClassDefFoundError: org/springframework/data/hadoop/batch/hive/HiveTasklet] with root cause  java.lang.ClassNotFoundException: org.springframework.data.hadoop.batch.hive.HiveTasklet    java.net.URLClassLoader$1.run(URLClassLoader.java:366)    java.net.URLClassLoader$1.run(URLClassLoader.java:355)    java.security.AccessController.doPrivileged(Native Method)    java.net.URLClassLoader.findClass(URLClassLoader.java:354)    java.lang.ClassLoader.loadClass(ClassLoader.java:425)    sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)    java.lang.ClassLoader.loadClass(ClassLoader.java:358)    org.springframework.xd.dirt.job.TaskletType.<clinit>(TaskletType.java:57)    org.springframework.xd.dirt.job.StepExecutionInfo.<init>(StepExecutionInfo.java:94)    org.springframework.xd.dirt.rest.BatchStepExecutionsController.details(BatchStepExecutionsController.java:98)    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)    java.lang.reflect.Method.invoke(Method.java:606)    org.springframework.web.method.support.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:215)",3
"Dependendcies for Hadoop distros are broken 
We used to have distro specific jars i the lib/[distro] directory. That is no longer working and all distros seem to contain mostly the same version (hadoop 2.2.0 dependencies)  This is the list for phd1 now:  avro-1.7.5.jar hadoop-annotations-2.2.0.jar hadoop-auth-2.2.0.jar hadoop-client-2.0.5-alpha-gphd-2.1.0.0.jar hadoop-common-2.2.0.jar hadoop-distcp-2.2.0.jar hadoop-hdfs-2.2.0.jar hadoop-mapreduce-client-app-2.2.0.jar hadoop-mapreduce-client-common-2.2.0.jar hadoop-mapreduce-client-core-2.2.0.jar hadoop-mapreduce-client-jobclient-2.2.0.jar hadoop-mapreduce-client-shuffle-2.2.0.jar hadoop-streaming-2.2.0.jar hadoop-yarn-api-2.2.0.jar hadoop-yarn-client-2.2.0.jar hadoop-yarn-common-2.2.0.jar hadoop-yarn-server-common-2.2.0.jar hadoop-yarn-server-nodemanager-2.2.0.jar jersey-core-1.9.jar jersey-server-1.9.jar jetty-util-6.1.26.jar protobuf-java-2.5.0.jar spring-data-hadoop-2.0.1.RELEASE.jar spring-data-hadoop-batch-2.0.1.RELEASE.jar spring-data-hadoop-core-2.0.1.RELEASE.jar spring-data-hadoop-store-2.0.1.RELEASE.jar ",5
"HdfsTest in Acceptance test fails sporadically (uses trigger as a source)
HdfsTest uses the following stream to test the hdfs sink.  trigger --payload='foobar' | hdfs.  In the test failure, the test reported that no file was created on the hdfs.    I'm wondering if the trigger fired before the hdfs was fully deployed.   I would say that we set the phase to the maximum, but the problem is that by default it is MAX_INT.  Thoughts?",3
"Send count check occasionally fails on Acceptance tests.
Acceptance tests check the number of ""sends"" for each module after a single event is triggered.  This should entail that each module in the stream should have a send count of ""1"".   Sporadically this test will fail on a sink, where the send count will be 2.    The stacktrace below occurred on a singleAdmin/2 container deployment with rabbit as its transport and this stream was used: ""tcp --port=1234 |file --binary=true --mode=REPLACE""  java.lang.AssertionError: java.lang.AssertionError: Module file.1 for channel input did not have expected count  expected:<1> but was:<2> java.lang.AssertionError: Module file.1 for channel input did not have expected count  expected:<1> but was:<2>   org.junit.Assert.fail(Assert.java:88)   org.junit.Assert.failNotEquals(Assert.java:743)   org.junit.Assert.assertEquals(Assert.java:118)   org.junit.Assert.assertEquals(Assert.java:555)   org.springframework.xd.integration.util.XdEc2Validation.verifySendCounts(XdEc2Validation.java:349)   org.springframework.xd.integration.util.XdEc2Validation.verifySendCounts(XdEc2Validation.java:323)   org.springframework.xd.integration.util.XdEc2Validation.assertReceived(XdEc2Validation.java:140)   org.springframework.xd.integration.test.AbstractIntegrationTest.assertReceived(AbstractIntegrationTest.java:490)   org.springframework.xd.integration.test.TcpTest.testTCPSourceCRLF(TcpTest.java:41)   sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)   sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)   sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)   java.lang.reflect.Method.invoke(Method.java:606)   org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)   org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)   org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)   org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)   org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)   org.springframework.test.context.junit4.statements.RunBeforeTestMethodCallbacks.evaluate(RunBeforeTestMethodCallbacks.java:74)   org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)   org.springframework.test.context.junit4.statements.RunAfterTestMethodCallbacks.evaluate(RunAfterTestMethodCallbacks.java:83)   org.springframework.test.context.junit4.statements.SpringRepeat.evaluate(SpringRepeat.java:72)   org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:233)   org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:87)   org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)   org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)   org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)   org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)   org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)   org.springframework.test.context.junit4.statements.RunBeforeTestClassCallbacks.evaluate(RunBeforeTestClassCallbacks.java:61)   org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)   org.springframework.test.context.junit4.statements.RunAfterTestClassCallbacks.evaluate(RunAfterTestClassCallbacks.java:71)   org.junit.runners.ParentRunner.run(ParentRunner.java:309)   org.springframework.test.context.junit4.SpringJUnit4ClassRunner.run(SpringJUnit4ClassRunner.java:176)   org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecuter.runTestClass(JUnitTestClassExecuter.java:86)   org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecuter.execute(JUnitTestClassExecuter.java:49)   org.gradle.api.internal.tasks.testing.junit.JUnitTestClassProcessor.processTestClass(JUnitTestClassProcessor.java:69)   org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.processTestClass(SuiteTestClassProcessor.java:48)   sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)   sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)   sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)   java.lang.reflect.Method.invoke(Method.java:606)   org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35)   org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)   org.gradle.messaging.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32)   org.gradle.messaging.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93)   com.sun.proxy.$Proxy2.processTestClass(Unknown Source)   org.gradle.api.internal.tasks.testing.worker.TestWorker.processTestClass(TestWorker.java:105)   sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)   sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)   sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)   java.lang.reflect.Method.invoke(Method.java:606)   org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35)   org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)   org.gradle.messaging.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:355)   org.gradle.internal.concurrent.DefaultExecutorFactory$StoppableExecutorImpl$1.run(DefaultExecutorFactory.java:64)   java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)   java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)   java.lang.Thread.run(Thread.java:724)",5
"Display errors in a persistent way
When a validation error occurs in the UI, it is displayed as a small box then fades away.  If it is a larger form or a less specific error, the user may not catch everything before it fades away.  We should display the error in a way that they can refer to as they are fixing it.",4
"Add ability to define nested jobs
h3.  Narrative  As a developer, I need to be able to create a Spring XD job module that consists of a job orchestrating the execution of other Spring Batch jobs using the Spring Batch Job Step (see section 5.3.6 here: http://docs.spring.io/spring-batch/reference/html/configureStep.html) within the same module definition.    h3.  Acceptance Criteria  # Define the ""contract"" for a job module  ## Currently the contract consists of a single job definition within the assembled {{ApplicationContext}} ({{context.getBean(Job.class)}}).  ## The new version will need to document what job definition within the assembled {{ApplicationContext}} should be run as the entry point.  I'm assuming it would be by id ({{context.getBean(""job"")}} for example) of the job but am open to other options.  # A custom job module that orchestrates multiple Spring Batch jobs via Job steps should be able to be deployed and executed as a single Spring XD module.  ## Spring XD launches the job that conforms to the previously defined ""contract"".  ## Spring Batch manages the execution of the child jobs.  # The existing OOTB jobs should work under the new ""contract"".    h3.  Assumptions  # The UI should ""just work"" in that child jobs update the job repository independently so no updates should be needed for an MVP of this functionality.  # *This will be a breaking change for users that have developed custom job modules.*    h3.  Out of Scope  # Execution of child jobs that are remote (deployed on another node / {{ApplicationContext}}).  # Dynamically assembling jobs via the shell's DSL or the UI.  ",3
"stream destroy does not need to validate metadata
In dev mode I created a twitter stream, rebuilt the server, and then wanted to destroy it.  My new install didn't have any twitter keys as I was also not using XD_CONFIG_LOCATION.  xd:>stream destroy --name tweets  Command failed org.springframework.xd.rest.client.impl.SpringXDException: Error with option(s) for module twitterstream of type source:     consumerKey: You must provide a 'consumerKey' token to use this module.     consumerSecret: You must provide a 'consumerSecret' token to use this module.   I should be able to destroy a stream w/o having to supply values for PPCs.",2
"Move [Back] button to top right
The [Back] button is at lower left of the page which requires scrolling all the way to the bottom - could we move it to top right? Would make clicking back and forth for job executions much easier.",3
"Undeploying twitterstream logs warning - MessageDeliveryException
To reproduce -     Download recent snapshot - http://repo.spring.io/libs-snapshot-local/org/springframework/xd/spring-xd/1.0.0.BUILD-SNAPSHOT/spring-xd-1.0.0.BUILD-20140715.101224-1-dist.zip    Start XD and shell -  xd:>stream create --name tweets --definition ""twitterstream | file"" --deploy   xd:>stream undeploy --name tweets     (Note: the IllegalStateException has been fixed for RC1, still need to fix the MessageDeliveryException)    There is an error logged in the logs:    {code}  08:37:57,022  INFO DeploymentsPathChildrenCache-0 module.ModuleDeployer - deployed SimpleModule [name=twitterstream, type=source, group=tweets, index=0 @581a12b9]  08:38:02,685  INFO main-EventThread server.ContainerRegistrar - Undeploying module [ModuleDescriptor@4807f3e2 moduleName = 'twitterstream', moduleLabel = 'twitterstream', group = 'tweets', sourceChannelName = [null], sinkChannelName = [null], sinkChannelName = [null], index = 0, type = source, parameters = map[[empty]], children = list[[empty]]]  08:38:02,687  INFO main-EventThread module.ModuleDeployer - removed SimpleModule [name=twitterstream, type=source, group=tweets, index=0 @581a12b9]  08:38:02,705  INFO DeploymentsPathChildrenCache-0 server.ContainerRegistrar - Path cache event: /deployments/modules/allocated/fa40cb45-3c16-4b19-81e9-eb6d357d186d/tweets.source.twitterstream.1, type: CHILD_REMOVED  08:38:02,779  WARN task-scheduler-4 twitter.TwitterStreamChannelAdapter - Exception while reading stream.  org.springframework.messaging.MessageDeliveryException: Dispatcher has no subscribers for channel 'SimpleModule [name=twitterstream, type=source, group=tweets, index=0 @581a12b9]:default,container:0.to.discardDeletes'.    org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:81)    org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:255)    org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:223)    sun.reflect.GeneratedMethodAccessor86.invoke(Unknown Source)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)    java.lang.reflect.Method.invoke(Method.java:606)    org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)    org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)    org.springframework.integration.monitor.DirectChannelMetrics.monitorSend(DirectChannelMetrics.java:113)    org.springframework.integration.monitor.DirectChannelMetrics.doInvoke(DirectChannelMetrics.java:97)    org.springframework.integration.monitor.DirectChannelMetrics.invoke(DirectChannelMetrics.java:91)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)    org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)    com.sun.proxy.$Proxy81.send(Unknown Source)    org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:109)    org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:44)    org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:94)    org.springframework.integration.endpoint.MessageProducerSupport.sendMessage(MessageProducerSupport.java:98)    org.springframework.integration.x.twitter.TwitterStreamChannelAdapter.doSendLine(TwitterStreamChannelAdapter.java:154)    org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter$StreamReadingTask$2.extractData(AbstractTwitterInboundChannelAdapter.java:200)    org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter$StreamReadingTask$2.extractData(AbstractTwitterInboundChannelAdapter.java:186)    org.springframework.web.client.RestTemplate.doExecute(RestTemplate.java:553)    org.springframework.web.client.RestTemplate.execute(RestTemplate.java:521)    org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter$StreamReadingTask.readStream(AbstractTwitterInboundChannelAdapter.java:180)    org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter$StreamReadingTask.run(AbstractTwitterInboundChannelAdapter.java:158)    org.springframework.scheduling.support.DelegatingErrorHandlingRunnable.run(DelegatingErrorHandlingRunnable.java:54)    java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)    java.util.concurrent.FutureTask.run(FutureTask.java:262)    java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)    java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)    java.lang.Thread.run(Thread.java:744)  Caused by: org.springframework.integration.MessageDispatchingException: Dispatcher has no subscribers    org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:107)    org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)    org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)  	... 33 more  08:38:02,780  WARN task-scheduler-4 twitter.TwitterStreamChannelAdapter - Exception while reading stream, waiting for 250 ms before restarting  08:38:02,781 ERROR task-scheduler-4 handler.LoggingHandler - java.lang.IllegalStateException: java.lang.InterruptedException: sleep interrupted    org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter.wait(AbstractTwitterInboundChannelAdapter.java:258)    org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter.waitLinearBackoff(AbstractTwitterInboundChannelAdapter.java:232)    org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter.access$600(AbstractTwitterInboundChannelAdapter.java:54)    org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter$StreamReadingTask.run(AbstractTwitterInboundChannelAdapter.java:174)    org.springframework.scheduling.support.DelegatingErrorHandlingRunnable.run(DelegatingErrorHandlingRunnable.java:54)    java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)    java.util.concurrent.FutureTask.run(FutureTask.java:262)    java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)    java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)    java.lang.Thread.run(Thread.java:744)  Caused by: java.lang.InterruptedException: sleep interrupted    java.lang.Thread.sleep(Native Method)    org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter.wait(AbstractTwitterInboundChannelAdapter.java:254)  	... 11 more  {code}  ",3
"Unable to deploy job in UI
This only happens when creating jobs via the CLI and deploying using the UI    On the job page:  http://localhost:9393/admin-ui/#/jobs/definitions    I click [Deploy] for a Job and get a screen asking for Container Match Criteria and Job Module Count - clicking on the [Deploy] button on that screen does nothing - I see this error reported:    Deploying Job Definition undefined angular.js:9778  TypeError: Cannot read property 'jobDefinition' of undefined      at Scope.$scope.deployDefinition (http://localhost:9393/admin-ui/scripts/job/controllers/definition-deploy.js:52:78)      at http://localhost:9393/admin-ui/lib/angular/angular.js:10567:21      at http://localhost:9393/admin-ui/lib/angular/angular.js:18627:17      at Scope.$eval (http://localhost:9393/admin-ui/lib/angular/angular.js:12412:28)      at Scope.$apply (http://localhost:9393/admin-ui/lib/angular/angular.js:12510:23)      at HTMLButtonElement.<anonymous> (http://localhost:9393/admin-ui/lib/angular/angular.js:18626:21)      at HTMLButtonElement.jQuery.event.dispatch (http://localhost:9393/admin-ui/lib/jquery/jquery.js:5095:9)      at HTMLButtonElement.elemData.handle (http://localhost:9393/admin-ui/lib/jquery/jquery.js:4766:46) angular.js:9778",5
"Mixins need to use Property PlaceHolders instead of hard coded values where possible
Many attributes in Mixins use hard coded values instead of using property placeholders.      JdbcConnectionMixin, JdbcConnectionPoolMixin, MqttConnectionMixin...",4
"SSL Support For RabbitMQ (Bus and Modules)
nan",3
"Change xd.sink logging level to INFO
The log sink is not writing information to the log.  Not the solution but, when log4j.rootLogger is set to INFO, the log sink information is written to the log.  ",2
"Remove duplicate logger info on application started
When the admin, container and singlenode servers start, the ""Started ..Application"" log message is displayed everytime the spring application is created.  We should only log when the server is started eventually.",1
"Automatically align version for tomcat components from platform
https://github.com/spring-projects/spring-xd/commit/db66aa2a329a6fc7ef89a340dd4d562fa70d14a4 introduces org.apache.tomcat.embed:tomcat-embed-logging-log4j which is not covered by platform. Yet, we should lookup the version to use from other tomcat artifacts, using some gradle magic",2
"Add Https Support to the HTTP Source
nan",7
"NodeExists Exception upon container disconnect/reconnect without admin leader
When the container which has modules deployed disconnects/reconnects to the cluster while the admin leader isn't available, following exception is thrown:  This is more likely to happen in single-node scenario as there is no admin leader re-election there. In distributed mode, we can always setup HA on admins so that the leadership re-election happens.    20:03:16,307 ERROR DeploymentsPathChildrenCache-0 cache.PathChildrenCache -   org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /xd/deployments/modules/allocated/53f41042-8abd-443b-abfb-ba42a24fb9fb/foo.sink.log.1/metadata    org.apache.zookeeper.KeeperException.create(KeeperException.java:119)    org.apache.zookeeper.KeeperException.create(KeeperException.java:51)    org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:783)    org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:676)    org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:660)    org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)    org.apache.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:656)    org.apache.curator.framework.imps.CreateBuilderImpl.protectedPathInForeground(CreateBuilderImpl.java:441)    org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:431)    org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:44)    org.springframework.xd.dirt.server.ContainerRegistrar.writeModuleMetadata(ContainerRegistrar.java:486)    org.springframework.xd.dirt.server.ContainerRegistrar.onChildAdded(ContainerRegistrar.java:461)    org.springframework.xd.dirt.server.ContainerRegistrar.access$8(ContainerRegistrar.java:426)    org.springframework.xd.dirt.server.ContainerRegistrar$DeploymentListener.childEvent(ContainerRegistrar.java:807)    org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:509)    org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:503)    org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)",3
"Avoid all modules deploying to the first container instance upon system restart
A possible approach is to set a configurable wait period when the first container arrives. If another container arrives during the wait period, reset the clock. When the wait period expires, start deploying modules. ",8
"Packaging of Guava 17 results in failure to deploy mapreduce job to Hadoop 2.4 based distros
Trying to deploy the hashtagcount batch sample [1] to Hadoop 2.4.1 or Hortonworks HDP 2.1 fails with an IllegalAccessError exception.    Looks like a Guava versioning issue - Swapping out guava-17.0.jar for guava-11.0.2.jar in the xd/lib directory solves it.    Mark P suggested we try 16.0.1 which is what Curator uses and that seems to work as well.     Looking into changing the build to not force 17.0 which is the IO platform version.    http://upstream-tracker.org/java/compat_reports/guava/16.0.1_to_17.0/src_compat_report.html       I get the following exception:    {code}  16:42:22,214  INFO Deployer server.JobDeploymentListener - Deployment status for job 'hashtagCountJob': DeploymentStatus{state=deployed}  16:42:27,315  WARN task-scheduler-2 mapreduce.JobSubmitter - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).  16:42:27,325 ERROR task-scheduler-2 step.AbstractStep - Encountered an error executing step hashtagcount in job hashtagCountJob  java.lang.IllegalAccessError: tried to access method com.google.common.base.Stopwatch.<init>()V from class org.apache.hadoop.mapreduce.lib.input.FileInputFormat    org.apache.hadoop.mapreduce.lib.input.FileInputFormat.getSplits(FileInputFormat.java:369)    org.apache.hadoop.mapreduce.JobSubmitter.writeNewSplits(JobSubmitter.java:493)    org.apache.hadoop.mapreduce.JobSubmitter.writeSplits(JobSubmitter.java:510)    org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:394)    org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)    org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)    java.security.AccessController.doPrivileged(Native Method)    javax.security.auth.Subject.doAs(Subject.java:415)    org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)    org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)    org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1303)    org.apache.hadoop.mapreduce.Job$$FastClassBySpringCGLIB$$a048cbfe.invoke(<generated>)    org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:204)    org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:708)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)    org.springframework.aop.support.DelegatingIntroductionInterceptor.doProceed(DelegatingIntroductionInterceptor.java:133)    org.springframework.aop.support.DelegatingIntroductionInterceptor.invoke(DelegatingIntroductionInterceptor.java:121)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)    org.springframework.aop.framework.CglibAopProxy$DynamicAdvisedInterceptor.intercept(CglibAopProxy.java:644)    org.apache.hadoop.mapreduce.Job$$EnhancerBySpringCGLIB$$875ec891.waitForCompletion(<generated>)    org.springframework.data.hadoop.mapreduce.JobExecutor$2.run(JobExecutor.java:199)    org.springframework.core.task.SyncTaskExecutor.execute(SyncTaskExecutor.java:50)    org.springframework.data.hadoop.mapreduce.JobExecutor.startJobs(JobExecutor.java:170)    org.springframework.data.hadoop.batch.mapreduce.JobTasklet.execute(JobTasklet.java:90)    org.springframework.batch.core.step.tasklet.TaskletStep$ChunkTransactionCallback.doInTransaction(TaskletStep.java:406)    org.springframework.batch.core.step.tasklet.TaskletStep$ChunkTransactionCallback.doInTransaction(TaskletStep.java:330)    org.springframework.transaction.support.TransactionTemplate.execute(TransactionTemplate.java:133)    org.springframework.batch.core.step.tasklet.TaskletStep$2.doInChunkContext(TaskletStep.java:271)    org.springframework.batch.core.scope.context.StepContextRepeatCallback.doInIteration(StepContextRepeatCallback.java:77)    org.springframework.batch.repeat.support.RepeatTemplate.getNextResult(RepeatTemplate.java:368)    org.springframework.batch.repeat.support.RepeatTemplate.executeInternal(RepeatTemplate.java:215)    org.springframework.batch.repeat.support.RepeatTemplate.iterate(RepeatTemplate.java:144)    org.springframework.batch.core.step.tasklet.TaskletStep.doExecute(TaskletStep.java:257)    org.springframework.batch.core.step.AbstractStep.execute(AbstractStep.java:198)    org.springframework.batch.core.job.SimpleStepHandler.handleStep(SimpleStepHandler.java:148)    org.springframework.batch.core.job.flow.JobFlowExecutor.executeStep(JobFlowExecutor.java:64)    org.springframework.batch.core.job.flow.support.state.StepState.handle(StepState.java:67)    org.springframework.batch.core.job.flow.support.SimpleFlow.resume(SimpleFlow.java:162)    org.springframework.batch.core.job.flow.support.SimpleFlow.start(SimpleFlow.java:141)    org.springframework.batch.core.job.flow.FlowJob.doExecute(FlowJob.java:134)    org.springframework.batch.core.job.AbstractJob.execute(AbstractJob.java:304)    org.springframework.batch.core.launch.support.SimpleJobLauncher$1.run(SimpleJobLauncher.java:135)    org.springframework.core.task.SyncTaskExecutor.execute(SyncTaskExecutor.java:50)    org.springframework.batch.core.launch.support.SimpleJobLauncher.run(SimpleJobLauncher.java:128)    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)    java.lang.reflect.Method.invoke(Method.java:606)    org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)    org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)    org.springframework.batch.core.configuration.annotation.SimpleBatchConfiguration$PassthruAdvice.invoke(SimpleBatchConfiguration.java:127)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)    org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)    com.sun.proxy.$Proxy44.run(Unknown Source)    org.springframework.batch.integration.launch.JobLaunchingMessageHandler.launch(JobLaunchingMessageHandler.java:50)    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)    java.lang.reflect.Method.invoke(Method.java:606)    org.springframework.expression.spel.support.ReflectiveMethodExecutor.execute(ReflectiveMethodExecutor.java:63)    org.springframework.expression.spel.ast.MethodReference.getValueInternal(MethodReference.java:122)    org.springframework.expression.spel.ast.MethodReference.access$000(MethodReference.java:44)    org.springframework.expression.spel.ast.MethodReference$MethodValueRef.getValue(MethodReference.java:258)    org.springframework.expression.spel.ast.CompoundExpression.getValueInternal(CompoundExpression.java:84)    org.springframework.expression.spel.ast.SpelNodeImpl.getTypedValue(SpelNodeImpl.java:114)    org.springframework.expression.spel.standard.SpelExpression.getValue(SpelExpression.java:111)    org.springframework.integration.util.AbstractExpressionEvaluator.evaluateExpression(AbstractExpressionEvaluator.java:159)    org.springframework.integration.util.MessagingMethodInvokerHelper.processInternal(MessagingMethodInvokerHelper.java:268)    org.springframework.integration.util.MessagingMethodInvokerHelper.process(MessagingMethodInvokerHelper.java:142)    org.springframework.integration.handler.MethodInvokingMessageProcessor.processMessage(MethodInvokingMessageProcessor.java:75)    org.springframework.integration.handler.ServiceActivatingHandler.handleRequestMessage(ServiceActivatingHandler.java:71)    org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:170)    org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)    sun.reflect.GeneratedMethodAccessor98.invoke(Unknown Source)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)    java.lang.reflect.Method.invoke(Method.java:606)    org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)    org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)    org.springframework.integration.monitor.SimpleMessageHandlerMetrics.handleMessage(SimpleMessageHandlerMetrics.java:106)    org.springframework.integration.monitor.SimpleMessageHandlerMetrics.invoke(SimpleMessageHandlerMetrics.java:86)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)    org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)    com.sun.proxy.$Proxy125.handleMessage(Unknown Source)    org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)    org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)    org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)    org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)    org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:255)    org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:223)    sun.reflect.GeneratedMethodAccessor97.invoke(Unknown Source)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)    java.lang.reflect.Method.invoke(Method.java:606)    org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)    org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)    org.springframework.integration.monitor.DirectChannelMetrics.monitorSend(DirectChannelMetrics.java:113)    org.springframework.integration.monitor.DirectChannelMetrics.doInvoke(DirectChannelMetrics.java:97)    org.springframework.integration.monitor.DirectChannelMetrics.invoke(DirectChannelMetrics.java:91)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)    org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)    com.sun.proxy.$Proxy123.send(Unknown Source)    org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:109)    org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:44)    org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:94)    org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendMessage(AbstractReplyProducingMessageHandler.java:260)    org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendReplyMessage(AbstractReplyProducingMessageHandler.java:241)    org.springframework.integration.handler.AbstractReplyProducingMessageHandler.produceReply(AbstractReplyProducingMessageHandler.java:205)    org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleResult(AbstractReplyProducingMessageHandler.java:199)    org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:177)    org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)    sun.reflect.GeneratedMethodAccessor98.invoke(Unknown Source)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)    java.lang.reflect.Method.invoke(Method.java:606)    org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)    org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)    org.springframework.integration.monitor.SimpleMessageHandlerMetrics.handleMessage(SimpleMessageHandlerMetrics.java:106)    org.springframework.integration.monitor.SimpleMessageHandlerMetrics.invoke(SimpleMessageHandlerMetrics.java:86)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)    org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)    com.sun.proxy.$Proxy125.handleMessage(Unknown Source)    org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)    org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)    org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)    org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)    org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:255)    org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:223)    sun.reflect.GeneratedMethodAccessor97.invoke(Unknown Source)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)    java.lang.reflect.Method.invoke(Method.java:606)    org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)    org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)    org.springframework.integration.monitor.DirectChannelMetrics.monitorSend(DirectChannelMetrics.java:113)    org.springframework.integration.monitor.DirectChannelMetrics.doInvoke(DirectChannelMetrics.java:97)    org.springframework.integration.monitor.DirectChannelMetrics.invoke(DirectChannelMetrics.java:91)    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)    org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)    com.sun.proxy.$Proxy123.send(Unknown Source)    org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:109)    org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:44)    org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:94)    org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendMessage(AbstractReplyProducingMessageHandler.java:260)    org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendReplyMessage(AbstractReplyProducingMessageHandler.java:241)    org.springframework.integration.handler.AbstractReplyProducingMessageHandler.produceReply(AbstractReplyProducingMessageHandler.java:205)    org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleResult(AbstractReplyProducingMessageHandler.java:199)    org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:177)    org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)    org.springframework.integration.endpoint.PollingConsumer.handleMessage(PollingConsumer.java:74)    org.springframework.integration.endpoint.AbstractPollingEndpoint.doPoll(AbstractPollingEndpoint.java:205)    org.springframework.integration.endpoint.AbstractPollingEndpoint.access$000(AbstractPollingEndpoint.java:55)    org.springframework.integration.endpoint.AbstractPollingEndpoint$1.call(AbstractPollingEndpoint.java:149)    org.springframework.integration.endpoint.AbstractPollingEndpoint$1.call(AbstractPollingEndpoint.java:146)    org.springframework.integration.endpoint.AbstractPollingEndpoint$Poller$1.run(AbstractPollingEndpoint.java:284)    org.springframework.integration.util.ErrorHandlingTaskExecutor$1.run(ErrorHandlingTaskExecutor.java:52)    org.springframework.core.task.SyncTaskExecutor.execute(SyncTaskExecutor.java:50)    org.springframework.integration.util.ErrorHandlingTaskExecutor.execute(ErrorHandlingTaskExecutor.java:49)    org.springframework.integration.endpoint.AbstractPollingEndpoint$Poller.run(AbstractPollingEndpoint.java:278)    org.springframework.scheduling.support.DelegatingErrorHandlingRunnable.run(DelegatingErrorHandlingRunnable.java:54)    org.springframework.scheduling.concurrent.ReschedulingRunnable.run(ReschedulingRunnable.java:81)    java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)    java.util.concurrent.FutureTask.run(FutureTask.java:262)    java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)    java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)    java.lang.Thread.run(Thread.java:745)  {code}    [1] https://github.com/spring-projects/spring-xd-samples",5
"Fix package tangle
Fix package tangle issue reported here:    https://build.spring.io/browse/XD-SONAR-490",1
"Create Acceptance Test for source Trigger
Create a test specific to for the trigger source instead of it being tested with other Acceptance tests.",3
"Remove Trigger as a source for Acceptance Tests
To remain consistent across all (processor, sink, job) tests, http should be used as the source.   ",5
"Remove warnings from Shell hadoop commands
Some hadoop commands generate warnings/deprecation messages. We should try to get rid of most of them.    {code}  xd:>hadoop fs ls /xd --recursive   Hadoop configuration changed, re-initializing shell...  lsr: DEPRECATED: Please use 'ls -R' instead.  13:01:07,120  WARN Spring Shell util.NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable  drwxr-xr-x   - trisberg supergroup          0 2014-07-17 11:19 /xd/hashtagcount  drwxr-xr-x   - trisberg supergroup          0 2014-07-17 11:19 /xd/hashtagcount/output  -rw-r--r--   3 trisberg supergroup          0 2014-07-17 11:19 /xd/hashtagcount/output/_SUCCESS  -rw-r--r--   3 trisberg supergroup        833 2014-07-17 11:19 /xd/hashtagcount/output/part-r-00000  drwxr-xr-x   - trisberg supergroup          0 2014-07-16 18:28 /xd/tweets  -rw-r--r--   3 trisberg supergroup     982993 2014-07-16 18:28 /xd/tweets/tweets-0.txt  {code}",5
"Add Docs (or Reference) For Standard Shell Commands (e.g. script)
http://stackoverflow.com/questions/24819401/how-to-get-spring-xd-to-deploy-a-predefined-set-of-streams-and-taps-on-startup    It is documented here https://github.com/spring-projects/spring-xd/wiki/Shell#executing-a-script    But maybe it should also be at the top of the appendix?    https://github.com/spring-projects/spring-xd/wiki/ShellReference",1
"Error message about memory leak when ctrl-c xd-container and xd-admin
e.g.,     ^C09:42:00,882 ERROR localhost-startStop-2 loader.WebappClassLoader - The web application [] appears to have started a thread named [Abandoned connection cleanup thread] but has failed to stop it. This is very likely to create a memory leak.    The thread name may be different...",4
"Release 1.0 RC1
nan",2
"Update rpm and brew recipes
nan",2
"ParentLastURLClassLoader should set itself as context ClassLoader
I am converting a Spring XD Sample (Batch notifications) from copying jars to (old way)    {code}  $XD_HOME/lib  {code}    To rather copy the module jar to (new preferred way)    {code}  $XD_HOME/modules/job/payment-import/lib  {code}    By doing so, I hit a classloader issue. Custom classes and resources are loaded in Spring XD using *org.springframework.xd.module.support.ParentLastURLClassLoader*.    However, the sample is initializing custom bean definitions and one of those creates a new *DataSource* using the *EmbeddedDatabaseBuilder*. This class however, under the hood, uses the *Default* class loader to load SQL scripts:    {code}  	public DefaultResourceLoader() {  		this.classLoader = ClassUtils.getDefaultClassLoader();  	}  {code}    Therefore, the SQL scripts are NOT FOUND.    *Possible Solution*    A possible solution seems to be for *ParentLastURLClassLoader* to set itself as the context ClassLoader for the current thread:    {code}  	public ParentLastURLClassLoader(URL[] classpath, ClassLoader parent) {  		...  		Thread.currentThread().setContextClassLoader(this);  		...  	}  {code}",4
"Step execution count is zero for the job execution list result
For the Job execution list, the step execution count for each job execution is always set to zero.    For a single job execution display command, the step execution count is set correctly.",4
"Inconsistent failure while deploying job from admin UI
After clicking 'deploy' on the definitions page, the 'deploy' button is deactivated and message says:    ""An error occurred. We were unable to retrieve the module name from the provided definition ....""  and web console says:    TypeError: Cannot read property '0' of null      at Object.getModuleNameFromJobDefinition (http://localhost:9393/admin-ui/scripts/shared/services.js:43:26)      at http://localhost:9393/admin-ui/scripts/job/controllers/definition-deploy.js:35:36      at wrappedCallback (http://localhost:9393/admin-ui/lib/angular/angular.js:11319:81)      at http://localhost:9393/admin-ui/lib/angular/angular.js:11405:26      at Scope.$eval (http://localhost:9393/admin-ui/lib/angular/angular.js:12412:28)      at Scope.$digest (http://localhost:9393/admin-ui/lib/angular/angular.js:12224:31)      at Scope.$apply (http://localhost:9393/admin-ui/lib/angular/angular.js:12516:24)      at done (http://localhost:9393/admin-ui/lib/angular/angular.js:8204:45)      at completeRequest (http://localhost:9393/admin-ui/lib/angular/angular.js:8412:7)      at XMLHttpRequest.xhr.onreadystatechange (http://localhost:9393/admin-ui/lib/angular/angular.js:8351:11) ",3
"Add comprehensive tests for AggregateCounterRepository
The AggregateCounterTests were created to satisfy XD-1462, but currently they only have a couple tests to validate the time field processing. More comprehensive tests need to be added (including the testing of the Redis-based implementation in addition to in-memory).    For more info, see the comment here:  https://github.com/spring-projects/spring-xd/pull/1087#issuecomment-49638189",2
"Remove jersey test framework for xd/lib distribution
The jars     jersey-test-framework-core-1.9.jar   jersey-test-framework-grizzly2-1.9.jar    are incorrectly classified as compile time deps in hadoop vs. testCompile.    ",1
"Remove unused post module references
nan",1
"Consider usage of jackson afterburner
see https://github.com/FasterXML/jackson-module-afterburner",3
"REST API for DSL completion should allow extension
The REST API for DSL completion currently returns a List<String>.  This prevents future backwards compatible extension. Should change to List<Completion> where Completion has e.g. a ""text"" property.",4
"Rename packages that is applicable for both stream/job
Determine a better package name for the following packages once we have a common model that applies to both stream/job:  `org.springframework.xd.dirt.stream`   `org.springframework.xd.dirt.stream.zookeeper`  ",1
"In EC2 deployment, Allow users to set download jars into the lib/xd directory 
In cases where the deployment requires jars that can not be included with the distribution, the user should be able to pull a jar from a http site and place it in lib/xd.      The use case is that when we removed the mysql jar from the distribution, the CI tests could not start the XD instances on EC2 without it.  It was suggested that we use the postgresql instead, but decided to continue the use of mysql for acceptance tests.",5
"Containers stopped responding to Admin
SHA = a205d43f0b59e1984bf55c3368b031a373a03712  Environment: Rabbit Transport Test 1 admin 2 containers.    [Initial Event]  During the run of FileJdbcTest.testPartitionedFileJdbcJob the containers quit responding to the admin server.   After the initial failure at 12:26:46 no other streams can be deployed.      [Secondary Event]  When shutting down one of the container 1 the following exception occurs on the admin server:  12:51:12,004  INFO DeploymentSupervisorCacheListener-0 server.DepartingContainerModuleRedeployer - Container departed: Container{name='5353dc4b-6068-49a0-8981-fa175869edf0', attributes={id=5353dc4b-6068-49a0-8981-fa175869edf0, host=domU-12-31-39-07-81-02, pid=1270, groups=, ip=10.209.130.240}}  12:51:12,004 ERROR DeploymentSupervisorCacheListener-0 cache.PathChildrenCache -   org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /xd/deployments/modules/allocated/5353dc4b-6068-49a0-8981-fa175869edf0    org.apache.zookeeper.KeeperException.create(KeeperException.java:111)    org.apache.zookeeper.KeeperException.create(KeeperException.java:51)    org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1590)    org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:214)    org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:203)    org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)    org.apache.curator.framework.imps.GetChildrenBuilderImpl.pathInForeground(GetChildrenBuilderImpl.java:199)    org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:191)    org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:38)    org.springframework.xd.dirt.server.DepartingContainerModuleRedeployer.deployModules(DepartingContainerModuleRedeployer.java:101)    org.springframework.xd.dirt.server.ContainerListener.childEvent(ContainerListener.java:104)    org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:509)    org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:503)    org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)    com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)    org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)    org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:500)    org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)    org.apache.curator.framework.recipes.cache.PathChildrenCache$10.run(PathChildrenCache.java:762)    java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)    java.util.concurrent.FutureTask.run(FutureTask.java:262)    java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)    java.util.concurrent.FutureTask.run(FutureTask.java:262)    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)    java.lang.Thread.run(Thread.java:745)    The attached container logs are only partial, because they have rolled over.  The attached admin log is fairly complete.  ",12
"IllegalStateException when shutting down container
{noformat}  13:23:57,643  INFO main-EventThread server.ContainerRegistrar - Undeploying module [ModuleDescriptor@1c736092 moduleName = 'log', moduleLabel = 'log', group = 'paymenttap', sourceChannelName = 'tap:job:payment', sinkChannelName = [null], sinkChannelName = [null], index = 0, type = sink, parameters = map[[empty]], children = list[[empty]]]  13:23:57,643 ERROR main-EventThread imps.CuratorFrameworkImpl - Watcher exception  java.lang.IllegalStateException: instance must be started before calling this method  at com.google.common.base.Preconditions.checkState(Preconditions.java:176)  at org.apache.curator.framework.imps.CuratorFrameworkImpl.delete(CuratorFrameworkImpl.java:344)  at org.springframework.xd.dirt.server.ContainerRegistrar.unregisterTap(ContainerRegistrar.java:292)  at org.springframework.xd.dirt.server.ContainerRegistrar.undeployModule(ContainerRegistrar.java:257)  at org.springframework.xd.dirt.server.ContainerRegistrar$StreamModuleWatcher.process(ContainerRegistrar.java:711)  at org.apache.curator.framework.imps.NamespaceWatcher.process(NamespaceWatcher.java:67)  at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:522)  at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)  {noformat}    Sequence of events:  * Stream module ZK path is removed  * Event is raised  * ZK connection is closed  * Event handler causes module undeployment which includes unregistration of tap  * Since connection is closed, exception is thrown  ",3
"Logging improvements
Propose the following changes to our logging:  * Create unique file names by including the pid in the file name - this allows each process (in particular containers) to maintain its own log file  * Use DailyRollingFileAppender to roll files over on a daily basis ",2
"Acceptance test must be able to handle log names with PID suffix
Introduced by XD-2006, admin and container logs will have a pid suffix appended to their filename.  The acceptance tests will have to identify the PID for the admin server and the container servers deployed in the cluster and then append the pid value to the filename contained in the xd_container_log_dir.",4
"Verify we meet all requirements to publish to maven central
https://docs.sonatype.org/display/Repository/Central+Sync+Requirements    has a list of requirements.  This also means that https://jira.spring.io/browse/XD-1509 is critical to fix.",3
"Cleanup Module Deployer
Container's module deployer (org.springframework.xd.dirt.module.ModuleDeployer) has some unused code and container-server.xml has listeners.xml which is no longer used. Also, all the extension code is moved to SharedContextConfiguration.",1
"Fix package-info.java warnings
nan",2
"Replace xd MongoItemWriter in hdfsmongodb with spring batch MongoItemWriter
When hdfsmongodb was written Spring Batch did not have a MongoItemWriter available, so Mark wrote one for XD.  The hdfsmongodb module now needs to be use Spring Batches MongoItemWriter.",3
"Upgrade to Spring Shell 1.1 GA
nan",1
"Build scripts can refer hadoop distro sub projects in a unique place
Based on this change, https://github.com/spring-projects/spring-xd/commit/87b97a0b4651f862e8a639697745ad232bb42e6a The gradle build scripts now refer to two different places to check for the list of hadoop distro sub projects. We can simplify this to make it available in one place so that maintenance will be easier.",3
"Investigate throwing of exception in BatchJobRegistryBeanPostProcessor
The condition that leads to this exception does not seem like it would ever occur, namely the BPP processing a job deployment for a job that was already deployed to the same container.",2
"Spring XD UI: end-to-end tests do not work
Currently, end-to-end tests of Spring XD UI will not run, as protractor relies on a non-existing chromedriver.exe file.    Either the configuration has to be removed from the Gruntfile or the necessary dependencies should be there.",2
"Reorganize TOC for manual
Here is a strawman {noformat} Getting Started  (rather meaty compared to other top level sections, maybe have a section - running in SingleNode) * Running in Distributed Mode * Running on YARN *Application Configuration Message Bus Configuration Monitoring and Management  Technical Documentation    Architecture Distributed Runtime  (remove 'XD' prefix) Interactive Shell Batch Jobs Streams Modules Tuples Sources Processors Analytics Sinks Taps Type Conversion Deployment  (better name?) Best Practices (new section)  Admin UI DSL Reference REST API  Samples  {noformat}",2
"Spring XD should log the address of the admin UI
When I start xd-singlenode for instance, I would expect to see http://localhost:9393/admin-ui listed in the logs.",2
"Fix images alignment in reference pdf doc
The reference pdf doc has some of the images not aligned well within the document.    For the latest doc from snapshot build, please refer here:    http://repo.spring.io/libs-snapshot-local/org/springframework/xd/spring-xd/1.0.0.BUILD-SNAPSHOT/",3
"Refactor StreamUtils in Acceptance Tests
1)StreamUtils should be removed (in that static util classes are frowned upon) and its functionality should be placed in the appropriate classes.   2) XdEnvironment - Many classes use this to obtain Environment variables.  The environment variables should be obtained via @Value in the classes that they are required. Only those values that require special setup ssh private key, Connection factories should remain in XdEnvironment. ",4
"Replace jps calls to get the PIDs for the container log with listRuntimeContainers
Replace the execution of JPS to retrieve PID for the containers in the acceptance tests with runtimeOperations().listRuntimeContainers().  ",4
"Admin UI: Deployment Status tooltip should close when the controller scope is lost
Please refer to: https://github.com/spring-projects/spring-xd/issues/1119",1
"0xData - investigate embedding
0xData is a rich JVM based machine learning and scoring engine.",24
"Update to Spring Hadoop 2.0.2
A bug in the HDFS Store was discovered that should be fixed. ",1
"Fix failing script integration tests
issue seems to be    error	25-Jul-2014 18:36:18	cat: gemfire.pid: No such file or directory  error	25-Jul-2014 18:36:18	Usage:  error	25-Jul-2014 18:36:18	  kill pid ...              Send SIGTERM to every process listed.  error	25-Jul-2014 18:36:18	  kill signal pid ...       Send a signal to every process listed.  error	25-Jul-2014 18:36:18	  kill -s signal pid ...    Send a signal to every process listed.  error	25-Jul-2014 18:36:18	  kill -l                   List all signal names.  error	25-Jul-2014 18:36:18	  kill -L                   List all signal names in a nice table.  error	25-Jul-2014 18:36:18	  kill -l signal            Convert between signal numbers and names.  error	25-Jul-2014 18:36:18	rm: cannot remove `gemfire.pid': No such file or directory",2
"Update spring-xd-yarn configuration options
Need a re-write of the configuration files for YARN deployments",3
"Handle random available http port for admin server
If the XD admin server lets tomcat chooses random http port by setting PORT or server.port to '0', the XD config logging and admin server context id still points to port zero as these are set before tomcat assigns the available random port.    We also need to persist the admin servers' ports into ZK so that this repo can be accessed by the client.",5
"Add docs for ""Creating a job Item Processor""
https://github.com/spring-projects/spring-xd/wiki/Creating-a-Job-Item-Processor    should be very brief introduction to this topic, before linking to relevant spring batch documentation.",2
"Add docs for creating a Job Module
There are a few places in the doc we can reference regarding overall lifecycle of jobs but this should provide a basic recipe for a single step job.    The focus should be on creating a job item processor.    In particular how List<Message<Tuple>> as the payload.    this should link back to a new section in the aggregator that also mentions List<Message<Tuple>>    Change title from  Creating a Job Item Processor to Creating a Job Module",2
"Use AlternativeJdkIdGenerator  instead of 3rd party library
Spring 4.0 provides a UUID generator (used by default in SI) that should be used instead of the com.eaoi.uuid library in the xd-tuple library",1
"Make producible media type to `application/json` for Job executions GET request endpoints 
As a temporary work around to fix XD-1935, make producible media type to 'application/json' for Job executions GET request endpoints.",1
"Improvements to Tuple project
Removed  various TODO comments in code and put here for proper triage.    DefaultTuple    * Error handling.  When delegating to the conversion service, the ConversionFailedException does not have the context of which key caused the failure.  Need to wrap ConversionFailedException with IllegalArgumentException and add that context back in.  (see method convert)   * Ctor visibility.  Consider making ctor final and package protect the ctor so as to always use TupleBuilder   * check for no duplicate values when initializing names/values list    tuple.  * top level methods to add.      String getComponentName... somethign that would indicate which stream or job this tuple is being processed in....    * TupleFieldSetMapper    Only one date format?    * JsonStringtoTupleConverter/JsonNodetoTupleConverter   * do we want to not map id and timestamp (believe the answer is don't map, preserve original)    ",1
"Fix misc doc formatting issues
Noticed a few issues while reviewing the documentation    * The sidebar for TOC is no longer there :(  That was really nice.  * Somehow the 'Using-MQTT-on-XD' section is giving an error.    {quote}  asciidoctor: WARNING: index.adoc: line 167: invalid style for paragraph: appendix  asciidoctor: WARNING: index.adoc: line 169: include file not found: /data/projects/spring-xd/build/asciidoc/guide/Using-MQTT-on-XD.asciidoc  :distZip  {quote}    but I don't notice anything different between that appendix and the others in index.adoc.      ",2
"Connection pool settings need to be in their own section in server.yml
The   #ConnectionPoolSettings    define this in the beginning -     {code}  #spring:  #  datasource:  {code}    uncommenting this will override/invalidate any changes made earlier in the section since it defines spring:datasource again    should either be removed or in separate section",1
"Custom location for modules.yml not working
tried local xd-admin/xd-container after setting    {code}  export XD_MODULE_CONFIG_LOCATION=file:./spring-xd-1.0.0.BUILD-SNAPSHOT-yarn/config/  {code}    have my twitter stuff in modules.yml in that directory but not picked up by the twitterstream module    Also not working for me deploying on YARN, this used to work at some point, not sure how long ago I actually tested this part - M6/M7?    The setting used for YARN deployment:    {code}  -Dxd.module.config.location: ""file:./""  {code}",3
"Exclude HealthIndicatorAutoConfiguration
With HealthIndicatorAutoConfiguration, the health endpoint shows up:    {""status"":""DOWN"",""healthIndicator"":{""status"":""UP""},""rabbit"":{""status"":""DOWN"",""error"":""org.springframework.amqp.AmqpConnectException: java.net.ConnectException: Connection refused""},""redis"":{""status"":""UP"",""version"":""2.8.9""},""mongo"":{""status"":""DOWN"",""error"":""org.springframework.dao.InvalidDataAccessResourceUsageException: Timed out while waiting to connect after 3894 ms; nested exception is com.mongodb.MongoTimeoutException: Timed out while waiting to connect after 3894 ms""},""db"":{""status"":""UP"",""database"":""HSQL Database Engine"",""hello"":1}}    We can only use vanilla health indicator, since other status may not be relevant.",1
"Create JDK6 CI build
Maybe only have it run after the publish build instead of triggering builds directly from jdk6->7->8.",2
"Log the servers/modules config locations and names
It would be good to log the server, module config locations and names when the admin, container, singlenode servers startup.",1
"Restore JDK6 compatibility
nan",2
"Add Spring IO Plugin to XD Gradle.
https://github.com/spring-projects/gradle-plugins/tree/master/spring-io-plugin Allows us to create a springIoCheck that will verify the status of XD's dependencies as it stands with SpringIO.    We would need to add the dependencies as show in the document.  But we would need to add the:  if (project.hasProperty('platformVersion')) {      ...     } code segment to the configure(javaProjects).",2
"Release 1.0 GA
nan",2
"Fix anchor links so that they work in both the wiki and generated docs
Part of the issue is likely with the build script https://github.com/spring-projects/spring-xd/blob/master/build.gradle#L1859  We should also add a link checker to the CI build.",8
"Update XD-EC2 & Acceptance Test Configs to use 1.0.1 repo
* Update XD-EC2 configs to Pull from 1.0.1 Repo * Update XD-EC2 Configs to use spring-xd-1.0.1.BUILD-SNAPSHOT dir  * Update test configs XD_HOME to spring-xd-1.0.1.BUILD-SNAPSHOT instead of spring-xd-1.0.0.BUILD-SNAPSHOT",3
"Add SFTP source
As a user, I'd like to have the option to use the _SFTP_ source module so that I can access, transfer, and mange files over any reliable data streams.    *Reference:*  [Spring Integration SFTP Adapter|http://docs.spring.io/spring-integration/reference/html/sftp.html]    Need to consider the infrastructure for testing.",2
"Deploy,Undeploy,Deploy Acceptance tests run back to back causes exception.
SHA: 33de93797106c8dd413dfb08f2fdbbb4931b528c Deployment: 1 Admin, 1 Container DataStore: MySQL  A SpringXDException is thrown when running the  testJobDeployUndeployFlow test in Acceptance tests back to back.  # Ran the test once.  Success # Ran the test a second time. ## The following exception is thrown: SpringXDException: Batch Job with the name deployundeployjob already exists  ## The XD_JOB_REGISTRY_STEP_NAMES & XD_JOB_REGISTRY still have job_name deployunderployjob still stored. ## The following Exception is seen on the admin server ### 20:45:59,214  INFO DeploymentSupervisorCacheListener-0 server.InitialDeploymentListener - Path cache event: /deployments/jobs/deployundeployjob, type: CHILD_ADDED 20:46:04,741  INFO Deployer server.JobDeploymentListener - Deployment status for job 'deployundeployjob': DeploymentStatus{state=deployed} 20:46:05,436  INFO DeploymentSupervisorCacheListener-0 server.InitialDeploymentListener - Path cache event: /deployments/jobs/deployundeployjob, type: CHILD_REMOVED 20:46:12,471  INFO DeploymentSupervisorCacheListener-0 server.InitialDeploymentListener - Path cache event: /deployments/jobs/deployundeployjob, type: CHILD_ADDED 20:46:16,330  INFO DeploymentSupervisorCacheListener-0 server.InitialDeploymentListener - Path cache event: /deployments/jobs/deployundeployjob, type: CHILD_REMOVED 20:46:17,819  INFO DeploymentSupervisorCacheListener-0 server.InitialDeploymentListener - Path cache event: /deployments/jobs/deployundeployjob, type: CHILD_ADDED 20:46:17,832  INFO Deployer server.JobDeploymentListener - Deployment status for job 'deployundeployjob': DeploymentStatus{state=deployed} 20:46:17,834 ERROR Deployer server.InitialDeploymentListener - Exception caught while handling event org.springframework.xd.dirt.zookeeper.ZooKeeperAccessException: KeeperErrorCode = NoNode for /xd/deployments/jobs/deployundeployjob/status   org.springframework.xd.dirt.zookeeper.ZooKeeperUtils.wrapThrowable(ZooKeeperUtils.java:111)   org.springframework.xd.dirt.zookeeper.ZooKeeperUtils.wrapThrowable(ZooKeeperUtils.java:95)   org.springframework.xd.dirt.server.JobDeploymentListener.deployJob(JobDeploymentListener.java:175)   org.springframework.xd.dirt.server.JobDeploymentListener.onChildAdded(JobDeploymentListener.java:99)   org.springframework.xd.dirt.server.InitialDeploymentListener$EventHandler.call(InitialDeploymentListener.java:217)   org.springframework.xd.dirt.server.InitialDeploymentListener$EventHandler.call(InitialDeploymentListener.java:186)   java.util.concurrent.FutureTask.run(FutureTask.java:262)   java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)   java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)   java.lang.Thread.run(Thread.java:745) Caused by: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /xd/deployments/jobs/deployundeployjob/status   org.apache.zookeeper.KeeperException.create(KeeperException.java:111)   org.apache.zookeeper.KeeperException.create(KeeperException.java:51)   org.apache.zookeeper.ZooKeeper.setData(ZooKeeper.java:1270)   org.apache.curator.framework.imps.SetDataBuilderImpl$4.call(SetDataBuilderImpl.java:274)   org.apache.curator.framework.imps.SetDataBuilderImpl$4.call(SetDataBuilderImpl.java:270)   org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)   org.apache.curator.framework.imps.SetDataBuilderImpl.pathInForeground(SetDataBuilderImpl.java:266)   org.apache.curator.framework.imps.SetDataBuilderImpl.forPath(SetDataBuilderImpl.java:253)   org.apache.curator.framework.imps.SetDataBuilderImpl.forPath(SetDataBuilderImpl.java:41)   org.springframework.xd.dirt.server.JobDeploymentListener.deployJob(JobDeploymentListener.java:165) 	... 7 more  # Clear  XD_JOB_REGISTRY_STEP_NAMES & XD_JOB_REGISTRY tables and run the test a third time  # Ran the test a third time. ## The following exception is thrown: SpringXDException: The job named 'deployundeployjob' is already deployed ## The following Exception was reported by the admin server. ### 21:18:29,355  WARN http-nio-9393-exec-4 zookeeper.ZooKeeperJobRepository - Exception while transitioning job 'deployundeployjob' state to undeploying org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /xd/deployments/jobs/deployundeployjob/status   org.apache.zookeeper.KeeperException.create(KeeperException.java:111)   org.apache.zookeeper.KeeperException.create(KeeperException.java:51)   org.apache.zookeeper.ZooKeeper.setData(ZooKeeper.java:1270)   org.apache.curator.framework.imps.SetDataBuilderImpl$4.call(SetDataBuilderImpl.java:274)   org.apache.curator.framework.imps.SetDataBuilderImpl$4.call(SetDataBuilderImpl.java:270)   org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)   org.apache.curator.framework.imps.SetDataBuilderImpl.pathInForeground(SetDataBuilderImpl.java:266)   org.apache.curator.framework.imps.SetDataBuilderImpl.forPath(SetDataBuilderImpl.java:253)   org.apache.curator.framework.imps.SetDataBuilderImpl.forPath(SetDataBuilderImpl.java:41)   org.springframework.xd.dirt.stream.zookeeper.ZooKeeperJobRepository.delete(ZooKeeperJobRepository.java:174)   org.springframework.xd.dirt.stream.zookeeper.ZooKeeperJobRepository.delete(ZooKeeperJobRepository.java:196)   org.springframework.xd.dirt.stream.zookeeper.ZooKeeperJobRepository.delete(ZooKeeperJobRepository.java:54)   org.springframework.xd.dirt.stream.AbstractInstancePersistingDeployer.undeploy(AbstractInstancePersistingDeployer.java:77)   org.springframework.xd.dirt.stream.AbstractInstancePersistingDeployer.undeployAll(AbstractInstancePersistingDeployer.java:103)   org.springframework.xd.dirt.stream.AbstractInstancePersistingDeployer.deleteAll(AbstractInstancePersistingDeployer.java:111)   org.springframework.xd.dirt.rest.XDController.deleteAll(XDController.java:114)   sun.reflect.GeneratedMethodAccessor161.invoke(Unknown Source)   sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)   java.lang.reflect.Method.invoke(Method.java:606)   org.springframework.web.method.support.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:215)   org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:132)   org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:104)   org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandleMethod(RequestMappingHandlerAdapter.java:749)   org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:689)   org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:83)   org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:938)   org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:870)   org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:961)   org.springframework.web.servlet.FrameworkServlet.doDelete(FrameworkServlet.java:885)   javax.servlet.http.HttpServlet.service(HttpServlet.java:652)   org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:837)   javax.servlet.http.HttpServlet.service(HttpServlet.java:727)   org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:303)   org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)   org.springframework.boot.actuate.trace.WebRequestTraceFilter.doFilterInternal(WebRequestTraceFilter.java:110)   org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)   org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)   org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)   org.springframework.boot.actuate.autoconfigure.EndpointWebMvcAutoConfiguration$ApplicationContextFilterConfiguration$1.doFilterInternal(EndpointWebMvcAutoConfiguration.java:257)   org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)   org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)   org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)   org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:77)   org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)   org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)   org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)   org.springframework.web.filter.HttpPutFormContentFilter.doFilterInternal(HttpPutFormContentFilter.java:88)   org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)   org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)   org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)   org.springframework.boot.actuate.autoconfigure.MetricFilterAutoConfiguration$MetricsFilter.doFilterInternal(MetricFilterAutoConfiguration.java:85)   org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)   org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)   org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)   org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:220)   org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:122)   org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:501)   org.apache.catalina.valves.RemoteIpValve.invoke(RemoteIpValve.java:683)   org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:171)   org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:102)   org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:116)   org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:408)   org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1040)   org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:607)   org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1720)   org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.run(NioEndpoint.java:1679)   java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)   java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)   org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)   java.lang.Thread.run(Thread.java:745) # Ran the test a fourth time.  Success ",5
"Source:file module read file line by line
I have a stream that watch output of multi file in a directory, process data and put it to HDFS. Here is my stream creat command:  stream create --name fileHdfs --definition ""file --dir=/var/log/supervisor/ --pattern=tracker.out-*.log --outputType=text/plain | logHdfsTransformer | hdfs --fsUri=hdfs://192.168.1.115:8020 --directory=/data/log/appsync --fileName=log --partitionPath=path(dateFormat('yyyy/MM/dd'))"" --deploy  Problem is source:file module send all data read from file to log processing module instead of one line each turn, becase of that, payload string have millions of char, i can't process it. Ex:  --- PAYLOAD LENGTH---- 9511284  Please tell me how to read line by line when use source:file module, thanks !!!",1
"Easier Customization of Headers Passed by RedisMessageBus
http://stackoverflow.com/questions/25072967/spring-xd-redis-message-bus-removing-headers-from-the-message/25081538#25081538    Currently, you have to modify {{redis-bus.xml}} in the dirt jar.",1
"Do you have plan to support Spark?
Do you have plans to support Spark? In version 1.0 GA, Spring XD has supported Hadoop, But it has not supported the brand new big data calculation platform Spark. do you have plans to support Spark in the future?",20
"Singlenode fails to start from external module
Singlenode fails to start using spring-xd-dirt-1.0.0.RELEASE.jar see https://github.com/dturanski/xd-test  The root cause of the error:  java.lang.NoSuchMethodError: javax.servlet.ServletContext.addServlet(Ljava/lang/String;Ljavax/servlet/Servlet;)Ljavax/servlet/ServletRegistration$Dynamic;  Indicating an incompatible servlet version is being pulled in by default.",4
"HDFS sink Partition Path causing writes to be slower in certain cases
I have not tested this on M7 but I believe it is the case with latest release as well.    Stream definition 1:  stream create logIngestion --definition ""rabbit --queues=demo --host=<rabbitmq-broker> | script --location=linemerge.groovy | hdfs --rollover=10M --idleTimeout=10000 --fileUuid=true --directory=/data/loganalysis --partitionPath=path(payload.split('\u0001')[1],dateFormat('yyyy/MM/dd/HH',payload.split('\u0001')[0],'yyyyMMddHHmmss'))""    we noticed this was causing writes to be slower    Stream definition 2:  stream create logIngestion --definition ""rabbit --queues=demo --host=<rabbitmq-broker> | script --location=linemerge.groovy | hdfs --rollover=10M --idleTimeout=10000 --fileUuid=true --directory=/data/loganalysis ""    but this definition caused the writes to be much faster.    Please note this was just a one time test I did and not reproduced multiple times.     Janne also seems to have reproduced this in another use case.    Thanks  Girish",8
"Add ability to copy job from http site to containers
Download zipped Job Modules from a HTTP Site and deploy them to modules on the admin & containers, before container is started.",5
"Add new source modules
As an user, I'd like to have OOTB source modules to integrate with various data sources to ingest data using Spring XD.   Note: The OOTB support, however, is limited to currently available Spring Integration adapters.   Acceptance Criteria: - User should be able to list the 'new source' through DSL commands  - User should be able to optionally choose the ""new source"" adapters for stream creation using XD shell - User should see appropriate error messages if the required attributes are missing while creating a stream with the 'new source' module - After successful stream creation with the 'new source' module, the definition should be included in stream listing - REST endpoints should include 'new source' definitions - Data ingested using the 'new source' should be validated for accurateness - Appropriate error/exception message needs logged if there's any problem ingesting data using 'new source' module     ",8
"Add new sink modules
As an user, I'd like to have OOTB sink modules to integrate with various data sources to egest data using Spring XD.   Note: The OOTB support, however, is limited to currently available Spring Integration adapters.  Acceptance Criteria: - User should be able to list the 'new sink' through DSL commands  - User should be able to optionally choose the ""new sink"" adapters for stream creation using XD shell - User should see appropriate error messages if the required attributes are missing while creating a stream with the 'new sink' module - After successful stream creation with the 'new sink' module, the definition should be included in stream listing - REST endpoints should include 'new sink' definitions - Data ingested into 'new sink' should be validated for accurateness - Appropriate error/exception message needs logged if there's any problem ingesting data into 'new sink' module",8
"Add infrastructure support for Admin UI
As an user, I'd like to have the ability to setup infrastructure to develop/enhance UI functionality.  This is including but not limited to: - UI designs (mockup's) - Unit testing - CI - JS 'minification'  ",8
"Add ElasticSearch sink
As a user, I should be able to leverage native _ElasticSearch_ sink so that I can aggregate, search and analyze data insights in real-time.",8
"Investigate Travis issues
We are currently facing issues with Travis. Determine the root cause, isolate the bottleneck, and resolve the issues.",3
"Investigate long running tests
The goal is to optimize the build process and at the same time validate the feature capabilities as quickly as we can. Investigate the long running tests. Look for long timeout window declarations. ",4
"FilePollHdfs sporadically fails to create files on remote machines
Need to add a retry to the createDataFileOnRemote machine, because the creation of the test file on the remote machine fails from time to time. Usually related to network issues. ",3
"Add JDBC source
As an user, I'd like to have a native _JDBC_ source module to ingest data directly from various databases. ",3
"Add Redis sink
As an user, I'd like to have the ability to ingest data into _Redis_ sink.",3
"Investigate, measure, validate and verify quality attributes
Non-functional requirements such as reliability, availability, scalability, and performance needs measured and validated.   Acceptance criteria: - Throughput: Measure the messaging infrastructure (broker/bus) capabilities in terms of messages consumed/sec. Compute the data ingestion rate/sec.  - HA/Scalability: Measure of DIRT architecture capabilities through linear increase/decrease/destroy xd-containers and xd-admins along with payload variations.  - Durability: Measure linear variations on payload with baseline DIRT infrastructure to compute end-to-end durability. ",32
"Investigate setting up performance test environment on cloud providers
Use a baseline DIRT infrastructure to measure throughput, HA and scalability for various payload sizes.    Depends on testing infrastructure setup, configuration and availability.",8
"Update JClouds to 1.8
Have to update the code because of deprecation and to get ready for 2.0.",4
"Update JClouds to 1.8 For XD-EC2
Update  to replace deprecated code.",4
"Tests sporadically fail when checking send counts with rabbit as transport
Tests that use verifySendCounts to validate whether data was sent to all the modules in a stream occasionally fail.  This is because, sometimes it takes 2 or more sends to get the data transmitted between modules.  With the current test structure this is considered a failure.  Is this the correct behavior?",5
"Upgrade asciidoctor toolchain
This will in turn allow us to get rid of the custom logic for handling crossref links between documents",3
"Add support for specifying an undeploy-condition for stream definitions
In some scenarios like when performing exploratory data-analysis on streaming data one often create a stream, keep it running for some time (or until some condition is met) and then stop the stream and start to investigate the collected data.  It would be cool to be able to specify some undeploy condition, like e.g. a timeout after x minutes, no. of events collected, a specific counter past a given threshold, file-size greater then x etc.",13
"Provide a way to debug the http source
The http source does not provide debug logging to see information such as http headers and requests, in particular if a non OK response is returned.    I updated the log4j config for org.jboss.netty but it had no effect.  I suspect this is due to the need to configure the netty logging system via InternalLoggerFactory.setDefaultFactory(new Log4JLoggerFactory()).    ",5
"Get rid of custom asciidoctor link: transformations
Remove   {noformat}  		filter { line ->  			// TODO: refine regex to only match local documents  			def match = (line =~ /link:(.*?)#(.*?)\[(.*?)\]/)  			if (match) match.replaceAll('xref:$2[$3]') else line  		}    {noformat}    and replace link:Foo#bar by xref:Foo#bar    ",2
"Modularize gradle build
split out build.gradle into multiple files.",2
"xd-shell should provide a command line argument ""--adminUri"" to specify the location of the admin node
xd-shell by default will try to connect to the admin node in the same host. You need to run ""admin config server"" to specify the URI where the admin node is - every time you run xd-shell.   xd-shell should have a command line argument ""--adminUri"" so that the user can invoke the xd-shell passing the Admin Node Uri as an argument when running the shell and then be able to use xd-shell in interactive mode without having to run ""admin config server"" every time.     ",2
"Create a Sink and Source for Riak
nan",8
"Modules that depend on HttpClient fail when running on YARN on Hadoop 2.4.x and later
Trying to run a twitterstream on YARN on HDP 2.1 and get the following:    {code}  14/08/08 12:12:50 ERROR server.ContainerRegistrar: Exception deploying module  org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.springframework.integration.x.twitter.TwitterStreamChannelAdapter#0' defined in URL [file:./spring-xd-yarn-1.0.0.RELEASE.zip/modules/source/twitterstream/config/twitterstream.xml]: Cannot resolve reference to bean 'twitterTemplate' while setting constructor argument; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'twitterTemplate' defined in URL [file:./spring-xd-yarn-1.0.0.RELEASE.zip/modules/source/twitterstream/config/twitterstream.xml]: Instantiation of bean failed; nested exception is org.springframework.beans.BeanInstantiationException: Could not instantiate bean class [org.springframework.social.twitter.api.impl.TwitterTemplate]: Constructor threw exception; nested exception is java.lang.NoClassDefFoundError: org/apache/http/impl/client/HttpClients    org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:336)    org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:108)    org.springframework.beans.factory.support.ConstructorResolver.resolveConstructorArguments(ConstructorResolver.java:646)    org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:140)    org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1114)    org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1017)    org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:504)    org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:475)    org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:302)    org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:228)    org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:298)    org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:193)    org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:703)    org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:760)    org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:482)    org.springframework.boot.SpringApplication.refresh(SpringApplication.java:691)    org.springframework.boot.SpringApplication.run(SpringApplication.java:320)    org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:142)    org.springframework.xd.module.core.SimpleModule.initialize(SimpleModule.java:203)    org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:98)    org.springframework.xd.dirt.module.ModuleDeployer.deployAndStore(ModuleDeployer.java:88)    org.springframework.xd.dirt.module.ModuleDeployer.deployAndStore(ModuleDeployer.java:78)    org.springframework.xd.dirt.server.ContainerRegistrar.deployModule(ContainerRegistrar.java:231)    org.springframework.xd.dirt.server.ContainerRegistrar.deployStreamModule(ContainerRegistrar.java:577)    org.springframework.xd.dirt.server.ContainerRegistrar.onChildAdded(ContainerRegistrar.java:447)    org.springframework.xd.dirt.server.ContainerRegistrar.access$800(ContainerRegistrar.java:95)    org.springframework.xd.dirt.server.ContainerRegistrar$DeploymentListener.childEvent(ContainerRegistrar.java:826)    org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:509)    org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:503)    org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)    com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)    org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)    org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:500)    org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)    org.apache.curator.framework.recipes.cache.PathChildrenCache$10.run(PathChildrenCache.java:762)    java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)    java.util.concurrent.FutureTask.run(FutureTask.java:262)    java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)    java.util.concurrent.FutureTask.run(FutureTask.java:262)    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)    java.lang.Thread.run(Thread.java:744)  Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'twitterTemplate' defined in URL [file:./spring-xd-yarn-1.0.0.RELEASE.zip/modules/source/twitterstream/config/twitterstream.xml]: Instantiation of bean failed; nested exception is org.springframework.beans.BeanInstantiationException: Could not instantiate bean class [org.springframework.social.twitter.api.impl.TwitterTemplate]: Constructor threw exception; nested exception is java.lang.NoClassDefFoundError: org/apache/http/impl/client/HttpClients    org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:278)    org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1114)    org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1017)    org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:504)    org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:475)    org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:302)    org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:228)    org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:298)    org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:193)    org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:328)  	... 41 more  Caused by: org.springframework.beans.BeanInstantiationException: Could not instantiate bean class [org.springframework.social.twitter.api.impl.TwitterTemplate]: Constructor threw exception; nested exception is java.lang.NoClassDefFoundError: org/apache/http/impl/client/HttpClients    org.springframework.beans.BeanUtils.instantiateClass(BeanUtils.java:164)    org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:125)    org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:270)  	... 50 more  Caused by: java.lang.NoClassDefFoundError: org/apache/http/impl/client/HttpClients    org.springframework.http.client.HttpComponentsClientHttpRequestFactory.<init>(HttpComponentsClientHttpRequestFactory.java:74)    org.springframework.social.support.ClientHttpRequestFactorySelector$HttpComponentsClientRequestFactoryCreator$1.<init>(ClientHttpRequestFactorySelector.java:77)    org.springframework.social.support.ClientHttpRequestFactorySelector$HttpComponentsClientRequestFactoryCreator.createRequestFactory(ClientHttpRequestFactorySelector.java:77)    org.springframework.social.support.ClientHttpRequestFactorySelector.getRequestFactory(ClientHttpRequestFactorySelector.java:52)    org.springframework.social.oauth1.AbstractOAuth1ApiBinding.createRestTemplateWithCulledMessageConverters(AbstractOAuth1ApiBinding.java:188)    org.springframework.social.oauth1.AbstractOAuth1ApiBinding.createRestTemplate(AbstractOAuth1ApiBinding.java:169)    org.springframework.social.oauth1.AbstractOAuth1ApiBinding.<init>(AbstractOAuth1ApiBinding.java:70)    org.springframework.social.twitter.api.impl.TwitterTemplate.<init>(TwitterTemplate.java:79)    sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)    sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)    sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)    java.lang.reflect.Constructor.newInstance(Constructor.java:526)    org.springframework.beans.BeanUtils.instantiateClass(BeanUtils.java:148)  	... 52 more  {code}",5
"Add --binary Option to MQTT Source
See http://stackoverflow.com/questions/25226527/mqtt-source-spring-xd/25227531#25227531",1
"Ability to send files as attachment (mail sink)
I'd be very pratical if the mail sink has the ability to send files as attachment. I.e. add the attribute ""attachment-filename"" to header-enricher.",1
"Problem using twittersearch when system where container is running has two network interfaces.
Problem using twittersearch when the system where the XD container is running has two network interfaces.  With the following config:  eth0      local network, resolves `hostname`  eth1      internet network  I get an error deploying the stream:  {code} 12:08:22,965  WARN twitterSource-1-1 client.RestTemplate - GET request for ""https://stream.twitter.com/1.1/statuses/sample.json"" resulted in 401 (Authorization Required); invoking error handler 12:08:22,972 ERROR twitterSource-1-1 twitter.TwitterStreamChannelAdapter - Twitter authentication failed: 401 Authorization Required {code}  If I flip the network interfaces to be:  eth0      internet network, resolves `hostname` eth1      local network    then it seems to work. ",3
"Add a Retry/Dead Letter Interceptor to the RabbitMQ Source
Provide for retry and/or dead-lettering for the rabbit source (similar to the rabbit message bus).",2
"Modules do not redeploy properly when Zookeeper node is lost.
* SHA baddfc24b08286a78392d5f565742c9bab5adfea * EC2 Environment ** Look at Zookeeper Ec2 Deployment Test Topology.png for a view of the topology  h2. The test scenario  # Bring up a up a 5 container 2 admin XD Cluster up using 3 ZK Server ensemble. # Create ticktock  stream ""time|log"" # Deploy with --properties ""module.log.count=5"" # Kill one of the ZK Servers in the ensemble  h2. Observed Behavior.  # In this particular scenario 3 containers were affected by killing (sudo kill <pid>) Zookeeper 2 # 2 Containers did not come back online even though they did show up in the runtime containers   h2. Timeline # 14:08:21 deployed stream # 14:09:10 kill server in ZK Ensemble # After waiting a few seconds ran runtime Modules (*Note:* log2 is undeployed and log5 is then deployed)  :  xd:>runtime modules   Module             Container Id                          Options                                     Deployment Properties   -----------------  ------------------------------------  ------------------------------------------  ---------------------   foo.sink.log.1     98a32c62-302a-484b-af9c-d670f2a3cfc2  {name=foo, expression=payload, level=INFO}  {count=5, sequence=1}   foo.sink.log.2     9a3a1846-bac4-4504-81fd-151665d851dc  {name=foo, expression=payload, level=INFO}  {count=5, sequence=2}   foo.source.time.1  9a3a1846-bac4-4504-81fd-151665d851dc  {fixedDelay=1, format=yyyy-MM-dd HH:mm:ss}  {count=1, sequence=1}  xd:>runtime modules   Module             Container Id                          Options                                     Deployment Properties   -----------------  ------------------------------------  ------------------------------------------  ---------------------   foo.sink.log.1     98a32c62-302a-484b-af9c-d670f2a3cfc2  {name=foo, expression=payload, level=INFO}  {count=5, sequence=1}   foo.sink.log.5     5c454a39-fc4c-4bd3-b828-08cd837dc4ba  {name=foo, expression=payload, level=INFO}  {count=5, sequence=5}   foo.source.time.1  9a3a1846-bac4-4504-81fd-151665d851dc  {fixedDelay=1, format=yyyy-MM-dd HH:mm:ss}  {count=1, sequence=1}  xd:>runtime modules   Module             Container Id                          Options                                     Deployment Properties   -----------------  ------------------------------------  ------------------------------------------  ---------------------   foo.sink.log.1     98a32c62-302a-484b-af9c-d670f2a3cfc2  {name=foo, expression=payload, level=INFO}  {count=5, sequence=1}   foo.sink.log.5     5c454a39-fc4c-4bd3-b828-08cd837dc4ba  {name=foo, expression=payload, level=INFO}  {count=5, sequence=5}   foo.source.time.1  9a3a1846-bac4-4504-81fd-151665d851dc  {fixedDelay=1, format=yyyy-MM-dd HH:mm:ss}  {count=1, sequence=1}  xd:>runtime containers   Container Id                          Host              IP Address     PID   Groups  Custom Attributes   ------------------------------------  ----------------  -------------  ----  ------  -----------------   0ba5e6ce-aedf-429c-b846-1cd4e32836c7  ip-10-2-209-174   10.2.209.174   1045   5c454a39-fc4c-4bd3-b828-08cd837dc4ba  ip-10-70-9-57     10.70.9.57     1099   707a968b-15a5-451f-9034-1e7f05cdcf97  ip-10-70-11-185   10.70.11.185   1055   98a32c62-302a-484b-af9c-d670f2a3cfc2  ip-10-110-186-48  10.110.186.48  1056  GROUPA   9a3a1846-bac4-4504-81fd-151665d851dc  ip-10-70-9-153    10.70.9.153    1020  GROUP0   h2.  Undeploy and redeploy stream   # 14:16:42 Undeploy and redploy with module.log.count=5 xd:>runtime modules   Module             Container Id                          Options                                     Deployment Properties   -----------------  ------------------------------------  ------------------------------------------  ---------------------   foo.sink.log.1     98a32c62-302a-484b-af9c-d670f2a3cfc2  {name=foo, expression=payload, level=INFO}  {count=5, sequence=1}   foo.sink.log.2     5c454a39-fc4c-4bd3-b828-08cd837dc4ba  {name=foo, expression=payload, level=INFO}  {count=5, sequence=2}   foo.sink.log.5     9a3a1846-bac4-4504-81fd-151665d851dc  {name=foo, expression=payload, level=INFO}  {count=5, sequence=5}   foo.source.time.1  98a32c62-302a-484b-af9c-d670f2a3cfc2  {fixedDelay=1, format=yyyy-MM-dd HH:mm:ss}  {count=1, sequence=1}  xd:>runtime containers   Container Id                          Host              IP Address     PID   Groups  Custom Attributes   ------------------------------------  ----------------  -------------  ----  ------  -----------------   0ba5e6ce-aedf-429c-b846-1cd4e32836c7  ip-10-2-209-174   10.2.209.174   1045   5c454a39-fc4c-4bd3-b828-08cd837dc4ba  ip-10-70-9-57     10.70.9.57     1099   707a968b-15a5-451f-9034-1e7f05cdcf97  ip-10-70-11-185   10.70.11.185   1055   98a32c62-302a-484b-af9c-d670f2a3cfc2  ip-10-110-186-48  10.110.186.48  1056  GROUPA   9a3a1846-bac4-4504-81fd-151665d851dc  ip-10-70-9-153    10.70.9.153    1020  GROUP0    # 14:21:06 undeploy foo  ",5
"Container does not fail if sharing same ports on same machine with another container.
* SHA baddfc24b08286a78392d5f565742c9bab5adfea * EC2 Environment ** Look at Zookeeper Ec2 Deployment Test Topology.png for a view of the topology  h2. The test scenario  # Can be duplicated with single server ZK ensemble # Start Container on a EC2 instance.  Wait till it joins cluster # Start 2nd container on same EC2 instance.  h2. Observed Behavior.  # Container 1 starts normally # Container 2 reports that failed to bind to address.  (shown in attached stack trace) ## But does not terminate  ## Shown as valid container when executing ""runtime containers"" command. xd:>runtime containers   Container Id                          Host              IP Address     PID   Groups  Custom Attributes    0c88a300-9469-4f21-a256-7733259b13c7  ip-10-70-11-185   10.70.11.185   2061   256c35b7-c9f4-43ba-81dd-e1bfff0fb7c1  ip-10-70-9-57     10.70.9.57     3604   31fedc48-2762-4c45-8075-1dce64af5391  ip-10-110-186-48  *10.110.186.48*  2396  GROUPA   524bb933-a8b5-4014-a0b5-06d4fa8b30c2  ip-10-2-209-174   10.2.209.174   2123   e993026e-e2ac-4d16-9890-0786149d7b75  ip-10-110-186-48  *10.110.186.48*  2524  GROUPA   f9437b15-1ee2-4827-99d4-e7957f9abdf2  ip-10-70-9-153    10.70.9.153    2366  GROUP0   ",5
"Investigate how to provide a means to share bean defintions across module instances
In some cases it maybe useful to share a specific bean instance contributed by a user across multiple module instances.  This story is a placeholder to collect requirements and discuss.",4
"singlestep-partition-support needs to allow grid size to be configurable
h3.  Narrative As a developer, I need to be able to configure a partitioned job's grid size so that the correct number of partitions are created (the current code is hard coded to 1 for the grid size).  h3.  Acceptance Criteria # Expose the gridSize attribute of the {{MessageChannelPartitionHandler}} as an option.  h3.  Assumptions # Existing OOTB jobs should not be impacted by this given they don't use the grid size. ",2
"Spring XD EC2 needs to setup cluster that uses static resources.
h1. Summary  User wants the ability to deploy an ec2 cluster where the admin & containers use a pre existing ZK ensemble, Rabbit and redis instance that are deployed on different machines.   h2. Current functionality Currently spring-xd-ec2 sets up its containers & admin server to use a ZK, rabbit and redis that are provisioned and collocated with the admin server.    h2. Detail The following properties will be added to the spring-xd-ec2.properties # *spring_zk_client_connect* - contains a comma delimited list of zk hosts:ports for a ensemble.  The application will check to see if the port is open on at least of the servers in the list, if not deployment will fail.  Default is adminServer_host:2181. # *spring_rabbitmq_addresses* - contains a comma delimited list of hosts:ports for a rabbit cluster.  The application will check to see if the port is open on at least of the servers in the list, if not deployment will fail.  Default is adminServer_host:5672. # *spring_redis* - contains a host:port for a redis instance. The application will check to see if the port is open , if not deployment will fail.  Default is adminServer_host:5672. # *ec2_zone* - user can specify the zone to which the containers and admin will be deployed.  If not present AWS will decide which zone to deploy the cluster.",8
"Test Recommended XD Cluster Strategy on slow/bad network
h1. Run Acceptance tests on the following  deployments.      h2. Slow Network  Simulate slow network by deploying a XD cluster where the ZK Ensemble is only available via WAN.      h2. Network packet loss  Simulate cases where a network packets can be lost.    ",5
"Research how to handle data encryption within pipeline
Design Spike: Investigate best approach to encrypt data pipeline. Consider all moving parts within the topology including the scenarios where data is at rest and as well as in transit. ",8
"Spike: Research how to leverage ZK's ACLs
As an Ops, I'd like to setup security for the ZK nodes, so I could restrict access to zNodes to the right users/apps. ",8
"Research how to extend 'Trigger' interface to implement custom triggers
As a developer, I'd like to have the option of extending the Trigger abstraction so that I can implement my own trigger.       ",8
"Process for creating and deploying custom XD modules
nan",16
"Custom module packaging strategy
As a user, I'd like to have guidance to create custom modules so that I can align the development practices with recommended approach.     11/20: Update: Scope of this task is to create an example to demonstrate and document the capability.",1
"Research approach to bootstrap custom modules
Design Spike: Investigate various approaches to bootstrap custom modules. Can Spring Boot be leveraged? Starter POM?",5
"Enhance Container domain object
Currently, org.springframework.xd.dirt.cluster.Container has name and attributes (container attributes). This can be enhanced to include all the deployed modules, number of deployed modules and any more useful info. and can subsequently be used to get a detailed runtime container info.",2
"List Streams/Jobs based with deployed modules
Currently, there is a ""stream list""/""job list"" which shows the status of a given stream/job along with the DSL. and, there is ""runtime modules"" which shows all the deployed modules with their container info.    We need a better REST endpoint that gives all the deployed modules for a given stream/job along with the status.",3
"UI: Cluster view of a container
We need a visual representation of the XD cluster with runtime container and deployed modules.",5
"UI: Ability to deploy stream with deployment properties
Admin UI currently allows job to be deployed with deployment properties, we need similar way to deploy stream with the deployment properties (module count, container matching criteria).",2
"UI: Visual representation of Stream/Job with deployed modules
For a given stream/job, we need a visual representation of the stream/job with any deployed modules.",5
"Enable shutdown containers from admin server
When the container starts up, it has a random http port for management configurations. If the container has management port enabled, then we can store it as container attribute. If the admin can reach out to the container on that port, then we can provide an option to shutdown container from the admin server.",5
"Implement KafkaMessageBus
Kafka lends itself well as a message bus, and kafka partitioning maps to MB partitioning.    Implement KafkaMessageBus and supporting classes and UT/IT.",3
"Remove unavailable jobs
If a job is deployed an the singlenode job is canceled, the job name cannot neither be reused nor destroyed. See screenshots. ",1
"Create XD source from POJO annotated @Source 
XD Module configured for component scanning classes (or methods?) annotated with @Source, (consider @Processor, and @Sink as well) and simply provide the POJOs and dependent jars in the module /lib directory. Custom processor is fairly straightforward currently, but still requires an XML module definition to wire up the POJO as a service activator or transformer to the input and output channel. A service activator works for a POJO backed sink. Writing a source that is not backed by an existing inbound channel adapter is a bit more involved and requires more than basic familiarity with SI. It should be possible for XD to automatically create a polling source by wiring a Java method to an inbound adapter configured with a poller. Ideally, we would require no XML, even to enable component scanning- this will require some changes to the module registry/module initializer.",3
"Provide tab completion for deployment manifest properties
nan",8
"Add Kafka sink
As a user, I'd like to have the option to write into _Kafka_ sink so that I can publish mass data into Kafka broker.",8
"Investigate and Fix E2E test execution for grunt task ""test:e2e""
As a result of fixing XD-2015 we still cannot execute:  {code} grunt test:e2e {code}  Basically running the tests AND the server together in one process fails. We see the following error: *Fatal error: socket hang up*.  If we separate the protractor execution into 2 separate steps, the tests pass:  {code} grunt serve (one console window) grunt protractor:run (second console window) {code}  In the *grunt serve* window, you can still observe *Fatal error: socket hang up* being printed out but the tests execute successfully.   ",8
"MessageBus should see custom SpEL property accessors
Currently, custom SI property accessors are registered by a plugin (org.springframework.xd.dirt.plugins.SpelPropertyAccessorPlugin) and are not visible by the bus.    I believe they should be.    This may just be a matter of moving them around.",5
"Provide the ability to visualize XD cluster view
As a user, I'd like to have the ability to visually explore XD's cluster view so that I'm aware where the components are deployed and how they are connected within the topology. ",2
"Allow aggregate-counter to increment by some value of the message
Currently, the aggregate counter only adds +1 to the individual values, even though support is there to add any increment.    This ticket is about surfacing a SpEL expression on the message to choose the increment",3
"Show Stream / Tap relationship in Admin UI
It would be nice if the admin ui would sort the streams in such a way that the taps that are created for a particular stream are somehow placed UNDER the tapped stream... {code} Stream A ...  +-Tap A.foo ...  +-Tap A.bar ... {code}  Same goes for the list command in the XD-Shell",13
"No way to set 'makeUnique' false when creating job in UI
The resulting definition starts with --makeUnique=true even if the MakeUnique checkbox is unchecked. I can check and uncheck the box and the --make unique parameter isn't included. Since the default for this parameter is true the end result is the same. There doesn't seem to be a way to set --makeUnique=false.",3
"XD-EC2 needs to provide ability to use a preinstalled zip vs downloading from s3
[Current Behavior] Currently XD-EC2 downloads an XD zip file from the location specified by the xd-dist-url after verifying that the file is accessible..   ",3
"JDBC driver classpath issues for custom job modules
There is a class-loader issue when defining JDBC driver/pool dataSource for a custom job module. If I use the Tomcat DataSource it pulls the driver classes from /xd/lib, but if I use a DataSource class that's not loaded by the XD runtime like SimpleDriverDataSource then it pulls the jdbc driver jars from my job module's lib directory which I was expecting since the JDBC drivers I'm using are not delivered as part of XD.   The whole class-loading scheme for custom modules needs either better documentation or improvement. Currently it's difficult to tell where classes will be loaded from and what classes are available on the server classpath and which classes have already been loaded. ",3
"User wants to select the ec2 zone when deploying XD
Currently the application allows AWS select which zone in the region to create an instance.",3
"Redis aggregate-counter fails when end of interval is on the hour
curl http://localhost:9393/metrics/aggregate-counters/smartgrid_h_28_load_predicted\?resolution\=minute\&from\=2013-09-01T12:55:00.000Z\&to\=2013-09-01T13:00:00.000Z     would return 0 as the last bucket, while    curl http://localhost:9393/metrics/aggregate-counters/smartgrid_h_28_load_predicted\?resolution\=minute\&from\=2013-09-01T12:55:00.000Z\&to\=2013-09-01T13:05:00.000Z    would correctly return the value for 13:00:00",2
"Job stuck in ""deploying"" state when no containers are available
A job gets stuck in ""deploying"" state when a job is deployed when there are no containers available.  When a container is started after this event, the job doesn't automatically start because of the job is stuck in the ""deploying"" state instead of the ""failed"" state.      Refer to https://github.com/spring-projects/spring-xd/blob/193088dc164c73e07d7b4509de22241b28bf42b3/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/server/JobDeploymentListener.java    Update of the status in Zookeeper is inside the NoContainerException catch block.    This works correctly for streams.",5
"Add REST resource sink
Would be nice to have a sink for REST resources.  Might be configurable with an endpoint URI. Basic auth details would be a nice to have too.  Would perform a POST to the endpoint passing the payload.",5
"Spring XD very poor performance when using redis as transport
When using redis as transport bus there is a problem when using many streams and taps. Basically the maxTotal parameter of org.apache.commons.pool2.GenericObjectPool default is 8. After some streams are deployed it starts to occur concurrency problems hence the number of inbound redis channel adapters is larger than that number.    A more detailed explanation is in stackoverflow:    http://stackoverflow.com/questions/25851660/spring-xd-very-poor-performance-when-using-redis-as-transport",8
"Create a shell command processor and sink
Create processor and sink modules that can execute a shell command using stdin and stdout to stream data.",5
"Support accessing admin server endpoints over HTTPS
As a user, I'd like to have the option to enable HTTPS so that I can access XD's Admin server [endpoints|https://github.com/spring-projects/spring-xd/wiki/REST-API#xd-resources] over secured communication.    Technical Implementation:    This functionality is available in Spring Boot 1.2.0 M1 and has been backported into the 1.1.x branch to be released under Spring 1.1.7.  We can test against 1.1.7 SNAPSHOT.    Working through the way to update the build file to pick up a new version of boot is a bit tricky :(",1
"Provide file based storage for users, groups (and roles)
As a user, I'd like to have the option to provide file based security configurations so that I can access the endpoints in a secured manner.    Ideally, all the listed *endpoints* (http://localhost:9393/) needs to be encapsulated within file based security layer.    Reference:  [Securing Web App|https://spring.io/guides/gs/securing-web/]",5
"Secure all endpoints using LDAP based security configurations
As a user, I'd like to have the option to provide LDAP based security configurations so that I can access the endpoints in a secured manner.    Ideally, all the listed *endpoints* (http://localhost:9393/) needs to be encapsulated within LDAP based security layer.    Reference:  [Authentication using LDAP|https://spring.io/guides/gs/authenticating-ldap/]",8
"Secure endpoints using either ROLE_VIEWER and ROLE_ADMIN
As a user, I'd like to have the option to configure default access control for endpoints so that I can grant access by _Admin_ or _Viewer_ roles.",8
"Provide kerberos support for HDFS sink
As a user, I'd like to have the option of _kerberized_ HDFS sink so that I can leverage Kerberos (open source distributed authentication system) for secured data writes into Hadoop.",3
"Research integration options for Sqoop 'tasklet'
As a user, I'd like to have the ability to mass-ingest data from various database systems so that I'm not restricted with the current approach (_jdbchdfs_) that is dependent on JDBC drivers.     *Spike Scope:*  * Identify integration options  * Collaborate to determine the design  * Document outcome (design specs)  ",5
"Add HBase sink
As a user, I would like to have an option to write data into HBase sink so that I can perform random, realtime read/write access on Big Data.",8
"Add File Roll sink
As a user, I'd like to have the option to write into _File Roll_ sink so that I can store events on the local file system.",8
"Add JMX source
As a user, I'd like to have the option of _JMX_ source module so that I can publish/subscribe to JMX notifications.  *Reference:* [Sprint Integration JMX Support|http://docs.spring.io/spring-integration/reference/html/system-management-chapter.html#jmx]",3
"Add WebSocket source
As a user, I'd like to have the option of _WebSocket_ source module so that I can create a interactive communication channel between user's browser session and the runtime to ingest browser based events and activities.  *Reference:* [Spring Integration WebSocket Support|https://github.com/spring-projects/spring-integration/tree/master/spring-integration-websocket/src]",3
"Add S3 source
As a user, I'd like to have an option of _AWS_ source module so that I can ingest data from Amazon S3 or use the Simple Email Service (SES).  *Reference:* [Spring Integration AWS Extension|https://github.com/spring-projects/spring-integration-extensions/tree/master/spring-integration-aws]",5
"Add Cassandra sink
As a user, I'd like to have the option of _Cassandra_ sink, so I can leverage the NoSQL database to write high volumes of variable data segments in high velocity.",3
"Add S3 sink
As a user, I'd like to have the option of _AWS_ sink so that I can write data into S3 directly.   *Reference:* [Spring Integration AWS Extension|https://github.com/spring-projects/spring-integration-extensions/tree/master/spring-integration-aws]",5
"Add HAWQ sink
As a user, I'd like to have the option of _HAWQ_ sink so that I can write data directly into HAWQ via PXF extensions through Avro/Parquet format.",3
"Add SOLR sink
As a user, I'd like to have the option of _SOLR_ sink so that I can perform full-text indexing and search through SOLR backend server. ",5
"Improve stream processing capabilities
nan",16
"Add explicit ""error channel"" support
As a user, I'd like to have the option to explicitly define/configure ""error channel"" so that I can stage and route the errors/exceptions through the dedicated channel and continue ingestion.  *Scenario:* * 'http' source ingest  * failure at either source, processor, or sink module * regardless of whether it is a custom module or not, traverse through the exception to propagate the actual _Caused by:..._, stage the error as payload, and route it to the error channel  *Example Configuration:* * error channel definition similar to ""topic.errors.stream.module"" * configure custom exception similar to ""catch=**Exception"" * exception hierarchy ** GlobalException ** DefaultException ** ModuleSpecificException ",8
"Restrict the use of reserved keywords
As a user, I should not be allowed to create a custom module with a _reserved_ keywords so that I it will avoid confusions from seeing duplicate strings in deployment manifest.  *Example:* We would like to avoid a _custom_ module name of *producer* to eliminate the confusion below: {code}  xd:>stream deploy --name test1 --properties ""module.producer.producer.deliveryMode= PERSISTENT,module.log.criteria=groups.contains('group1')""  {code}  [List of available reserved keywords|https://github.com/spring-projects/spring-xd/wiki/Deployment#deployment-properties]",5
"Preserve partition state on container restarts
As a user, I'd like to retain the data partitioning state so that when I restart the containers, I continue to write based on the original partitioning strategy.   Currently, the state is not preserved; hence, on restarts the definition of partitioning strategy is lost due to different _hashCode()_.  *Design consideration:* Mine through the container info to derive the ""partition index"" instead of relying on _hashCode()_.",5
"Test partitions by dynamic additions/deletions of modules
As a user, I'd like to have the data partition strategy state preserved so that when I add/delete modules, they are able to dynamically adapt to the strategy.  This is already included as part of the GA release. This story is to account for the testing effort.",1
"Add ftp sink to default sink modules
It would be nice to have a simple ftp sink. I had to do it for one of my projects. Therefore, the sink already exists. I would like to contribute but I don't know how you do the 'testing' part for that kind of module.",2
"Deployment properties should use label instead of name
stream create foo --definition ""label: bar | xxxx""  stream deploy foo --properties ""module.label.yyy=zzz"" seems to work but it does not. The pre-validation is correct, but downstream, deployment logic still looks for module.bar (instead of module.label)",3
"XD Cluster view: improve hover over capabilities 
Please see the discussion here:  https://github.com/spring-projects/spring-xd/pull/1183#issuecomment-55917701",2
"XD Cluster view: create container details page
Please see the discussion here:  https://github.com/spring-projects/spring-xd/pull/1183#issuecomment-55917701 ",2
"Add Basic Auth support
As a user, I'd like to have the option of _Basic Auth_ so that I'm challenged to provide _user name_ and _password_ when making a request.    Technical Implementation:    This functionality is provided in Spring Boot 1.1.x, it should be a matter of adding the spring boot security starter dependency to the spring-xd-dirt project.      It will be controlled using the spring boot property server.basic.enabled = true/false.  Our default in application.yml for this property should be false.    ",1
"Support Spring Boot's single-user security configurations
As a user, I'd like to have the option to provide single-user security configurations so that I can override them as needed.    *Reference:*  [Spring Boot - Security|http://docs.spring.io/spring-boot/docs/1.1.x-SNAPSHOT/reference/html/boot-features-security.html]    *Scope:*  Configurations can be provided through _servers.yml_ file.",1
"XD Shell needs to be be able to authenticate using basic auth to admin server
As a user, I want to be able to provide security credentials to the XD Shell so that I can interact with an xd admin server that is secured via basic auth    Technical implementation:    Add ---password and --username to the admin config command.      ",5
"Upgrade to spring boot 1.1.7.SNAPSHOT
As a user, I'd like to have latest Spring Boot snapshot pulled as a dependency so that I can inherit and implement the OOTB security features.",1
"Upgrade to spring boot 1.1.7.RELEASE
As a user, I'd like to have latest Spring Boot RELEASE pulled as a dependency so that I can inherit and implement the OOTB security features.",1
"Create separate distribution for shell
Create zip distribution for shell",1
"Remove un-necessary libs from shell
Shell currently adds all jars from xd/lib to its classpath.  Remove jars that are not needed to run shell.",1
"FilePollHdfsTest fails intermittently 
From time to time FilePollHdfsTest fails in the CI Acceptance Tests. The exception that is fired is as follows:  java.lang.AssertionError: java.lang.AssertionError: The data returned from hadoop was different than was sent.   expected:<942b9f47-8169-4dc3-a2ba-3d8fab04a4dc > but was:<null>  Need to investigate if this is a timing issue with the test (more than likely) or with the actual module.",3
"HDFS sink should honor --fileExtension parameter for bzip2 compressed files
Looks like the --fileExtension isn't used when compressing files with bzip2, some use cases requirer bz2 instead of bzip2 as the extension. Also, '.bz2' should be the default extension. At the same time we should change the default gzip extension to '.gz'.",3
"Support XD as a service on PCF
nan",16
"Update Wiki to reflect the change from runtime x to cluster x
Update the wiki to reflect the change from: runtime containers to cluster containers and runtime modules to cluster modules.  ",1
"Research REST endpoint approach to push custom module
As a user, I'd like to have a REST API to point and push an archive that includes custom module definitions and configurations so that I don't have to manually move and set it up.    *Scope of this spike:*  * Assess customer requirement, brainstorm, and document options  * Socialize with the team to collect feedback  * Identify phases  * Create new stories",8
"UI: Add Tooltip Directive
Small follow up story to XD-2094 to improve tooltip handling.",1
"Support XD Runtime on Docker
nan",16
"Support for Mesos based s-c-d deployment 
nan",16
"Provide Docker image for developers
As a user, I need a 'sandbox' Docker Image so that I can get started to experiment XD deployment with the following setup:    * Ubuntu OS  * Full XD Jar  * Java 7.x  * Redis  * RabbitMQ",5
"Provide Docker image for production deployment
As a user, I need a ""production-ready' Docker Image so that I can use that as a baseline to deploy XD with the following setup.  * Ubuntu OS * Full XD Jar * Java 7.x",8
"Provide ability to configure Docker containers
As a user, I need the ability to configure Docker XD Containers so that I can link to external services such as _Rabbit, Redis, Zookeeper, Hadoop, Mongo, etc_.  Includes pointers to: * Linking/binding attributes * Environment variables ",8
"Provide support for Docker Service Discovery
As a user, I want to configure Docker XD Containers using Service Discovery so that I can have tools to manage how processes and services in a cluster can find and talk to one another.",8
"Provide CI build support for Docker images
As a user, I need to have the ability to create docker images via CI build so that I can build all the components/configurations I need into a Docker image, test it, and deploy the image to various environments. ",8
"Research the approach for XD Runtime on Mesos 
As a user, I need a document covering our recommendations for deploying a XD cluster using Mesos with the Marathon Framework. ",8
"Provide support for XD Runtime on Vagrant
nan",16
"Evaluate Spring Boot dependency upgrade
As a user, I'd like to evaluate Spring Boot dependency upgrades so that I can make sure there aren't any side effects or impacts to existing functionalities. ",3
"NoNodeException for job creation
The following exception was encountered by a few parties: for example https://gopivotal-com.socialcast.com/messages/21678398    {noformat}  ERROR LeaderSelector-0 leader.LeaderSelector - The leader threw an exception  org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /xd/deployments/jobs/NNNN/modules    org.apache.zookeeper.KeeperException.create(KeeperException.java:111)    org.apache.zookeeper.KeeperException.create(KeeperException.java:51)    org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1586)    org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:214)    org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:203)    org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)    org.apache.curator.framework.imps.GetChildrenBuilderImpl.pathInForeground(GetChildrenBuilderImpl.java:199)    org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:191)    org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:38)    org.springframework.xd.dirt.server.JobDeploymentListener.recalculateJobStates(JobDeploymentListener.java:197)    org.springframework.xd.dirt.server.DeploymentSupervisor$LeaderListener.takeLeadership(DeploymentSupervisor.java:389)    org.apache.curator.framework.recipes.leader.LeaderSelector$WrappedListener.takeLeadership(LeaderSelector.java:536)    org.apache.curator.framework.recipes.leader.LeaderSelector.doWork(LeaderSelector.java:398)    org.apache.curator.framework.recipes.leader.LeaderSelector.doWorkLoop(LeaderSelector.java:443)    org.apache.curator.framework.recipes.leader.LeaderSelector.access$100(LeaderSelector.java:63)    org.apache.curator.framework.recipes.leader.LeaderSelector$2.call(LeaderSelector.java:244)    org.apache.curator.framework.recipes.leader.LeaderSelector$2.call(LeaderSelector.java:238)    java.util.concurrent.FutureTask.run(FutureTask.java:266)    java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)    java.util.concurrent.FutureTask.run(FutureTask.java:266)    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)    java.lang.Thread.run(Thread.java:745)  {noformat}    No specific details on reproducing yet; although the Socialcast thread indicates:    {quote}  I only hit this when I have tried to deploy a job that fails deployment the first time  {quote}",5
"Document Kafka source
nan",1
"Provide a way to customize the isolation level of the JobRepository
The Gemfire XD database cannot be used to store the Spring XD metadata because the former doesn't support the default Spring Batch transaction isolation level ISOLATION_SERIALIZABLE.  There looks to be no way to configure the Spring XD's internal Spring Batch JobRepository with another isolation level.  The JobRepository instance is getting created with default settings by the Spring Batch'es {{SimpleBatchConfiguration}} and there are no custom {{BatchConfigurer}}s available to change the default settings of the JobRepository.",1
"Shell does not report failed deploy attempt 
During the slow network tests the user undeployed a stream and then immediately redeployed the same stream to get the modules on different containers.  The deployment failed as reflected in the stacktrace below from the admin server, however the the shell did not report an error and the user could not deploy the stream.   * The stream in question is ""http|log""  * The Shell did not report any error.   * Stream list does show the state of the stream as failed.  * executing a stream deploy fails with the following error:     ** Command failed org.springframework.xd.rest.client.impl.SpringXDException: The stream named 'foo' is already deployed  * Undeploy and deploy of the stream worked.   17:54:39,487  INFO Deployer server.StreamDeploymentListener - Deployment status for stream 'foo': DeploymentStatus{state=failed,error(s)=org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /xd/deployments/modules/allocated/73f0a93d-e213-414d-8337-6c04409ec210/foo.source.http.1/status   org.apache.zookeeper.KeeperException.create(KeeperException.java:111)   org.apache.zookeeper.KeeperException.create(KeeperException.java:51)   org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:1155)   org.apache.curator.framework.imps.GetDataBuilderImpl$4.call(GetDataBuilderImpl.java:302)   org.apache.curator.framework.imps.GetDataBuilderImpl$4.call(GetDataBuilderImpl.java:291)   org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)   org.apache.curator.framework.imps.GetDataBuilderImpl.pathInForeground(GetDataBuilderImpl.java:287)   org.apache.curator.framework.imps.GetDataBuilderImpl.forPath(GetDataBuilderImpl.java:279)   org.apache.curator.framework.imps.GetDataBuilderImpl.forPath(GetDataBuilderImpl.java:41)   org.springframework.xd.dirt.server.ModuleDeploymentWriter.writeDeployment(ModuleDeploymentWriter.java:205)   org.springframework.xd.dirt.server.ModuleDeploymentWriter.writeDeployment(ModuleDeploymentWriter.java:163)   org.springframework.xd.dirt.server.StreamDeploymentListener.deployStream(StreamDeploymentListener.java:166)   org.springframework.xd.dirt.server.StreamDeploymentListener.onChildAdded(StreamDeploymentListener.java:100)   org.springframework.xd.dirt.server.InitialDeploymentListener$EventHandler.call(InitialDeploymentListener.java:217)   org.springframework.xd.dirt.server.InitialDeploymentListener$EventHandler.call(InitialDeploymentListener.java:186)   java.util.concurrent.FutureTask.run(FutureTask.java:262)   java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)   java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)   java.lang.Thread.run(Thread.java:745) } 17:54:39,496  INFO Deployer server.StreamDeploymentListener - Stream Stream{name='foo'} deployment attempt complete",3
"Health Actuator Endpoint (JMX)  Needs to portray an accurate status for Admin or Containers
In cases where the zookeeper ensemble is down or does not have quorum, the xd cluster effectively stops all work.  However the health status of the containers is ""Up"".  The container health status should be ""Awaiting Zookeeper Session""  until it can get a successful zookeeper session.",3
"Throughput in a stream with any processor
One of the goal for a micro benchmark is to compare throughput difference  between two types of streams: 1. source | sink 2. source | processor | sink  For this test I used reactor-tcp source, throughput-sampler as sink and created a NoOp processor. Tests were performed on a single node container with direct binding turned on for all streams.  1. Throughput for ""source|sink""  {noformat} stream create reactortcp --definition ""reactor-ip --transport=tcp --port=4000 | throughput-sampler"" stream deploy reactortcp --properties module.*.count=0 {noformat}  On my system I get following numbers: Throughput sampled for 5000000 items: 345423/s in 14475ms elapsed time  2. Throughput for ""source|processor|sink""  Code for NoOpProcessor is available here: https://github.com/parikhkc/xd-noop-processor  {noformat} stream create reactornoop --definition ""reactor-ip --transport=tcp --port=5000 | noopprocessor | throughput-sampler"" stream deploy reactornoop --properties module.*.count=0 {noformat}  On the same system the throughput reduces to less then 70K/sec. Throughput sampled for 5000000 items: 67250/s in 74349ms elapsed time  Yourkit shows 50% of CPU time on following thread:  {noformat} * ringBuffer-17 [RUNNABLE] [DAEMON] java.lang.reflect.Method.getParameterAnnotations() Method.java:770 org.springframework.xd.integration.reactor.net.NetServerInboundChannelAdapter$1.accept(Object) NetServerInboundChannelAdapter.java:53 reactor.net.AbstractNetChannel$3.accept(Event) AbstractNetChannel.java:131 reactor.net.AbstractNetChannel$3.accept(Object) AbstractNetChannel.java:128 reactor.event.routing.ArgumentConvertingConsumerInvoker.invoke(Consumer, Class, Object) ArgumentConvertingConsumerInvoker.java:73 reactor.event.routing.ConsumerFilteringEventRouter.route(Object, Event, List, Consumer, Consumer) ConsumerFilteringEventRouter.java:78 reactor.event.dispatch.AbstractLifecycleDispatcher.route(AbstractLifecycleDispatcher$Task) AbstractLifecycleDispatcher.java:64 reactor.event.dispatch.AbstractSingleThreadDispatcher$SingleThreadTask.run() AbstractSingleThreadDispatcher.java:50 reactor.event.dispatch.RingBufferDispatcher$3.onEvent(RingBufferDispatcher$RingBufferTask, long, boolean) RingBufferDispatcher.java:115 reactor.event.dispatch.RingBufferDispatcher$3.onEvent(Object, long, boolean) RingBufferDispatcher.java:112 com.lmax.disruptor.BatchEventProcessor.run() BatchEventProcessor.java:128 java.lang.Thread.run() Thread.java:745  {noformat} ",1
"Module (re) deployment failed after ZK Cluster Ensemble lost quorum
When simulating a slow network by deploying a Zookeeper with 3 nodes. * Zookeeper 1 (follower)was located at US-East-1 * Zookeeper 2 (follower) was in Sydney  * Zookeeper 3 (Leader) was in Sydney XD Admin and containers were running in US-East-1 Zone  In this case we simulated a loss of quorum by killing the zookeeper 2 (follower in Sydney).  All modules were undeployed.  When I restarted the zookeeper 2 all containers and admin recognized the ensemble was up and tried to redeploy the modules, but the stream was left in a ""Failed"" state.  Steps to reproduce:  * Deploy Stream ""http|file"" * Start Data Flow to stream from JMeter * Terminate ZK Follower in AU * Restart ZK Follower in AU  The workaround is to undeploy and deploy the stream.",3
"Add support for Pivotal HD 2.1 (XD 1.0.2 Release)
*XD 1.0.2 Release + PHD 2.1 Upgrade - Action Items:*    * Update to SHDP 2.0.3  * Add Hadoop 2.5 (hadoop25)  * Change PHD 2.x from phd20 to phd21  * Test PHD 2.0 with phd21   * Document that both PHD 2.1 and PHD 2.0 is supported with phd21    ",3
"Add remote partitioning on 'jdbchdfs' job
As a user, I'd like to have the option to supply data partitioning strategy so that I can parallelize ingest of data from RDBMS to HDFS.",8
"Add HDFS source
As a user, I'd like to have the option of _HDFS_ source module so that I can ingest data directly from HDFS file system.",3
"Expose property to change ""commit-interval""
As a user, I'd like to override the default ""commit-interval"" so that I can configure commit interval depending on data volume.    *Note:*  This would apply for all OOTB jobs that has partition support. The property could be part of _servers.yml_ file.",3
"Document how to enable SSL and Basic authentication 
As a user, I want to know my configuration options are for enabling SSL/HTTPS and Basic authentication for administration endpoints, so that I can secure my application.",1
"Document how to enable LDAP security for admin endpoints
As a user, I want to know how to enable and configure LDAP as an authentication provider for the administration server, so that I can set up my security configuration accordingly.",1
"Fix 'cluster/containers' REST endpoint with security enabled
Once the container's management server is secured, the admin server needs to know which REST template to use to get the message rates from the deployed modules inside the containers.",3
"Fix 'cluster/containers' REST endpoint with security enabled
Once the container's management server is secured, the admin server needs to know which REST template to use to get the message rates from the deployed modules inside the containers.",3
"Fix 'cluster/containers' REST endpoint with security enabled
Once the container's management server is secured, the admin server needs to know which REST template to use to get the message rates from the deployed modules inside the containers.",3
"Fix 'cluster/containers' REST endpoint with security enabled
Once the container's management server is secured, the admin server needs to know which REST template to use to get the message rates from the deployed modules inside the containers.",3
"testHdfsSink  sporadically fails because twitter data is written to file.
The hdfs sink needs to have a unique stream name.  Because the twitterSearch test uses the ""defaultName"" and it may broadcast more than one message to the sink for the search.  So when the stream is destroyed the message is abandoned until hdfsTest starts up using the same  ""defaultName"" and the message is delivered to hdfs sink and thus the twitter data is written to the file.  ",3
"FilePollHdfs sporadically fails 
Need to add a retry to the mkdir command, in the case that it fails.",3
"Fix 'cluster/containers' REST endpoint if security is enabled
If the security is enabled for the container, then admin server won't be able to fetch the message rates for the deployed modules in that container.  The REST endpoint 'cluster/containers' needs to be fixed.",3
"xd-shell from 1.0.1 doesn't work with 1.0.0 GA admin
Targeting xd-shell from 1.0.1 to 1.0.0 GA admin server fails    server-unknown:>admin config info    -------------  -------------------------------------------------------------    Result         Unable to contact XD Admin Server at 'http://localhost:9393'.    Target         http://localhost:9393    Timezone used  Pacific Standard Time (UTC -8:00)    -------------  -------------------------------------------------------------  -------------------------------------------------------------------------------  An exception ocurred during targeting:  java.lang.NullPointerException      at org.springframework.xd.rest.client.impl.SpringXDTemplate.<init>(SpringXDTemplate.java:110)      at org.springframework.xd.rest.client.impl.SpringXDTemplate.<init>(SpringXDTemplate.java:137)      at org.springframework.xd.shell.command.ConfigCommands.target(ConfigCommands.java:106)      at org.springframework.xd.shell.command.ConfigCommands.afterPropertiesSet(ConfigCommands.java:191)  ",1
"Fix doc related to ZK install
Amongst (maybe) other things, the doc says we don't ship ZK:  http://docs.spring.io/spring-xd/docs/1.0.0.BUILD-SNAPSHOT/reference/html/#_setting_up_zookeeper",1
"Fix failing script integration test
See https://build.spring.io/browse/XD-SCRIPTS-723",3
"Add authentication support to the mongo sink
nan",2
"JsonStringToObjectTransformerTests fail on mac if local machine name is not in /etc/hosts
When running the gradlew build the JsonStringToObjectTransformerTests unit test failed.  The cause is enumerated below: Caused by: com.gemstone.gemfire.internal.tcp.ConnectionException: While creating ServerSocket and Stub on port 0 with address glenns-mbp/192.168.1.254  If I added 127.0.0.1 glenns-mbp to my /etc/hosts the tests succeeded.  ",2
"Have counters use Double arithmetic instead of Long
Now that we have --incrementExpression, we often want to increment by some domain specific value that may not be integral (e.g. Watts in smartgrid demo)",4
"Refactor ContainerRegistrar
Decouple from DeploymentListener. Make DL a public class to handle deployment related events.  Remove createSimpleModule() and createComposedModule() from DL.  This will be delegated to ModuleDeployer which will eventually be further refactored to use the proposed ModuleFactory.",3
"Create ModuleFactory
ModuleFactory is responsible for determining how the Module application context is created from available sources at the resource location exposed via the ModuleDefinition. The factory creates the application context and creates a SimpleModule or CompositeModule as appropropriate. For example, if an XML file is present, it is assumed to be the bean definition file used to create a CXMLAC. If no XML file is present, inspect the properties file for the existence of well-known properties such as base-package-name for component scanning for an @Configuration, or a module-class-name for an Annotated POJO based module (see XD-2100).  The MF is also responsible for creating composite modules.   Also includes Module refactoring, add getApplicationContext() and probably setApplicationContext().  Also refactor CompositeModule code to use boot SpringApplicationBuilder    ",5
"Create CompositeModuleRegistry
Currently there is no ModuleRegistry contract for composite modules. The composed module definition is maintained in ZooKeeperModuleDefinitionRepository which delegates to the ModuleRegistry to define and validate component modules. The repository should be converted to a ModuleRegistry.",5
"Simplify ModuleRegistry
MR is responsible for looking up existing module definitions by name and type.  ModuleDefinition should contain name, type, and resource; where resource is a springframework.core.io.Resource containing the root path of the module definition. This could be file, classpath, http, hdfs or something else but the contents under this path will not be inspected or processed by MR. The exception is for composite modules which should contain a list of corresponding Resources.    Simplify ModuleDefinition - remove classpath. Provide support for CompositeModuleDefinition - requires a list of resources (possibly a subclass)    Also retire RedisModuleRegistry  ",8
"Add option to set PARTITION_HEADER
Build on top of https://github.com/spring-projects/spring-integration-extensions/pull/116/files wrt the partition header to use (see https://github.com/spring-projects/spring-xd/pull/1200#issuecomment-57622400)",5
"Exception in a tap will stop the tapped stream from sinking data
Exception in a tap will stop the tapped stream from sinking data.  h2. Background Running xd-singlenode. We experienced this when streaming data from a rabbit queue to hdfs. The stream was tapped and we had a groovy processor on the tap stream. Any exceptions in the processor stopped the main stream from writing data to the hdfs sink.  h2. Steps to reproduce. 1: Create a groovy script that throws an exception in  modules/processor/scripts/exceptionthrower.groovy. Code below {code} /**  * Custom processor to be wired into a tap to throw an exception.  */  throw new RuntimeException(""Error from processor"") {code} 2: Create a sample main stream {code} xd:>stream create --name ticktock --definition ""time | log"" --deploy {code} 3: Tail the log to confirm the data is going to the sink. We see  'sink.ticktock' appearing in the log as expected. 4: Add a tap to the stream that will throw an exception. {code} xd:>stream create --name exTap --definition ""tap:stream:ticktock > script --location=exceptionthrower.groovy | log"" --deploy {code} 5: Tail the log and we see that there are no more 'sink.ticktock' strings being logged. Looks like the main stream is no longer sending messages to the sink. ",5
"Release 1.0.1
nan",5
"Add Python processor
As a user, I'd like to have a _Python_ processor so that I can efficiently perform data computations and statistical analysis.     Investigate the right approach (native or via stdin/stdout) that fits Spring XD model.    [Integrate Java and Python|https://wiki.python.org/moin/IntegratingPythonWithOtherLanguages#Java]",8
"Add R processor
As a user, I'd like to have a _R_ processor, so I can efficiently perform data computations and statistical analysis in the context of streaming pipeline.   Investigate the right approach that fits Spring XD model.  *R Java Libraries* [rJava|http://rforge.net/rJava/] [Renjin|http://www.renjin.org/]",8
"Add Hive sink
As a user, I would like to have an option to write data into _Hive_ sink so that I can query and manage large datasets in distributed storage.",5
"Add acceptance test to extract and assert payload from JSON object
To enrich the acceptance test, I'd like to evaluate JSON object to extract ""Good"" and ""Bad"" instead of just relying on a basic filter test to assert the payload content. ",3
"Add acceptance test to include Gemfire use case
To enrich acceptance test, I'd like to have basic coverage to evaluate Gemfire use cases.     An example would be to ingest data from HTTP source and write it to Gemfire server.",2
"Enhance JDBC sink test to include more options
To enrich acceptance test, I'd like to add coverage to JDBC sink by including *-- driverclass* and *-- url* options.",2
"Exclude DB initialization for JDBC sink test
To enrich acceptance test, I'd like to lazy initialize DB for JDBC Sink acceptance tests. Check to see if it is already initialized and decide. ",2
"Add acceptance tests for FieldValueCounts and AggregateCounts
To enrich acceptance tests, I'd like to have test coverage to evaluate _FieldValueCounts_ and _AggregateCounts_ for a given scenario.",3
"Add stress test
To enrich acceptance test coverage, I'd like to have stress test scenario that includes ingesting data from _HTTP_ and then writing it to _Log_ sink.",3
"HTTPS Source Configuration issues
1. The sample {{httpSSLproperties}} file that is included in the distribution contains the line:  {quote} keystore.passPhrase=secret {quote}  The correct key value is {{keyStore.passPhrase}}. This issue causes HTTPS sources to deploy, but not bind to the port.  2. The password is always defaulting to ""secret""",1
"NPE in ContainerRedeploymentTests
Running the distributed tests ({{-Drun_distributed_tests=true}}) against d109a3a and got the following:    {noformat}     java.lang.NullPointerException       org.springframework.xd.dirt.module.store.ZooKeeperModuleMetadataRepository.updateDeploymentStatus(ZooKeeperModuleMetadataRepository.java:209)       org.springframework.xd.dirt.module.store.ZooKeeperModuleMetadataRepository.findAllByContainerId(ZooKeeperModuleMetadataRepository.java:313)       org.springframework.xd.dirt.container.store.ZooKeeperContainerRepository.findAllRuntimeContainers(ZooKeeperContainerRepository.java:339)       org.springframework.xd.dirt.rest.ContainersController.list(ContainersController.java:97)       sun.reflect.GeneratedMethodAccessor100.invoke(Unknown Source)       sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)       java.lang.reflect.Method.invoke(Method.java:483)       org.springframework.web.method.support.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:215)       org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:132)       org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:104)       org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandleMethod(RequestMappingHandlerAdapter.java:749)       org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:689)       org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:83)       org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:938)       org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:870)       org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:961)       org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:852)       javax.servlet.http.HttpServlet.service(HttpServlet.java:620)       org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:837)       javax.servlet.http.HttpServlet.service(HttpServlet.java:727)       org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:  {noformat}",2
"Upgrade to Spring 4.1.2, SI 4.1.0, SA 1.4.0
As a user, I'd like to have Spring 'Core' upgraded to 4.1.1 (_milestone_ ) so that I can benefit from performance improvements associated with 'compiled' SpEL and other enhancements.   ",3
"Generated pom does not include spring boot 1.1. dependency
The change in XD 1.0.1 to use Spring Boot 1.1.7 meant that we can't pickup the boot dependency by importing the platform-bom.  Andy W's suggestion is to change the generated pom to refer to the platform-bom as a parent (not import) and then declare the explicit dependency on Boot 1.1.7",5
"Add support to define granular security definitions by entity (ACL)
As a user, I'd like to define security definitions so that I can configure entity (REST API) specific group/role access policies.",8
"Add support to configure security definitions via Shell (ACL)
As a user, I'd like to have the ability to configure ACLs so that I can restrict access to resources accessed via DSL Shell.  *Examples:* * Who can create streams? * Who can destroy the streams? * Who can view the streams? ??(defaults to all)??",8
"Add support to configure security definitions via Admin UI (ACL)
As a user, I'd like to have the ability to configure ACLs so that I can restrict access to resources accessed via Admin UI.    *Examples:*  * Who can create streams?  * Who can destroy the streams?  * Who can view the streams? ??(defaults to all)??",8
"global context reference is incorrect in ModuleDeployer
see ModuleDeployer.setApplicationContext(). It appears the shared server context (child of global) is incorrectly set as the module context's parent. However, the first attempt caused some test failures, so this needs further investigation. Things still function correctly, but is not according to design. ",1
"Provide proper ordering for all REST endpoints
With the implementation of XD-1864, we need to make sure that the (paginated) data returned from the REST endpoints has proper default ordering.    Up to now we have done client-side ordering in the Admin UI, but with server-side pagination, the server-side should support proper pagination as well.    Eventually, we may even decide to provide more flexible ordering options (ASC vs DESC, sort on different properties etc.), which may be a separate Jira.",8
"REST: Make the Configurations REST endpoint pagination-aware
Add pagination for:    http://localhost:9393/jobs/configurations    Related to XD-1864",2
"REST: Make the Job Execution REST endpoint pagination-aware
Implement pagination for:    http://localhost:9393/jobs/executions",8
"Add support for configurable ZK namespace
As a user, I'd like to have the flexibility to change the namespace so that I can isolate ZK _metadata_ based on each _tenant_ profile. ",5
"Need to set small commit level for Acceptance tests.  
XD-2180 introduced a default commit level for jobs to be 1000, vs the original 100.  Now tests sporadically fail.  Need to set the --commitInterval for the tests to a small value.",3
"Remove logging of password in Shell
As a user, I'd like to type the _username_ and _password_ to gain access to Admin server so that I don't have to add it in some file; hence I don't have to worry about having the password getting logged somewhere.",5
"When using --deletefile option for filejdbc all files are deleted before processing complete
In this scenario I copied 2 files (fakeFile.csv, anotherFile.csv) into the bar directory.   * Created thew following job: job create myjob2 --definition ""filejdbc --resources=file:bar/*.csv --names=name --tableName=people2 --initializeDatabase=true --commitInterval=1 --deleteFiles=true"" --deploy * Then launched the job: job launch myjob2  * The result was that the first file was processed, but then the module deleted all *.csv files in the directory before processing the 2nd file.   * The net result was that the job failed and the 2nd file was not processed. ",3
"Fix the configuration problem with Filter and Transform modules
As a user, I'd like to leverage _propertieis-location_ parameter while using *Filter* or *Transform* modules so that I can load the user-defined properties included in the external properties file.     Attempting to include the _propertieis-location_ attribute errors out - refer to the attachment.    It could also be beneficial to load user-defined properties through stream definition similar to deployment properties.    Example:  --script=myscript.groovy --variables=foo=bar,goo=gaz  ",5
"Document job repo schema overrides
As a user, I'd like to have the flexibility to configure DB creds so that I can use a DB of choice for batch job repository (metadata persistence).     The scope of this task is to have the configuration specifics documented in the wiki.",2
"Add support for estimating the median value of a data stream.
Often it is useful to have access to the median value for fields of a data stream since they are more robust with respect to outliers.  The median is defined as the value of a dataset such that, when sorted, 50% of the data is smaller than the value and 50% of the data is larger then the value. Ordinarily this is difficult to calculate on a stream because it requires the collection and sorting of all data.  The median of a data stream can be approximated with a technique called stochastic averaging. To approximate the median value of a data stream one could use the following approach: Given the current estimate of the median {{M}}. If the next observed value in the stream is larger than {{M}}, increase the current estimate by {{r}} (= the learning rate). If it is smaller, decrease the estimate by {{r}}. When {{M}} is close to the median, it increases as often as it decreases, and therefore it stabilizes.   The following example shows a primitive implementation of the above mentioned algorithm in groovy (to be placed under ${XD_HOME}/modules/processor/scripts). {code:groovy} import org.springframework.xd.tuple.TupleBuilder; import org.springframework.xd.tuple.Tuple;  /**  * Stochastic averaging to compute the median of a data stream.  * To approximate this value using stochastic optimization, the value  * of interest is the current estimate of the median M. If the next observed   * value of the stream is larger than M, increase by r. If it is smaller decrease the  * estimate by r. When M is close to the median, it increases as often as it decreases,  * and therefore stablilizes.  * r denotes the learningRate.  */ enum MedianEstimator{      INSTANCE    //TODO Add support for estimating multiple medians   /*    * The current median value.    */   double current = Double.POSITIVE_INFINITY      public double update(double value, double learningRate){          //Initialize current with given value     if(current == Double.POSITIVE_INFINITY){        current = value     }          if(current == value){       return current     }      //Move current value towards the median     current = current + (current < value ? learningRate : -learningRate)      return current   } }  //TODO Make learning rate configurable def learningRate = 0.7 //TODO Add support for dynamic field selection. double median = MedianEstimator.INSTANCE.update(payload.getDouble('value'), learningRate)  def fieldNames = new ArrayList<String>(payload.getFieldNames()) def fieldValues = new ArrayList<Object>(payload.getValues())  //TODO Add make median output field configurable fieldNames.add(""value_median"") fieldValues.add(median)  //Return the original tuple values extended with the computed median. TupleBuilder.tuple().ofNamesAndValues(fieldNames, fieldValues) {code}  Stream definition: {code} xd:>stream create median --definition ""http --outputType=application/x-xd-tuple | script --location=median-est.groovy | log"" --deploy Created and deployed new stream 'median' {code}  Post some JSON data... {code} xd:>http post --contentType application/json --data '{""value"":2}' > POST (application/json;charset=UTF-8) http://localhost:9000 {""value"":2} > 200 OK {code}  Output: {code} 20:44:37,829  INFO pool-35-thread-4 sink.median - {""id"":""cd9719b3-eeff-59c9-fdf1-fdb628c7fbb8"",""timestamp"":1413139477829,""value"":""2"",""value_median"":2.0} {code}  ... After ~15 the median value should stabilize.  This approach was taken from the book ""Real-time Analytics - Techniques to Analyze and Visualize Streaming Data"" P. 296 / Byron Ellis / Wiley  Open points: - Support for resetting the median value - Better state management (Redis?) - Support median estimation for multiple fields - Make learning rate configurable from outside - Maybe add this as aggregate-counter aggregation strategy?",5
"REST representation of an aggregate-counter can lead to mixed up output
The current representation of REST resources of time-series data (e.g. aggregate counter) can lead to problems in consuming applications.  Despite the time series data provided by the ""counts"" data structure is logically ordered by key (timestamps) it doesn't guarantee an ordering, since many consuming applications interpret JSON data as an unordered map like data structure.  Because of this consuming applications have to apply special ordering / transformation logic to get the data in an chronologically ordered fashion. It would be helpful if one could configure the rendering of the time series data, e.g. as a list of json object like: {code:json} {  ""ts"":""Sun Oct 12 23:10:23 CEST 2014""  ,""value"": 42 }  {code} Where {{ts}} denotes the timestamp and {{value}} denotes the value. It would also be helpful if one could adjust the date format either with a pattern or a well known date format like, c.f.  {{ISO 8601}}.  I attached a python example for this that demonstrates the problem.  Steps to reproduce: Create stream {code} xd:>stream create test --definition ""http | filter --expression=payload.contains('pivotal') | log"" {code}  Create tap on stream with aggregate-counter {code} xd:>stream create test_tap --definition ""tap:stream:test.filter > aggregate-counter"" {code}  Post some http data {code} xd:>http post --data ""Hello pivotal data labs"" #...some more data... {code}  Display aggregate counter {code} xd:>aggregate-counter display test_tap --resolution minute   AggregateCounter=test_tap   -----------------------------  -  -----   TIME                           -  COUNT   Sun Oct 12 23:10:23 CEST 2014  |  0   Sun Oct 12 23:11:23 CEST 2014  |  0   Sun Oct 12 23:12:23 CEST 2014  |  0   Sun Oct 12 23:13:23 CEST 2014  |  0   ...   Mon Oct 13 00:02:23 CEST 2014  |  0   Mon Oct 13 00:03:23 CEST 2014  |  0   Mon Oct 13 00:04:23 CEST 2014  |  0   Mon Oct 13 00:05:23 CEST 2014  |  0   Mon Oct 13 00:06:23 CEST 2014  |  0   Mon Oct 13 00:07:23 CEST 2014  |  0   Mon Oct 13 00:08:23 CEST 2014  |  3   Mon Oct 13 00:09:23 CEST 2014  |  1 {code}  Install python Requests library (REST support) {code} pip install Requests {code}  Start a python console (or IPythonNotebook) and run the following program: {code:python} import requests import json  res = requests.get(""http://localhost:9393/metrics/aggregate-counters/test_tap?resolution=minute"") status_object = json.loads(res.content) print(json.dumps(status_object, indent=4)) {code}  The above program should result in a similar output, but as one can see, due to pythons interpretation of the JSON object as a dict, the order of the keys in the output got mixed up.   Instead of showing the counts ....3 and then 1 ... as in the example above. This is just one example of how the current representation of the rest resource could lead to problems in consuming applications.   {code:json} {     ""counts"": {         ""2014-10-12T21:26:28.553Z"": 0,         ""2014-10-12T22:15:28.553Z"": 0,         ""2014-10-12T22:22:28.553Z"": 0,         ""2014-10-12T21:49:28.553Z"": 0,         ""2014-10-12T21:35:28.553Z"": 0,         ""2014-10-12T21:47:28.553Z"": 0,         ""2014-10-12T22:18:28.553Z"": 0,         ""2014-10-12T22:12:28.553Z"": 0,         ""2014-10-12T22:16:28.553Z"": 0,         ""2014-10-12T21:57:28.553Z"": 0,         ""2014-10-12T21:28:28.553Z"": 0,         ""2014-10-12T22:08:28.553Z"": 3,         ""2014-10-12T21:40:28.553Z"": 0,         ""2014-10-12T22:06:28.553Z"": 0,         ""2014-10-12T21:27:28.553Z"": 0,         ""2014-10-12T21:52:28.553Z"": 0,         ""2014-10-12T22:11:28.553Z"": 0,         ""2014-10-12T22:05:28.553Z"": 0,         ""2014-10-12T21:29:28.553Z"": 0,         ""2014-10-12T21:24:28.553Z"": 0,         ""2014-10-12T21:56:28.553Z"": 0,         ""2014-10-12T21:43:28.553Z"": 0,         ""2014-10-12T22:00:28.553Z"": 0,         ""2014-10-12T22:10:28.553Z"": 0,         ""2014-10-12T21:58:28.553Z"": 0,         ""2014-10-12T22:21:28.553Z"": 0,         ""2014-10-12T21:32:28.553Z"": 0,         ""2014-10-12T21:46:28.553Z"": 0,         ""2014-10-12T22:04:28.553Z"": 0,         ""2014-10-12T22:02:28.553Z"": 0,         ""2014-10-12T21:51:28.553Z"": 0,         ""2014-10-12T21:38:28.553Z"": 0,         ""2014-10-12T21:31:28.553Z"": 0,         ""2014-10-12T22:20:28.553Z"": 0,         ""2014-10-12T21:54:28.553Z"": 0,         ""2014-10-12T22:07:28.553Z"": 0,         ""2014-10-12T22:03:28.553Z"": 0,         ""2014-10-12T21:34:28.553Z"": 0,         ""2014-10-12T22:09:28.553Z"": 1,         ""2014-10-12T21:44:28.553Z"": 0,         ""2014-10-12T22:17:28.553Z"": 0,         ""2014-10-12T21:53:28.553Z"": 0,         ""2014-10-12T22:19:28.553Z"": 0,         ""2014-10-12T21:30:28.553Z"": 0,         ""2014-10-12T22:23:28.553Z"": 0,         ""2014-10-12T21:36:28.553Z"": 0,         ""2014-10-12T21:41:28.553Z"": 0,         ""2014-10-12T22:13:28.553Z"": 0,         ""2014-10-12T21:59:28.553Z"": 0,         ""2014-10-12T22:01:28.553Z"": 0,         ""2014-10-12T21:33:28.553Z"": 0,         ""2014-10-12T21:45:28.553Z"": 0,         ""2014-10-12T21:39:28.553Z"": 0,         ""2014-10-12T21:50:28.553Z"": 0,         ""2014-10-12T21:37:28.553Z"": 0,         ""2014-10-12T22:14:28.553Z"": 0,         ""2014-10-12T21:25:28.553Z"": 0,         ""2014-10-12T21:55:28.553Z"": 0,         ""2014-10-12T21:42:28.553Z"": 0,         ""2014-10-12T21:48:28.553Z"": 0     },     ""name"": ""test_tap"",     ""links"": [         {             ""href"": ""http://localhost:9393/metrics/aggregate-counters/test_tap"",             ""rel"": ""self""         }     ] } {code}",8
"Incorrect port in resource manager address overwrite
the resource manager address overwrite is setting the port to 8032; the value cannot be set in servers.yml.  this occurs when pushing the config to hdfs and also when attempting to start the admin server on yarn.       [ConfigurationFactoryBean] - Overwriting rmAddress=[0.0.0.0:8032] with rmAddress=[host:8032]    [root spring-xd-1.0.1.RELEASE-yarn]# ./bin/xd-yarn start admin      .   ____          _            __ _ _   /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \  ( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \   \\/  ___)| |_)| | | | | || (_| |  ) ) ) )    '  |____| .__|_| |_|_| |_\__, | / / / /   =========|_|==============|___/=/_/_/_/   :: Spring Boot ::        (v1.1.7.RELEASE)    2014-10-13 16:50:28,710 INFO [ConfiguringBeanFactoryPostProcessor] - No bean named 'taskScheduler' has been explicitly defined. Therefore, a default ThreadPoolTaskScheduler will be created.  2014-10-13 16:50:28,724 INFO [ConfiguringBeanFactoryPostProcessor] - No bean named 'taskExecutor' has been explicitly defined. Therefore, a default SyncTaskExecutor will be created.  2014-10-13 16:50:30,311 INFO [SpringYarnConfiguration] - Enabling CLIENT for Yarn  2014-10-13 16:50:30,335 INFO [SpringYarnConfiguration] - We couldn't figure out if we could use existing configuration  2014-10-13 16:50:30,335 INFO [SpringYarnConfiguration] - Building configuration for bean 'yarnConfiguration'  2014-10-13 16:50:30,383 INFO [SpringYarnConfigBuilder] - Existing yarnConfiguration: null  2014-10-13 16:50:30,658 INFO [ConfigurationFactoryBean] - Overwriting fsUri=[hdfs://host:8020] with fsUri=[hdfs://host:8020]  2014-10-13 16:50:30,659 INFO [ConfigurationFactoryBean] - Overwriting rmAddress=[0.0.0.0:8032] with rmAddress=[host:8032]          ",1
"Basic authentication realm is always 'null'
Besides the Basic authentication realm being always {{null}}, {{security.basic.realm}} is always ignored.",1
"Provide Python module to handle I/O for implementing a Python shell processor 
nan",2
"Improve module deployment distribution
When a container joins the XD cluster (via ZooKeeper) it triggers stream/job module deployments for modules that need to be deployed. If multiple containers are being started at around the same time, this can result in the first few containers taking all of the deployments while leaving the rest without any deployments.    To solve this, we will introduce a ""quiet period"" where no deployments will be triggered within _n_ seconds of a container joining, where _n_ will have a default value (perhaps 5 to 10 seconds). This value will be configurable.",5
"Fix incorrect IP Address associated with containers
The XD Container IP address displayed on both 'singlenode' and Distributed modes are incorrect both on _Shell_ as well as _Admin-UI_.     *Example:*  Noticed IP address as 10.10.10.*    Following function in [RuntimeUtils|http://docs.spring.io/autorepo/docs/spring-xd/1.0.1.RELEASE/api/org/springframework/xd/dirt/util/RuntimeUtils.html] could be flawed:  {code}  public static String getIpAddress() {  	try {  		for(Enumeration<NetworkInterface> enumNic = NetworkInterface.getNetworkInterfaces();  				enumNic.hasMoreElements();) {  			NetworkInterface ifc = enumNic.nextElement();  			if (ifc.isUp()) {  				for (Enumeration<InetAddress> enumAddr = ifc.getInetAddresses();  						enumAddr.hasMoreElements(); ) {  					InetAddress address = enumAddr.nextElement();  					if (address instanceof Inet4Address && !address.isLoopbackAddress()) {  						return address.getHostAddress();  					}  				}  			}  		}  	}  	catch (IOException e) {  		// ignore  	}    	return ""unknown"";  }  {code}  ",5
"Redis sink: better handling of module options/profile activation
Please see the discussion here:  https://github.com/spring-projects/spring-xd/pull/1188#discussion_r18788216",2
"NoNode Exception in SpringXD Admin
Sorry to set it ""Blocker""; but the problem makes SpringXD unusable. We are getting this weird, NoNode exception on the status ZNode. Example and Log given below. Once this happens, both streams and jobs cannot be deployed. For whatever reason the ""status"" znode goes missing.    The only way for us to get the cluster back to working state, is to clear the zk znode /xd tree and restart spring-xd. At which point, we have to recreate all our streams and jobs back again..   /xd/deployments/streams/testCreateStream_SrcHttp_SinkFile1413234903170/status  NoNode Exception:  13 Oct 2014 14:16:16,044   INFO DeploymentSupervisorCacheListener-0 server.InitialDeploymentListener - Path cache event: /deployments/streams/testDestroyStream1413234903170, type: CHILD_REMOVED 13 Oct 2014 14:16:16,705   INFO DeploymentSupervisorCacheListener-0 server.InitialDeploymentListener - Path cache event: /deployments/streams/testListStreams1413234903170, type: CHILD_ADDED 13 Oct 2014 14:16:22,818   INFO DeploymentSupervisorCacheListener-0 server.InitialDeploymentListener - Path cache event: /deployments/streams/testListStreams1413234903170, type: CHILD_REMOVED 13 Oct 2014 14:16:23,585   INFO DeploymentSupervisorCacheListener-0 server.InitialDeploymentListener - Path cache event: /deployments/streams/testStreamSrcHttpTimeseriesSink1413234903170, type: CHILD_ADDED 13 Oct 2014 14:16:37,694   INFO DeploymentSupervisorCacheListener-0 server.InitialDeploymentListener - Path cache event: /deployments/jobs/filetsjob-newjob001, type: CHILD_ADDED 13 Oct 2014 14:16:49,950   INFO DeploymentSupervisorCacheListener-0 server.InitialDeploymentListener - Path cache event: /deployments/streams/testStreamSrcHttpTimeseriesSink1413234903170, type: CHILD_REMOVED 13 Oct 2014 14:16:54,490   INFO Deployer server.StreamDeploymentListener - Deployment status for stream 'testCreateStream_SrcHttp_SinkFile1413234903170': DeploymentStatus{state=failed,error(s)=Deployment of module 'ModuleDeploymentKey{stream='testCreateStream_SrcHttp_SinkFile1413234903170', type=sink, label='file'}' to container 'd03bccd6-524b-4ff8-84d2-88f3f6daac42' timed out after 30000 ms; Deployment of module 'ModuleDeploymentKey{stream='testCreateStream_SrcHttp_SinkFile1413234903170', type=source, label='http'}' to container '52abf1c8-ba45-4994-8324-6079b03c670c' timed out after 30000 ms} 13 Oct 2014 14:16:54,493  ERROR Deployer server.InitialDeploymentListener - Exception caught while handling event org.springframework.xd.dirt.zookeeper.ZooKeeperAccessException: KeeperErrorCode = NoNode for /xd/deployments/streams/testCreateStream_SrcHttp_SinkFile1413234903170/status         at org.springframework.xd.dirt.zookeeper.ZooKeeperUtils.wrapThrowable(ZooKeeperUtils.java:111)         at org.springframework.xd.dirt.zookeeper.ZooKeeperUtils.wrapThrowable(ZooKeeperUtils.java:95)         at org.springframework.xd.dirt.server.StreamDeploymentListener.deployStream(StreamDeploymentListener.java:185)         at org.springframework.xd.dirt.server.StreamDeploymentListener.onChildAdded(StreamDeploymentListener.java:100)         at org.springframework.xd.dirt.server.InitialDeploymentListener$EventHandler.call(InitialDeploymentListener.java:217)         at org.springframework.xd.dirt.server.InitialDeploymentListener$EventHandler.call(InitialDeploymentListener.java:186)         at java.util.concurrent.FutureTask.run(FutureTask.java:262)         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)         at java.lang.Thread.run(Thread.java:744) Caused by: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /xd/deployments/streams/testCreateStream_SrcHttp_SinkFile1413234903170/status         at org.apache.zookeeper.KeeperException.create(KeeperException.java:111)         at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)         at org.apache.zookeeper.ZooKeeper.setData(ZooKeeper.java:1270)         at org.apache.curator.framework.imps.SetDataBuilderImpl$4.call(SetDataBuilderImpl.java:274)         at org.apache.curator.framework.imps.SetDataBuilderImpl$4.call(SetDataBuilderImpl.java:270)         at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)         at org.apache.curator.framework.imps.SetDataBuilderImpl.pathInForeground(SetDataBuilderImpl.java:266)         at org.apache.curator.framework.imps.SetDataBuilderImpl.forPath(SetDataBuilderImpl.java:253)         at org.apache.curator.framework.imps.SetDataBuilderImpl.forPath(SetDataBuilderImpl.java:41)         at org.springframework.xd.dirt.server.StreamDeploymentListener.deployStream(StreamDeploymentListener.java:179)         ... 7 more 13 Oct 2014 14:16:56,251   INFO DeploymentSupervisorCacheListener-0 server.InitialDeploymentListener - Path cache event: /deployments/jobs/filetsjob-newjob001, type: CHILD_REMOVED 13 Oct 2014 14:17:08,179   INFO Deployer server.JobDeploymentListener - Deployment status for job 'filetsjob-newjob001': DeploymentStatus{state=failed,error(s)=Deployment of module 'ModuleDeploymentKey{stream='filetsjob-newjob001', type=job, label='filepolltimeseries'}' to container '244d5076-f69d-42a4-8110-3b046cea2667' timed out after 30000 ms} 13 Oct 2014 14:17:08,181  ERROR Deployer server.InitialDeploymentListener - Exception caught while handling event org.springframework.xd.dirt.zookeeper.ZooKeeperAccessException: KeeperErrorCode = NoNode for /xd/deployments/jobs/filetsjob-newjob001/status         at org.springframework.xd.dirt.zookeeper.ZooKeeperUtils.wrapThrowable(ZooKeeperUtils.java:111)         at org.springframework.xd.dirt.zookeeper.ZooKeeperUtils.wrapThrowable(ZooKeeperUtils.java:95)         at org.springframework.xd.dirt.server.JobDeploymentListener.deployJob(JobDeploymentListener.java:175)         at org.springframework.xd.dirt.server.JobDeploymentListener.onChildAdded(JobDeploymentListener.java:99)         at org.springframework.xd.dirt.server.InitialDeploymentListener$EventHandler.call(InitialDeploymentListener.java:217)         at org.springframework.xd.dirt.server.InitialDeploymentListener$EventHandler.call(InitialDeploymentListener.java:186)         at java.util.concurrent.FutureTask.run(FutureTask.java:262)         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)         at java.lang.Thread.run(Thread.java:744) Caused by: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /xd/deployments/jobs/filetsjob-newjob001/status         at org.apache.zookeeper.KeeperException.create(KeeperException.java:111)         at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)         at org.apache.zookeeper.ZooKeeper.setData(ZooKeeper.java:1270)         at org.apache.curator.framework.imps.SetDataBuilderImpl$4.call(SetDataBuilderImpl.java:274)         at org.apache.curator.framework.imps.SetDataBuilderImpl$4.call(SetDataBuilderImpl.java:270)         at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)         at org.apache.curator.framework.imps.SetDataBuilderImpl.pathInForeground(SetDataBuilderImpl.java:266)         at org.apache.curator.framework.imps.SetDataBuilderImpl.forPath(SetDataBuilderImpl.java:253)         at org.apache.curator.framework.imps.SetDataBuilderImpl.forPath(SetDataBuilderImpl.java:41)         at org.springframework.xd.dirt.server.JobDeploymentListener.deployJob(JobDeploymentListener.java:165)         ... 7 more 13 Oct 2014 14:17:10,553   INFO DeploymentSupervisorCacheListener-0 server.InitialDeploymentListener - Path cache event: /deployments/jobs/filetsjob-newjob001, type: CHILD_ADDED  ",1
"NullPointerException while fetching runtime containers
In SpringXD ver 1.0.1, runtime/containers fetches additional runtime modules information for each container.  When a user queries the runtime containers while a stream is being deploy it throws a NullPointerException.  See below:  15:56:02,829  INFO DeploymentSupervisorCacheListener-0 server.InitialDeploymentListener - Path cache event: path=/deployments/streams/testCreateHTTPStream_postData1413327352991, type=CHILD_ADDED 15:56:02,935  INFO Deployer server.StreamDeploymentListener - Deploying stream Stream{name='testCreateHTTPStream_postData1413327352991'} 15:56:05,069 ERROR http-nio-9393-exec-9 rest.RestControllerAdvice - Caught exception while handling a request java.lang.NullPointerException   org.springframework.xd.dirt.module.store.ZooKeeperModuleMetadataRepository.updateDeploymentStatus(ZooKeeperModuleMetadataRepository.java:209)   org.springframework.xd.dirt.module.store.ZooKeeperModuleMetadataRepository.findAllByContainerId(ZooKeeperModuleMetadataRepository.java:313)   org.springframework.xd.dirt.container.store.ZooKeeperContainerRepository.findAllRuntimeContainers(ZooKeeperContainerRepository.java:339)   org.springframework.xd.dirt.rest.ContainersController.list(ContainersController.java:97)   sun.reflect.GeneratedMethodAccessor133.invoke(Unknown Source)   sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)   java.lang.reflect.Method.invoke(Method.java:606)   org.springframework.web.method.support.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:215)   org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:132)   org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:104)   org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandleMethod(RequestMappingHandlerAdapter.java:749)   org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:689)   org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:83)   org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:938)   org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:870)   org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:961)   org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:852)   javax.servlet.http.HttpServlet.service(HttpServlet.java:620)   org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:837)   javax.servlet.http.HttpServlet.service(HttpServlet.java:727)   org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:303)   org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)   org.springframework.boot.actuate.trace.WebRequestTraceFilter.doFilterInternal(WebRequestTraceFilter.java:110)   org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)   org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)   org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)   org.springframework.boot.actuate.autoconfigure.EndpointWebMvcAutoConfiguration$ApplicationContextHeaderFilter.doFilterInternal(EndpointWebMvcAutoConfiguration.java:280)   org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)   org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)   org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)   org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:186)   org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:160)   org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)   org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)   org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:77)   org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)   org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)   org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)   org.springframework.web.filter.HttpPutFormContentFilter.doFilterInternal(HttpPutFormContentFilter.java:88)   org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)   org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)   org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)   org.springframework.boot.actuate.autoconfigure.MetricFilterAutoConfiguration$MetricsFilter.doFilterInternal(MetricFilterAutoConfiguration.java:89)   org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)   org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)   org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)   org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:220)   org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:122)   org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:501)   org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:171)   org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:103)   org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:116)   org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:408)   org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1070)   org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:611)   org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1736)   org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.run(NioEndpoint.java:1695)   java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)   java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)   org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)   java.lang.Thread.run(Thread.java:724) ",1
"Stream Definition Calls Times-out often
We are using the SpringXD REST endpoints for creating and managing streams. With Version 1.0.0 and 1.0.1, the Stream Definition API Call Times-out at times. Here is the log from the admin node.  Look at the 30000 ms in the logs. I have also left a few other lines around for context.   API Call: http://<hostname>:9393/jobs/definitions  We need to come up with a fix for this.   14 Oct 2014 17:40:28,062   INFO DeploymentSupervisorCacheListener-0 server.InitialDeploymentListener - Path cache event: /deployments/streams/sample, type: CHILD_ADDED 14 Oct 2014 17:40:28,198   INFO Deployer server.StreamDeploymentListener - Deploying stream Stream{name='sample'} 14 Oct 2014 17:40:38,847   INFO Deployer server.JobDeploymentListener - Deployment status for job 'filetsjob-sample002': DeploymentStatus{state=failed,error(s)=Deployment of module 'ModuleDeploymentKey{stream='filetsjob-sample002', type=job, label='filepollsomething'}' to container 'c77bc83e-bcba-4e4d-9753-e71f603566b1' timed out after 30000 ms} 14 Oct 2014 17:41:28,225   INFO Deployer server.StreamDeploymentListener - Deployment status for stream 'sample': DeploymentStatus{state=failed,error(s)=Deployment of module 'ModuleDeploymentKey{stream='sample', type=sink, label='something'}' to container 'f877a8e8-08b3-44f9-8f73-bf163acb0cef' timed out after 30000 ms; Deployment of module 'ModuleDeploymentKey{stream='sample', type=source, label='http'}' to container 'c77bc83e-bcba-4e4d-9753-e71f603566b1' timed out after 30000 ms} 14 Oct 2014 17:41:28,227   INFO Deployer server.StreamDeploymentListener - Stream Stream{name='sample'} deployment attempt complete ",1
"Streams sending to Job Queue issue
Look at the below Stream definition. This gets to ""deployed"" state even without the corresponding job. And then from there the same Job or any other Job can't be deployed and it goes to a hung state.     Here is an example of the Stream definition:    stream create --name jobName --definition ""file --ref=true --dir=/tmp/springxdsource/dropbox --pattern=*.csv > queue:job:filetsjob-sample002"" --deploy  ",1
"Gemfire Source throws classnotfound 
* Steps to reproduce.  ** Start admin & container.  ** Follow the instructions from https://github.com/spring-projects/spring-xd/wiki/Sources#gemfire-source  *** When you post the message is when the stacktrace shows up in the container.    Copying the gemfire-7.0.2.jar to the lib directory will resolve the error.    The stacktrace is attached.  ",1
"Document default behavior if config option is not present
As a user, I'd like to have the flexibility to specify config options for IP and Hostname so that I can list the correct configuration for XD Admin and XD Container servers in the Admin-UI and Shell. ",1
"Log version number in log files
When answering support questions, the first step is to determine what version of the software the customer is using. This question can be easily answered if we log the version as one of the fields in the log file. For example:    {noformat}  10:44:21,212 1.0.2.BUILD-SNAPSHOT  INFO DeploymentSupervisorCacheListener-0 server.ContainerListener - Container arrived: Container{name='431baa56-b23b-48fc-b37d-18b52231e799', attributes={ip=192.168.25.177, host=Patrick-Peralta-MacBook-Pro.local, groups=, pid=38004, id=431baa56-b23b-48fc-b37d-18b52231e799}}  {noformat}    This way when we receive log snippets (initial support inquires rarely include the entire log file) we can immediately determine if the issue has already been fixed in a later release.  ",2
"Gemfire Source and Sink deployments cause OOME PermGen
If you create more than 4 gemfire modules, the container will throw a OOME Permgen on Java 7.    The solution is to put a technote into the wiki for the gemfire modules stating that for Java 7 they will need to add the following to environment.   export JAVA_OPTS=""-XX:PermSize=256m""",2
"Automatically disable autostartup in module SmartLifeCycle components
Provide a BPP to do this in the StreamPlugin.  (Possibly JobPlugin as well). Remove auto-startup=""false"" in existing module configs.",2
"Improve type handling for jdbc sink
The jdbc sink is currently limited to handling the entire payload as a string and converting a single json object to row data. We should improve that and support the following input types:   - LinkedCaseInsensitiveMap (single row)  - List<LinkedCaseInsensitiveMap> (multiple rows as a batch insert)  - JSON string {""ID"":74488,""NAME"":""Foo"",""YEAR"":""2014""} (single row)  - JSON array [{""ID"":74488,""NAME"":""Foo"",""YEAR"":""2014""},{""ID"":74489,""NAME"":""Bar"",""YEAR"":""2014""}] (multiple rows as a batch insert)  - none of the above use payload.toString()  The above matches what the new jdbc source puts out (depending on outputType used)",5
"The HTTP Source creates the ChannelPipeline inefficiently
The ChannelPipelineFactory used by the HTTP source should cache expensive objects used by the ChannelPipeline between requests, because creating them every time is inefficient (and in the case of HTTPS it can become even more expensive). ",1
"Measure baseline performance of RabbitMQ using PerfTest (In-house)
nan",16
"Baseline tcp measurements (DB-1)
Using the [iperf tool|https://iperf.fr/], find out the transfer rate in MB/sec between three machines in a four machine configuration.",1
"Vary message size (DB-2)
Use a single producer, single consumer, prefetch size = 50.  Send 1M messages and increase or decrease so that a given test iteration takes about 2 minutes.    Vary the message size and measure the msg/sec rate and calculate data transfer rate in MB/sec.    *Message Sizes:*  100 bytes  1000  10,000  100,000     During the measurements look at the RabbitMQ Admin UI and see if the queue is backing up.",1
"Vary prefetch size (DB-3)
Use a single producer, single consumer, message size of 1000 bytes.   Send 1M messages and increase or decrease so that a given test iteration takes about 2 minutes.    Vary the prefetch size.  Measure the msg/sec rate and calculate the data transfer rate in MB/sec.    *Prefetch Sizes:*  * 1  * 10  * 50  * 100  * 10000    During the measurements look at the RabbitMQ Admin UI and see if the queue is backing up.",1
"Vary consumers size (DB-4)
Using a single producer, message size of 1000 bytes, Pretch of 100.   Send 1M messages and increase or decrease so that a given test iteration takes about 2 minutes.    Vary the number of consumers.   Measure the msg/sec rate and calculate the data transfer rate in MB/sec.    *Number of consumers:*  * 1  * 2  * 4  * 6  * 10  * 50    During the measurements look at the RabbitMQ Admin UI and see if the queue is backing up.",1
"Vary producers size (DB-5)
Use the number of consumers gave a maximum throughput in the previous test (say 10 consumers), message size 100 bytes, Prefetch 100.   Send 1M messages    Vary the number of producers.  Measure the msg/sec rate and calculate the data transfer rate in MB/sec.    *Number of producers:*  * 2  * 4  * 6  * 10  * 50    During the measurements look at the RabbitMQ Admin UI and see if the queue is backing up.",1
"Research adding support for 'spring-cloud-config' to configure Modules
Please refer to the GH Issue reported here: https://github.com/spring-projects/spring-xd/issues/1218",8
"Infrastructure for RabbitMQ Cluster (DB)
Pre-requisite for Rabbit MQ Benchmarks:    * Infrastructure setup  * Configuration changes  * Tool-chain setup",1
"Document user-defined permission to role mapping
As a user, I'd like to have the option to configure permissions so that I'll have the flexibility to bind permissions (REST endpoint) to a specific role.   Default Roles: * Admin (CRUD) * Viewer (R)",2
"Document 'idleTimeout' setting 
Document --idleTimeout setting to not exceed the HDFS timeout value.",1
"Infrastructure for RabbitMQ Cluster (ECB)
Pre-requisite for Rabbit MQ Benchmarks:    * Infrastructure setup  * Configuration changes  * Tool-chain setup    * IPerf",5
"Vary consumers size (ECB-4)
Using a single producer, message size of 1000 bytes, Pretch of 100.   Send 1M messages and increase or decrease so that a given test iteration takes about 2 minutes.    Vary the number of consumers.   Measure the msg/sec rate and calculate the data transfer rate in MB/sec.    *Number of consumers:*  * 1  * 2  * 4  * 6  * 10  * 50    During the measurements look at the RabbitMQ Admin UI and see if the queue is backing up.",3
"Vary producers size (ECB-5)
Use the number of consumers gave a maximum throughput in the previous test (say 10 consumers), message size 100 bytes, Prefetch 100.   Send 1M messages    Vary the number of producers.  Measure the msg/sec rate and calculate the data transfer rate in MB/sec.    *Number of producers:*  * 2  * 4  * 6  * 10  * 50    During the measurements look at the RabbitMQ Admin UI and see if the queue is backing up.",3
"Create a maintenance branch
As a developer, I'd like to have a maintenance branch so that I can commit MINOR release _(ex: 1.0.2)_ code changes instead of committing to MASTER.",5
"Remove deprecated functions 
As a follow-up action from module registry refactoring we would have to clean-up deprecated functions _(ex: download of module definitions)_ within our codebase.     It may also be necessary to clean-up Shell and Admin-UI modules. ",3
