Issue,Storypoint
"GarbageCollectorIntegrationTest.Restart is slow
The {{GarbageCollectorIntegrationTest.Restart}} test takes more than {{5s}} to finish on my Mac OS 10.10.4:  {code}  GarbageCollectorIntegrationTest.Restart (5102 ms)  {code}",3
"HookTest.VerifySlaveLaunchExecutorHook is slow.
The {{HookTest.VerifySlaveLaunchExecutorHook}} test takes more than {{5s}} to finish on my Mac OS 10.10.4:  {code}  HookTest.VerifySlaveLaunchExecutorHook (5061 ms)  {code}",1
"ContentType/SchedulerTest.Decline is slow.
The {{ContentType/SchedulerTest.Decline}} test takes more than {{1s}} to finish on my Mac OS 10.10.4:  {code}  ContentType/SchedulerTest.Decline/0 (1022 ms)  {code}",1
"Create a user doc for Executor HTTP API
We need a user doc similar to the corresponding one for the Scheduler HTTP API.",3
"Add persistent volume support to the Authorizer
This ticket is the first in a series that adds authorization support for persistent volume creation and destruction.    Persistent volumes should be authorized with the {{principal}} of the reserving entity (framework or master). The idea is to introduce {{Create}} and {{Destroy}} into the ACL.    {code}    message Create {      // Subjects.      required Entity principals = 1;        // Objects? Perhaps the kind of volume? allowed permissions?    }      message Destroy {      // Subjects.      required Entity principals = 1;        // Objects.      required Entity creator_principals = 2;    }  {code}    ACLs for volume creation and destruction must be added to {{authorizer.proto}}, and the appropriate function overloads must be added to the Authorizer.",1
"Extend `Master` to authorize persistent volumes
This ticket is the second in a series that adds authorization support for persistent volumes.    Methods {{Master::authorizeCreateVolume()}} and {{Master::authorizeDestroyVolume}} must be added to allow the Master to authorize these operations.",1
"Move operator<< definitions to .cpp files and include <iosfwd> in .hpp where possible.
We often include complex headers like {{<ostream>}} in "".hpp"" files to define {{operator<<()}} inline (e.g. ""mesos/authorizer/authorizer.hpp""). Instead, we can move definitions to corresponding "".cpp"" files and replace stream headers with {{iosfwd}}, for example, this is partially done for {{URI}} in ""mesos/uri/uri.hpp"".",3
"Jenkins builds for Centos fail with missing 'which' utility and incorrect 'java.home'
Jenkins builds are now consistently failing for centos 7, withe the failure:    checking value of Java system property 'java.home'...  /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.65-3.b17.el7.x86_64/jre  configure: error: could not guess JAVA_HOME    They also fail early on during 'bootstrap' with a missing 'which' command.    The solution is to update support/docker_build.sh to install 'which' as well as make sure the proper versions of java are installed during the installation process.    The problem here is that we install maven BEFORE installing java-1.7.0-openjdk-devel, causing maven to pull in a dependency on java-1.8.0-openjdk. This causes problems with finding the proper java.home in our mesos/configure script because of the mismatch between the most up to date jre (1.8.0) and the most up to date development tools (1.7.0).  We can either update the script to pull in the 1.8 devel tools or move our dependence on maven until AFTER our installation of java-1.7.0-openjdk-devel.  Unclear what the best solution is.",3
"Serialize docker v1 image spec as protobuf
Currently we only support v2 docker manifest serialization method. When we read docker image spec locally from disk, we should be able to parse v1 docker manifest as protobuf, which will make it easier to gather runtime config and other necessary info.",2
"Avoid using absolute URLs in documentation pages
Links from one documentation page to another should not use absolute URLs (e.g., {{http://mesos.apache.org/documentation/latest/...}}) for several good reasons. For instance, absolute URLs break when the docs are generated/previewed locally.",1
"Create a Design Doc for dynamic weights.
A short design doc for dynamic weights, it will focus on /weights API and the changes to the allocator API.",3
"Design doc for fixed point resources
nan",5
"Add documentation for API Versioning
Currently, we don't have any documentation for:    - How Mesos implements API versioning ?  - How are protobufs versioned and how does mesos handle them internally ?  - What do contributors need to do when they make a change to a external user facing protobuf ?    The relevant design doc:  https://docs.google.com/document/d/1-iQjo6778H_fU_1Zi_Yk6szg8qj-wqYgVgnx7u3h6OU/edit#heading=h.2gkbjz6amn7b  ",3
"Port `process/file.hpp`
nan",3
"MesosContainerizer* tests leak FDs (pipes)
If you run:  {{bin/mesos-tests.sh --gtest_filter=""*MesosContainerizer*"" --gtest_repeat=-1 --gtest_break_on_failure}}    And then check:  {{lsof | grep mesos}}    The number of open pipes will grow linearly with the number of test repetitions.",2
"Add dynamic reservation tests with no principal
Currently, there exist no dynamic reservation tests that include authorization of a framework that is registered with no principal. This should be added in order to more comprehensively test the dynamic reservation code.",1
"Enable running tests without authorizer.
We do not support creating {{Master}} instance without an {{Authorizer}} in tests: https://github.com/apache/mesos/blob/aa497e81c945677c570484a8aa1a8c8b2e979dfd/src/tests/cluster.cpp#L217. This leads to a segfault when {{masterFlags.acls = None();}} is used in a test, while it's a valid use case and should be allowed.    Alternatively, we use {{masterFlags.acls = ACLs();}}, which triggers creation of {{LocalAuthorizer}} with emtpy {{ACLs}}, which seems to be semantically equal to the absence of an authorizer, given {{permissive}} flag is {{true}}. This equivalence should be verified by a test.",3
"Disk Resource Reservation is NOT Enforced for Persistent Volumes
If I create a persistent volume on a reserved disk resource, I am able to write data in excess of my reserved size.    Disk resource reservation should be enforced just as ""cpus"" and ""mem"" reservations are enforced.",3
"Test case(s) for weights + allocation behavior
As far as I can see, we currently have NO test cases for behavior when weights are defined.",2
"Race in SSL socket shutdown 
libprocess Socket shares the ownership of the file descriptor with libevent. In  the destructor of the libprocess libevent_ssl socket, we call ssl shutdown which  is executed asynchronously. This causes the libprocess socket file descriptor tobe closed (and possibly reused) when the same file descriptor could be used bylibevent/ssl. Since we set the shutdown options as SSL_RECEIVED_SHUTDOWN, we leave the any write operations to continue with possibly closed file descriptor.    This issue manifests as junk characters written to the file that has been handled the closed socket file descriptor (by OS) that has the above issue.",5
"Document that frameworks that participate in a role should cooperate
nan",2
"Write new log-related documentation
This should include:  * Default logging behavior for master, agent, framework, executor, task.  * Master/agent:  ** A summary of log-related flags.  ** {{glog}} specific options.  * Separation of master/agent logs from container logs.  * The {{ContainerLogger}} module.",3
"Add an example bug due to a lack of defer() to the defer() documentation
In the past, some bugs have been introduced into the codebase due to a lack of {{defer()}} where it should have been used. It would be useful to add an example of this to the {{defer()}} documentation.",2
"PersistentVolumeTest.BadACLDropCreateAndDestroy is flaky
{noformat}  [ RUN      ] PersistentVolumeTest.BadACLDropCreateAndDestroy  I1219 09:51:32.623245 31878 leveldb.cpp:174] Opened db in 4.393596ms  I1219 09:51:32.624084 31878 leveldb.cpp:181] Compacted db in 709447ns  I1219 09:51:32.624186 31878 leveldb.cpp:196] Created db iterator in 21252ns  I1219 09:51:32.624290 31878 leveldb.cpp:202] Seeked to beginning of db in 11391ns  I1219 09:51:32.624378 31878 leveldb.cpp:271] Iterated through 0 keys in the db in 611ns  I1219 09:51:32.624505 31878 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned  I1219 09:51:32.625195 31904 recover.cpp:447] Starting replica recovery  I1219 09:51:32.625641 31904 recover.cpp:473] Replica is in EMPTY status  I1219 09:51:32.627305 31904 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (6740)@172.17.0.3:36408  I1219 09:51:32.627749 31904 recover.cpp:193] Received a recover response from a replica in EMPTY status  I1219 09:51:32.628330 31904 recover.cpp:564] Updating replica status to STARTING  I1219 09:51:32.629068 31906 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 410494ns  I1219 09:51:32.629169 31906 replica.cpp:320] Persisted replica status to STARTING  I1219 09:51:32.629598 31906 recover.cpp:473] Replica is in STARTING status  I1219 09:51:32.630782 31912 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (6741)@172.17.0.3:36408  I1219 09:51:32.631166 31901 recover.cpp:193] Received a recover response from a replica in STARTING status  I1219 09:51:32.632467 31902 recover.cpp:564] Updating replica status to VOTING  I1219 09:51:32.633600 31907 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 311370ns  I1219 09:51:32.633627 31907 replica.cpp:320] Persisted replica status to VOTING  I1219 09:51:32.633719 31907 recover.cpp:578] Successfully joined the Paxos group  I1219 09:51:32.633874 31907 recover.cpp:462] Recover process terminated  I1219 09:51:32.636409 31909 master.cpp:365] Master bded856d-1c7f-4fad-a8bc-3629ba8c59d3 (60ab6e727501) started on 172.17.0.3:36408  I1219 09:51:32.636593 31909 master.cpp:367] Flags at startup: --acls=""create_volumes {    principals {      values: ""creator-principal""    }    volume_types {      type: ANY    }  }  create_volumes {    principals {      type: ANY    }    volume_types {      type: NONE    }  }  "" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""false"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/SpPF7B/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""25secs"" --registry_strict=""true"" --roles=""role1"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.27.0/_inst/share/mesos/webui"" --work_dir=""/tmp/SpPF7B/master"" --zk_session_timeout=""10secs""  I1219 09:51:32.637055 31909 master.cpp:414] Master allowing unauthenticated frameworks to register  I1219 09:51:32.637068 31909 master.cpp:417] Master only allowing authenticated slaves to register  I1219 09:51:32.637094 31909 credentials.hpp:35] Loading credentials for authentication from '/tmp/SpPF7B/credentials'  I1219 09:51:32.637403 31909 master.cpp:456] Using default 'crammd5' authenticator  I1219 09:51:32.637555 31909 master.cpp:493] Authorization enabled  W1219 09:51:32.637575 31909 master.cpp:553] The '--roles' flag is deprecated. This flag will be removed in the future. See the Mesos 0.27 upgrade notes for more information  I1219 09:51:32.637806 31897 whitelist_watcher.cpp:77] No whitelist given  I1219 09:51:32.637820 31910 hierarchical.cpp:147] Initialized hierarchical allocator process  I1219 09:51:32.639677 31909 master.cpp:1629] The newly elected leader is master@172.17.0.3:36408 with id bded856d-1c7f-4fad-a8bc-3629ba8c59d3  I1219 09:51:32.639768 31909 master.cpp:1642] Elected as the leading master!  I1219 09:51:32.639892 31909 master.cpp:1387] Recovering from registrar  I1219 09:51:32.640136 31907 registrar.cpp:307] Recovering registrar  I1219 09:51:32.640929 31901 log.cpp:659] Attempting to start the writer  I1219 09:51:32.642199 31912 replica.cpp:493] Replica received implicit promise request from (6742)@172.17.0.3:36408 with proposal 1  I1219 09:51:32.642719 31912 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 445876ns  I1219 09:51:32.642755 31912 replica.cpp:342] Persisted promised to 1  I1219 09:51:32.643478 31904 coordinator.cpp:238] Coordinator attempting to fill missing positions  I1219 09:51:32.645009 31909 replica.cpp:388] Replica received explicit promise request from (6743)@172.17.0.3:36408 for position 0 with proposal 2  I1219 09:51:32.645356 31909 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 310064ns  I1219 09:51:32.645382 31909 replica.cpp:712] Persisted action at 0  I1219 09:51:32.646662 31909 replica.cpp:537] Replica received write request for position 0 from (6744)@172.17.0.3:36408  I1219 09:51:32.646721 31909 leveldb.cpp:436] Reading position from leveldb took 29298ns  I1219 09:51:32.647047 31909 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 283424ns  I1219 09:51:32.647073 31909 replica.cpp:712] Persisted action at 0  I1219 09:51:32.647722 31909 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0  I1219 09:51:32.648052 31909 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 300825ns  I1219 09:51:32.648077 31909 replica.cpp:712] Persisted action at 0  I1219 09:51:32.648095 31909 replica.cpp:697] Replica learned NOP action at position 0  I1219 09:51:32.655295 31899 log.cpp:675] Writer started with ending position 0  I1219 09:51:32.656543 31905 leveldb.cpp:436] Reading position from leveldb took 32788ns  I1219 09:51:32.658164 31905 registrar.cpp:340] Successfully fetched the registry (0B) in 0ns  I1219 09:51:32.658604 31905 registrar.cpp:439] Applied 1 operations in 38183ns; attempting to update the 'registry'  I1219 09:51:32.660102 31905 log.cpp:683] Attempting to append 170 bytes to the log  I1219 09:51:32.660538 31906 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1  I1219 09:51:32.661872 31906 replica.cpp:537] Replica received write request for position 1 from (6745)@172.17.0.3:36408  I1219 09:51:32.662719 31906 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 483018ns  I1219 09:51:32.663054 31906 replica.cpp:712] Persisted action at 1  I1219 09:51:32.664008 31902 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0  I1219 09:51:32.664330 31902 leveldb.cpp:341] Persisting action (191 bytes) to leveldb took 287310ns  I1219 09:51:32.664355 31902 replica.cpp:712] Persisted action at 1  I1219 09:51:32.664376 31902 replica.cpp:697] Replica learned APPEND action at position 1  I1219 09:51:32.665365 31902 registrar.cpp:484] Successfully updated the 'registry' in 0ns  I1219 09:51:32.665493 31902 registrar.cpp:370] Successfully recovered registrar  I1219 09:51:32.665894 31902 master.cpp:1439] Recovered 0 slaves from the Registry (131B) ; allowing 10mins for slaves to re-register  I1219 09:51:32.665990 31902 hierarchical.cpp:165] Skipping recovery of hierarchical allocator: nothing to recover  I1219 09:51:32.666266 31902 log.cpp:702] Attempting to truncate the log to 1  I1219 09:51:32.666424 31902 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2  I1219 09:51:32.667181 31907 replica.cpp:537] Replica received write request for position 2 from (6746)@172.17.0.3:36408  I1219 09:51:32.667768 31907 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 335947ns  I1219 09:51:32.668067 31907 replica.cpp:712] Persisted action at 2  I1219 09:51:32.668942 31906 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0  I1219 09:51:32.669240 31906 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 266566ns  I1219 09:51:32.669292 31906 leveldb.cpp:399] Deleting ~1 keys from leveldb took 27852ns  I1219 09:51:32.669314 31906 replica.cpp:712] Persisted action at 2  I1219 09:51:32.669334 31906 replica.cpp:697] Replica learned TRUNCATE action at position 2  I1219 09:51:32.691251 31878 containerizer.cpp:141] Using isolation: posix/cpu,posix/mem,filesystem/posix  W1219 09:51:32.691759 31878 backend.cpp:48] Failed to create 'bind' backend: BindBackend requires root privileges  I1219 09:51:32.697428 31901 slave.cpp:191] Slave started on 228)@172.17.0.3:36408  I1219 09:51:32.697459 31901 slave.cpp:192] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/PersistentVolumeTest_BadACLDropCreateAndDestroy_gWLtnc/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""auth.docker.io"" --docker_auth_server_port=""443"" --docker_kill_orphans=""true"" --docker_local_archives_dir=""/tmp/mesos/images/docker"" --docker_puller=""local"" --docker_puller_timeout=""60"" --docker_registry=""registry-1.docker.io"" --docker_registry_port=""443"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/PersistentVolumeTest_BadACLDropCreateAndDestroy_gWLtnc/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.27.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk(role1):2048"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/PersistentVolumeTest_BadACLDropCreateAndDestroy_gWLtnc""  I1219 09:51:32.697963 31901 credentials.hpp:83] Loading credential for authentication from '/tmp/PersistentVolumeTest_BadACLDropCreateAndDestroy_gWLtnc/credential'  I1219 09:51:32.698210 31901 slave.cpp:322] Slave using credential for: test-principal  I1219 09:51:32.698449 31901 resources.cpp:478] Parsing resources as JSON failed: cpus:2;mem:1024;disk(role1):2048  Trying semicolon-delimited string format instead  I1219 09:51:32.699065 31901 slave.cpp:392] Slave resources: cpus(*):2; mem(*):1024; disk(role1):2048; ports(*):[31000-32000]  I1219 09:51:32.699137 31901 slave.cpp:400] Slave attributes: [  ]  I1219 09:51:32.699151 31901 slave.cpp:405] Slave hostname: 60ab6e727501  I1219 09:51:32.699161 31901 slave.cpp:410] Slave checkpoint: true  I1219 09:51:32.699364 31878 sched.cpp:164] Version: 0.27.0  I1219 09:51:32.700614 31911 sched.cpp:262] New master detected at master@172.17.0.3:36408  I1219 09:51:32.700703 31911 sched.cpp:272] No credentials provided. Attempting to register without authentication  I1219 09:51:32.700724 31911 sched.cpp:714] Sending SUBSCRIBE call to master@172.17.0.3:36408  I1219 09:51:32.700839 31911 sched.cpp:747] Will retry registration in 620.399428ms if necessary  I1219 09:51:32.701244 31903 master.cpp:2197] Received SUBSCRIBE call for framework 'default' at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408  I1219 09:51:32.701313 31903 master.cpp:1668] Authorizing framework principal 'test-principal' to receive offers for role 'role1'  I1219 09:51:32.701625 31903 master.cpp:2268] Subscribing framework default with checkpointing disabled and capabilities [  ]  I1219 09:51:32.702308 31903 hierarchical.cpp:260] Added framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000  I1219 09:51:32.702386 31903 hierarchical.cpp:1329] No resources available to allocate!  I1219 09:51:32.702422 31903 hierarchical.cpp:1423] No inverse offers to send out!  I1219 09:51:32.702448 31903 hierarchical.cpp:1079] Performed allocation for 0 slaves in 114358ns  I1219 09:51:32.702638 31903 sched.cpp:641] Framework registered with bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000  I1219 09:51:32.702688 31903 sched.cpp:655] Scheduler::registered took 25558ns  I1219 09:51:32.703553 31901 state.cpp:58] Recovering state from '/tmp/PersistentVolumeTest_BadACLDropCreateAndDestroy_gWLtnc/meta'  I1219 09:51:32.704118 31897 status_update_manager.cpp:200] Recovering status update manager  I1219 09:51:32.704407 31907 containerizer.cpp:383] Recovering containerizer  I1219 09:51:32.705373 31907 slave.cpp:4427] Finished recovery  I1219 09:51:32.705991 31907 slave.cpp:4599] Querying resource estimator for oversubscribable resources  I1219 09:51:32.706277 31907 slave.cpp:4613] Received oversubscribable resources  from the resource estimator  I1219 09:51:32.706666 31907 slave.cpp:729] New master detected at master@172.17.0.3:36408  I1219 09:51:32.706738 31907 slave.cpp:792] Authenticating with master master@172.17.0.3:36408  I1219 09:51:32.706760 31907 slave.cpp:797] Using default CRAM-MD5 authenticatee  I1219 09:51:32.706886 31899 status_update_manager.cpp:174] Pausing sending status updates  I1219 09:51:32.706941 31907 slave.cpp:765] Detecting new master  I1219 09:51:32.707036 31899 authenticatee.cpp:121] Creating new client SASL connection  I1219 09:51:32.707291 31910 master.cpp:5423] Authenticating slave(228)@172.17.0.3:36408  I1219 09:51:32.707479 31910 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(510)@172.17.0.3:36408  I1219 09:51:32.707849 31910 authenticator.cpp:98] Creating new server SASL connection  I1219 09:51:32.708082 31910 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5  I1219 09:51:32.708112 31910 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'  I1219 09:51:32.708196 31910 authenticator.cpp:203] Received SASL authentication start  I1219 09:51:32.708395 31910 authenticator.cpp:325] Authentication requires more steps  I1219 09:51:32.708611 31902 authenticatee.cpp:258] Received SASL authentication step  I1219 09:51:32.708773 31910 authenticator.cpp:231] Received SASL authentication step  I1219 09:51:32.708889 31910 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '60ab6e727501' server FQDN: '60ab6e727501' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false   I1219 09:51:32.708976 31910 auxprop.cpp:179] Looking up auxiliary property '*userPassword'  I1219 09:51:32.709096 31910 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'  I1219 09:51:32.709200 31910 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '60ab6e727501' server FQDN: '60ab6e727501' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true   I1219 09:51:32.709285 31910 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true  I1219 09:51:32.709363 31910 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true  I1219 09:51:32.709452 31910 authenticator.cpp:317] Authentication success  I1219 09:51:32.709707 31910 authenticatee.cpp:298] Authentication success  I1219 09:51:32.710252 31910 slave.cpp:860] Successfully authenticated with master master@172.17.0.3:36408  I1219 09:51:32.710525 31910 slave.cpp:1254] Will retry registration in 17.44437ms if necessary  I1219 09:51:32.709839 31908 master.cpp:5453] Successfully authenticated principal 'test-principal' at slave(228)@172.17.0.3:36408  I1219 09:51:32.710985 31908 master.cpp:4132] Registering slave at slave(228)@172.17.0.3:36408 (60ab6e727501) with id bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0  I1219 09:51:32.711645 31908 registrar.cpp:439] Applied 1 operations in 83191ns; attempting to update the 'registry'  I1219 09:51:32.709908 31912 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(510)@172.17.0.3:36408  I1219 09:51:32.713407 31908 log.cpp:683] Attempting to append 343 bytes to the log  I1219 09:51:32.713646 31912 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3  I1219 09:51:32.714884 31911 replica.cpp:537] Replica received write request for position 3 from (6758)@172.17.0.3:36408  I1219 09:51:32.715221 31911 leveldb.cpp:341] Persisting action (362 bytes) to leveldb took 288909ns  I1219 09:51:32.715250 31911 replica.cpp:712] Persisted action at 3  I1219 09:51:32.716145 31912 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0  I1219 09:51:32.716689 31912 leveldb.cpp:341] Persisting action (364 bytes) to leveldb took 512217ns  I1219 09:51:32.716716 31912 replica.cpp:712] Persisted action at 3  I1219 09:51:32.716737 31912 replica.cpp:697] Replica learned APPEND action at position 3  I1219 09:51:32.718426 31911 registrar.cpp:484] Successfully updated the 'registry' in 0ns  I1219 09:51:32.719441 31902 slave.cpp:3371] Received ping from slave-observer(228)@172.17.0.3:36408  I1219 09:51:32.719843 31909 log.cpp:702] Attempting to truncate the log to 3  I1219 09:51:32.719908 31911 master.cpp:4200] Registered slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 at slave(228)@172.17.0.3:36408 (60ab6e727501) with cpus(*):2; mem(*):1024; disk(role1):2048; ports(*):[31000-32000]  I1219 09:51:32.720064 31911 slave.cpp:904] Registered with master master@172.17.0.3:36408; given slave ID bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0  I1219 09:51:32.720088 31911 fetcher.cpp:81] Clearing fetcher cache  I1219 09:51:32.720491 31911 slave.cpp:927] Checkpointing SlaveInfo to '/tmp/PersistentVolumeTest_BadACLDropCreateAndDestroy_gWLtnc/meta/slaves/bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0/slave.info'  I1219 09:51:32.720844 31909 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4  I1219 09:51:32.720929 31911 slave.cpp:963] Forwarding total oversubscribed resources   I1219 09:51:32.721017 31903 status_update_manager.cpp:181] Resuming sending status updates  I1219 09:51:32.721099 31911 master.cpp:4542] Received update of slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 at slave(228)@172.17.0.3:36408 (60ab6e727501) with total oversubscribed resources   I1219 09:51:32.721141 31905 hierarchical.cpp:465] Added slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 (60ab6e727501) with cpus(*):2; mem(*):1024; disk(role1):2048; ports(*):[31000-32000] (allocated: )  I1219 09:51:32.721879 31911 replica.cpp:537] Replica received write request for position 4 from (6759)@172.17.0.3:36408  I1219 09:51:32.722293 31905 hierarchical.cpp:1423] No inverse offers to send out!  I1219 09:51:32.722337 31905 hierarchical.cpp:1101] Performed allocation for slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 in 1.155563ms  I1219 09:51:32.722681 31905 hierarchical.cpp:521] Slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 (60ab6e727501) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(role1):2048; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):1024; ports(*):[31000-32000]; disk(role1):2048)  I1219 09:51:32.722713 31909 master.cpp:5252] Sending 1 offers to framework bded856d-...",1
"Document ""how to program with dynamic reservations and persistent volumes""
Specifically, some of the gotchas around:    * Retrying reservation attempts after a timeout  * Fuzzy-matching resources to determine whether a reservation/PV is successful  * Represent client state as a state machine and repeatedly move ""toward"" successful terminate stats    Should also point to persistent volume example framework. We should also ask Gabriel and others (Arango?) who have built frameworks with PVs/DRs for feedback.",3
"Introduce HTTP endpoint /weights for updating weight
nan",5
"Test for Quota Status Endpoint
nan",3
"Document containerizer from user perspective.
Add documentation that covers:    * Purpose of containerizers from a use case perspective.  * What purpose does each containerizer (mesos. docker, compose) serve.  * What criteria could be used to choose a containerizer.",3
"Document isolators from user perspective.
The documentation should cover:    * Purpose of isolators (business/user perspective).  * What is the criteria for choosing/picking any set of isolators.",4
"Document isolator internals.
Document isolators from developer perspective, possibly covering:    * linux isolators  * posix isolators  * filesystem, network isolators",4
"Exposed docker/appc image manifest to mesos containerizer.
Collect docker image manifest from disk(which contains all runtime configurations), and pass it back to provisioner, so that mesos containerizer can grab all necessary info from provisioner.",2
"Enable passing docker image environment variables runtime config to provisioner
Collect environment variables runtime config information from a docker image, and save as a map. Pass it back to provisioner, and handling environment variables merge issue.",1
"Enable passing docker image cmd runtime config to provisioner
Cmd is the command to run when starting a container. We should be able to collect Cmd config information from a docker image, and pass it back to provisioner.",1
"Pull provisioner from linux filesystem isolator to Mesos containerizer.
The rationale behind this change is that many of the image specifications (e.g., Docker/Appc) are not just for filesystems. They also specify runtime configurations (e.g., environment variables, volumes, etc) for the container.    Provisioner should return those runtime configurations to the Mesos containerizer and Mesos containerizer will delegate the isolation of those runtime configurations to the relevant isolator.    Here is what it will be look like eventually. We could do those changes in phases:  1) Provisioner will return a ProvisionInfo which includes a 'rootfs' and image specific runtime configurations (could be the Docker/Appc manifest).  2) Then, the Mesos containerizer will generate a ContainerConfig (a protobuf which includes rootfs, sandbox, docker/appc manifest, similar to OCI's host independent config.json) and pass that to each isolator in 'prepare'. Imaging in the future, a DockerRuntimeIsolator takes the docker manifest from ContainerConfig and prepare the container.  3) The isolator's prepare function will return a ContainerLaunchInfo (contains environment variables, namespaces, etc.) which will be used by Mesos containerize to launch containers. Imaging that information will be passed to the launcher in the future.    We can do the renaming (ContainerPrepareInfo -> ContainerLaunchInfo) later.    ",5
"Consolidate docker store slave flags
Currently there are too many slave flags for configuring the docker store/puller.  We can remove the following flags:    docker_auth_server_port  docker_local_archives_dir  docker_registry_port  docker_puller    And consolidate them into the existing flags.",3
"Add mechanism for testing recovery of HTTP based executors
Currently, the slave process generates a process ID every time it is initialized via {{process::ID::generate}} function call. This is a problem for testing HTTP executors as it can't retry if there is a disconnection after an agent restart since the prefix is incremented.     {code}  Agent PID before:  slave(1)@127.0.0.1:43915    Agent PID after restart:  slave(2)@127.0.0.1:43915  {code}    There are a couple of ways to fix this:  - Add a constructor to {{Slave}} exclusively for testing that passes on a fixed {{ID}} instead of relying on {{ID::generate}}.  - Currently we delegate to slave(1)@ i.e. (1) when nothing is specified as the URL in libprocess i.e. {{127.0.0.1:43915/api/v1/executor}} would delegate to {{slave(1)@127.0.0.1:43915/api/v1/executor}}. Instead of defaulting to (1), we can default to the last known active ID.",3
"ExamplesTest.NoExecutorFramework runs forever.
{noformat: title=Good Run}  [ RUN      ] ExamplesTest.NoExecutorFramework  I1221 23:10:02.721617 32528 exec.cpp:444] Ignoring exited event because the driver is aborted!  Using temporary directory '/tmp/ExamplesTest_NoExecutorFramework_fCmFLn'  I1221 23:10:02.721675 32539 exec.cpp:444] Ignoring exited event because the driver is aborted!  I1221 23:10:02.722024 32554 exec.cpp:444] Ignoring exited event because the driver is aborted!  WARNING: Logging before InitGoogleLogging() is written to STDERR  I1221 23:10:05.179466 32569 resources.cpp:478] Parsing resources as JSON failed: cpus:0.1;mem:32;disk:32  Trying semicolon-delimited string format instead  I1221 23:10:05.180269 32569 logging.cpp:172] Logging to STDERR  I1221 23:10:05.185768 32569 process.cpp:998] libprocess is initialized on 172.17.0.2:40874 for 16 cpus  I1221 23:10:05.200728 32569 leveldb.cpp:174] Opened db in 4.184362ms  I1221 23:10:05.202234 32569 leveldb.cpp:181] Compacted db in 1.459268ms  I1221 23:10:05.202353 32569 leveldb.cpp:196] Created db iterator in 73761ns  I1221 23:10:05.202383 32569 leveldb.cpp:202] Seeked to beginning of db in 3382ns  I1221 23:10:05.202405 32569 leveldb.cpp:271] Iterated through 0 keys in the db in 633ns  I1221 23:10:05.202674 32569 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned  I1221 23:10:05.205301 32604 recover.cpp:447] Starting replica recovery  I1221 23:10:05.206414 32569 local.cpp:239] Using 'local' authorizer  I1221 23:10:05.206405 32604 recover.cpp:473] Replica is in EMPTY status  I1221 23:10:05.209595 32594 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (4)@172.17.0.2:40874  I1221 23:10:05.210916 32596 recover.cpp:193] Received a recover response from a replica in EMPTY status  I1221 23:10:05.211515 32597 master.cpp:365] Master 3931c1a8-1cd6-49eb-94c8-d01b33bb008e (6ccf2ee56b13) started on 172.17.0.2:40874  I1221 23:10:05.211699 32605 recover.cpp:564] Updating replica status to STARTING  I1221 23:10:05.211539 32597 master.cpp:367] Flags at startup: --acls=""permissive: false  register_frameworks {    principals {      type: SOME      values: ""test-principal""    }    roles {      type: SOME      values: ""*""    }  }  run_tasks {    principals {      type: SOME      values: ""test-principal""    }    users {      type: SOME      values: ""mesos""    }  }  "" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_slaves=""false"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/ExamplesTest_NoExecutorFramework_fCmFLn/credentials"" --framework_sorter=""drf"" --help=""true"" --hostname_lookup=""true"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""5secs"" --registry_strict=""false"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.27.0/src/webui"" --work_dir=""/tmp/mesos-otpdch"" --zk_session_timeout=""10secs""  I1221 23:10:05.212323 32597 master.cpp:412] Master only allowing authenticated frameworks to register  I1221 23:10:05.212337 32597 master.cpp:419] Master allowing unauthenticated slaves to register  I1221 23:10:05.212347 32597 credentials.hpp:35] Loading credentials for authentication from '/tmp/ExamplesTest_NoExecutorFramework_fCmFLn/credentials'  W1221 23:10:05.212442 32597 credentials.hpp:50] Permissions on credentials file '/tmp/ExamplesTest_NoExecutorFramework_fCmFLn/credentials' are too open. It is recommended that your credentials file is NOT accessible by others.  I1221 23:10:05.212606 32600 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 656857ns  I1221 23:10:05.212620 32597 master.cpp:456] Using default 'crammd5' authenticator  I1221 23:10:05.212631 32600 replica.cpp:320] Persisted replica status to STARTING  I1221 23:10:05.212893 32597 authenticator.cpp:518] Initializing server SASL  I1221 23:10:05.213091 32608 recover.cpp:473] Replica is in STARTING status  I1221 23:10:05.213958 32595 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (5)@172.17.0.2:40874  I1221 23:10:05.214323 32594 recover.cpp:193] Received a recover response from a replica in STARTING status  I1221 23:10:05.214689 32595 recover.cpp:564] Updating replica status to VOTING  I1221 23:10:05.215353 32596 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 487419ns  I1221 23:10:05.215384 32596 replica.cpp:320] Persisted replica status to VOTING  I1221 23:10:05.215481 32605 recover.cpp:578] Successfully joined the Paxos group  I1221 23:10:05.215867 32605 recover.cpp:462] Recover process terminated  I1221 23:10:05.216111 32569 containerizer.cpp:141] Using isolation: filesystem/posix,posix/cpu,posix/mem  W1221 23:10:05.217021 32569 backend.cpp:48] Failed to create 'bind' backend: BindBackend requires root privileges  I1221 23:10:05.221482 32608 slave.cpp:191] Slave started on 1)@172.17.0.2:40874  I1221 23:10:05.221521 32608 slave.cpp:192] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""auth.docker.io"" --docker_auth_server_port=""443"" --docker_kill_orphans=""true"" --docker_local_archives_dir=""/tmp/mesos/images/docker"" --docker_puller=""local"" --docker_puller_timeout=""60"" --docker_registry=""registry-1.docker.io"" --docker_registry_port=""443"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/mesos/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""filesystem/posix,posix/cpu,posix/mem"" --launcher=""posix"" --launcher_dir=""/mesos/mesos-0.27.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""1secs"" --resources=""cpus:2;mem:10240"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/mesos-otpdch/0""  I1221 23:10:05.222578 32608 resources.cpp:478] Parsing resources as JSON failed: cpus:2;mem:10240  Trying semicolon-delimited string format instead  I1221 23:10:05.223465 32608 slave.cpp:392] Slave resources: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]  I1221 23:10:05.223621 32569 containerizer.cpp:141] Using isolation: filesystem/posix,posix/cpu,posix/mem  I1221 23:10:05.223610 32608 slave.cpp:400] Slave attributes: [  ]  I1221 23:10:05.223677 32608 slave.cpp:405] Slave hostname: 6ccf2ee56b13  I1221 23:10:05.223697 32608 slave.cpp:410] Slave checkpoint: true  W1221 23:10:05.224143 32569 backend.cpp:48] Failed to create 'bind' backend: BindBackend requires root privileges  I1221 23:10:05.226668 32604 slave.cpp:191] Slave started on 2)@172.17.0.2:40874  I1221 23:10:05.226692 32604 slave.cpp:192] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""auth.docker.io"" --docker_auth_server_port=""443"" --docker_kill_orphans=""true"" --docker_local_archives_dir=""/tmp/mesos/images/docker"" --docker_puller=""local"" --docker_puller_timeout=""60"" --docker_registry=""registry-1.docker.io"" --docker_registry_port=""443"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/mesos/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""filesystem/posix,posix/cpu,posix/mem"" --launcher=""posix"" --launcher_dir=""/mesos/mesos-0.27.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""1secs"" --resources=""cpus:2;mem:10240"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/mesos-otpdch/1""  I1221 23:10:05.227520 32604 resources.cpp:478] Parsing resources as JSON failed: cpus:2;mem:10240  Trying semicolon-delimited string format instead  I1221 23:10:05.228037 32604 slave.cpp:392] Slave resources: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]  I1221 23:10:05.228148 32604 slave.cpp:400] Slave attributes: [  ]  I1221 23:10:05.228169 32604 slave.cpp:405] Slave hostname: 6ccf2ee56b13  I1221 23:10:05.228184 32604 slave.cpp:410] Slave checkpoint: true  I1221 23:10:05.229123 32569 containerizer.cpp:141] Using isolation: filesystem/posix,posix/cpu,posix/mem  I1221 23:10:05.229641 32605 state.cpp:58] Recovering state from '/tmp/mesos-otpdch/0/meta'  W1221 23:10:05.229645 32569 backend.cpp:48] Failed to create 'bind' backend: BindBackend requires root privileges  I1221 23:10:05.229636 32595 state.cpp:58] Recovering state from '/tmp/mesos-otpdch/1/meta'  I1221 23:10:05.230242 32605 status_update_manager.cpp:200] Recovering status update manager  I1221 23:10:05.230254 32598 status_update_manager.cpp:200] Recovering status update manager  I1221 23:10:05.230515 32601 containerizer.cpp:383] Recovering containerizer  I1221 23:10:05.230562 32602 containerizer.cpp:383] Recovering containerizer  I1221 23:10:05.232681 32597 auxprop.cpp:71] Initialized in-memory auxiliary property plugin  I1221 23:10:05.232803 32597 master.cpp:493] Authorization enabled  I1221 23:10:05.232867 32600 slave.cpp:4427] Finished recovery  I1221 23:10:05.232980 32598 slave.cpp:191] Slave started on 3)@172.17.0.2:40874  I1221 23:10:05.233039 32594 slave.cpp:4427] Finished recovery  I1221 23:10:05.233376 32599 whitelist_watcher.cpp:77] No whitelist given  I1221 23:10:05.233428 32601 hierarchical.cpp:147] Initialized hierarchical allocator process  I1221 23:10:05.233003 32598 slave.cpp:192] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""auth.docker.io"" --docker_auth_server_port=""443"" --docker_kill_orphans=""true"" --docker_local_archives_dir=""/tmp/mesos/images/docker"" --docker_puller=""local"" --docker_puller_timeout=""60"" --docker_registry=""registry-1.docker.io"" --docker_registry_port=""443"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/mesos/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""filesystem/posix,posix/cpu,posix/mem"" --launcher=""posix"" --launcher_dir=""/mesos/mesos-0.27.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""1secs"" --resources=""cpus:2;mem:10240"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/mesos-otpdch/2""  I1221 23:10:05.233744 32600 slave.cpp:4599] Querying resource estimator for oversubscribable resources  I1221 23:10:05.233749 32598 resources.cpp:478] Parsing resources as JSON failed: cpus:2;mem:10240  Trying semicolon-delimited string format instead  I1221 23:10:05.234222 32598 slave.cpp:392] Slave resources: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]  I1221 23:10:05.234284 32598 slave.cpp:400] Slave attributes: [  ]  I1221 23:10:05.234299 32598 slave.cpp:405] Slave hostname: 6ccf2ee56b13  I1221 23:10:05.234311 32598 slave.cpp:410] Slave checkpoint: true  I1221 23:10:05.234338 32600 slave.cpp:729] New master detected at master@172.17.0.2:40874  I1221 23:10:05.234376 32604 status_update_manager.cpp:174] Pausing sending status updates  I1221 23:10:05.234424 32600 slave.cpp:754] No credentials provided. Attempting to register without authentication  I1221 23:10:05.234522 32600 slave.cpp:765] Detecting new master  I1221 23:10:05.234616 32569 sched.cpp:164] Version: 0.27.0  I1221 23:10:05.234658 32600 slave.cpp:4613] Received oversubscribable resources  from the resource estimator  I1221 23:10:05.234671 32594 slave.cpp:4599] Querying resource estimator for oversubscribable resources  I1221 23:10:05.234884 32606 slave.cpp:4613] Received oversubscribable resources  from the resource estimator  I1221 23:10:05.235038 32595 status_update_manager.cpp:174] Pausing sending status updates  I1221 23:10:05.235043 32606 slave.cpp:729] New master detected at master@172.17.0.2:40874  I1221 23:10:05.235111 32606 slave.cpp:754] No credentials provided. Attempting to register without authentication  I1221 23:10:05.235147 32606 slave.cpp:765] Detecting new master  I1221 23:10:05.235240 32594 state.cpp:58] Recovering state from '/tmp/mesos-otpdch/2/meta'  I1221 23:10:05.235443 32608 status_update_manager.cpp:200] Recovering status update manager  I1221 23:10:05.235625 32594 containerizer.cpp:383] Recovering containerizer  I1221 23:10:05.236549 32599 slave.cpp:4427] Finished recovery  I1221 23:10:05.236984 32593 sched.cpp:262] New master detected at master@172.17.0.2:40874  I1221 23:10:05.237004 32599 slave.cpp:4599] Querying resource estimator for oversubscribable resources  I1221 23:10:05.237221 32593 sched.cpp:318] Authenticating with master master@172.17.0.2:40874  I1221 23:10:05.237277 32593 sched.cpp:325] Using default CRAM-MD5 authenticatee  I1221 23:10:05.237285 32604 status_update_manager.cpp:174] Pausing sending status updates  I1221 23:10:05.237288 32599 slave.cpp:729] New master detected at master@172.17.0.2:40874  I1221 23:10:05.237361 32599 slave.cpp:754] No credentials provided. Attempting to register without authentication  I1221 23:10:05.237433 32599 slave.cpp:765] Detecting new master  I1221 23:10:05.237565 32599 slave.cpp:4613] Received oversubscribable resources  from the resource estimator  I1221 23:10:05.238154 32605 authenticatee.cpp:97] Initializing client SASL  I1221 23:10:05.238315 32605 authenticatee.cpp:121] Creating new client SASL connection  I1221 23:10:05.239640 32597 master.cpp:1200] Dropping 'mesos.internal.AuthenticateMessage' message since not elected yet  I1221 23:10:05.239765 32597 master.cpp:1629] The newly elected leader is master@172.17.0.2:40874 with id 3931c1a8-1cd6-49eb-94c8-d01b33bb008e  I1221 23:10:05.239794 32597 master.cpp:1642] Elected as the leading master!  I1221 23:10:05.239843 32597 master.cpp:1387] Recovering from registrar  I1221 23:10:05.240056 32600 registrar.cpp:307] Recovering registrar  I1221 23:10:05.241477 32608 log.cpp:659] Attempting to start the writer  I1221 23:10:05.244540 32600 replica.cpp:493] Replica received implicit promise request from (39)@172.17.0.2:40874 with proposal 1  I1221 23:10:05.245358 32600 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 776937ns  I1221 23:10:05.245393 32600 replica.cpp:342] Persisted promised to 1  I1221 23:10:05.246625 32601 coordinator.cpp:238] Coordinator attempting to fill missing positions  I1221 23:10:05.248757 32605 replica.cpp:388] Replica received explicit promise request from (40)@172.17.0.2:40874 for position 0 with proposal 2  I1221 23:10:05.249214 32605 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 366567ns  I1221 23:10:05.249246 32605 replica.cpp:712] Persisted action at 0  I1221 23:10:05.250998 32599 replica.cpp:537] Replica received write request for position 0 from (41)@172.17.0.2:40874  I1221 23:10:05.251111 32599 leveldb.cpp:436] Reading position from leveldb took 66773ns  I1221 23:10:05.251734 32599 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 379612ns  I1221 23:10:05.251759 32599 replica.cpp:712] Persisted action at 0  I1221 23:10:05.252555 32601 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0  I1221 23:10:05.253010 32601 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 381858ns  I1221 23:10:05.253036 32601 replica.cpp:712] Persisted action at 0  I1221 23:10:05.253068 32601 replica.cpp:697] Replica learned NOP action at position 0  I1221 23:10:05.254043 32595 log.cpp:675] Writer started with ending position 0  I1221 23:10:05.256741 32595 leveldb.cpp:436] Reading position from leveldb took 48607ns  I1221 23:10:05.260617 32601 registrar.cpp:340] Successfully fetched the registry (0B) in 20.47616ms  I1221 23:10:05.260988 32601 registrar.cpp:439] Applied 1 operations in 103123ns; attempting to update the 'registry'  I1221 23:10:05.264700 32604 log.cpp:683] Attempting to append 170 bytes to the log  I1221 23:10:05.265138 32601 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1  I1221 23:10:05.266208 32603 replica.cpp:537] Replica received write request for position 1 from (42)@172.17.0.2:40874  I1221 23:10:05.266829 32603 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 551087ns  I1221 23:10:05.266861 32603 replica.cpp:712] Persisted action at 1  I1221 23:10:05.267918 32605 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0  I1221 23:10:05.268442 32605 leveldb.cpp:341] Persisting action (191 bytes) to leveldb took 453416ns  I1221 23:10:05.268470 32605 replica.cpp:712] Persisted action at 1  I1221 23:10:05.268506 32605 replica.cpp:697] Replica learned APPEND action at position 1  I1221 23:10:05.270512 32606 registrar.cpp:484] Successfully updated the 'registry' in 9.375232ms  I1221 23:10:05.270705 32606 registrar.cpp:370] Successfully recovered registrar  I1221 23:10:05.271045 32602 log.cpp:702] Attempting to truncate the log to 1  I1221 23:10:05.271178 32603 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2  I1221 23:10:05.271695 32605...",3
"Remove docker auth server flag
We currently use a configured docker auth server from a slave flag to get token auth for docker registry. However this doesn't work for private registries as docker registry supports sending down the correct auth server to contact.    We should remove docker auth server flag completely and ask the docker registry for auth server.",3
"Enable net_cls subsytem in cgroup infrastructure
Currently the control group infrastructure within mesos supports only the memory and CPU subsystems. We need to enhance this infrastructure to support the net_cls subsystem as well. Details of the net_cls subsystem and its use-cases can be found here:  https://www.kernel.org/doc/Documentation/cgroups/net_cls.txt    Enabling the net_cls will allow us to provide operators to, potentially, regulate framework traffic on a per-container basis.  ",5
"Report volume usage through ResourceStatistics.
POSIX disk isolator does not currently report volume usage through ResourceStatistics. {{PosixDiskIsolatorProcess::usage()}} should be amended to take into account volume usage as well. ",3
"Docker executor truncates task's output when the task is killed.
I'm implementing a graceful restarts of our mesos-marathon-docker setup and I came to a following issue:    (it was already discussed on https://github.com/mesosphere/marathon/issues/2876 and guys form mesosphere got to a point that its probably a docker containerizer problem...)  To sum it up:    When i deploy simple python script to all mesos-slaves:  {code}  #!/usr/bin/python    from time import sleep  import signal  import sys  import datetime    def sigterm_handler(_signo, _stack_frame):      print ""got %i"" % _signo      print datetime.datetime.now().time()      sys.stdout.flush()      sleep(2)      print datetime.datetime.now().time()      print ""ending""      sys.stdout.flush()      sys.exit(0)    signal.signal(signal.SIGTERM, sigterm_handler)  signal.signal(signal.SIGINT, sigterm_handler)    try:      print ""Hello""      i = 0      while True:          i += 1          print datetime.datetime.now().time()          print ""Iteration #%i"" % i          sys.stdout.flush()          sleep(1)  finally:      print ""Goodbye""  {code}    and I run it through Marathon like  {code:javascript}  data = {  	args: [""/tmp/script.py""],  	instances: 1,  	cpus: 0.1,  	mem: 256,  	id: ""marathon-test-api""  }  {code}    During the app restart I get expected result - the task receives sigterm and dies peacefully (during my script-specified 2 seconds period)    But when i wrap this python script in a docker:  {code}  FROM node:4.2    RUN mkdir /app  ADD . /app  WORKDIR /app  ENTRYPOINT []  {code}  and run appropriate application by Marathon:  {code:javascript}  data = {  	args: [""./script.py""],  	container: {  		type: ""DOCKER"",  		docker: {  			image: ""bydga/marathon-test-api""  		},  		forcePullImage: yes  	},  	cpus: 0.1,  	mem: 256,  	instances: 1,  	id: ""marathon-test-api""  }  {code}    The task during restart (issued from marathon) dies immediately without having a chance to do any cleanup.  ",5
"Correctly handle disk quota usage when volumes are bind mounted into the container.
In its current implementation disk quota enforcement on the task sandbox will not work correctly when disk volumes are bind mounted into the task sandbox (this happens when Linux filesystem isolator is used).",3
"Update isolator prepare function to use ContainerLaunchInfo
Currently we have the isolator's prepare function returning ContainerPrepareInfo protobuf. We should enable ContainerLaunchInfo (contains environment variables, namespaces, etc.) to be returned which will be used by Mesos containerize to launch containers.     By doing this (ContainerPrepareInfo -> ContainerLaunchInfo), we can select any necessary information and passing then to launcher.",2
"Draft design doc for multi-role frameworks
Create a document that describes the problems with having only single-role frameworks and proposes an MVP solution and implementation approach.",8
"Mesos command task doesn't support volumes with image
Currently volumes are stripped when an image is specified running a command task with Mesos containerizer. ",3
"Design doc for simple appc image discovery
Create a design document describing the following:    - Model and abstraction of the Discoverer  - Workflow of the discovery process  ",5
"fs::enter(rootfs) does not work if 'rootfs' is read only.
I noticed this when I was testing the unified containerizer with the bind mount backend and no volumes.    The current implementation of fs::enter will put the old root under /tmp/._old_root_.XXXXXX in the new rootfs. It assumes that /tmp is writable in the new rootfs, but this might not be true, especially if the bind mount backend is used.    To solve the problem, what we can do is to mount tmpfs to /tmp in the new rootfs and umount it after pivot_root.",2
"Tests for quota with implicit roles.
With the introduction of implicit roles (MESOS-3988), we should make sure quota can be set for an inactive role (unknown to the master) and maybe transition it to the active state.",3
"Protobuf parse should support parsing JSON object containing JSON Null.
(This bug was exposed by MESOS-4184, when serializing docker v1 image manifest as protobuf).    Currently protobuf::parse returns failures when parsing any JSON containing JSON::Null. If we have any protobuf field set as `JSON::Null`, any other non-repeated field cannot capture their value. For example, assuming we have a protobuf message:  {noformat}  message Nested {    optional string str = 1;    repeated string json_null = 2;  }  {noformat}    If there exists any field containing JSON::Null, like below:  {noformat}        {          \""str\"": \""message\"",          \""json_null\"": null        }  {noformat}  When we do protobuf::parse, it would return the following failure:  {noformat}  Failure parse: Not expecting a JSON null  {noformat}",1
"Change documentation links to ""*.md""
Right now, links either use the form {noformat}[label](/documentation/latest/foo/){noformat} or {noformat}[label](foo.md){noformat}. We should probably switch to using the latter form consistently -- it previews better on Github, and it will make it easier to have multiple versions of the docs on the website at once in the future.",3
"Add docker URI fetcher plugin based on curl.
The existing registry client for docker assumes that Mesos is built using SSL support and SSL is enabled. That means Mesos built with libev (or if SSL is disabled) won't be able to use docker registry client to provision docker images.    Given the new URI fetcher (MESOS-3918) work has been committed, we can add a new URI fetcher plugin for docker. The plugin will be based on curl so that https and 3xx redirects will be handled automatically. The docker registry puller will just use the URI fetcher to get docker images.",8
"Sync up configuration.md and flags.cpp
The https://reviews.apache.org/r/39923/ made some clean up for configuration.md but the related flags.cpp was not updated, we should update those files as well.",1
"Add AuthN and AuthZ to maintenance endpoints.
Maintenance endpoints are currently only restricted by firewall settings.  They should also support authentication/authorization like other HTTP endpoints.",3
"Accepting an inverse offer prints misleading logs
Whenever a scheduler accepts an inverse offer, Mesos will print a line like this in the master logs:  {code}  W1125 10:05:53.155109 29362 master.cpp:2897] ACCEPT call used invalid offers '[ 932f7d7b-f2d4-42c7-9391-222c19b9d35b-O2 ]': Offer 932f7d7b-f2d4-42c7-9391-222c19b9d35b-O2 is no longer valid  {code}    Inverse offers should not trigger this warning.",1
"hdfs operations fail due to prepended / on path for non-hdfs hadoop clients.
This bug was resolved for the hdfs protocol for MESOS-3602 but since the process checks for the ""hdfs"" protocol at the beginning of the URI, the fix does not extend itself to non-hdfs hadoop clients.  {code}  I0107 01:22:01.259490 17678 logging.cpp:172] INFO level logging started!  I0107 01:22:01.259856 17678 fetcher.cpp:422] Fetcher Info: {""cache_directory"":""\/tmp\/mesos\/fetch\/slaves\/530dda5a-481a-4117-8154-3aee637d3b38-S3\/root"",""items"":[{""action"":""BYPASS_CACHE"",""uri"":{""extract"":true,""value"":""maprfs:\/\/\/mesos\/storm-mesos-0.9.3.tgz""}},{""action"":""BYPASS_CACHE"",""uri"":{""extract"":true,""value"":""http:\/\/s0121.stag.urbanairship.com:36373\/conf\/storm.yaml""}}],""sandbox_directory"":""\/mnt\/data\/mesos\/slaves\/530dda5a-481a-4117-8154-3aee637d3b38-S3\/frameworks\/530dda5a-481a-4117-8154-3aee637d3b38-0000\/executors\/word-count-1-1452129714\/runs\/4443d5ac-d034-49b3-bf12-08fb9b0d92d0"",""user"":""root""}  I0107 01:22:01.262171 17678 fetcher.cpp:377] Fetching URI 'maprfs:///mesos/storm-mesos-0.9.3.tgz'  I0107 01:22:01.262212 17678 fetcher.cpp:248] Fetching directly into the sandbox directory  I0107 01:22:01.262243 17678 fetcher.cpp:185] Fetching URI 'maprfs:///mesos/storm-mesos-0.9.3.tgz'  I0107 01:22:01.671777 17678 fetcher.cpp:110] Downloading resource with Hadoop client from 'maprfs:///mesos/storm-mesos-0.9.3.tgz' to '/mnt/data/mesos/slaves/530dda5a-481a-4117-8154-3aee637d3b38-S3/frameworks/530dda5a-481a-4117-8154-3aee637d3b38-0000/executors/word-count-1-1452129714/runs/4443d5ac-d034-49b3-bf12-08fb9b0d92d0/storm-mesos-0.9.3.tgz'  copyToLocal: java.net.URISyntaxException: Expected scheme-specific part at index 7: maprfs:  Usage: java FsShell [-copyToLocal [-ignoreCrc] [-crc] <src> <localdst>]  E0107 01:22:02.435556 17678 shell.hpp:90] Command 'hadoop fs -copyToLocal '/maprfs:///mesos/storm-mesos-0.9.3.tgz' '/mnt/data/mesos/slaves/530dda5a-481a-4117-8154-3aee637d3b38-S3/frameworks/530dda5a-481a-4117-8154-3aee637d3b38-0000/executors/word-count-1-1452129714/runs/4443d5ac-d034-49b3-bf12-08fb9b0d92d0/storm-mesos-0.9.3.tgz'' failed; this is the output:  Failed to fetch 'maprfs:///mesos/storm-mesos-0.9.3.tgz': HDFS copyToLocal failed: Failed to execute 'hadoop fs -copyToLocal '/maprfs:///mesos/storm-mesos-0.9.3.tgz' '/mnt/data/mesos/slaves/530dda5a-481a-4117-8154-3aee637d3b38-S3/frameworks/530dda5a-481a-4117-8154-3aee637d3b38-0000/executors/word-count-1-1452129714/runs/4443d5ac-d034-49b3-bf12-08fb9b0d92d0/storm-mesos-0.9.3.tgz''; the command was either not found or exited with a non-zero exit status: 255  Failed to synchronize with slave (it's probably exited)  {code}    After a brief chat with [~jieyu], it was recommended to fix the current hdfs client code because the new hadoop fetcher plugin is slated to use it.",1
"Expand the ""Getting Started"" installation instructions
The ""Getting Started"" documentation currently contains basic instructions to prepare several platforms for compilation and installation of Mesos. However, these instructions are not sufficient to run and pass all tests in the test suite, using all configuration options. The installation instructions should be made comprehensive in this respect.    It may also be desirable to provide scripts that have been verified to prepare a particular base OS to build, install, and test Mesos. This would be very useful for both developers and users of Mesos.    Note that using some features on some platforms requires the installation of software packages from sources that may not be completely reliable in the long-term; for example, packages which are maintained as personal projects of individuals. This should be noted in the instructions accordingly.",5
"Reliably report executor terminations to framework schedulers.
Now that executor terminations are reported (unreliably), we should investigate queuing up these messages (on the agent?) and resending them periodically until we get an acknowledgement, much like status updates do.    From MESOS-313: The Scheduler interface has a callback for executorLost, but currently it is never called.",5
"Protobuf parse should pass error messages when parsing nested JSON.
Currently when protobuf::parse handles nested JSON objects, it cannot pass any error message out. We should enable showing those error messages.",1
"Publish Quota Documentation
Publish and finish the operator guide draft  for quota which describes basic usage of the endpoints and few basic and advanced usage cases.",3
"Support get non-default weights by /weights
Like /quota, we should also add query logic for /weights to keep consistent. Then /roles no longer needs to show weight information.",5
"PersistentVolumeTest.BadACLNoPrincipal is flaky
https://builds.apache.org/job/Mesos/1457/COMPILER=gcc,CONFIGURATION=--verbose%20--enable-libevent%20--enable-ssl,OS=centos:7,label_exp=docker%7C%7CHadoop/consoleFull    {noformat}  [ RUN      ] PersistentVolumeTest.BadACLNoPrincipal  I0108 01:13:16.117883  1325 leveldb.cpp:174] Opened db in 2.614722ms  I0108 01:13:16.118650  1325 leveldb.cpp:181] Compacted db in 706567ns  I0108 01:13:16.118702  1325 leveldb.cpp:196] Created db iterator in 24489ns  I0108 01:13:16.118723  1325 leveldb.cpp:202] Seeked to beginning of db in 2436ns  I0108 01:13:16.118738  1325 leveldb.cpp:271] Iterated through 0 keys in the db in 397ns  I0108 01:13:16.118793  1325 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned  I0108 01:13:16.119627  1348 recover.cpp:447] Starting replica recovery  I0108 01:13:16.120352  1348 recover.cpp:473] Replica is in EMPTY status  I0108 01:13:16.121750  1357 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (7084)@172.17.0.2:32801  I0108 01:13:16.122297  1353 recover.cpp:193] Received a recover response from a replica in EMPTY status  I0108 01:13:16.122747  1350 recover.cpp:564] Updating replica status to STARTING  I0108 01:13:16.123625  1354 master.cpp:365] Master 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2 (d9632dd1c41e) started on 172.17.0.2:32801  I0108 01:13:16.123946  1347 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 728242ns  I0108 01:13:16.123999  1347 replica.cpp:320] Persisted replica status to STARTING  I0108 01:13:16.123708  1354 master.cpp:367] Flags at startup: --acls=""create_volumes {    principals {      values: ""test-principal""    }    volume_types {      type: ANY    }  }  create_volumes {    principals {      type: ANY    }    volume_types {      type: NONE    }  }  "" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""false"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/f2rA75/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""25secs"" --registry_strict=""true"" --roles=""role1"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.27.0/_inst/share/mesos/webui"" --work_dir=""/tmp/f2rA75/master"" --zk_session_timeout=""10secs""  I0108 01:13:16.124219  1354 master.cpp:414] Master allowing unauthenticated frameworks to register  I0108 01:13:16.124236  1354 master.cpp:417] Master only allowing authenticated slaves to register  I0108 01:13:16.124248  1354 credentials.hpp:35] Loading credentials for authentication from '/tmp/f2rA75/credentials'  I0108 01:13:16.124294  1358 recover.cpp:473] Replica is in STARTING status  I0108 01:13:16.124644  1354 master.cpp:456] Using default 'crammd5' authenticator  I0108 01:13:16.124820  1354 master.cpp:493] Authorization enabled  W0108 01:13:16.124843  1354 master.cpp:553] The '--roles' flag is deprecated. This flag will be removed in the future. See the Mesos 0.27 upgrade notes for more information  I0108 01:13:16.125154  1348 hierarchical.cpp:147] Initialized hierarchical allocator process  I0108 01:13:16.125334  1345 whitelist_watcher.cpp:77] No whitelist given  I0108 01:13:16.126065  1346 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (7085)@172.17.0.2:32801  I0108 01:13:16.126806  1348 recover.cpp:193] Received a recover response from a replica in STARTING status  I0108 01:13:16.128237  1354 recover.cpp:564] Updating replica status to VOTING  I0108 01:13:16.128402  1359 master.cpp:1629] The newly elected leader is master@172.17.0.2:32801 with id 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2  I0108 01:13:16.128489  1359 master.cpp:1642] Elected as the leading master!  I0108 01:13:16.128523  1359 master.cpp:1387] Recovering from registrar  I0108 01:13:16.128756  1355 registrar.cpp:307] Recovering registrar  I0108 01:13:16.129259  1344 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 531437ns  I0108 01:13:16.129292  1344 replica.cpp:320] Persisted replica status to VOTING  I0108 01:13:16.129425  1358 recover.cpp:578] Successfully joined the Paxos group  I0108 01:13:16.129680  1358 recover.cpp:462] Recover process terminated  I0108 01:13:16.130187  1358 log.cpp:659] Attempting to start the writer  I0108 01:13:16.131613  1352 replica.cpp:493] Replica received implicit promise request from (7086)@172.17.0.2:32801 with proposal 1  I0108 01:13:16.131983  1352 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 333646ns  I0108 01:13:16.132004  1352 replica.cpp:342] Persisted promised to 1  I0108 01:13:16.132627  1348 coordinator.cpp:238] Coordinator attempting to fill missing positions  I0108 01:13:16.133896  1349 replica.cpp:388] Replica received explicit promise request from (7087)@172.17.0.2:32801 for position 0 with proposal 2  I0108 01:13:16.134289  1349 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 349652ns  I0108 01:13:16.134317  1349 replica.cpp:712] Persisted action at 0  I0108 01:13:16.135470  1351 replica.cpp:537] Replica received write request for position 0 from (7088)@172.17.0.2:32801  I0108 01:13:16.135537  1351 leveldb.cpp:436] Reading position from leveldb took 36181ns  I0108 01:13:16.135901  1351 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 308752ns  I0108 01:13:16.135924  1351 replica.cpp:712] Persisted action at 0  I0108 01:13:16.136529  1347 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0  I0108 01:13:16.136889  1347 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 327106ns  I0108 01:13:16.136916  1347 replica.cpp:712] Persisted action at 0  I0108 01:13:16.136943  1347 replica.cpp:697] Replica learned NOP action at position 0  I0108 01:13:16.137707  1359 log.cpp:675] Writer started with ending position 0  I0108 01:13:16.138844  1348 leveldb.cpp:436] Reading position from leveldb took 31371ns  I0108 01:13:16.139878  1356 registrar.cpp:340] Successfully fetched the registry (0B) in 0ns  I0108 01:13:16.140012  1356 registrar.cpp:439] Applied 1 operations in 42063ns; attempting to update the 'registry'  I0108 01:13:16.140797  1355 log.cpp:683] Attempting to append 170 bytes to the log  I0108 01:13:16.140974  1345 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1  I0108 01:13:16.141744  1354 replica.cpp:537] Replica received write request for position 1 from (7089)@172.17.0.2:32801  I0108 01:13:16.142226  1354 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 441971ns  I0108 01:13:16.142251  1354 replica.cpp:712] Persisted action at 1  I0108 01:13:16.142860  1351 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0  I0108 01:13:16.143198  1351 leveldb.cpp:341] Persisting action (191 bytes) to leveldb took 305928ns  I0108 01:13:16.143223  1351 replica.cpp:712] Persisted action at 1  I0108 01:13:16.143241  1351 replica.cpp:697] Replica learned APPEND action at position 1  I0108 01:13:16.144271  1354 registrar.cpp:484] Successfully updated the 'registry' in 0ns  I0108 01:13:16.144435  1354 registrar.cpp:370] Successfully recovered registrar  I0108 01:13:16.144567  1359 log.cpp:702] Attempting to truncate the log to 1  I0108 01:13:16.144780  1359 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2  I0108 01:13:16.144989  1348 hierarchical.cpp:165] Skipping recovery of hierarchical allocator: nothing to recover  I0108 01:13:16.144928  1354 master.cpp:1439] Recovered 0 slaves from the Registry (131B) ; allowing 10mins for slaves to re-register  I0108 01:13:16.145690  1357 replica.cpp:537] Replica received write request for position 2 from (7090)@172.17.0.2:32801  I0108 01:13:16.146072  1357 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 345113ns  I0108 01:13:16.146097  1357 replica.cpp:712] Persisted action at 2  I0108 01:13:16.146667  1358 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0  I0108 01:13:16.147060  1358 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 283648ns  I0108 01:13:16.147116  1358 leveldb.cpp:399] Deleting ~1 keys from leveldb took 32174ns  I0108 01:13:16.147135  1358 replica.cpp:712] Persisted action at 2  I0108 01:13:16.147153  1358 replica.cpp:697] Replica learned TRUNCATE action at position 2  I0108 01:13:16.166832  1325 containerizer.cpp:139] Using isolation: posix/cpu,posix/mem,filesystem/posix  W0108 01:13:16.167556  1325 backend.cpp:48] Failed to create 'bind' backend: BindBackend requires root privileges  I0108 01:13:16.170526  1349 slave.cpp:191] Slave started on 231)@172.17.0.2:32801  I0108 01:13:16.170718  1349 slave.cpp:192] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/PersistentVolumeTest_BadACLNoPrincipal_yqJjLY/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""https://auth.docker.io"" --docker_kill_orphans=""true"" --docker_puller_timeout=""60"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/PersistentVolumeTest_BadACLNoPrincipal_yqJjLY/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.27.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk(role1):2048"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/PersistentVolumeTest_BadACLNoPrincipal_yqJjLY""  I0108 01:13:16.171269  1349 credentials.hpp:83] Loading credential for authentication from '/tmp/PersistentVolumeTest_BadACLNoPrincipal_yqJjLY/credential'  I0108 01:13:16.171505  1349 slave.cpp:322] Slave using credential for: test-principal  I0108 01:13:16.171747  1349 resources.cpp:481] Parsing resources as JSON failed: cpus:2;mem:1024;disk(role1):2048  Trying semicolon-delimited string format instead  I0108 01:13:16.172266  1349 slave.cpp:392] Slave resources: cpus(*):2; mem(*):1024; disk(role1):2048; ports(*):[31000-32000]  I0108 01:13:16.172327  1349 slave.cpp:400] Slave attributes: [  ]  I0108 01:13:16.172340  1349 slave.cpp:405] Slave hostname: d9632dd1c41e  I0108 01:13:16.172353  1349 slave.cpp:410] Slave checkpoint: true  I0108 01:13:16.173418  1353 state.cpp:58] Recovering state from '/tmp/PersistentVolumeTest_BadACLNoPrincipal_yqJjLY/meta'  I0108 01:13:16.173521  1325 sched.cpp:164] Version: 0.27.0  I0108 01:13:16.174054  1345 status_update_manager.cpp:200] Recovering status update manager  I0108 01:13:16.174289  1353 containerizer.cpp:387] Recovering containerizer  I0108 01:13:16.174295  1356 sched.cpp:268] New master detected at master@172.17.0.2:32801  I0108 01:13:16.174387  1356 sched.cpp:278] No credentials provided. Attempting to register without authentication  I0108 01:13:16.174409  1356 sched.cpp:722] Sending SUBSCRIBE call to master@172.17.0.2:32801  I0108 01:13:16.174515  1356 sched.cpp:755] Will retry registration in 1.699889272secs if necessary  I0108 01:13:16.174653  1349 master.cpp:2197] Received SUBSCRIBE call for framework 'no-principal' at scheduler-bf0ed267-b4c4-412d-9fb0-84c85cd2fbce@172.17.0.2:32801  I0108 01:13:16.174823  1349 master.cpp:1668] Authorizing framework principal '' to receive offers for role 'role1'  I0108 01:13:16.175250  1347 master.cpp:2268] Subscribing framework no-principal with checkpointing disabled and capabilities [  ]  I0108 01:13:16.175359  1353 slave.cpp:4429] Finished recovery  I0108 01:13:16.175715  1345 hierarchical.cpp:260] Added framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000  I0108 01:13:16.175734  1351 sched.cpp:649] Framework registered with 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000  I0108 01:13:16.175792  1345 hierarchical.cpp:1329] No resources available to allocate!  I0108 01:13:16.175833  1345 hierarchical.cpp:1423] No inverse offers to send out!  I0108 01:13:16.175853  1353 slave.cpp:4601] Querying resource estimator for oversubscribable resources  I0108 01:13:16.175869  1345 hierarchical.cpp:1079] Performed allocation for 0 slaves in 127881ns  I0108 01:13:16.175923  1351 sched.cpp:663] Scheduler::registered took 27956ns  I0108 01:13:16.176110  1353 slave.cpp:729] New master detected at master@172.17.0.2:32801  I0108 01:13:16.176187  1353 slave.cpp:792] Authenticating with master master@172.17.0.2:32801  I0108 01:13:16.176216  1353 slave.cpp:797] Using default CRAM-MD5 authenticatee  I0108 01:13:16.176398  1357 status_update_manager.cpp:174] Pausing sending status updates  I0108 01:13:16.176404  1353 slave.cpp:765] Detecting new master  I0108 01:13:16.176463  1358 authenticatee.cpp:121] Creating new client SASL connection  I0108 01:13:16.176553  1353 slave.cpp:4615] Received oversubscribable resources  from the resource estimator  I0108 01:13:16.176709  1353 master.cpp:5445] Authenticating slave(231)@172.17.0.2:32801  I0108 01:13:16.176823  1359 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(516)@172.17.0.2:32801  I0108 01:13:16.177135  1348 authenticator.cpp:98] Creating new server SASL connection  I0108 01:13:16.177373  1356 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5  I0108 01:13:16.177399  1356 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'  I0108 01:13:16.177502  1344 authenticator.cpp:203] Received SASL authentication start  I0108 01:13:16.177563  1344 authenticator.cpp:325] Authentication requires more steps  I0108 01:13:16.177680  1346 authenticatee.cpp:258] Received SASL authentication step  I0108 01:13:16.177848  1354 authenticator.cpp:231] Received SASL authentication step  I0108 01:13:16.177883  1354 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'd9632dd1c41e' server FQDN: 'd9632dd1c41e' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false   I0108 01:13:16.177894  1354 auxprop.cpp:179] Looking up auxiliary property '*userPassword'  I0108 01:13:16.177944  1354 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'  I0108 01:13:16.177994  1354 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'd9632dd1c41e' server FQDN: 'd9632dd1c41e' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true   I0108 01:13:16.178014  1354 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true  I0108 01:13:16.178040  1354 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true  I0108 01:13:16.178066  1354 authenticator.cpp:317] Authentication success  I0108 01:13:16.178256  1355 authenticatee.cpp:298] Authentication success  I0108 01:13:16.178315  1354 master.cpp:5475] Successfully authenticated principal 'test-principal' at slave(231)@172.17.0.2:32801  I0108 01:13:16.178356  1355 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(516)@172.17.0.2:32801  I0108 01:13:16.178710  1354 slave.cpp:860] Successfully authenticated with master master@172.17.0.2:32801  I0108 01:13:16.178865  1354 slave.cpp:1254] Will retry registration in 13.009431ms if necessary  I0108 01:13:16.179138  1350 master.cpp:4154] Registering slave at slave(231)@172.17.0.2:32801 (d9632dd1c41e) with id 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0  I0108 01:13:16.179628  1345 registrar.cpp:439] Applied 1 operations in 71663ns; attempting to update the 'registry'  I0108 01:13:16.180505  1356 log.cpp:683] Attempting to append 343 bytes to the log  I0108 01:13:16.180711  1352 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3  I0108 01:13:16.181499  1350 replica.cpp:537] Replica received write request for position 3 from (7103)@172.17.0.2:32801  I0108 01:13:16.182080  1350 leveldb.cpp:341] Persisting action (362 bytes) to leveldb took 537757ns  I0108 01:13:16.182112  1350 replica.cpp:712] Persisted action at 3  I0108 01:13:16.182749  1351 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0  I0108 01:13:16.183120  1351 leveldb.cpp:341] Persisting action (364 bytes) to leveldb took 340999ns  I0108 01:13:16.183151  1351 replica.cpp:712] Persisted action at 3  I0108 01:13:16.183177  1351 replica.cpp:697] Replica learned APPEND action at position 3  I0108 01:13:16.184787  1348 registrar.cpp:484] Successfully updated the 'registry' in 0ns  I0108 01:13:16.185287  1348 log.cpp:702] Attempting to truncate the log to 3  I0108 01:13:16.185484  1349 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4  I0108 01:13:16.186043  1353 slave.cpp:3371] Received ping from slave-observer(230)@172.17.0.2:32801  I0108 01:13:16.186074  1345 master.cpp:4222] Registered slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0 at slave(231)@172.17.0.2:32801 (d9632dd1c41e) with cpus(*):2; mem(*):1024; disk(role1):2048; ports(*):[31000-32000]  I0108 01:13:16.186224  1353 slave.cpp:904] Registered with master master@172.17.0.2:32801; given slave ID 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0  I0108 01:13:16.186441  1353 fetcher.cpp:81] Clearing fetcher cache  I0108 01:13:16.186486  1349 hierarchical.cpp:465] Added slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0 (d9632dd1c41e) with cpus(*):2; mem(*):1024; disk(role1):2048; ports(*):[31000-32000] (allocated: )  I0108 01:13:16.186658  1346 status_update_manager.cpp:181] Resuming sending status updates  I0108 01:13:16.186885  1353 slave.cpp:927] Checkpointing SlaveInfo to '/tmp/PersistentVolumeTest_BadACLNoPrincipal_yqJjLY/meta/slaves/773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0/slave.info'  I0108 01:13:16.186905  1350 replica.cpp:537] Replica received write request for position 4 from (7104)@172.17.0.2:32801  I0108 01:13:16.187595  1350 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 645704ns  I0108 01:13:16.187628  1350 replica.cpp:712] Persisted action at 4  I0108 01:13:16.188347  1349 hierarchical.cpp:1423] No inverse offers to send out!  I0108 01:13:16.188475  1349 hierarchical.cpp:1101] Performed allocation for slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0 in 1.861833ms  I0108 01:13:16.188560  1348 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0  I0108 01:13:16.188385  1353 slave.cpp:963] Forwarding total oversubscribed resources   I0108 01:13:16.189275  1344 master.cpp:5274] Sending 1 offers to framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000 (no-principal) at scheduler-bf0ed267-b4c4-412d-9fb0-84c85cd2fbce@172.17.0.2:32801  I0108 01:13:16.189792  1344 master.cpp:4564] Received update of slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0 at slave(231)@1...",1
"SlaveTest.LaunchTaskInfoWithContainerInfo cannot be execute in isolation
Executing {{SlaveTest.LaunchTaskInfoWithContainerInfo}} from {{468b8ec}} under OS X 10.10.5 in isolation fails due to missing cleanup,  {code}  % ./bin/mesos-tests.sh --gtest_filter=SlaveTest.LaunchTaskInfoWithContainerInfo  Source directory: /ABC/DEF/src/mesos  Build directory: /ABC/DEF/src/mesos/build  -------------------------------------------------------------  We cannot run any Docker tests because:  Docker tests not supported on non-Linux systems  -------------------------------------------------------------  /usr/bin/nc  /usr/bin/curl  Note: Google Test filter = SlaveTest.LaunchTaskInfoWithContainerInfo-HealthCheckTest.ROOT_DOCKER_DockerHealthyTask:HealthCheckTest.ROOT_DOCKER_DockerHealthStatusChange:HierarchicalAllocator_BENCHMARK_Test.DeclineOffers:HookTest.ROOT_DOCKER_VerifySlavePreLaunchDockerHook:SlaveTest.ROOT_RunTaskWithCommandInfoWithoutUser:SlaveTest.DISABLED_ROOT_RunTaskWithCommandInfoWithUser:DockerContainerizerTest.ROOT_DOCKER_Launch:DockerContainerizerTest.ROOT_DOCKER_Kill:DockerContainerizerTest.ROOT_DOCKER_Usage:DockerContainerizerTest.ROOT_DOCKER_Recover:DockerContainerizerTest.ROOT_DOCKER_SkipRecoverNonDocker:DockerContainerizerTest.ROOT_DOCKER_Logs:DockerContainerizerTest.ROOT_DOCKER_Default_CMD:DockerContainerizerTest.ROOT_DOCKER_Default_CMD_Override:DockerContainerizerTest.ROOT_DOCKER_Default_CMD_Args:DockerContainerizerTest.ROOT_DOCKER_SlaveRecoveryTaskContainer:DockerContainerizerTest.DISABLED_ROOT_DOCKER_SlaveRecoveryExecutorContainer:DockerContainerizerTest.ROOT_DOCKER_NC_PortMapping:DockerContainerizerTest.ROOT_DOCKER_LaunchSandboxWithColon:DockerContainerizerTest.ROOT_DOCKER_DestroyWhileFetching:DockerContainerizerTest.ROOT_DOCKER_DestroyWhilePulling:DockerContainerizerTest.ROOT_DOCKER_ExecutorCleanupWhenLaunchFailed:DockerContainerizerTest.ROOT_DOCKER_FetchFailure:DockerContainerizerTest.ROOT_DOCKER_DockerPullFailure:DockerContainerizerTest.ROOT_DOCKER_DockerInspectDiscard:DockerTest.ROOT_DOCKER_interface:DockerTest.ROOT_DOCKER_parsing_version:DockerTest.ROOT_DOCKER_CheckCommandWithShell:DockerTest.ROOT_DOCKER_CheckPortResource:DockerTest.ROOT_DOCKER_CancelPull:DockerTest.ROOT_DOCKER_MountRelative:DockerTest.ROOT_DOCKER_MountAbsolute:CopyBackendTest.ROOT_CopyBackend:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/0:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/1:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/2:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/3:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/4:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/5:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/6:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/7:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/8:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/9:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/10:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/11:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/12:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/13:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/14:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/15:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/16:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/17:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/18:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/19:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/20:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/21:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/22:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/23:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/24:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/25:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/26:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/27:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/28:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/29:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/30:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/31:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/32:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/33:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/34:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/35:SlaveCount/Registrar_BENCHMARK_Test.Performance/0:SlaveCount/Registrar_BENCHMARK_Test.Performance/1:SlaveCount/Registrar_BENCHMARK_Test.Performance/2:SlaveCount/Registrar_BENCHMARK_Test.Performance/3  [==========] Running 1 test from 1 test case.  [----------] Global test environment set-up.  [----------] 1 test from SlaveTest  [ RUN      ] SlaveTest.LaunchTaskInfoWithContainerInfo  [       OK ] SlaveTest.LaunchTaskInfoWithContainerInfo (79 ms)  [----------] 1 test from SlaveTest (79 ms total)    [----------] Global test environment tear-down  ../../src/tests/environment.cpp:569: Failure  Failed  Tests completed with child processes remaining:  -+- 54487 /ABC/DEF/src/mesos/build/src/.libs/mesos-tests --gtest_filter=SlaveTest.LaunchTaskInfoWithContainerInfo   \--- 54503 /bin/sh /ABC/DEF/src/mesos/build/src/mesos-containerizer launch --command={""shell"":true,""value"":""\/ABC\/DEF\/src\/mesos\/build\/src\/mesos-executor""} --commands={""commands"":[]} --directory=/tmp --help=false --pipe_read=10 --pipe_write=13 --user=test  [==========] 1 test from 1 test case ran. (87 ms total)  [  PASSED  ] 1 test.  [  FAILED  ] 0 tests, listed below:     0 FAILED TESTS  {code}  ",1
"Refactor Appc provisioner tests  
Current tests can be refactored so that we can reuse some common tasks like test image creation. This will benefit future tests like appc image puller tests.",2
"Document supported file types for archive extraction by fetcher
The Mesos fetcher extracts specified URIs if requested to do so by the scheduler. However, the documentation at http://mesos.apache.org/documentation/latest/fetcher/ doesn't list the file types /extensions that will be extracted by the fetcher.    [The relevant code|https://github.com/apache/mesos/blob/master/src/launcher/fetcher.cpp#L63] specifies an exhaustive list of extensions that will be extracted, the documentation should be updated to match.",1
"Implement a simple Windows version of dirent.hpp, for compatibility.
nan",5
"Create utilities for common shell commands used. 
We spawn shell for command line utilities like tar, untar, sha256 etc. Would be great for resuse if we can create a common utilities class/file for all these utilities.    ",5
"Add parameters to apply patches quiet
Added a parameters to apply the patches quiet; so it's easy for contributor to apply patches with -c.",1
"Allow operators to assign net_cls major handles to mesos agents
The net_cls cgroup allows operators to assign a 16-bit major and 16-bit minor network handle to tasks associated with a specific net_cls cgroup. In mesos we need to give the operator the ability to fix the 16-bit major handle used in an agent. Fixing the parent handle on the agent allows operators to install default firewall rules using the parent handle to enforce a default policy (say DENY ALL) for all container traffic till the container is allocated a minor handle.     A simple way to achieve this requirement is to pass the major handle as a flag to the agent at startup. ",1
"Implement a network-handle manager for net_cls cgroup subsystem
As part of implementing the net_cls cgroup isolator we need a mechanism to manage the minor handles that will allocated to containers when they are associated with a net_cls cgroup. The network-handle manager needs to provide the following functionality:    a) During normal operation keep track of the free and allocated network handles. There can be a total of 64K such network handles.  b) On startup, learn the allocated network handle by walking the net_cls cgroup tree for mesos and build a map of free network handles available to the agent. ",8
"GMock warning in ReservationTest.ACLMultipleOperations
{noformat}  [ RUN      ] ReservationTest.ACLMultipleOperations    GMOCK WARNING:  Uninteresting mock function call - returning directly.      Function call: shutdown(0x7fa2a311b300)  Stack trace:  [       OK ] ReservationTest.ACLMultipleOperations (174 ms)  [----------] 1 test from ReservationTest (174 ms total)  {noformat}    Seems to occur non-deterministically for me, maybe once per 50 runs or so. OSX 10.10",1
"GMock warning in HookTest.VerifySlaveRunTaskHook, HookTest.VerifySlaveTaskStatusDecorator
{noformat}  [ RUN      ] HookTest.VerifySlaveRunTaskHook    GMOCK WARNING:  Uninteresting mock function call - returning directly.      Function call: shutdown(0x7ff079cb2420)  Stack trace:  [       OK ] HookTest.VerifySlaveRunTaskHook (51 ms)  [ RUN      ] HookTest.VerifySlaveTaskStatusDecorator    GMOCK WARNING:  Uninteresting mock function call - returning directly.      Function call: shutdown(0x7ff079cbb790)  Stack trace:  [       OK ] HookTest.VerifySlaveTaskStatusDecorator (54 ms)  {noformat}    Occurs non-deterministically for me. OSX 10.10.",1
"GMock warning in SlaveTest.ContainerUpdatedBeforeTaskReachesExecutor
{noformat}  [ RUN      ] SlaveTest.ContainerUpdatedBeforeTaskReachesExecutor    GMOCK WARNING:  Uninteresting mock function call - returning directly.      Function call: shutdown(0x7fe189cae850)  Stack trace:  [       OK ] SlaveTest.ContainerUpdatedBeforeTaskReachesExecutor (51 ms)  {noformat}    Occurs non-deterministically for me on OSX 10.10, perhaps one run in ten.",1
"GMock warning on `offerRescinded` in `ReservationTest` fixture
Several tests involving checkpointing of resources in the {{ReservationTest}} fixture are throwing GMock warnings occasionally. Here is the output of {{GTEST_FILTER=""ReservationTest.*"" bin/mesos-tests.sh --gtest_repeat=10000 --gtest_break_on_failure=1 | grep -B 3 -A 6 WARNING}}:    {code}  -------------------------------------------------------------  We cannot run any Docker tests because:  Docker tests not supported on non-Linux systems  -------------------------------------------------------------  [       OK ] ReservationTest.MasterFailover (89 ms)  [ RUN      ] ReservationTest.CompatibleCheckpointedResources    GMOCK WARNING:  Uninteresting mock function call - returning directly.      Function call: offerRescinded(0x7fff5f014960, @0x7feec320fab0 65537c10-285c-419e-b89f-191283402d85-O1)  Stack trace:  [       OK ] ReservationTest.CompatibleCheckpointedResources (52 ms)  [ RUN      ] ReservationTest.CompatibleCheckpointedResourcesWithPersistentVolumes  [       OK ] ReservationTest.CompatibleCheckpointedResourcesWithPersistentVolumes (45 ms)  --  --  [       OK ] ReservationTest.CompatibleCheckpointedResources (46 ms)  [ RUN      ] ReservationTest.CompatibleCheckpointedResourcesWithPersistentVolumes    GMOCK WARNING:  Uninteresting mock function call - returning directly.      Function call: offerRescinded(0x7fff5f014960, @0x7feec796f220 bf4e1b52-02db-4763-8be0-3c759c80f1ba-O1)  Stack trace:  [       OK ] ReservationTest.CompatibleCheckpointedResourcesWithPersistentVolumes (63 ms)  [ RUN      ] ReservationTest.IncompatibleCheckpointedResources  [       OK ] ReservationTest.IncompatibleCheckpointedResources (45 ms)  --  --  [       OK ] ReservationTest.CompatibleCheckpointedResources (42 ms)  [ RUN      ] ReservationTest.CompatibleCheckpointedResourcesWithPersistentVolumes    GMOCK WARNING:  Uninteresting mock function call - returning directly.      Function call: offerRescinded(0x7fff5f014960, @0x7feec7ad92b0 42a9f1ff-122e-4df7-9530-a96126e36f84-O1)  Stack trace:  [       OK ] ReservationTest.CompatibleCheckpointedResourcesWithPersistentVolumes (65 ms)  [ RUN      ] ReservationTest.IncompatibleCheckpointedResources  [       OK ] ReservationTest.IncompatibleCheckpointedResources (46 ms)  --  --  [       OK ] ReservationTest.CompatibleCheckpointedResourcesWithPersistentVolumes (49 ms)  [ RUN      ] ReservationTest.IncompatibleCheckpointedResources    GMOCK WARNING:  Uninteresting mock function call - returning directly.      Function call: offerRescinded(0x7fff5f014960, @0x7feec7af4310 d5e1005f-abb8-4bfd-92e0-3976ee150fbf-O1)  Stack trace:  [       OK ] ReservationTest.IncompatibleCheckpointedResources (94 ms)  [ RUN      ] ReservationTest.GoodACLReserveThenUnreserve  [       OK ] ReservationTest.GoodACLReserveThenUnreserve (57 ms)  --  --  [       OK ] ReservationTest.CompatibleCheckpointedResources (43 ms)  [ RUN      ] ReservationTest.CompatibleCheckpointedResourcesWithPersistentVolumes    GMOCK WARNING:  Uninteresting mock function call - returning directly.      Function call: offerRescinded(0x7fff5f014960, @0x7feec7cdadc0 36e15f52-3299-46fa-850d-970097fef8e2-O1)  Stack trace:  [       OK ] ReservationTest.CompatibleCheckpointedResourcesWithPersistentVolumes (62 ms)  [ RUN      ] ReservationTest.IncompatibleCheckpointedResources  [       OK ] ReservationTest.IncompatibleCheckpointedResources (46 ms)  --  --  [       OK ] ReservationTest.CompatibleCheckpointedResources (47 ms)  [ RUN      ] ReservationTest.CompatibleCheckpointedResourcesWithPersistentVolumes    GMOCK WARNING:  Uninteresting mock function call - returning directly.      Function call: offerRescinded(0x7fff5f014960, @0x7feec8c1b580 c8dd35ab-7363-40e0-8e20-8c7dc76a8497-O1)  Stack trace:  [       OK ] ReservationTest.CompatibleCheckpointedResourcesWithPersistentVolumes (62 ms)  [ RUN      ] ReservationTest.IncompatibleCheckpointedResources  [       OK ] ReservationTest.IncompatibleCheckpointedResources (45 ms)  --  --  [       OK ] ReservationTest.CompatibleCheckpointedResources (47 ms)  [ RUN      ] ReservationTest.CompatibleCheckpointedResourcesWithPersistentVolumes    GMOCK WARNING:  Uninteresting mock function call - returning directly.      Function call: offerRescinded(0x7fff5f014960, @0x7feecbd9b5b0 031c2148-8a20-4532-b77f-b6200c3791c8-O1)  Stack trace:  [       OK ] ReservationTest.CompatibleCheckpointedResourcesWithPersistentVolumes (62 ms)  [ RUN      ] ReservationTest.IncompatibleCheckpointedResources  [       OK ] ReservationTest.IncompatibleCheckpointedResources (46 ms)  --  --  [       OK ] ReservationTest.CompatibleCheckpointedResourcesWithPersistentVolumes (47 ms)  [ RUN      ] ReservationTest.IncompatibleCheckpointedResources    GMOCK WARNING:  Uninteresting mock function call - returning directly.      Function call: offerRescinded(0x7fff5f014960, @0x7feecd52adb0 edc5a322-b220-4b13-a39b-99a523b172ba-O1)  Stack trace:  [       OK ] ReservationTest.IncompatibleCheckpointedResources (76 ms)  [ RUN      ] ReservationTest.GoodACLReserveThenUnreserve  [       OK ] ReservationTest.GoodACLReserveThenUnreserve (63 ms)  --  --  [       OK ] ReservationTest.SendingCheckpointResourcesMessage (45 ms)  [ RUN      ] ReservationTest.ResourcesCheckpointing    GMOCK WARNING:  Uninteresting mock function call - returning directly.      Function call: offerRescinded(0x7fff5f015df8, @0x7feecfe16f00 09a90e67-a40f-4e42-8802-1a5644733a06-O1)  Stack trace:  [       OK ] ReservationTest.ResourcesCheckpointing (60 ms)  [ RUN      ] ReservationTest.MasterFailover  [       OK ] ReservationTest.MasterFailover (89 ms)  --  --  [       OK ] ReservationTest.CompatibleCheckpointedResources (43 ms)  [ RUN      ] ReservationTest.CompatibleCheckpointedResourcesWithPersistentVolumes    GMOCK WARNING:  Uninteresting mock function call - returning directly.      Function call: offerRescinded(0x7fff5f014960, @0x7feecacceba0 84965984-28cd-4bc8-b25b-746583477d09-O1)  Stack trace:  [       OK ] ReservationTest.CompatibleCheckpointedResourcesWithPersistentVolumes (58 ms)  [ RUN      ] ReservationTest.IncompatibleCheckpointedResources  [       OK ] ReservationTest.IncompatibleCheckpointedResources (68 ms)  {code}",2
"Limit the number of processes created by libprocess
Currently libprocess will create {{max(8, number of CPU cores)}} processes during the initialization, see https://github.com/apache/mesos/blob/0.26.0/3rdparty/libprocess/src/process.cpp#L2146 for details. This should be OK for a normal machine which has no much cores (e.g., 16, 32), but for a powerful machine which may have a large number of cores (e.g., an IBM Power machine may have 192 cores), this will cause too much worker threads which are not necessary.    And since libprocess is widely used in Mesos (master, agent, scheduler, executor), it may also cause some performance issue. For example, when user creates a Docker container via Mesos in a Mesos agent which is running on a powerful machine with 192 cores, the DockerContainerizer in Mesos agent will create a dedicated executor for the container, and there will be 192 worker threads in that executor. And if user creates 1000 Docker containers in that machine, then there will be 1000 executors, i.e., 1000 * 192 worker threads which is a large number and may thrash the OS.    ",1
"GMock warning in RoleTest.ImplicitRoleStaticReservation
{noformat}  [ RUN      ] RoleTest.ImplicitRoleStaticReservation    GMOCK WARNING:  Uninteresting mock function call - returning directly.      Function call: shutdown(0x7fe37a4752f0)  Stack trace:  [       OK ] RoleTest.ImplicitRoleStaticReservation (52 ms)  {noformat}",1
"Expose net_cls network handles in agent's state endpoint
We need to expose net_cls network handles, associated with containers, to operators and network utilities that would use these network handles to enforce network policy.     In order to achieve the above we need to add a new field in the `NetworkInfo` protobuf (say NetHandles) and update this field when a container gets assigned to a net_cls cgroup. The `ContainerStatus` protobuf already has the `NetworkInfo` protobuf as a nested message, and the `ContainerStatus` itself is exposed to operators as part of TaskInfo (for tasks associated with the container) in an agent's state.json. ",2
"GMock warning in DockerContainerizerTest.ROOT_DOCKER_DockerInspectDiscard
The following GMock warning was seen on CentOS 7.1:    {code}  [ RUN      ] DockerContainerizerTest.ROOT_DOCKER_DockerInspectDiscard    GMOCK WARNING:  Uninteresting mock function call - returning directly.      Function call: executorLost(0x7ffdd74f73e0, @0x7f3e3c00fa20 e1, @0x7f3e3c00f4b0 cf212bb4-c8c5-4a43-b71f-c17b27458627-S0, -1)  Stack trace:  [       OK ] DockerContainerizerTest.ROOT_DOCKER_DockerInspectDiscard (405 ms)  {code}",2
"Create common tar/untar utility function.
As part of refactoring and creating a common place to add all command utilities, add *tar* and *untar* as the first POC.",3
"Formating issues and broken links in documentation.
The online documentation has a number of bad formatting issues and broken links (e.g., mesos-provider.md).",1
"Add a roles field to FrameworkInfo
To represent multiple roles per framework a new repeated string field for roles is needed.",1
"Add roles validation code to master
A {{FrameworkInfo}} can only have one of role or roles. A natural location for this appears to be under {{validation::operation::validate}}.",5
"Add internal migration from role to roles to master
If only the {{role}} field is given, add it as single entry to {{roles}}. Add a note to {{CHANGELOG}}/release notes on deprecation of the existing {{role}} field. File a JIRA issue for removal of that migration code once the deprecation cycle is over.  ",3
"Migrate all existing uses of FrameworkInfo.role to FrameworkInfo.roles
nan",3
"Add tracking of the role a Resource was offered for
If a framework can have multiple roles, we need a way to identify for which of the framework's role a resource was offered for (e.g., for resource recovery and reconciliation).",5
"Make HierarchicalAllocatorProcess set a Resource's active role during allocation
The concrete implementation here depends on the implementation strategy used to solve MESOS-4367.",3
"Document semantics of `slaveLost`
We should clarify the semantics of this callback:    * Is it always invoked, or just a hint?  * Can a slave ever come back from `slaveLost`?  * What happens to persistent resources on a lost slave?    The new HA framework development guide might be a good place to put (some of?) this information.  ",2
"Document units associated with resource types
We should document the units associated with memory and disk resources.",1
"Add Source to Resource.DiskInfo.
Source is used to describe the extra information about the source of a Disk resource. We will support 'PATH' type first and then 'BLOCK' later.    {noformat}  message Source {        enum Type {          PATH = 1;          BLOCK = 2,        }          message Path {          // Path to the folder (e.g., /mnt/raid/disk0).          required string root = 1;          required double total_size = 2;        }          message Block {          // Path to the device file (e.g., /dev/sda1, /dev/vg/v1).          // It can be a physical partition, or a logical volume (LVM).          required string device = 1;        }          required Type type = 1;        optional Path path = 2;        optional Block block = 3;      }  }  {noformat}",1
"Design doc for reservation IDs
nan",3
"Adjust Resource arithmetics for DiskInfo.Source.
Since we added the Source for DiskInfo, we need to adjust the Resource arithmetics for that. That includes equality check, addable check, subtractable check, etc.",2
"Improve upgrade compatibility documentation.
Investigate and document upgrade compatibility for 0.27 release.",3
"Change the `principal` in `ReservationInfo` to optional
With the addition of HTTP endpoints for {{/reserve}} and {{/unreserve}}, it is now desirable to allow dynamic reservations without a principal, in the case where HTTP authentication is disabled. To allow for this, we will change the {{principal}} field in {{ReservationInfo}} from required to optional. For backwards-compatibility, however, the master should currently invalidate any {{ReservationInfo}} messages that do not have this field set.",1
"Support docker runtime configuration env var from image.
We need to support env var configuration returned from docker image in mesos containerizer.",2
"Offers and InverseOffers cannot be accepted in the same ACCEPT call
*Problem*  * In {{Master::accept}}, {{validation::offer::validate}} returns an error when an {{InverseOffer}} is included in the list of {{OfferIDs}} in an {{ACCEPT}} call.  * If an {{Offer}} is part of the same {{ACCEPT}}, the master sees {{error.isSome()}} and returns a {{TASK_LOST}} for normal offers.  (https://github.com/apache/mesos/blob/fafbdca610d0a150b9fa9cb62d1c63cb7a6fdaf3/src/master/master.cpp#L3117)    Here's a regression test:  https://reviews.apache.org/r/42092/    *Proprosal*  The question is whether we want to allow the mixing of {{Offers}} and {{InverseOffers}}.    Arguments for mixing:  * The design/structure of the maintenance originally intended to overload {{ACCEPT}} and {{DECLINE}} to take inverse offers.  * Enforcing non-mixing may require breaking changes to {{scheduler.proto}}.    Arguments against mixing:  * Some semantics are difficult to explain.  What does it mean to supply {{InverseOffers}} with {{Offer::Operations}}?  What about {{DECLINE}} with {{Offers}} and {{InverseOffers}}, including a ""reason""?  * What happens if we presumably add a third type of offer?  * Does it make sense to {{TASK_LOST}} valid normal offers if {{InverseOffers}} are invalid?",2
"Deprecate 'authenticate' master flag in favor of 'authenticate_frameworks' flag
To be consistent with `authenticate_slaves` and `authenticate_http` flags, we should rename `authenticate` to `authenticate_frameworks` flag.    This should be done via deprecation cycle.     1) Release X supports both `authenticate` and `authenticate_frameworks` flags    2)  Release X + n supports only `authenticate_frameworks` flag.   ",1
"Shared Volumes Design Doc
Review & Approve design doc",3
"Draft design document for resource revocability by default.
Create a design document for setting offered resources as ""revocable by default"". Greedy frameworks can then temporarily use resources set aside to satisfy quota.  ",8
"Add persistent volume endpoint tests with no principal
There are currently no persistent volume endpoint tests that do not use a principal; they should be added.",1
"Rename ContainerPrepareInfo to ContainerLaunchInfo for isolators.
The name ""ContainerPrepareInfo"" does not really capture the purpose of this struct. ContainerLaunchInfo better captures the purpose of this struct. ContainerLaunchInfo is returned by the isolator 'prepare' function. It contains information about how a container should be launched (e.g., environment variables, namespaces, commands, etc.). The information will be used by the Mesos Containerizer when launching the container.",2
"Synchronously handle AuthZ errors for the Scheduler endpoint.
Currently, any AuthZ errors for the {{/scheduler}} endpoint are handled asynchronously as {{FrameworkErrorMessage}}. Here is an example:    {code}    if (authorizationError.isSome()) {      LOG(INFO) << ""Refusing subscription of framework""                << "" '"" << frameworkInfo.name() << ""'""                << "": "" << authorizationError.get().message;        FrameworkErrorMessage message;      message.set_message(authorizationError.get().message);      http.send(message);      http.close();      return;    }  {code}    We would like to handle such errors synchronously when the request is received similar to what other endpoints like {{/reserve}}/{{/quota}} do. We already have the relevant functions {{authorizeXXX}} etc in {{master.cpp}}. We should just make the requests pass through once the relevant {{Future}} from the {{authorizeXXX}} function is fulfilled.",5
"Create persistent volume directories based on DiskInfo.Source.
Currently, we always create persistent volumes from root disk, and the persistent volumes are directories. With DiskInfo.Source being added, we should create the persistent volume accordingly based on the information in DiskInfo.Source.    This ticket handles the case where DiskInfo.Source.type is PATH. In that case, we should create sub-directories and use the same layout as slave.work_dir.    See the relevant code here:  {code}  void Slave::checkpointResources(...)  {    // Creates persistent volumes that do not exist and schedules    // releasing those persistent volumes that are no longer needed.    //    // TODO(jieyu): Consider introducing a volume manager once we start    // to support multiple disks, or raw disks. Depending on the    // DiskInfo, we may want to create either directories under a root    // directory, or LVM volumes from a given device.    Resources volumes = newCheckpointedResources.persistentVolumes();      foreach (const Resource& volume, volumes) {      // This is validated in master.      CHECK_NE(volume.role(), ""*"");        string path = paths::getPersistentVolumePath(          flags.work_dir,          volume.role(),          volume.disk().persistence().id());        if (!os::exists(path)) {        CHECK_SOME(os::mkdir(path, true))          << ""Failed to create persistent volume at '"" << path << ""'"";      }    }  }  {code}",2
"Update filesystem isolators to look for persistent volume directories from the correct location.
This is related to MESOS-4400.    Since persistent volume directories can be created from non root disk now. We need to adjust both posix and linux filesystem isolator to look for volumes from the correct location based on the information in DiskInfo.Source.    See relevant code in:  {code}  Future<Nothing> PosixFilesystemIsolatorProcess::update(..);  Future<Nothing> LinuxFilesystemIsolatorProcess::update(..);  {code}",2
"Check paths in DiskInfo.Source.Path exist during slave initialization.
We have two options here. We can either check and fail if it does not exists. Or we can create if it does not exist like we did for slave.work_dir.",2
"Introduce protobuf for quota set request.
To document quota request JSON schema and simplify request processing, introduce a {{QuotaRequest}} protobuf wrapper.",3
"Traverse all roles for quota allocation.
There might be a bug in how resources are allocated to multiple quota'ed roles if one role's quota is met. We need to investigate this behavior.",3
"Implement stout/os/windows/rmdir.hpp
nan",5
"Prevent allocator from crashing on successful recovery.
There might be a bug that may crash the master as pointed out by [~bmahler] in https://reviews.apache.org/r/42222/:  {noformat}  It looks like if we trip the resume call in addSlave, this delayed resume will crash the master   due to the CHECK(paused) that currently resides in resume.  {noformat}",3
"Document that /reserve, /create-volumes endpoints can return misleading ""success""
The docs for the {{/reserve}} endpoint say:    {noformat}  200 OK: Success (the requested resources have been reserved).  {noformat}    This is not true: the master returns {{200}} when the request has been validated and a {{CheckpointResourcesMessage}} has been sent to the agent, but the master does not attempt to verify that the message has been received or that the agent successfully checkpointed. Same behavior applies to {{/unreserve}}, {{/create-volumes}}, and {{/destroy-volumes}}.    We should _either_:    1. Accurately document what {{200}} return code means.  2. Change the implementation to wait for the agent's next checkpoint to succeed (and to include the effect of the operation) before returning success to the HTTP client.",3
"Introduce filtering test abstractions for HTTP events to libprocess
We need a test abstraction for {{HttpEvent}} similar to the already existing one's for {{DispatchEvent}}, {{MessageEvent}} in libprocess.    The abstraction can look similar in semantics to the already existing {{FUTURE_DISPATCH}}/{{FUTURE_MESSAGE}}.",3
"Implement a callback testing interface for the Executor Library
Currently, we do not have a mocking based callback interface for the executor library. This should look similar to the ongoing work for MESOS-3339 i.e. the corresponding issue for the scheduler library.    The interface should allow us to set expectations like we do for the driver. An example:    {code}  EXPECT_CALL(executor, connected())    .Times(1)  {code}",3
"Install 3rdparty package boost, glog, protobuf and picojson when installing Mesos
Mesos modules depend on having these packages installed with the exact version as Mesos was compiled with.",3
"Update `Master::Http::stateSummary` to use `jsonify`.
Update {{state-summary}} to use {{jsonify}} to stay consistent with {{state}} HTTP endpoint.",3
"Disable the test RegistryClientTest.BadTokenServerAddress.
As we are retiring registry client, disable this test which looks flaky.",1
"Add 'dependency' message to 'AppcImageManifest' protobuf.
AppcImageManifest protobuf currently lacks 'dependencies' which is necessary for image discovery.",1
"Fix appc CachedImage image validation
Currently image validation is done assuming that the image's filename will have  digest (SHA-512) information. This is not part of the spec      (https://github.com/appc/spec/blob/master/spec/discovery.md).            The spec specifies the tuple <image name, labels> as unique identifier for  discovering an image.  ",1
"Allocate revocable resources beyond quota guarantee.
h4. Status Quo  Currently resources allocated to frameworks in a role with quota (aka quota'ed role) beyond quota guarantee are marked non-revocable. This impacts our flexibility for revoking them if we decide so in the future.    h4. Proposal  Once quota guarantee is satisfied we must not necessarily further allocate resources as non-revocable. Instead we can mark all offers resources beyond guarantee as revocable. When in the future {{RevocableInfo}} evolves frameworks will get additional information about ""revocability"" of the resource (i.e. allocation slack)    h4. Caveats  Though it seems like a simple change, it has several implications.    h6. Fairness  Currently the hierarchical allocator considers revocable resources as regular resources when doing fairness calculations. This may prevent frameworks getting non-revocable resources as part of their role's quota guarantee if they accept some revocable resources as well.    Consider the following scenario. A single framework in a role with quota set to {{10}} CPUs is allocated {{10}} CPUs as non-revocable resources as part of its quota and additionally {{2}} revocable CPUs. Now a task using {{2}} non-revocable CPUs finishes and its resources are returned. Total allocation for the role is {{8}} non-revocable + {{2}} revocable. However, the role may not be offered additional {{2}} non-revocable since its total allocation satisfies quota.    h6. Resource math  If we allocate non-revocable resources as revocable, we should make sure we do accounting right: either we should update total agent resources and mark them as revocable as well, or bookkeep resources as non-revocable and convert them to revocable when necessary.    h6. Coarse-grained nature of allocation  The hierarchical allocator performs ""coarse-grained"" allocation, meaning it always allocates the entire remaining agent resources to a single framework. This may lead to over-allocating some resources as non-revocable beyond quota guarantee.    h6. Quotas smaller than fair share  If a quota set for a role is smaller than its fair share, it may reduce the amount of resources offered to this role, if frameworks in it do not accept revocable resources. This is probably the most important consequence of the proposed change. Operators may set quota to get guarantees, but may observe a decrease in amount of resources a role gets, which is not intuitive.",8
"Refactor allocator recovery.
Allocator recovery code can be improved for readability. [~bmahler] left some thoughts about it in https://reviews.apache.org/r/42222/.",3
"Design doc for reservation labels
nan",3
"Labels equality behavior is wrong
{noformat}  TEST(RevocableResourceTest, LabelSemantics)  {    Labels labels1;    Labels labels2;      labels1.add_labels()->CopyFrom(createLabel(""foo"", ""bar""));    labels1.add_labels()->CopyFrom(createLabel(""foo"", ""bar""));      labels2.add_labels()->CopyFrom(createLabel(""foo"", ""bar""));    labels2.add_labels()->CopyFrom(createLabel(""baz"", ""qux""));      bool eq = (labels1 == labels2);    LOG(INFO) << ""Equal? "" << (eq ? ""true"" : ""false"");  }  {noformat}    Output:  {noformat}  [ RUN      ] RevocableResourceTest.LabelSemantics  I0120 13:15:25.207223 2078158848 resources_tests.cpp:1990] Equal? true  [       OK ] RevocableResourceTest.LabelSemantics (0 ms)  {noformat}    This behavior seems pretty problematic.",5
"SegFault on agent during executor startup
When repeatedly performing our system tests we have found that we get a segfault on one of the agents. It probably occurs about one time in ten. I have attached the full log from that agent. I've attached the log from the agent that failed and the master (although I think this is less helpful).    To reproduce  - I have no idea. It seems to occur at certain times. E.g. like if a packet is created right on a minute boundary or something. But I don't think it's something caused by our code because the timestamps are stamped by mesos. I was surprised not to find a bug already open.",1
"Improve documentation around roles, principals, authz, and reservations
* What is the difference between a role and a principal?  * Why do some ACL entities reference ""roles"" but others reference ""principals""? In a typical organization, what real-world entities would my roles vs. principals map to? The ACL documentation could use more information about the motivation of ACLs and examples of configuring ACLs to meet real-world security policies.  * We should give some examples of making reservations when the role and principal are different, and why you would want to do that  * We should add an example to the ACL page that includes setting ACLs for reservations and/or persistent volumes",2
"Create common sha512 compute utility function.
Add common utility function for computing digests. Start with `sha512` since its immediately needed by appc image fetcher. ",2
"Implement tests for the new Executor library
We need to add tests for the executor library {{src/executor/executor.cpp}}. One possible approach would be to use the existing tests in {{src/tests/scheduler_tests.cpp}} and make them use the new executor library.",3
"Implement AuthN handling on the scheduler library
Currently, we do not have the ability of passing {{Credentials}} via the scheduler library. Once the master supports AuthN handling for the {{/scheduler}} endpoint, we would need to add this support to the library.",3
"Enable Framework->Executor message optimization for HTTP API
Currently, we support sending framework->executor messages directly as an optimization. This is not currently possible with using the Scheduler HTTP API.  We should think about exploring possible alternatives for supporting this optimization.",13
"Enable Executor->Framework message optimization for HTTP API
Currently, we support sending executor->framework messages directly as an optimization. This is not currently possible with using the Scheduler HTTP API. We should think about exploring possible alternatives for supporting this optimization.",13
"Implement `waitpid` in Windows
nan",5
"Implement process querying/counting in Windows
nan",2
"ReviewBot seemed to be crashing ReviewBoard server when posting large reviews
The bot is currently tripping on this review  https://reviews.apache.org/r/42506/ (see builds #10973 to #10978).    [~jfarrell] looked at the server logs and said he saw 'MySQL going away' message when the mesos bot was making these requests. I think that error is a bit misleading because it happens only for this review (which has a huge error log due to bad patch). The bot has successfully posted reviews for other review requests which had no error log (good patch).    One way to fix this would be to just post a tail of the error log (and perhaps link to Jenkins Console or some other service for the longer error text).",2
"Implement reservation labels
nan",5
"Introduce status() interface in `Containerizer`
In the Containerizer, during container isolation, the isolators end up modifying the state of the containers. Examples would be IP address allocation to a container by the 'network isolator, or net_cls handle allocation by the cgroup/net_cls isolator.     Often times the state of the container, needs to be exposed to operators through the state.json end-point. For e.g. operators or frameworks might want to know the IP-address configured on a particular container, or the net_cls handle associated with a container to configure the right TC rules. However, at present, there is no clean interface for the slave to retrieve the state of a container from the Containerizer for any of the launched containers. Thus, we need to introduce a `status` interface in the `Containerizer` base class, in order for the slave to expose container state information in its state.json.   ",2
"Define a CgroupInfo protobuf to expose cgroup isolator configuration.
Within `MesosContainerizer` we have an isolator associated with each linux cgroup subsystem. The isolators apply subsystem specific configuration on the containers before launching the containers. For e.g cgroup/net_cls isolator applies net_cls handles, cgroup/mem isolator applies memory quotas, cgroups/cpu-share isolator configures cpu shares.     Currently, there is no message structure defined to capture the configuration information of the container, for each cgroup isolator that has been applied to the container. We therefore need to define a protobuf that can capture the cgroup configuration of each cgroup isolator that has been applied to the container. This protobuf will be filled in by the cgroup isolator and will be stored as part of `ContainerConfig` in the containerizer. ",1
"The `cgroups/net_cls` isolator needs to expose handles in the ContainerStatus
The `cgroup/net_cls` isolator is responsible for allocating network handles to containers launched within a net_cls cgroup. The `cgroup/net_cls` isolator needs to expose these handles to the containerizer as part of the `ContainerStatus` when the containerizer queries the status() method of the isolator. The information itself will go as part of a `CgroupInfo` protobuf that will be defined as part of MESOS-4488 .  ",1
"Get container status information in slave. 
As part of MESOS-4487 an interface will be introduce into the `Containerizer` to allow agents to retrieve container state information. The agent needs to use this interface to retrieve container state information during status updates from the executor. The container state information can be then use by the agent to expose various isolator specific configuration (for e.g., IP address allocated by network isolators, net_cls handles allocated by `cgroups/net_cls` isolator), that has been applied to the container, in the state.json endpoint.  ",3
"Add ability to create symlink on Windows
nan",3
"Implement `size`, `usage`, and other disk metrics reporting on Windows.
nan",3
"Delete `os::chown` on Windows
nan",1
"Refactor os.hpp to be less monolithic, and more cross-platform compatible
nan",1
"Docker provisioner store should reuse existing layers in the cache.
Currently, the docker provisioner store will download all the layers associated with an image if the image is not found locally, even though some layers of it might already exist in the cache.    This is problematic because anytime a user deploys a new image, Mesos will fetch all layers of that new image, even though most of the layers are already cached locally.    ",5
"Expose ExecutorInfo and TaskInfo for isolators.
Currently we do not have these info for isolator. Image once we have docker runtime isolator, CommandInfo is necessary to support either custom executor or command executor. ",2
"Hierarchical allocator performance is slow due to Quota
Since we do not strip the non-scalar resources during the resource arithmetic for quota, the performance can degrade significantly, as currently resource arithmetic is expensive.    One approach to resolving this is to filter the resources we use to perform this arithmetic to only use scalars. This is valid as quota can currently only be set for scalar resource types.",3
"Posix disk isolator should ignore disk quota enforcement for MOUNT type disk resources.
We assume MOUNT type disk is exclusive and the underlying filesystem will enforce the quota (i.e., the application won't be able to exceed the quota, and will get a write error it the disk is full).    Therefore, there's no need to enforce it's quota in posix disk isolator.",2
"Render quota status consistently with other endpoints.
Currently quota status endpoint returns a collection of {{QuotaInfo}} protos converted to JSON. An example response looks like this:  {code:xml}  {    ""infos"": [      {        ""role"": ""role1"",        ""guarantee"": [          {            ""name"": ""cpus"",            ""role"": ""*"",            ""type"": ""SCALAR"",            ""scalar"": { ""value"": 12 }          },          {            ""name"": ""mem"",            ""role"": ""*"",            ""type"": ""SCALAR"",            ""scalar"": { ""value"": 6144 }          }        ]      }    ]  }  {code}    Presence of some fields, e.g. ""role"", is misleading. To address this issue and make the output more informative, we should probably introduce a  {{model()}} function for {{QuotaStatus}}.",3
"Build failure when using gcc-4.9 - signed/unsigned mismatch.
When building the current master, the following happens when using gcc-4.9:    {noformat}  mv -f examples/.deps/persistent_volume_framework-persistent_volume_framework.Tpo examples/.deps/persistent_volume_framework-persistent_volume_framework.Po  g++-4.9 -DPACKAGE_NAME=\""mesos\"" -DPACKAGE_TARNAME=\""mesos\"" -DPACKAGE_VERSION=\""0.27.0\"" -DPACKAGE_STRING=\""mesos\ 0.27.0\"" -DPACKAGE_BUGREPORT=\""\"" -DPACKAGE_URL=\""\"" -DPACKAGE=\""mesos\"" -DVERSION=\""0.27.0\"" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_DLFCN_H=1 -DLT_OBJDIR=\"".libs/\"" -DHAVE_PTHREAD_PRIO_INHERIT=1 -DHAVE_PTHREAD=1 -DHAVE_LIBZ=1 -DHAVE_LIBCURL=1 -DHAVE_APR_POOLS_H=1 -DHAVE_LIBAPR_1=1 -DHAVE_SVN_VERSION_H=1 -DHAVE_LIBSVN_SUBR_1=1 -DHAVE_SVN_DELTA_H=1 -DHAVE_LIBSVN_DELTA_1=1 -DHAVE_LIBSASL2=1 -I. -I../../src   -Wall -Werror -DLIBDIR=\""/usr/local/lib\"" -DPKGLIBEXECDIR=\""/usr/local/libexec/mesos\"" -DPKGDATADIR=\""/usr/local/share/mesos\"" -I../../include -I../../3rdparty/libprocess/include -I../../3rdparty/libprocess/3rdparty/stout/include -I../include -I../include/mesos -isystem ../3rdparty/libprocess/3rdparty/boost-1.53.0 -I../3rdparty/libprocess/3rdparty/picojson-1.3.0 -DPICOJSON_USE_INT64 -D__STDC_FORMAT_MACROS -I../3rdparty/libprocess/3rdparty/protobuf-2.5.0/src -I../3rdparty/libprocess/3rdparty/glog-0.3.3/src -I../3rdparty/libprocess/3rdparty/glog-0.3.3/src -I../3rdparty/leveldb/include -I../3rdparty/zookeeper-3.4.5/src/c/include -I../3rdparty/zookeeper-3.4.5/src/c/generated -I../3rdparty/libprocess/3rdparty/protobuf-2.5.0/src -DSOURCE_DIR=\""/Users/till/Development/mesos-private/build/..\"" -DBUILD_DIR=\""/Users/till/Development/mesos-private/build\"" -I../3rdparty/libprocess/3rdparty/gmock-1.7.0/gtest/include -I../3rdparty/libprocess/3rdparty/gmock-1.7.0/include  -I/usr/local/opt/openssl/include -I/usr/local/opt/libevent/include -I/usr/local/opt/subversion/include/subversion-1 -I/usr/include/apr-1 -I/usr/include/apr-1.0  -D_THREAD_SAFE -pthread -g1 -O0 -Wno-unused-local-typedefs -std=c++11 -DGTEST_USE_OWN_TR1_TUPLE=1 -DGTEST_LANG_CXX11 -MT tests/mesos_tests-container_logger_tests.o -MD -MP -MF tests/.deps/mesos_tests-container_logger_tests.Tpo -c -o tests/mesos_tests-container_logger_tests.o `test -f 'tests/container_logger_tests.cpp' || echo '../../src/'`tests/container_logger_tests.cpp  mv -f slave/qos_controllers/.deps/mesos_tests-load.Tpo slave/qos_controllers/.deps/mesos_tests-load.Po  g++-4.9 -DPACKAGE_NAME=\""mesos\"" -DPACKAGE_TARNAME=\""mesos\"" -DPACKAGE_VERSION=\""0.27.0\"" -DPACKAGE_STRING=\""mesos\ 0.27.0\"" -DPACKAGE_BUGREPORT=\""\"" -DPACKAGE_URL=\""\"" -DPACKAGE=\""mesos\"" -DVERSION=\""0.27.0\"" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_DLFCN_H=1 -DLT_OBJDIR=\"".libs/\"" -DHAVE_PTHREAD_PRIO_INHERIT=1 -DHAVE_PTHREAD=1 -DHAVE_LIBZ=1 -DHAVE_LIBCURL=1 -DHAVE_APR_POOLS_H=1 -DHAVE_LIBAPR_1=1 -DHAVE_SVN_VERSION_H=1 -DHAVE_LIBSVN_SUBR_1=1 -DHAVE_SVN_DELTA_H=1 -DHAVE_LIBSVN_DELTA_1=1 -DHAVE_LIBSASL2=1 -I. -I../../src   -Wall -Werror -DLIBDIR=\""/usr/local/lib\"" -DPKGLIBEXECDIR=\""/usr/local/libexec/mesos\"" -DPKGDATADIR=\""/usr/local/share/mesos\"" -I../../include -I../../3rdparty/libprocess/include -I../../3rdparty/libprocess/3rdparty/stout/include -I../include -I../include/mesos -isystem ../3rdparty/libprocess/3rdparty/boost-1.53.0 -I../3rdparty/libprocess/3rdparty/picojson-1.3.0 -DPICOJSON_USE_INT64 -D__STDC_FORMAT_MACROS -I../3rdparty/libprocess/3rdparty/protobuf-2.5.0/src -I../3rdparty/libprocess/3rdparty/glog-0.3.3/src -I../3rdparty/libprocess/3rdparty/glog-0.3.3/src -I../3rdparty/leveldb/include -I../3rdparty/zookeeper-3.4.5/src/c/include -I../3rdparty/zookeeper-3.4.5/src/c/generated -I../3rdparty/libprocess/3rdparty/protobuf-2.5.0/src -DSOURCE_DIR=\""/Users/till/Development/mesos-private/build/..\"" -DBUILD_DIR=\""/Users/till/Development/mesos-private/build\"" -I../3rdparty/libprocess/3rdparty/gmock-1.7.0/gtest/include -I../3rdparty/libprocess/3rdparty/gmock-1.7.0/include  -I/usr/local/opt/openssl/include -I/usr/local/opt/libevent/include -I/usr/local/opt/subversion/include/subversion-1 -I/usr/include/apr-1 -I/usr/include/apr-1.0  -D_THREAD_SAFE -pthread -g1 -O0 -Wno-unused-local-typedefs -std=c++11 -DGTEST_USE_OWN_TR1_TUPLE=1 -DGTEST_LANG_CXX11 -MT tests/mesos_tests-containerizer.o -MD -MP -MF tests/.deps/mesos_tests-containerizer.Tpo -c -o tests/mesos_tests-containerizer.o `test -f 'tests/containerizer.cpp' || echo '../../src/'`tests/containerizer.cpp  In file included from ../3rdparty/libprocess/3rdparty/gmock-1.7.0/include/gmock/internal/gmock-internal-utils.h:47:0,                   from ../3rdparty/libprocess/3rdparty/gmock-1.7.0/include/gmock/gmock-actions.h:46,                   from ../3rdparty/libprocess/3rdparty/gmock-1.7.0/include/gmock/gmock.h:58,                   from ../../src/tests/container_logger_tests.cpp:21:  ../3rdparty/libprocess/3rdparty/gmock-1.7.0/gtest/include/gtest/gtest.h: In instantiation of 'testing::AssertionResult testing::internal::CmpHelperLE(const char*, const char*, const T1&, const T2&) [with T1 = int; T2 = long long unsigned int]':  ../../src/tests/container_logger_tests.cpp:467:3:   required from here  ../3rdparty/libprocess/3rdparty/gmock-1.7.0/gtest/include/gtest/gtest.h:1579:28: error: comparison between signed and unsigned integer expressions [-Werror=sign-compare]   GTEST_IMPL_CMP_HELPER_(LE, <=);                              ^  ../3rdparty/libprocess/3rdparty/gmock-1.7.0/gtest/include/gtest/gtest.h:1562:12: note: in definition of macro 'GTEST_IMPL_CMP_HELPER_'     if (val1 op val2) {\              ^  ../3rdparty/libprocess/3rdparty/gmock-1.7.0/gtest/include/gtest/gtest.h: In instantiation of 'testing::AssertionResult testing::internal::CmpHelperGE(const char*, const char*, const T1&, const T2&) [with T1 = int; T2 = long long unsigned int]':  ../../src/tests/container_logger_tests.cpp:468:3:   required from here  ../3rdparty/libprocess/3rdparty/gmock-1.7.0/gtest/include/gtest/gtest.h:1583:28: error: comparison between signed and unsigned integer expressions [-Werror=sign-compare]   GTEST_IMPL_CMP_HELPER_(GE, >=);                              ^  ../3rdparty/libprocess/3rdparty/gmock-1.7.0/gtest/include/gtest/gtest.h:1562:12: note: in definition of macro 'GTEST_IMPL_CMP_HELPER_'     if (val1 op val2) {\              ^  mv -f tests/.deps/mesos_tests-anonymous_tests.Tpo tests/.deps/mesos_tests-anonymous_tests.Po  {noformat}",1
"ContainerLoggerTest.LOGROTATE_RotateInSandbox breaks when running on Centos6.
{noformat}  [17:24:58][Step 7/7] logrotate: bad argument --version: unknown error  [17:24:58][Step 7/7] F0126 17:24:57.913729  4503 container_logger_tests.cpp:380] CHECK_SOME(containerizer): Failed to create container logger: Failed to create container logger module 'org_apache_mesos_LogrotateContainerLogger': Error creating Module instance for 'org_apache_mesos_LogrotateContainerLogger'   [17:24:58][Step 7/7] *** Check failure stack trace: ***  [17:24:58][Step 7/7]     @     0x7f11ae0d2d40  google::LogMessage::Fail()  [17:24:58][Step 7/7]     @     0x7f11ae0d2c9c  google::LogMessage::SendToLog()  [17:24:58][Step 7/7]     @     0x7f11ae0d2692  google::LogMessage::Flush()  [17:24:58][Step 7/7]     @     0x7f11ae0d544c  google::LogMessageFatal::~LogMessageFatal()  [17:24:58][Step 7/7]     @           0x983927  _CheckFatal::~_CheckFatal()  [17:24:58][Step 7/7]     @           0xa9a18b  mesos::internal::tests::ContainerLoggerTest_LOGROTATE_RotateInSandbox_Test::TestBody()  [17:24:58][Step 7/7]     @          0x1623a4e  testing::internal::HandleSehExceptionsInMethodIfSupported<>()  [17:24:58][Step 7/7]     @          0x161eab2  testing::internal::HandleExceptionsInMethodIfSupported<>()  [17:24:58][Step 7/7]     @          0x15ffdfd  testing::Test::Run()  [17:24:58][Step 7/7]     @          0x160058b  testing::TestInfo::Run()  [17:24:58][Step 7/7]     @          0x1600bc6  testing::TestCase::Run()  [17:24:58][Step 7/7]     @          0x1607515  testing::internal::UnitTestImpl::RunAllTests()  [17:24:58][Step 7/7]     @          0x16246dd  testing::internal::HandleSehExceptionsInMethodIfSupported<>()  [17:24:58][Step 7/7]     @          0x161f608  testing::internal::HandleExceptionsInMethodIfSupported<>()  [17:24:58][Step 7/7]     @          0x1606245  testing::UnitTest::Run()  [17:24:58][Step 7/7]     @           0xde36b6  RUN_ALL_TESTS()  [17:24:58][Step 7/7]     @           0xde32cc  main  [17:24:58][Step 7/7]     @     0x7f11a8896d5d  __libc_start_main  [17:24:58][Step 7/7]     @           0x981fc9  (unknown)  {noformat}",1
"Introduce docker runtime isolator.
Currently docker image default configuration are included in `ProvisionInfo`. We should grab necessary config from `ProvisionInfo` into `ContainerInfo`, and handle all these runtime informations inside of docker runtime isolator. Return a `ContainerLaunchInfo` containing `working_dir`, `env` and merged `commandInfo`, etc.",3
"Introduce a status() interface for isolators
While launching a container mesos isolators end up configuring/modifying various properties of the container. For e.g., cgroup isolators (mem, cpu, net_cls) configure/change the properties associated with their respective subsystems before launching a container. Similary network isolator (net-modules, port mapping) configure the IP address and ports associated with a container.     Currently, there are not interface in the isolator to extract the run time state of these properties for a given container. Therefore a status() method needs to be implemented in the isolators to allow the containerizer to extract the container status information from the isolator. ",1
"Enable benchmark tests in ASF CI
It would be nice to enable benchmark tests in the ASF CI so that we can catch performance regressions (esp. during releases).",3
"Include the allocated portion of reserved resources in the role sorter for DRF.
Reserved resources should be accounted for fairness calculation whether they are allocated or not, since they model a long or forever running task. That is, the effect of reserving resources is equivalent to launching a task in that the resources that make up the reservation are not available to other roles as non-revocable.    In the short-term, we should at least account for the allocated portion of the reservation.",1
"Include allocated portion of the reserved resources in the quota role sorter for DRF.
Similar to MESOS-4526, reserved resources should be accounted for in the quota role sorter regardless of their allocation state. In the short-term, we should at least account them if they are allocated.",1
"Account for reserved resources in the quota guarantee check.
Reserved resources should be accounted for in the quota guarantee check so that frameworks cannot continually reserve resources to pull them out of the quota pool.",2
"Update the allocator to not offer unreserved resources beyond quota.
Eventually, we will want to offer unreserved resources as revocable beyond the role's quota. Rather than offering non-revocable resources beyond the role's quota's guarantee, in the short term, we choose to not offer resources beyond a role's quota.",2
"NetClsIsolatorTest.ROOT_CGROUPS_NetClsIsolate is flaky
While running the command  {noformat}  sudo ./bin/mesos-tests.sh --gtest_filter=""-CgroupsAnyHierarchyWithCpuMemoryTest.ROOT_CGROUPS_Listen:CgroupsAnyHierarchyMemoryPressureTest.ROOT_IncreaseRSS"" --gtest_repeat=10 --gtest_break_on_failure  {noformat}  One eventually gets the following output:  {noformat}  [ RUN      ] NetClsIsolatorTest.ROOT_CGROUPS_NetClsIsolate  ../../src/tests/containerizer/isolator_tests.cpp:870: Failure  containerizer: Could not create isolator 'cgroups/net_cls': Unexpected subsystems found attached to the hierarchy /sys/fs/cgroup/net_cls,net_prio  [  FAILED  ] NetClsIsolatorTest.ROOT_CGROUPS_NetClsIsolate (75 ms)  {noformat}",1
"Document multi-disk support.
nan",2
"Resources object can be mutated through the public API
The {{Resources}} object current allows mutation of it's internal state through the public mutable iterator interface.  This can cause issues when the mutation involved stripping certain qualifiers on a {{Resource}}, as they will not be summed together at the end of the mutation (even though they should be).    The {{contains()}} math will not work correctly if two {{addable}} resources are not summed together on the {{lhs}} of the contains check.",3
"Logrotate ContainerLogger may not handle FD ownership correctly
One of the patches for [MESOS-4136] introduced the {{FDType::OWNED}} enum for {{Subprocess::IO::FD}}.    The way the logrotate module uses this is slightly incorrect:  # The module starts a subprocess with an output {{Subprocess::PIPE()}}.  # That pipe's FD is passed into another subprocess via {{Subprocess::IO::FD(pipe, IO::OWNED)}}.  # When the second subprocess starts, the pipe's FD is closed in the parent.  # When the first subprocess terminates, the existing code will try to close the pipe again.  This effectively closes a random FD.",1
"Add abstractions of ""owned"" and ""shared"" file descriptors to libprocess.
Libprocess currently manages file descriptors as plain {{int}} s.  This leads to some easily missed bugs regarding duplicated or closed FDs.    We should introduce an abstraction (like {{unique_ptr}} and {{shared_ptr}}) so that FD ownership can be expressed alongside the affected code.",3
"Exclude paths in Posix disk isolator should be absolute paths.
Since du --exclude uses pattern matching. A relative path might accidentally matches an irrelevant directory/file. For instance,    {noformat}  /tmp/testpath $ tree  .  ├── aaa  │   └── exc  │       └── file  └── exc      └── file    3 directories, 2 files  /tmp/testpath $ du --exclude /tmp/testpath/exc /tmp/testpath/  8    /tmp/testpath/aaa/exc  12    /tmp/testpath/aaa  16    /tmp/testpath/  /tmp/testpath $ du --exclude exc /tmp/testpath/  4    /tmp/testpath/aaa  8    /tmp/testpath/  /tmp/testpath $  {noformat}",2
"NetClsIsolatorTest.ROOT_CGROUPS_NetClsIsolate fails on CentOS 6
This test fails in my CentOS 6 VM due to a cgroups issue:    {code}  [ RUN      ] NetClsIsolatorTest.ROOT_CGROUPS_NetClsIsolate  I0127 19:15:06.637328 25347 exec.cpp:134] Version: 0.28.0  I0127 19:15:06.648378 25378 exec.cpp:208] Executor registered on slave 6edafba0-9dbd-4e6e-b10e-c6f935e58d41-S0  Registered executor on localhost  Starting task b745d88e-3fbe-4af9-80b3-e43484e37acf  sh -c 'sleep 1000'  Forked command at 25385  ../../src/tests/containerizer/isolator_tests.cpp:926: Failure  pids: Failed to read cgroups control 'cgroup.procs': '/sys/fs/cgroup/net_cls' is not a valid hierarchy  I0127 19:15:06.662083 25376 exec.cpp:381] Executor asked to shutdown  Shutting down  Sending SIGTERM to process tree at pid 25385  [  FAILED  ] NetClsIsolatorTest.ROOT_CGROUPS_NetClsIsolate (335 ms)  {code}",1
"MasterQuotaTest.AvailableResourcesAfterRescinding is flaky.
Can be reproduced by running {{GLOG_v=1 GTEST_FILTER=""MasterQuotaTest.AvailableResourcesAfterRescinding"" ./bin/mesos-tests.sh --gtest_shuffle --gtest_break_on_failure --gtest_repeat=1000 --verbose}}.    h5. Verbose log from a bad run:  {code}  [ RUN      ] MasterQuotaTest.AvailableResourcesAfterRescinding  I0128 12:20:27.568657 2080858880 resources.cpp:564] Parsing resources as JSON failed: cpus:2;mem:1024;disk:1024;ports:[31000-32000]  Trying semicolon-delimited string format instead  I0128 12:20:27.570142 2080858880 resources.cpp:564] Parsing resources as JSON failed: cpus:2;mem:1024;disk:1024;ports:[31000-32000]  Trying semicolon-delimited string format instead  I0128 12:20:27.583225 2080858880 leveldb.cpp:174] Opened db in 6241us  I0128 12:20:27.584353 2080858880 leveldb.cpp:181] Compacted db in 1026us  I0128 12:20:27.584429 2080858880 leveldb.cpp:196] Created db iterator in 12us  I0128 12:20:27.584442 2080858880 leveldb.cpp:202] Seeked to beginning of db in 7us  I0128 12:20:27.584453 2080858880 leveldb.cpp:271] Iterated through 0 keys in the db in 6us  I0128 12:20:27.584475 2080858880 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned  I0128 12:20:27.584918 300445696 recover.cpp:447] Starting replica recovery  I0128 12:20:27.585113 300445696 recover.cpp:473] Replica is in EMPTY status  I0128 12:20:27.585916 297226240 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (18274)@192.168.178.24:51278  I0128 12:20:27.586086 297762816 recover.cpp:193] Received a recover response from a replica in EMPTY status  I0128 12:20:27.586449 297226240 recover.cpp:564] Updating replica status to STARTING  I0128 12:20:27.587204 300445696 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 624us  I0128 12:20:27.587242 300445696 replica.cpp:320] Persisted replica status to STARTING  I0128 12:20:27.587376 299372544 recover.cpp:473] Replica is in STARTING status  I0128 12:20:27.588050 300982272 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (18275)@192.168.178.24:51278  I0128 12:20:27.588235 300445696 recover.cpp:193] Received a recover response from a replica in STARTING status  I0128 12:20:27.588572 297762816 recover.cpp:564] Updating replica status to VOTING  I0128 12:20:27.588850 297226240 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 140us  I0128 12:20:27.588879 297226240 replica.cpp:320] Persisted replica status to VOTING  I0128 12:20:27.588975 299909120 recover.cpp:578] Successfully joined the Paxos group  I0128 12:20:27.589154 299909120 recover.cpp:462] Recover process terminated  I0128 12:20:27.599486 298835968 master.cpp:374] Master 531344bd-56f4-4e4f-8f6f-a6a9d36058c7 (alexr.fritz.box) started on 192.168.178.24:51278  I0128 12:20:27.599520 298835968 master.cpp:376] Flags at startup: --acls="""" --allocation_interval=""50ms"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_http=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/private/tmp/NlzPSo/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""25secs"" --registry_strict=""true"" --roles=""role1,role2"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/private/tmp/NlzPSo/master"" --zk_session_timeout=""10secs""  I0128 12:20:27.599753 298835968 master.cpp:421] Master only allowing authenticated frameworks to register  I0128 12:20:27.599769 298835968 master.cpp:426] Master only allowing authenticated slaves to register  I0128 12:20:27.599781 298835968 credentials.hpp:35] Loading credentials for authentication from '/private/tmp/NlzPSo/credentials'  I0128 12:20:27.600082 298835968 master.cpp:466] Using default 'crammd5' authenticator  I0128 12:20:27.600163 298835968 master.cpp:535] Using default 'basic' HTTP authenticator  I0128 12:20:27.600327 298835968 master.cpp:569] Authorization enabled  W0128 12:20:27.600345 298835968 master.cpp:629] The '--roles' flag is deprecated. This flag will be removed in the future. See the Mesos 0.27 upgrade notes for more information  I0128 12:20:27.600497 297762816 whitelist_watcher.cpp:77] No whitelist given  I0128 12:20:27.600503 297226240 hierarchical.cpp:144] Initialized hierarchical allocator process  I0128 12:20:27.601965 297226240 master.cpp:1710] The newly elected leader is master@192.168.178.24:51278 with id 531344bd-56f4-4e4f-8f6f-a6a9d36058c7  I0128 12:20:27.601995 297226240 master.cpp:1723] Elected as the leading master!  I0128 12:20:27.602007 297226240 master.cpp:1468] Recovering from registrar  I0128 12:20:27.602083 300445696 registrar.cpp:307] Recovering registrar  I0128 12:20:27.602460 297226240 log.cpp:659] Attempting to start the writer  I0128 12:20:27.603514 299909120 replica.cpp:493] Replica received implicit promise request from (18277)@192.168.178.24:51278 with proposal 1  I0128 12:20:27.603734 299909120 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 205us  I0128 12:20:27.603768 299909120 replica.cpp:342] Persisted promised to 1  I0128 12:20:27.604194 299909120 coordinator.cpp:238] Coordinator attempting to fill missing positions  I0128 12:20:27.605311 299372544 replica.cpp:388] Replica received explicit promise request from (18278)@192.168.178.24:51278 for position 0 with proposal 2  I0128 12:20:27.605468 299372544 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 133us  I0128 12:20:27.605494 299372544 replica.cpp:712] Persisted action at 0  I0128 12:20:27.606441 298835968 replica.cpp:537] Replica received write request for position 0 from (18279)@192.168.178.24:51278  I0128 12:20:27.606492 298835968 leveldb.cpp:436] Reading position from leveldb took 29us  I0128 12:20:27.606665 298835968 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 151us  I0128 12:20:27.606688 298835968 replica.cpp:712] Persisted action at 0  I0128 12:20:27.607244 297226240 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0  I0128 12:20:27.607409 297226240 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 152us  I0128 12:20:27.607441 297226240 replica.cpp:712] Persisted action at 0  I0128 12:20:27.607457 297226240 replica.cpp:697] Replica learned NOP action at position 0  I0128 12:20:27.607853 297226240 log.cpp:675] Writer started with ending position 0  I0128 12:20:27.608649 299372544 leveldb.cpp:436] Reading position from leveldb took 158us  I0128 12:20:27.609539 298835968 registrar.cpp:340] Successfully fetched the registry (0B) in 7.426816ms  I0128 12:20:27.609763 298835968 registrar.cpp:439] Applied 1 operations in 54us; attempting to update the 'registry'  I0128 12:20:27.610216 300982272 log.cpp:683] Attempting to append 186 bytes to the log  I0128 12:20:27.610297 298835968 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1  I0128 12:20:27.611016 299909120 replica.cpp:537] Replica received write request for position 1 from (18280)@192.168.178.24:51278  I0128 12:20:27.611188 299909120 leveldb.cpp:341] Persisting action (205 bytes) to leveldb took 153us  I0128 12:20:27.611222 299909120 replica.cpp:712] Persisted action at 1  I0128 12:20:27.611843 299909120 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0  I0128 12:20:27.612004 299909120 leveldb.cpp:341] Persisting action (207 bytes) to leveldb took 147us  I0128 12:20:27.612035 299909120 replica.cpp:712] Persisted action at 1  I0128 12:20:27.612052 299909120 replica.cpp:697] Replica learned APPEND action at position 1  I0128 12:20:27.612742 300982272 registrar.cpp:484] Successfully updated the 'registry' in 2.924032ms  I0128 12:20:27.612846 300982272 registrar.cpp:370] Successfully recovered registrar  I0128 12:20:27.612936 298835968 log.cpp:702] Attempting to truncate the log to 1  I0128 12:20:27.613005 297762816 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2  I0128 12:20:27.613323 298299392 master.cpp:1520] Recovered 0 slaves from the Registry (147B) ; allowing 10mins for slaves to re-register  I0128 12:20:27.613364 298835968 hierarchical.cpp:171] Skipping recovery of hierarchical allocator: nothing to recover  I0128 12:20:27.613966 300445696 replica.cpp:537] Replica received write request for position 2 from (18281)@192.168.178.24:51278  I0128 12:20:27.614131 300445696 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 151us  I0128 12:20:27.614166 300445696 replica.cpp:712] Persisted action at 2  I0128 12:20:27.614660 299372544 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0  I0128 12:20:27.614828 299372544 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 158us  I0128 12:20:27.614876 299372544 leveldb.cpp:399] Deleting ~1 keys from leveldb took 28us  I0128 12:20:27.614898 299372544 replica.cpp:712] Persisted action at 2  I0128 12:20:27.614915 299372544 replica.cpp:697] Replica learned TRUNCATE action at position 2  I0128 12:20:27.625591 2080858880 containerizer.cpp:143] Using isolation: posix/cpu,posix/mem,filesystem/posix  I0128 12:20:27.629758 298299392 slave.cpp:192] Slave started on 871)@192.168.178.24:51278  I0128 12:20:27.629791 298299392 slave.cpp:193] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/MasterQuotaTest_AvailableResourcesAfterRescinding_gS9Qcf/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""https://auth.docker.io"" --docker_kill_orphans=""true"" --docker_puller_timeout=""60"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/MasterQuotaTest_AvailableResourcesAfterRescinding_gS9Qcf/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/Users/alex/Projects/mesos/build/default/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --version=""false"" --work_dir=""/tmp/MasterQuotaTest_AvailableResourcesAfterRescinding_gS9Qcf""  I0128 12:20:27.630067 298299392 credentials.hpp:83] Loading credential for authentication from '/tmp/MasterQuotaTest_AvailableResourcesAfterRescinding_gS9Qcf/credential'  I0128 12:20:27.630223 298299392 slave.cpp:323] Slave using credential for: test-principal  I0128 12:20:27.630360 298299392 resources.cpp:564] Parsing resources as JSON failed: cpus:2;mem:1024;disk:1024;ports:[31000-32000]  Trying semicolon-delimited string format instead  I0128 12:20:27.630818 298299392 slave.cpp:463] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]  I0128 12:20:27.630869 298299392 slave.cpp:471] Slave attributes: [  ]  I0128 12:20:27.630882 298299392 slave.cpp:476] Slave hostname: alexr.fritz.box  I0128 12:20:27.631352 300982272 state.cpp:58] Recovering state from '/tmp/MasterQuotaTest_AvailableResourcesAfterRescinding_gS9Qcf/meta'  I0128 12:20:27.631515 299909120 status_update_manager.cpp:200] Recovering status update manager  I0128 12:20:27.631702 298835968 containerizer.cpp:390] Recovering containerizer  I0128 12:20:27.632589 297226240 provisioner.cpp:245] Provisioner recovery complete  I0128 12:20:27.632807 298835968 slave.cpp:4495] Finished recovery  I0128 12:20:27.633539 298835968 slave.cpp:4667] Querying resource estimator for oversubscribable resources  I0128 12:20:27.633752 300445696 status_update_manager.cpp:174] Pausing sending status updates  I0128 12:20:27.633754 298835968 slave.cpp:795] New master detected at master@192.168.178.24:51278  I0128 12:20:27.633806 298835968 slave.cpp:858] Authenticating with master master@192.168.178.24:51278  I0128 12:20:27.633824 298835968 slave.cpp:863] Using default CRAM-MD5 authenticatee  I0128 12:20:27.633903 298835968 slave.cpp:831] Detecting new master  I0128 12:20:27.633913 299372544 authenticatee.cpp:121] Creating new client SASL connection  I0128 12:20:27.634016 298835968 slave.cpp:4681] Received oversubscribable resources  from the resource estimator  I0128 12:20:27.634076 297226240 master.cpp:5521] Authenticating slave(871)@192.168.178.24:51278  I0128 12:20:27.634130 299372544 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(1741)@192.168.178.24:51278  I0128 12:20:27.634255 297226240 authenticator.cpp:98] Creating new server SASL connection  I0128 12:20:27.634348 300982272 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5  I0128 12:20:27.634367 300982272 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'  I0128 12:20:27.634454 298835968 authenticator.cpp:203] Received SASL authentication start  I0128 12:20:27.634515 298835968 authenticator.cpp:325] Authentication requires more steps  I0128 12:20:27.634572 298835968 authenticatee.cpp:258] Received SASL authentication step  I0128 12:20:27.634706 297226240 authenticator.cpp:231] Received SASL authentication step  I0128 12:20:27.634757 297226240 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'alexr.fritz.box' server FQDN: 'alexr.fritz.box' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false   I0128 12:20:27.634771 297226240 auxprop.cpp:179] Looking up auxiliary property '*userPassword'  I0128 12:20:27.634793 297226240 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'  I0128 12:20:27.634809 297226240 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'alexr.fritz.box' server FQDN: 'alexr.fritz.box' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true   I0128 12:20:27.634819 297226240 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true  I0128 12:20:27.634827 297226240 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true  I0128 12:20:27.634893 297226240 authenticator.cpp:317] Authentication success  I0128 12:20:27.634958 298835968 authenticatee.cpp:298] Authentication success  I0128 12:20:27.635030 298299392 master.cpp:5551] Successfully authenticated principal 'test-principal' at slave(871)@192.168.178.24:51278  I0128 12:20:27.635079 300445696 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(1741)@192.168.178.24:51278  I0128 12:20:27.635195 299372544 slave.cpp:926] Successfully authenticated with master master@192.168.178.24:51278  I0128 12:20:27.635273 299372544 slave.cpp:1320] Will retry registration in 5.823453ms if necessary  I0128 12:20:27.635365 299909120 master.cpp:4235] Registering slave at slave(871)@192.168.178.24:51278 (alexr.fritz.box) with id 531344bd-56f4-4e4f-8f6f-a6a9d36058c7-S0  I0128 12:20:27.635542 297762816 registrar.cpp:439] Applied 1 operations in 41us; attempting to update the 'registry'  I0128 12:20:27.635889 299372544 log.cpp:683] Attempting to append 358 bytes to the log  I0128 12:20:27.636011 298299392 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3  I0128 12:20:27.636693 300982272 replica.cpp:537] Replica received write request for position 3 from (18295)@192.168.178.24:51278  I0128 12:20:27.636860 300982272 leveldb.cpp:341] Persisting action (377 bytes) to leveldb took 139us  I0128 12:20:27.636885 300982272 replica.cpp:712] Persisted action at 3  I0128 12:20:27.637380 299909120 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0  I0128 12:20:27.637547 299909120 leveldb.cpp:341] Persisting action (379 bytes) to leveldb took 132us  I0128 12:20:27.637573 299909120 replica.cpp:712] Persisted action at 3  I0128 12:20:27.637589 299909120 replica.cpp:697] Replica learned APPEND action at position 3  I0128 12:20:27.638362 298835968 registrar.cpp:484] Successfully updated the 'registry' in 2.77504ms  I0128 12:20:27.638589 300445696 log.cpp:702] Attempting to truncate the log to 3  I0128 12:20:27.638684 298299392 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4  I0128 12:20:27.638825 300445696 slave.cpp:3435] Received ping from slave-observer(871)@192.168.178.24:51278  I0128 12:20:27.639081 300982272 hierarchical.cpp:473] Added slave 531344bd-56f4-4e4f-8f6f-a6a9d36058c7-S0 (alexr.fritz.box) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )  I0128 12:20:27.639117 299909120 master.cpp:4303] Registered slave 531344bd-56f4-4e4f-8f6f-a6a9d36058c7-S0 at slave(871)@192.168.178.24:51278 (alexr.fritz.box) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]  I0128 12:20:27.639165 300982272 hierarchical.cpp:1403] No resources available to allocate!  I0128 12:20:27.639168 297226240 slave.cpp:970] Registered with master master@192.168.178.24:51278; given slave ID 531344bd-56f4-4e4f-8f6f-a6a9d36058c7-S0  I0128 12:20:27.639189 297226240 fetcher.cpp:81] Clearing fetcher cache  I0128 12:20:27.639183 300982272 hierarchical.cpp:1116] Performed allocation for slave 531344bd-56f4-4e4f-8f6f-a6a9d36058c7-S0 in 77us  I0128 12:20:27.639348 297762816 status_update_manager.cpp:181] Resuming sending status updates  I0128 12:20:27.639519 298835968 replica.cpp:537] Replica received write request for position 4 from (18296)@192.168.178.24:51278  I0128 12:20:27.639678 298835968 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 142us  I0128 12:20:27.639708 298835968 replica.cpp:712] Persisted action at 4  I0128 12:20:27.640115 300982272 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0  I0128 12:20:27.640276 300982272 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 137us  I0128 12:20:27.640312 300982272 leveldb.cpp:399] Deleting ~2 keys from leveldb took 21us  I0128 12:20:27.640326 300982272 replica.cpp:712] Persisted action at 4  I0128 12:20:27.640336 300982272 replica.cpp:697] Replica learned TRUNCATE action at position 4  I0128 12:20:27.642145 297226240 slave.cpp:993] Checkpointing SlaveInfo to '/tmp/MasterQuotaTest_AvailableResourcesAfterRescinding_gS9Qcf/meta/slaves/531344bd-56f4-4e4f-8f6f-a6a9d36058c7-S0/slave.info'  I0128 12:20:27.643354 297226240 slave.cpp:1029] Forwarding total oversubscribed resources   I0128 12:20:27.643458 300445696 master.cpp:4644] Received update of slave 531344bd-56f4-4e4f-8f6f-a6a9d36058c7-S0 at slave(871)@192.168.178.24:51278 (alexr.fritz.box) with total oversubscribed resources   I0128 12:20:27.643710 298299392 hierarchical.cpp:531] Slave 531344bd-56f4-4e4f-8f6f-a6a9d36058c7-S0 (alexr.fritz.box) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: )  I0128 12:20:27.643769 298299392 hierarchical.cpp:1403] No resources available to allocate!  I0128 12:20:27.643805 2...",3
"Propose design doc for agent partitioning behavior
nan",8
"Propose design doc for reliable floating point behavior
nan",3
"Mesos Agents needs to re-resolve hosts in zk string on leader change / failure to connect
Sample Mesos Agent log: https://gist.github.com/brndnmtthws/fb846fa988487250a809    Note, zookeeper has a function to change the list of servers at runtime: https://github.com/apache/zookeeper/blob/735ea78909e67c648a4978c8d31d63964986af73/src/c/src/zookeeper.c#L1207-L1232    This comes up when using an AWS AutoScalingGroup for managing the set of masters.     The agent when it comes up the first time, resolves the zk:// string. Once all the hosts that were in the original string fail (Each fails, is replaced by a new machine, which has the same DNS name), the agent just keeps spinning in an internal loop, never re-resolving the DNS names.    Two solutions I see are   1. Update the list of servers / re-resolve  2. Have the agent detect it hasn't connected recently, and kill itself (Which will force a re-resolution when the agent starts back up)",3
"Investigate test suite crashes after ZK socket disconnections.
Showed up on ASF CI:  https://builds.apache.org/job/Mesos/COMPILER=clang,CONFIGURATION=--verbose%20--enable-libevent%20--enable-ssl,OS=ubuntu:14.04,label_exp=docker%7C%7CHadoop/1579/console    The test crashed with the following logs:  {code}  [ RUN      ] ContentType/ExecutorHttpApiTest.DefaultAccept/1  I0129 02:00:35.137161 31926 leveldb.cpp:174] Opened db in 118.902333ms  I0129 02:00:35.187021 31926 leveldb.cpp:181] Compacted db in 49.836241ms  I0129 02:00:35.187088 31926 leveldb.cpp:196] Created db iterator in 33825ns  I0129 02:00:35.187109 31926 leveldb.cpp:202] Seeked to beginning of db in 7965ns  I0129 02:00:35.187121 31926 leveldb.cpp:271] Iterated through 0 keys in the db in 6350ns  I0129 02:00:35.187165 31926 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned  I0129 02:00:35.188433 31950 recover.cpp:447] Starting replica recovery  I0129 02:00:35.188796 31950 recover.cpp:473] Replica is in EMPTY status  I0129 02:00:35.190021 31949 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (11817)@172.17.0.3:60904  I0129 02:00:35.190569 31958 recover.cpp:193] Received a recover response from a replica in EMPTY status  I0129 02:00:35.190994 31959 recover.cpp:564] Updating replica status to STARTING  I0129 02:00:35.191522 31953 master.cpp:374] Master 823f2212-bf28-4dd6-959d-796029d32afb (90665f991b70) started on 172.17.0.3:60904  I0129 02:00:35.191640 31953 master.cpp:376] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_http=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/B9O6zq/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""25secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.28.0/_inst/share/mesos/webui"" --work_dir=""/tmp/B9O6zq/master"" --zk_session_timeout=""10secs""  I0129 02:00:35.191926 31953 master.cpp:421] Master only allowing authenticated frameworks to register  I0129 02:00:35.191936 31953 master.cpp:426] Master only allowing authenticated slaves to register  I0129 02:00:35.191943 31953 credentials.hpp:35] Loading credentials for authentication from '/tmp/B9O6zq/credentials'  I0129 02:00:35.192229 31953 master.cpp:466] Using default 'crammd5' authenticator  I0129 02:00:35.192366 31953 master.cpp:535] Using default 'basic' HTTP authenticator  I0129 02:00:35.192530 31953 master.cpp:569] Authorization enabled  I0129 02:00:35.192719 31950 whitelist_watcher.cpp:77] No whitelist given  I0129 02:00:35.192756 31957 hierarchical.cpp:144] Initialized hierarchical allocator process  I0129 02:00:35.194291 31955 master.cpp:1710] The newly elected leader is master@172.17.0.3:60904 with id 823f2212-bf28-4dd6-959d-796029d32afb  I0129 02:00:35.194335 31955 master.cpp:1723] Elected as the leading master!  I0129 02:00:35.194350 31955 master.cpp:1468] Recovering from registrar  I0129 02:00:35.194545 31958 registrar.cpp:307] Recovering registrar  I0129 02:00:35.220226 31948 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 29.150097ms  I0129 02:00:35.220262 31948 replica.cpp:320] Persisted replica status to STARTING  I0129 02:00:35.220484 31959 recover.cpp:473] Replica is in STARTING status  I0129 02:00:35.221220 31954 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (11819)@172.17.0.3:60904  I0129 02:00:35.221539 31959 recover.cpp:193] Received a recover response from a replica in STARTING status  I0129 02:00:35.221871 31954 recover.cpp:564] Updating replica status to VOTING  I0129 02:00:35.245329 31949 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 23.326002ms  I0129 02:00:35.245367 31949 replica.cpp:320] Persisted replica status to VOTING  I0129 02:00:35.245522 31955 recover.cpp:578] Successfully joined the Paxos group  I0129 02:00:35.245800 31955 recover.cpp:462] Recover process terminated  I0129 02:00:35.246181 31951 log.cpp:659] Attempting to start the writer  I0129 02:00:35.247228 31953 replica.cpp:493] Replica received implicit promise request from (11820)@172.17.0.3:60904 with proposal 1  I0129 02:00:35.270472 31953 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 23.225846ms  I0129 02:00:35.270510 31953 replica.cpp:342] Persisted promised to 1  I0129 02:00:35.271306 31957 coordinator.cpp:238] Coordinator attempting to fill missing positions  I0129 02:00:35.272373 31949 replica.cpp:388] Replica received explicit promise request from (11821)@172.17.0.3:60904 for position 0 with proposal 2  I0129 02:00:35.295600 31949 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 23.181008ms  I0129 02:00:35.295639 31949 replica.cpp:712] Persisted action at 0  I0129 02:00:35.296815 31950 replica.cpp:537] Replica received write request for position 0 from (11822)@172.17.0.3:60904  I0129 02:00:35.296879 31950 leveldb.cpp:436] Reading position from leveldb took 43203ns  I0129 02:00:35.320659 31950 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 23.753935ms  I0129 02:00:35.320699 31950 replica.cpp:712] Persisted action at 0  I0129 02:00:35.321394 31950 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0  I0129 02:00:35.345837 31950 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 24.358655ms  I0129 02:00:35.345877 31950 replica.cpp:712] Persisted action at 0  I0129 02:00:35.345898 31950 replica.cpp:697] Replica learned NOP action at position 0  I0129 02:00:35.346683 31950 log.cpp:675] Writer started with ending position 0  I0129 02:00:35.347913 31957 leveldb.cpp:436] Reading position from leveldb took 55621ns  I0129 02:00:35.349047 31947 registrar.cpp:340] Successfully fetched the registry (0B) in 154.395904ms  I0129 02:00:35.349185 31947 registrar.cpp:439] Applied 1 operations in 46347ns; attempting to update the 'registry'  I0129 02:00:35.350008 31952 log.cpp:683] Attempting to append 170 bytes to the log  I0129 02:00:35.350132 31957 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1  I0129 02:00:35.351042 31953 replica.cpp:537] Replica received write request for position 1 from (11823)@172.17.0.3:60904  I0129 02:00:35.370906 31953 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 19.829257ms  I0129 02:00:35.370946 31953 replica.cpp:712] Persisted action at 1  I0129 02:00:35.371840 31952 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0  I0129 02:00:35.396082 31952 leveldb.cpp:341] Persisting action (191 bytes) to leveldb took 24.218894ms  I0129 02:00:35.396122 31952 replica.cpp:712] Persisted action at 1  I0129 02:00:35.396144 31952 replica.cpp:697] Replica learned APPEND action at position 1  I0129 02:00:35.397250 31954 registrar.cpp:484] Successfully updated the 'registry' in 47.99104ms  I0129 02:00:35.397452 31954 registrar.cpp:370] Successfully recovered registrar  I0129 02:00:35.397678 31946 log.cpp:702] Attempting to truncate the log to 1  I0129 02:00:35.397881 31956 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2  I0129 02:00:35.398066 31951 master.cpp:1520] Recovered 0 slaves from the Registry (131B) ; allowing 10mins for slaves to re-register  I0129 02:00:35.398111 31957 hierarchical.cpp:171] Skipping recovery of hierarchical allocator: nothing to recover  I0129 02:00:35.398982 31955 replica.cpp:537] Replica received write request for position 2 from (11824)@172.17.0.3:60904  I0129 02:00:35.421293 31955 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 22.286476ms  I0129 02:00:35.421339 31955 replica.cpp:712] Persisted action at 2  I0129 02:00:35.422046 31944 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0  I0129 02:00:35.446316 31944 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 24.246177ms  I0129 02:00:35.446406 31944 leveldb.cpp:399] Deleting ~1 keys from leveldb took 84415ns  I0129 02:00:35.446466 31944 replica.cpp:712] Persisted action at 2  I0129 02:00:35.446491 31944 replica.cpp:697] Replica learned TRUNCATE action at position 2  I0129 02:00:35.452579 31957 slave.cpp:192] Slave started on 372)@172.17.0.3:60904  I0129 02:00:35.452620 31957 slave.cpp:193] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/ContentType_ExecutorHttpApiTest_DefaultAccept_1_r4GUhM/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""https://auth.docker.io"" --docker_kill_orphans=""true"" --docker_puller_timeout=""60"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/ContentType_ExecutorHttpApiTest_DefaultAccept_1_r4GUhM/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.28.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/ContentType_ExecutorHttpApiTest_DefaultAccept_1_r4GUhM""  I0129 02:00:35.453012 31957 credentials.hpp:83] Loading credential for authentication from '/tmp/ContentType_ExecutorHttpApiTest_DefaultAccept_1_r4GUhM/credential'  I0129 02:00:35.453191 31957 slave.cpp:323] Slave using credential for: test-principal  I0129 02:00:35.453368 31957 resources.cpp:564] Parsing resources as JSON failed: cpus:2;mem:1024;disk:1024;ports:[31000-32000]  Trying semicolon-delimited string format instead  I0129 02:00:35.453853 31957 slave.cpp:463] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]  I0129 02:00:35.453938 31957 slave.cpp:471] Slave attributes: [  ]  I0129 02:00:35.453953 31957 slave.cpp:476] Slave hostname: 90665f991b70  I0129 02:00:35.454794 31950 state.cpp:58] Recovering state from '/tmp/ContentType_ExecutorHttpApiTest_DefaultAccept_1_r4GUhM/meta'  I0129 02:00:35.455080 31948 status_update_manager.cpp:200] Recovering status update manager  I0129 02:00:35.455225 31926 sched.cpp:222] Version: 0.28.0  I0129 02:00:35.455535 31956 slave.cpp:4495] Finished recovery  I0129 02:00:35.455798 31945 sched.cpp:326] New master detected at master@172.17.0.3:60904  I0129 02:00:35.455879 31945 sched.cpp:382] Authenticating with master master@172.17.0.3:60904  I0129 02:00:35.455904 31945 sched.cpp:389] Using default CRAM-MD5 authenticatee  I0129 02:00:35.455943 31956 slave.cpp:4667] Querying resource estimator for oversubscribable resources  I0129 02:00:35.456167 31950 authenticatee.cpp:121] Creating new client SASL connection  I0129 02:00:35.456218 31953 status_update_manager.cpp:174] Pausing sending status updates  I0129 02:00:35.456219 31956 slave.cpp:795] New master detected at master@172.17.0.3:60904  I0129 02:00:35.456298 31956 slave.cpp:858] Authenticating with master master@172.17.0.3:60904  I0129 02:00:35.456323 31956 slave.cpp:863] Using default CRAM-MD5 authenticatee  I0129 02:00:35.456490 31948 authenticatee.cpp:121] Creating new client SASL connection  I0129 02:00:35.456492 31956 slave.cpp:831] Detecting new master  I0129 02:00:35.456588 31946 master.cpp:5521] Authenticating scheduler-93e745f0-0e48-4a8f-b227-93569976c5e8@172.17.0.3:60904  I0129 02:00:35.456686 31956 slave.cpp:4681] Received oversubscribable resources  from the resource estimator  I0129 02:00:35.456805 31953 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(804)@172.17.0.3:60904  I0129 02:00:35.456878 31946 master.cpp:5521] Authenticating slave(372)@172.17.0.3:60904  I0129 02:00:35.457124 31953 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(805)@172.17.0.3:60904  I0129 02:00:35.457157 31948 authenticator.cpp:98] Creating new server SASL connection  I0129 02:00:35.457373 31946 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5  I0129 02:00:35.457381 31951 authenticator.cpp:98] Creating new server SASL connection  I0129 02:00:35.457491 31946 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'  I0129 02:00:35.457598 31946 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5  I0129 02:00:35.457612 31951 authenticator.cpp:203] Received SASL authentication start  I0129 02:00:35.457635 31946 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'  I0129 02:00:35.457680 31951 authenticator.cpp:325] Authentication requires more steps  I0129 02:00:35.457767 31954 authenticator.cpp:203] Received SASL authentication start  I0129 02:00:35.457768 31948 authenticatee.cpp:258] Received SASL authentication step  I0129 02:00:35.457830 31954 authenticator.cpp:325] Authentication requires more steps  I0129 02:00:35.457885 31948 authenticator.cpp:231] Received SASL authentication step  I0129 02:00:35.457918 31948 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '90665f991b70' server FQDN: '90665f991b70' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false   I0129 02:00:35.457933 31948 auxprop.cpp:179] Looking up auxiliary property '*userPassword'  I0129 02:00:35.457954 31959 authenticatee.cpp:258] Received SASL authentication step  I0129 02:00:35.457993 31948 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'  I0129 02:00:35.458031 31948 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '90665f991b70' server FQDN: '90665f991b70' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true   I0129 02:00:35.458050 31948 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true  I0129 02:00:35.458065 31948 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true  I0129 02:00:35.458096 31948 authenticator.cpp:317] Authentication success  I0129 02:00:35.458112 31944 authenticator.cpp:231] Received SASL authentication step  I0129 02:00:35.458142 31944 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '90665f991b70' server FQDN: '90665f991b70' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false   I0129 02:00:35.458173 31944 auxprop.cpp:179] Looking up auxiliary property '*userPassword'  I0129 02:00:35.458206 31954 authenticatee.cpp:298] Authentication success  I0129 02:00:35.458256 31957 master.cpp:5551] Successfully authenticated principal 'test-principal' at scheduler-93e745f0-0e48-4a8f-b227-93569976c5e8@172.17.0.3:60904  I0129 02:00:35.458206 31944 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'  I0129 02:00:35.458360 31944 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '90665f991b70' server FQDN: '90665f991b70' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true   I0129 02:00:35.458382 31944 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true  I0129 02:00:35.458397 31944 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true  I0129 02:00:35.458489 31944 authenticator.cpp:317] Authentication success  I0129 02:00:35.458623 31953 sched.cpp:471] Successfully authenticated with master master@172.17.0.3:60904  I0129 02:00:35.458649 31953 sched.cpp:780] Sending SUBSCRIBE call to master@172.17.0.3:60904  I0129 02:00:35.458653 31956 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(804)@172.17.0.3:60904  I0129 02:00:35.458673 31951 authenticatee.cpp:298] Authentication success  I0129 02:00:35.458709 31952 master.cpp:5551] Successfully authenticated principal 'test-principal' at slave(372)@172.17.0.3:60904  I0129 02:00:35.458906 31955 slave.cpp:926] Successfully authenticated with master master@172.17.0.3:60904  I0129 02:00:35.458983 31956 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(805)@172.17.0.3:60904  I0129 02:00:35.459033 31955 slave.cpp:1320] Will retry registration in 7.075135ms if necessary  I0129 02:00:35.459128 31953 sched.cpp:813] Will retry registration in 86.579738ms if necessary  I0129 02:00:35.459193 31950 master.cpp:4235] Registering slave at slave(372)@172.17.0.3:60904 (90665f991b70) with id 823f2212-bf28-4dd6-959d-796029d32afb-S0  I0129 02:00:35.459489 31950 master.cpp:2278] Received SUBSCRIBE call for framework 'default' at scheduler-93e745f0-0e48-4a8f-b227-93569976c5e8@172.17.0.3:60904  I0129 02:00:35.459513 31950 master.cpp:1749] Authorizing framework principal 'test-principal' to receive offers for role '*'  I0129 02:00:35.459516 31959 registrar.cpp:439] Applied 1 operations in 62499ns; attempting to update the 'registry'  I0129 02:00:35.459766 31956 master.cpp:2349] Subscribing framework default with checkpointing disabled and capabilities [  ]  I0129 02:00:35.460095 31955 log.cpp:683] Attempting to append 339 bytes to the log  I0129 02:00:35.460192 31948 hierarchical.cpp:265] Added framework 823f2212-bf28-4dd6-959d-796029d32afb-0000  I0129 02:00:35.460247 31956 sched.cpp:707] Framework registered with 823f2212-bf28-4dd6-959d-796029d32afb-0000  I0129 02:00:35.460314 31958 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3  I0129 02:00:35.460388 31948 hierarchical.cpp:1403] No resources available to allocate!  I0129 02:00:35.460449 31948 hierarchical.cpp:1498] No inverse offers to send out!  I0129 02:00:35.460402 31956 sched.cpp:721] Scheduler::registered took 136519ns  I0129 02:00:35.460482 31948 hierarchical.cpp:1096] Performed allocation for 0 slaves in 158218ns  I0129 02:00:35.461187 31944 replica.cpp:537] Replica received write request for position 3 from (11829)@172.17.0.3:60904  I0129 02:00:35.467929 31954 slave.cpp:1320] Will retry registration in 14.701381ms if necessary  I0129 02:00:35.468183 31952 master.cpp:4223] Ignoring register slave message from slave(372)@172.17.0.3:60904 (90665f991b70) as admission is already in progress  I0129 02:00:35.483300 31959 slave.cpp:1320] Will retry registration in 8.003223ms if necessary  I0129 02:00:35.483500 31946 master.cpp:4223] Ignoring register slave message from slave(372)@172.17.0.3:60904 (90665f991b70) as admission is already in progress  I0129 02:00:35.491843 31945 slave.cpp:1320] Will retry registration in 52.952447ms if necessary  I0129 02:00:35.491962 31948 master.cpp:4223]...",3
"Automatically generate command-line flag documentation
To ensure that the command-line flag documentation in {{configuration.md}} stays in sync with the help strings in the various {{flags.cpp}} files, it could be beneficial to automate the generation of those docs. Such a script could be run as part of the build process, ensuring that changes to the help strings would show up in the documentation as well.    In addition to parsing and formatting the help strings for display as HTML, this could also involve specifying collections of flags to be grouped together in order to provide logical structure to the {{configuration.md}} documentation.",3
"Reduce the running time of benchmark tests.
Currently benchmark tests take a long time (>5 hours). It would be nice to reduce the total time taken by the benchmark tests to enable us to run them on ASF CI.    Command to run only benchmark tests  {code}  MESOS_BENCHMARK=1 GTEST_FILTER=""*BENCHMARK*"" make check  {code}",2
"Run benchmark tests in ASF CI
The build job is already created on ASF CI (https://builds.apache.org/job/Mesos-Benchmarks/) but is currently disabled due to MESOS-4558.",2
"Mesos UI shows wrong count for ""started"" tasks
The task started field shows the number of tasks in state ""TASKS_STARTING"" as opposed to those in ""TASK_RUNNING"" state.",2
"Separate Appc protobuf messages to its own file.
It would be cleaner to keep the Appc protobuf messages separate from other mesos messages.",2
"Avoid unnecessary temporary `std::string` constructions and copies in `jsonify`.
A few of the critical code paths in {{jsonify}} involve unnecessary temporary string construction and copies (inherited from the {{JSON::*}}). For example, {{strings::trim}} is used to remove trailing 0s from printing {{double}}s. We print {{double}}s a lot, and therefore constructing a temporary {{std::string}} on printing of every double is extremely costly. This ticket captures the work involved in avoiding them.",1
"Deprecate TASK_STARTING state
We currently have the following task stages:    * TASK_STAGING -> set by slave  * TASK_STARTING -> set by the executor (?)  * TASK_RUNNING -> set by the executor when the task is running   * TASK_XXX -> task termination statuses    The confusion here is about TASK_STARTING. This is the state between TASK_STAGING and TASK_RUNNING and is somewhat non-intuitive for the reader. Further, looks like no where in the source code, we are setting the TASK_STARTING state.    Why shouldn't we just deprecate/remove it?",2
"DockerFetcherPluginTest.INTERNET_CURL_FetchImage seems flaky.
{noformat}  ../configure --enable-ssl --enable-libevent && make check  {noformat}    {noformat}  --gtest_repeat=-1 --gtest_break_on_failure  --gtest_filter=DockerFetcherPluginTest.INTERNET_CURL_FetchImage   {noformat}    Failed at the 22nd run.     {noformat}  [ RUN      ] DockerFetcherPluginTest.INTERNET_CURL_FetchImage  ../../src/tests/uri_fetcher_tests.cpp:276: Failure  Failed to wait 15secs for fetcher.get()->fetch(uri, dir)  *** Aborted at 1454207653 (unix time) try ""date -d @1454207653"" if you are using GNU date ***  PC: @          0x167023a testing::UnitTest::AddTestPartResult()  *** SIGSEGV (@0x0) received by PID 19868 (TID 0x7f500fc877c0) from PID 0; stack trace: ***      @     0x7f5008f368d0 (unknown)      @          0x167023a testing::UnitTest::AddTestPartResult()      @          0x1664c73 testing::internal::AssertHelper::operator=()      @          0x146ac6f mesos::internal::tests::DockerFetcherPluginTest_INTERNET_CURL_FetchImage_Test::TestBody()      @          0x168dc70 testing::internal::HandleSehExceptionsInMethodIfSupported<>()      @          0x1688cc8 testing::internal::HandleExceptionsInMethodIfSupported<>()      @          0x166a013 testing::Test::Run()      @          0x166a7a1 testing::TestInfo::Run()      @          0x166addc testing::TestCase::Run()      @          0x167172b testing::internal::UnitTestImpl::RunAllTests()      @          0x168e8ff testing::internal::HandleSehExceptionsInMethodIfSupported<>()      @          0x168981e testing::internal::HandleExceptionsInMethodIfSupported<>()      @          0x167045b testing::UnitTest::Run()      @           0xe2d476 RUN_ALL_TESTS()      @           0xe2d08c main      @     0x7f5008b9fb45 (unknown)      @           0x9c6bf9 (unknown)  {noformat}",1
"Design doc for scheduler HTTP Stream IDs
This ticket is for the design of HTTP stream IDs, for use with HTTP schedulers. These IDs allow Mesos to distinguish between different instances of HTTP framework schedulers.",5
"Fix Appc image caching to share with image fetcher
As Appc image fetcher is being developed, Image cache needs to be shared between store and the image fetcher.",3
"Introduce a stout helper for ""which""
We may want to add a helper to {{stout/os.hpp}} that will natively emulate the functionality of the Linux utility {{which}}.  i.e.  {code}  Option<string> which(const string& command)  {    Option<string> path = os::getenv(""PATH"");      // Loop through path and return the first one which os::exists(...).      return None();  }  {code}    This helper may be useful:  * for test filters in {{src/tests/environment.cpp}}  * a few tests in {{src/tests/containerizer/port_mapping_tests.cpp}}  * the {{sha512}} utility in {{src/common/command_utils.cpp}}  * as runtime checks in the {{LogrotateContainerLogger}}  * etc.",2
"state.json serving duplicate ""active"" fields
state.json is serving duplicate ""active"" fields in frameworks.  See the framework ""47df96c2-3f85-4bc5-b781-709b2c30c752-0000"" In the attached file",1
"Rename `examples/event_call_framework.cpp` to `examples/test_http_framework.cpp`
We already have {{examples/test_framework.cpp}} for testing {{PID}} based frameworks. We would ideally want to rename {{event_call_framework}} to correctly reflect that it's an example for HTTP based framework.",1
"Update Rakefile for mesos site generation
The stuff in site/ directory needs some updates to make it easier to generate updates for mesos.apache.org site.",2
"Add test case for reservations with same role, different principals
We don't have a test case that covers $SUBJECT; we probably should.",2
"`/reserve` and `/create-volumes` endpoints allow operations for any role
When frameworks reserve resources, the validation of the operation ensures that the {{role}} of the reservation matches the {{role}} of the framework. For the case of the {{/reserve}} operator endpoint, however, the operator has no role to validate, so this check isn't performed.    This means that if an ACL exists which authorizes a framework's principal to reserve resources, that same principal can be used to reserve resources for _any_ role through the operator endpoint.    We should restrict reservations made through the operator endpoint to specified roles. A few possibilities:  * The {{object}} of the {{reserve_resources}} ACL could be changed from {{resources}} to {{roles}}  * A second ACL could be added for authorization of {{reserve}} operations, with an {{object}} of {{role}}  * Our conception of the {{resources}} object in the {{reserve_resources}} ACL could be expanded to include role information, i.e., {{disk(role1);mem(role1)}}",3
"Add common Appc spec utilities.
 Add common utility functions such as :        - validating image information against actual data in the image directory.        - getting list of dependencies at depth 1 for an image.        - getting image path simple image discovery.  ",2
"Logrotate ContainerLogger should not remove IP from environment.
The {{LogrotateContainerLogger}} starts libprocess-using subprocesses.  Libprocess initialization will attempt to resolve the IP from the hostname.  If a DNS service is not available, this step will fail, which terminates the logger subprocess prematurely.    Since the logger subprocesses live on the agent, they should use the same {{LIBPROCESS_IP}} supplied to the agent.",1
"Use `std::quoted` for strings in error messages
We'd like to have a consistent format for error strings through the code base.   As per this comment:  [MESOS-3772|https://issues.apache.org/jira/browse/MESOS-3772?focusedCommentId=14965652&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14965652]    We can then overload the stream operator to make sur strings are quoted as needed.    Note: We need to first require compilers that support C++14. For now we have to wait for MSVC to be part of that list.",3
"ROOT_DOCKER_DockerHealthyTask is flaky.
Log from Teamcity that is running {{sudo ./bin/mesos-tests.sh}} on AWS EC2 instances:  {noformat}  [18:27:14][Step 8/8] [----------] 8 tests from HealthCheckTest  [18:27:14][Step 8/8] [ RUN      ] HealthCheckTest.HealthyTask  [18:27:17][Step 8/8] [       OK ] HealthCheckTest.HealthyTask (2222 ms)  [18:27:17][Step 8/8] [ RUN      ] HealthCheckTest.ROOT_DOCKER_DockerHealthyTask  [18:27:36][Step 8/8] ../../src/tests/health_check_tests.cpp:388: Failure  [18:27:36][Step 8/8] Failed to wait 15secs for termination  [18:27:36][Step 8/8] F0204 18:27:35.981302 23085 logging.cpp:64] RAW: Pure virtual method called  [18:27:36][Step 8/8]     @     0x7f7077055e1c  google::LogMessage::Fail()  [18:27:36][Step 8/8]     @     0x7f707705ba6f  google::RawLog__()  [18:27:36][Step 8/8]     @     0x7f70760f76c9  __cxa_pure_virtual  [18:27:36][Step 8/8]     @           0xa9423c  mesos::internal::tests::Cluster::Slaves::shutdown()  [18:27:36][Step 8/8]     @          0x1074e45  mesos::internal::tests::MesosTest::ShutdownSlaves()  [18:27:36][Step 8/8]     @          0x1074de4  mesos::internal::tests::MesosTest::Shutdown()  [18:27:36][Step 8/8]     @          0x1070ec7  mesos::internal::tests::MesosTest::TearDown()  [18:27:36][Step 8/8]     @          0x16eb7b2  testing::internal::HandleSehExceptionsInMethodIfSupported<>()  [18:27:36][Step 8/8]     @          0x16e61a9  testing::internal::HandleExceptionsInMethodIfSupported<>()  [18:27:36][Step 8/8]     @          0x16c56aa  testing::Test::Run()  [18:27:36][Step 8/8]     @          0x16c5e89  testing::TestInfo::Run()  [18:27:36][Step 8/8]     @          0x16c650a  testing::TestCase::Run()  [18:27:36][Step 8/8]     @          0x16cd1f6  testing::internal::UnitTestImpl::RunAllTests()  [18:27:36][Step 8/8]     @          0x16ec513  testing::internal::HandleSehExceptionsInMethodIfSupported<>()  [18:27:36][Step 8/8]     @          0x16e6df1  testing::internal::HandleExceptionsInMethodIfSupported<>()  [18:27:36][Step 8/8]     @          0x16cbe26  testing::UnitTest::Run()  [18:27:36][Step 8/8]     @           0xe54c84  RUN_ALL_TESTS()  [18:27:36][Step 8/8]     @           0xe54867  main  [18:27:36][Step 8/8]     @     0x7f7071560a40  (unknown)  [18:27:36][Step 8/8]     @           0x9b52d9  _start  [18:27:36][Step 8/8] Aborted (core dumped)  [18:27:36][Step 8/8] Process exited with code 134  {noformat}  Happens with Ubuntu 15.04, CentOS 6, CentOS 7 _quite_ often. ",2
"Subprocess should be more intelligent about setting/inheriting libprocess environment variables 
Mostly copied from [this comment|https://issues.apache.org/jira/browse/MESOS-4598?focusedCommentId=15133497&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15133497]    A subprocess inheriting the environment variables {{LIBPROCESS_*}} may run into some accidental fatalities:    | || Subprocess uses libprocess || Subprocess is something else ||  || Subprocess sets/inherits the same {{PORT}} by accident | Bind failure -> exit | Nothing happens (?) |  || Subprocess sets a different {{PORT}} on purpose | Bind success (?) | Nothing happens (?) |    (?) = means this is usually the case, but not 100%.    A complete fix would look something like:  * If the {{subprocess}} call gets {{environment = None()}}, we should automatically remove {{LIBPROCESS_PORT}} from the inherited environment.    * The parts of [{{executorEnvironment}}|https://github.com/apache/mesos/blame/master/src/slave/containerizer/containerizer.cpp#L265] dealing with libprocess & libmesos should be refactored into libprocess as a helper.  We would use this helper for the Containerizer, Fetcher, and ContainerLogger module.  * If the {{subprocess}} call is given {{LIBPROCESS_PORT == os::getenv(""LIBPROCESS_PORT"")}}, we can LOG(WARN) and unset the env var locally.",2
"Passing a lambda to dispatch() always matches the template returning void
The following idiom does not currently compile:    {code}    Future<Nothing> initialized = dispatch(pid, [] () -> Nothing {      return Nothing();    });  {code}    This seems non-intuitive because the following template exists for dispatch:    {code}  template <typename R>  Future<R> dispatch(const UPID& pid, const std::function<R()>& f)  {    std::shared_ptr<Promise<R>> promise(new Promise<R>());        std::shared_ptr<std::function<void(ProcessBase*)>> f_(        new std::function<void(ProcessBase*)>(            [=](ProcessBase*) {              promise->set(f());            }));      internal::dispatch(pid, f_);        return promise->future();  }       {code}    However, lambdas cannot be implicitly cast to a corresponding std::function<R()> type.  To make this work, you have to explicitly type the lambda before passing it to dispatch.    {code}    std::function<Nothing()> f = []() { return Nothing(); };    Future<Nothing> initialized = dispatch(pid, f);  {code}    We should add template support to allow lambdas to be passed to dispatch() without explicit typing.   ",5
"Update vendored ZooKeeper to 3.4.8
See: http://zookeeper.apache.org/doc/r3.4.8/releasenotes.html for improvements / bug fixes    Added a new patch that solved [ZOOKEEPER-1643](https://issues.apache.org/jira/browse/ZOOKEEPER-1643)    The original patch: <https://github.com/apache/zookeeper/commit/46b565e6abd8423c43f1bb8da782d76bac7c392c>",3
"SlaveRecoveryTest/0.CleanupHTTPExecutor is flaky
Just saw this failure on the ASF CI:    {code}  [ RUN      ] SlaveRecoveryTest/0.CleanupHTTPExecutor  I0206 00:22:44.791671  2824 leveldb.cpp:174] Opened db in 2.539372ms  I0206 00:22:44.792459  2824 leveldb.cpp:181] Compacted db in 740473ns  I0206 00:22:44.792510  2824 leveldb.cpp:196] Created db iterator in 24164ns  I0206 00:22:44.792532  2824 leveldb.cpp:202] Seeked to beginning of db in 1831ns  I0206 00:22:44.792548  2824 leveldb.cpp:271] Iterated through 0 keys in the db in 342ns  I0206 00:22:44.792605  2824 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned  I0206 00:22:44.793256  2847 recover.cpp:447] Starting replica recovery  I0206 00:22:44.793480  2847 recover.cpp:473] Replica is in EMPTY status  I0206 00:22:44.794538  2847 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (9472)@172.17.0.2:43484  I0206 00:22:44.795040  2848 recover.cpp:193] Received a recover response from a replica in EMPTY status  I0206 00:22:44.795644  2848 recover.cpp:564] Updating replica status to STARTING  I0206 00:22:44.796519  2850 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 752810ns  I0206 00:22:44.796545  2850 replica.cpp:320] Persisted replica status to STARTING  I0206 00:22:44.796725  2848 recover.cpp:473] Replica is in STARTING status  I0206 00:22:44.797828  2857 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (9473)@172.17.0.2:43484  I0206 00:22:44.798355  2850 recover.cpp:193] Received a recover response from a replica in STARTING status  I0206 00:22:44.799193  2850 recover.cpp:564] Updating replica status to VOTING  I0206 00:22:44.799583  2855 master.cpp:376] Master 0b206a40-a9c3-4d44-a5bd-8032d60a32ca (6632562f1ade) started on 172.17.0.2:43484  I0206 00:22:44.799609  2855 master.cpp:378] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_http=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/n2FxQV/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.28.0/_inst/share/mesos/webui"" --work_dir=""/tmp/n2FxQV/master"" --zk_session_timeout=""10secs""  I0206 00:22:44.799991  2855 master.cpp:423] Master only allowing authenticated frameworks to register  I0206 00:22:44.800009  2855 master.cpp:428] Master only allowing authenticated slaves to register  I0206 00:22:44.800020  2855 credentials.hpp:35] Loading credentials for authentication from '/tmp/n2FxQV/credentials'  I0206 00:22:44.800245  2850 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 679345ns  I0206 00:22:44.800370  2850 replica.cpp:320] Persisted replica status to VOTING  I0206 00:22:44.800397  2855 master.cpp:468] Using default 'crammd5' authenticator  I0206 00:22:44.800693  2855 master.cpp:537] Using default 'basic' HTTP authenticator  I0206 00:22:44.800815  2855 master.cpp:571] Authorization enabled  I0206 00:22:44.801216  2850 recover.cpp:578] Successfully joined the Paxos group  I0206 00:22:44.801604  2850 recover.cpp:462] Recover process terminated  I0206 00:22:44.801759  2856 whitelist_watcher.cpp:77] No whitelist given  I0206 00:22:44.801725  2847 hierarchical.cpp:144] Initialized hierarchical allocator process  I0206 00:22:44.803982  2855 master.cpp:1712] The newly elected leader is master@172.17.0.2:43484 with id 0b206a40-a9c3-4d44-a5bd-8032d60a32ca  I0206 00:22:44.804026  2855 master.cpp:1725] Elected as the leading master!  I0206 00:22:44.804059  2855 master.cpp:1470] Recovering from registrar  I0206 00:22:44.804424  2855 registrar.cpp:307] Recovering registrar  I0206 00:22:44.805202  2855 log.cpp:659] Attempting to start the writer  I0206 00:22:44.806782  2856 replica.cpp:493] Replica received implicit promise request from (9475)@172.17.0.2:43484 with proposal 1  I0206 00:22:44.807368  2856 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 547939ns  I0206 00:22:44.807395  2856 replica.cpp:342] Persisted promised to 1  I0206 00:22:44.808375  2856 coordinator.cpp:238] Coordinator attempting to fill missing positions  I0206 00:22:44.809460  2848 replica.cpp:388] Replica received explicit promise request from (9476)@172.17.0.2:43484 for position 0 with proposal 2  I0206 00:22:44.809929  2848 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 427561ns  I0206 00:22:44.809967  2848 replica.cpp:712] Persisted action at 0  I0206 00:22:44.811035  2850 replica.cpp:537] Replica received write request for position 0 from (9477)@172.17.0.2:43484  I0206 00:22:44.811149  2850 leveldb.cpp:436] Reading position from leveldb took 36452ns  I0206 00:22:44.811532  2850 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 318924ns  I0206 00:22:44.811615  2850 replica.cpp:712] Persisted action at 0  I0206 00:22:44.812532  2850 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0  I0206 00:22:44.813117  2850 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 476530ns  I0206 00:22:44.813143  2850 replica.cpp:712] Persisted action at 0  I0206 00:22:44.813166  2850 replica.cpp:697] Replica learned NOP action at position 0  I0206 00:22:44.813984  2848 log.cpp:675] Writer started with ending position 0  I0206 00:22:44.815549  2848 leveldb.cpp:436] Reading position from leveldb took 31800ns  I0206 00:22:44.817061  2848 registrar.cpp:340] Successfully fetched the registry (0B) in 12.591104ms  I0206 00:22:44.817319  2848 registrar.cpp:439] Applied 1 operations in 63480ns; attempting to update the 'registry'  I0206 00:22:44.818780  2845 log.cpp:683] Attempting to append 170 bytes to the log  I0206 00:22:44.818981  2845 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1  I0206 00:22:44.819941  2845 replica.cpp:537] Replica received write request for position 1 from (9478)@172.17.0.2:43484  I0206 00:22:44.820582  2845 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 600949ns  I0206 00:22:44.820608  2845 replica.cpp:712] Persisted action at 1  I0206 00:22:44.821552  2845 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0  I0206 00:22:44.821934  2845 leveldb.cpp:341] Persisting action (191 bytes) to leveldb took 352813ns  I0206 00:22:44.821960  2845 replica.cpp:712] Persisted action at 1  I0206 00:22:44.821979  2845 replica.cpp:697] Replica learned APPEND action at position 1  I0206 00:22:44.823447  2845 registrar.cpp:484] Successfully updated the 'registry' in 5.987072ms  I0206 00:22:44.823580  2845 registrar.cpp:370] Successfully recovered registrar  I0206 00:22:44.823833  2845 log.cpp:702] Attempting to truncate the log to 1  I0206 00:22:44.824203  2845 master.cpp:1522] Recovered 0 slaves from the Registry (131B) ; allowing 10mins for slaves to re-register  I0206 00:22:44.824291  2845 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2  I0206 00:22:44.824645  2845 hierarchical.cpp:171] Skipping recovery of hierarchical allocator: nothing to recover  I0206 00:22:44.825222  2850 replica.cpp:537] Replica received write request for position 2 from (9479)@172.17.0.2:43484  I0206 00:22:44.825742  2850 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 481617ns  I0206 00:22:44.825772  2850 replica.cpp:712] Persisted action at 2  I0206 00:22:44.826748  2852 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0  I0206 00:22:44.827368  2852 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 588591ns  I0206 00:22:44.827432  2852 leveldb.cpp:399] Deleting ~1 keys from leveldb took 33059ns  I0206 00:22:44.827450  2852 replica.cpp:712] Persisted action at 2  I0206 00:22:44.827468  2852 replica.cpp:697] Replica learned TRUNCATE action at position 2  I0206 00:22:44.838011  2824 containerizer.cpp:149] Using isolation: posix/cpu,posix/mem,filesystem/posix  W0206 00:22:44.838873  2824 backend.cpp:48] Failed to create 'bind' backend: BindBackend requires root privileges  I0206 00:22:44.843785  2857 slave.cpp:193] Slave started on 172.17.0.2:43484  I0206 00:22:44.843819  2857 slave.cpp:194] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""https://auth.docker.io"" --docker_kill_orphans=""true"" --docker_puller_timeout=""60"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.28.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw""  I0206 00:22:44.844292  2857 credentials.hpp:83] Loading credential for authentication from '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/credential'  I0206 00:22:44.844518  2857 slave.cpp:324] Slave using credential for: test-principal  I0206 00:22:44.844696  2857 resources.cpp:564] Parsing resources as JSON failed: cpus:2;mem:1024;disk:1024;ports:[31000-32000]  Trying semicolon-delimited string format instead  I0206 00:22:44.845243  2857 slave.cpp:464] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]  I0206 00:22:44.845326  2857 slave.cpp:472] Slave attributes: [  ]  I0206 00:22:44.845342  2857 slave.cpp:477] Slave hostname: 6632562f1ade  I0206 00:22:44.845953  2824 sched.cpp:222] Version: 0.28.0  I0206 00:22:44.846853  2848 sched.cpp:326] New master detected at master@172.17.0.2:43484  I0206 00:22:44.846936  2848 sched.cpp:382] Authenticating with master master@172.17.0.2:43484  I0206 00:22:44.846958  2848 sched.cpp:389] Using default CRAM-MD5 authenticatee  I0206 00:22:44.847692  2858 state.cpp:58] Recovering state from '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/meta'  I0206 00:22:44.848108  2850 status_update_manager.cpp:200] Recovering status update manager  I0206 00:22:44.848325  2852 containerizer.cpp:397] Recovering containerizer  I0206 00:22:44.848603  2845 authenticatee.cpp:121] Creating new client SASL connection  I0206 00:22:44.849719  2845 master.cpp:5523] Authenticating scheduler-63899759-d7fc-42b2-8371-57484f352895@172.17.0.2:43484  I0206 00:22:44.850052  2852 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(662)@172.17.0.2:43484  I0206 00:22:44.850227  2854 provisioner.cpp:245] Provisioner recovery complete  I0206 00:22:44.850410  2852 authenticator.cpp:98] Creating new server SASL connection  I0206 00:22:44.850692  2852 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5  I0206 00:22:44.850720  2852 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'  I0206 00:22:44.850805  2852 authenticator.cpp:203] Received SASL authentication start  I0206 00:22:44.850862  2852 authenticator.cpp:325] Authentication requires more steps  I0206 00:22:44.850939  2852 authenticatee.cpp:258] Received SASL authentication step  I0206 00:22:44.851027  2852 authenticator.cpp:231] Received SASL authentication step  I0206 00:22:44.851052  2852 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '6632562f1ade' server FQDN: '6632562f1ade' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false   I0206 00:22:44.851063  2852 auxprop.cpp:179] Looking up auxiliary property '*userPassword'  I0206 00:22:44.851102  2852 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'  I0206 00:22:44.851121  2852 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '6632562f1ade' server FQDN: '6632562f1ade' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true   I0206 00:22:44.851130  2852 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true  I0206 00:22:44.851136  2852 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true  I0206 00:22:44.851150  2852 authenticator.cpp:317] Authentication success  I0206 00:22:44.851219  2850 authenticatee.cpp:298] Authentication success  I0206 00:22:44.851310  2850 master.cpp:5553] Successfully authenticated principal 'test-principal' at scheduler-63899759-d7fc-42b2-8371-57484f352895@172.17.0.2:43484  I0206 00:22:44.851485  2849 slave.cpp:4496] Finished recovery  I0206 00:22:44.852154  2843 sched.cpp:471] Successfully authenticated with master master@172.17.0.2:43484  I0206 00:22:44.852175  2843 sched.cpp:776] Sending SUBSCRIBE call to master@172.17.0.2:43484  I0206 00:22:44.852262  2843 sched.cpp:809] Will retry registration in 939.183679ms if necessary  I0206 00:22:44.852375  2844 master.cpp:2280] Received SUBSCRIBE call for framework 'default' at scheduler-63899759-d7fc-42b2-8371-57484f352895@172.17.0.2:43484  I0206 00:22:44.852448  2844 master.cpp:1751] Authorizing framework principal 'test-principal' to receive offers for role '*'  I0206 00:22:44.852699  2852 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(662)@172.17.0.2:43484  I0206 00:22:44.852782  2844 master.cpp:2351] Subscribing framework default with checkpointing enabled and capabilities [  ]  I0206 00:22:44.853056  2849 slave.cpp:4668] Querying resource estimator for oversubscribable resources  I0206 00:22:44.853421  2856 hierarchical.cpp:265] Added framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000  I0206 00:22:44.853513  2856 hierarchical.cpp:1403] No resources available to allocate!  I0206 00:22:44.853582  2844 sched.cpp:703] Framework registered with 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000  I0206 00:22:44.853613  2852 slave.cpp:4682] Received oversubscribable resources  from the resource estimator  I0206 00:22:44.853663  2844 sched.cpp:717] Scheduler::registered took 53762ns  I0206 00:22:44.853899  2843 slave.cpp:796] New master detected at master@172.17.0.2:43484  I0206 00:22:44.853955  2854 status_update_manager.cpp:174] Pausing sending status updates  I0206 00:22:44.853997  2856 hierarchical.cpp:1498] No inverse offers to send out!  I0206 00:22:44.853960  2843 slave.cpp:859] Authenticating with master master@172.17.0.2:43484  I0206 00:22:44.854035  2843 slave.cpp:864] Using default CRAM-MD5 authenticatee  I0206 00:22:44.854030  2856 hierarchical.cpp:1096] Performed allocation for 0 slaves in 581355ns  I0206 00:22:44.854182  2843 slave.cpp:832] Detecting new master  I0206 00:22:44.854277  2854 authenticatee.cpp:121] Creating new client SASL connection  I0206 00:22:44.854517  2843 master.cpp:5523] Authenticating slave@172.17.0.2:43484  I0206 00:22:44.854603  2854 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(663)@172.17.0.2:43484  I0206 00:22:44.854836  2855 authenticator.cpp:98] Creating new server SASL connection  I0206 00:22:44.855013  2852 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5  I0206 00:22:44.855044  2852 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'  I0206 00:22:44.855139  2855 authenticator.cpp:203] Received SASL authentication start  I0206 00:22:44.855186  2855 authenticator.cpp:325] Authentication requires more steps  I0206 00:22:44.855263  2855 authenticatee.cpp:258] Received SASL authentication step  I0206 00:22:44.855352  2855 authenticator.cpp:231] Received SASL authentication step  I0206 00:22:44.855381  2855 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '6632562f1ade' server FQDN: '6632562f1ade' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false   I0206 00:22:44.855389  2855 auxprop.cpp:179] Looking up auxiliary property '*userPassword'  I0206 00:22:44.855419  2855 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'  I0206 00:22:44.855438  2855 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '6632562f1ade' server FQDN: '6632562f1ade' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true   I0206 00:22:44.855448  2855 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true  I0206 00:22:44.855453  2855 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true  I0206 00:22:44.855464  2855 authenticator.cpp:317] Authentication success  I0206 00:22:44.855540  2851 authenticatee.cpp:298] Authentication success  I0206 00:22:44.855721  2851 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(663)@172.17.0.2:43484  I0206 00:22:44.855832  2852 slave.cpp:927] Successfully authenticated with master master@172.17.0.2:43484  I0206 00:22:44.855615  2855 master.cpp:5553] Successfully authenticated principal 'test-principal' at slave@172.17.0.2:43484  I0206 00:22:44.855973  2852 slave.cpp:1321] Will retry registration in 9.327708ms if necessary  I0206 00:22:44.856145  2854 master.cpp:4237] Registering slave at slave@172.17.0.2:43484 (6632562f1ade) with id 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0  I0206 00:22:44.856598  2851 registrar.cpp:439] Applied 1 operations in 59112ns; attempting to update the 'registry'  I0206 00:22:44.857403  2851 log.cpp:683] Attempting to append 339 bytes to the log  I0206 00:22:44.857525  2855 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3  I0206 00:22:44.858482  2844 replica.cpp:537] Replica received write request for position 3 from (9493)@172.17.0.2:43484  I0206 00:22:44.858755  2844 leveldb.cpp:341] Persisting action (358 bytes) to leveldb took 228484ns  I0206 00:22:44.858855  2844 replica.cpp:712] Persisted action at 3  I0206 00:22:44.859751  2852 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0  I0206 00:22:44.860332  2852 leveldb.cpp:341] Persisting action (360 bytes) to leveldb took 549638ns  I0206 00:22:44.860358  2852 replica.cpp:712] Persisted action at 3  I0206 00:22:44.860411  2852 replica.cpp:697] Replica learned APPEND action at position 3  I0206 00:22:44.862709  2856 registrar.cpp:484] Succe...",3
"ContainerLoggerTest.DefaultToSandbox is flaky
Just saw this failure on the ASF CI:    {code}  [ RUN      ] ContainerLoggerTest.DefaultToSandbox  I0206 01:25:03.766458  2824 leveldb.cpp:174] Opened db in 72.979786ms  I0206 01:25:03.811712  2824 leveldb.cpp:181] Compacted db in 45.162067ms  I0206 01:25:03.811810  2824 leveldb.cpp:196] Created db iterator in 26090ns  I0206 01:25:03.811828  2824 leveldb.cpp:202] Seeked to beginning of db in 3173ns  I0206 01:25:03.811839  2824 leveldb.cpp:271] Iterated through 0 keys in the db in 497ns  I0206 01:25:03.811900  2824 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned  I0206 01:25:03.812785  2849 recover.cpp:447] Starting replica recovery  I0206 01:25:03.813043  2849 recover.cpp:473] Replica is in EMPTY status  I0206 01:25:03.814668  2854 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (371)@172.17.0.8:37843  I0206 01:25:03.815210  2849 recover.cpp:193] Received a recover response from a replica in EMPTY status  I0206 01:25:03.815732  2854 recover.cpp:564] Updating replica status to STARTING  I0206 01:25:03.819664  2857 master.cpp:376] Master 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de (74ef606c4063) started on 172.17.0.8:37843  I0206 01:25:03.819703  2857 master.cpp:378] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_http=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/h5vu5I/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.28.0/_inst/share/mesos/webui"" --work_dir=""/tmp/h5vu5I/master"" --zk_session_timeout=""10secs""  I0206 01:25:03.820241  2857 master.cpp:423] Master only allowing authenticated frameworks to register  I0206 01:25:03.820257  2857 master.cpp:428] Master only allowing authenticated slaves to register  I0206 01:25:03.820269  2857 credentials.hpp:35] Loading credentials for authentication from '/tmp/h5vu5I/credentials'  I0206 01:25:03.821110  2857 master.cpp:468] Using default 'crammd5' authenticator  I0206 01:25:03.821311  2857 master.cpp:537] Using default 'basic' HTTP authenticator  I0206 01:25:03.821636  2857 master.cpp:571] Authorization enabled  I0206 01:25:03.821979  2846 hierarchical.cpp:144] Initialized hierarchical allocator process  I0206 01:25:03.822057  2846 whitelist_watcher.cpp:77] No whitelist given  I0206 01:25:03.825460  2847 master.cpp:1712] The newly elected leader is master@172.17.0.8:37843 with id 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de  I0206 01:25:03.825512  2847 master.cpp:1725] Elected as the leading master!  I0206 01:25:03.825533  2847 master.cpp:1470] Recovering from registrar  I0206 01:25:03.825835  2847 registrar.cpp:307] Recovering registrar  I0206 01:25:03.848212  2854 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 32.226093ms  I0206 01:25:03.848299  2854 replica.cpp:320] Persisted replica status to STARTING  I0206 01:25:03.848702  2854 recover.cpp:473] Replica is in STARTING status  I0206 01:25:03.850728  2858 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (373)@172.17.0.8:37843  I0206 01:25:03.851230  2854 recover.cpp:193] Received a recover response from a replica in STARTING status  I0206 01:25:03.852018  2854 recover.cpp:564] Updating replica status to VOTING  I0206 01:25:03.881681  2854 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 29.184163ms  I0206 01:25:03.881772  2854 replica.cpp:320] Persisted replica status to VOTING  I0206 01:25:03.882058  2854 recover.cpp:578] Successfully joined the Paxos group  I0206 01:25:03.882258  2854 recover.cpp:462] Recover process terminated  I0206 01:25:03.883076  2854 log.cpp:659] Attempting to start the writer  I0206 01:25:03.885040  2854 replica.cpp:493] Replica received implicit promise request from (374)@172.17.0.8:37843 with proposal 1  I0206 01:25:03.915132  2854 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 29.980589ms  I0206 01:25:03.915215  2854 replica.cpp:342] Persisted promised to 1  I0206 01:25:03.916038  2856 coordinator.cpp:238] Coordinator attempting to fill missing positions  I0206 01:25:03.917659  2856 replica.cpp:388] Replica received explicit promise request from (375)@172.17.0.8:37843 for position 0 with proposal 2  I0206 01:25:03.948698  2856 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 30.974607ms  I0206 01:25:03.948786  2856 replica.cpp:712] Persisted action at 0  I0206 01:25:03.950920  2849 replica.cpp:537] Replica received write request for position 0 from (376)@172.17.0.8:37843  I0206 01:25:03.951011  2849 leveldb.cpp:436] Reading position from leveldb took 44263ns  I0206 01:25:03.982026  2849 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 30.947321ms  I0206 01:25:03.982225  2849 replica.cpp:712] Persisted action at 0  I0206 01:25:03.983867  2849 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0  I0206 01:25:04.015499  2849 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 30.957888ms  I0206 01:25:04.015591  2849 replica.cpp:712] Persisted action at 0  I0206 01:25:04.015682  2849 replica.cpp:697] Replica learned NOP action at position 0  I0206 01:25:04.016666  2849 log.cpp:675] Writer started with ending position 0  I0206 01:25:04.017881  2855 leveldb.cpp:436] Reading position from leveldb took 56779ns  I0206 01:25:04.018934  2852 registrar.cpp:340] Successfully fetched the registry (0B) in 193.048064ms  I0206 01:25:04.019076  2852 registrar.cpp:439] Applied 1 operations in 38180ns; attempting to update the 'registry'  I0206 01:25:04.020100  2844 log.cpp:683] Attempting to append 170 bytes to the log  I0206 01:25:04.020288  2855 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1  I0206 01:25:04.021323  2844 replica.cpp:537] Replica received write request for position 1 from (377)@172.17.0.8:37843  I0206 01:25:04.054726  2844 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 33.309419ms  I0206 01:25:04.054818  2844 replica.cpp:712] Persisted action at 1  I0206 01:25:04.055933  2844 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0  I0206 01:25:04.088142  2844 leveldb.cpp:341] Persisting action (191 bytes) to leveldb took 32.116643ms  I0206 01:25:04.088230  2844 replica.cpp:712] Persisted action at 1  I0206 01:25:04.088265  2844 replica.cpp:697] Replica learned APPEND action at position 1  I0206 01:25:04.090070  2856 registrar.cpp:484] Successfully updated the 'registry' in 70.90816ms  I0206 01:25:04.090338  2851 log.cpp:702] Attempting to truncate the log to 1  I0206 01:25:04.090358  2856 registrar.cpp:370] Successfully recovered registrar  I0206 01:25:04.090507  2847 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2  I0206 01:25:04.090867  2858 master.cpp:1522] Recovered 0 slaves from the Registry (131B) ; allowing 10mins for slaves to re-register  I0206 01:25:04.091449  2858 hierarchical.cpp:171] Skipping recovery of hierarchical allocator: nothing to recover  I0206 01:25:04.092280  2857 replica.cpp:537] Replica received write request for position 2 from (378)@172.17.0.8:37843  I0206 01:25:04.125702  2857 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 33.192265ms  I0206 01:25:04.125804  2857 replica.cpp:712] Persisted action at 2  I0206 01:25:04.127400  2857 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0  I0206 01:25:04.157727  2857 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 30.268594ms  I0206 01:25:04.157905  2857 leveldb.cpp:399] Deleting ~1 keys from leveldb took 88436ns  I0206 01:25:04.157941  2857 replica.cpp:712] Persisted action at 2  I0206 01:25:04.157984  2857 replica.cpp:697] Replica learned TRUNCATE action at position 2  I0206 01:25:04.166174  2824 containerizer.cpp:149] Using isolation: posix/cpu,posix/mem,filesystem/posix  W0206 01:25:04.166954  2824 backend.cpp:48] Failed to create 'bind' backend: BindBackend requires root privileges  I0206 01:25:04.172008  2844 slave.cpp:193] Slave started on 9)@172.17.0.8:37843  I0206 01:25:04.172046  2844 slave.cpp:194] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""https://auth.docker.io"" --docker_kill_orphans=""true"" --docker_puller_timeout=""60"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.28.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw""  I0206 01:25:04.172569  2844 credentials.hpp:83] Loading credential for authentication from '/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw/credential'  I0206 01:25:04.172886  2844 slave.cpp:324] Slave using credential for: test-principal  I0206 01:25:04.173141  2844 resources.cpp:564] Parsing resources as JSON failed: cpus:2;mem:1024;disk:1024;ports:[31000-32000]  Trying semicolon-delimited string format instead  I0206 01:25:04.173620  2844 slave.cpp:464] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]  I0206 01:25:04.173686  2844 slave.cpp:472] Slave attributes: [  ]  I0206 01:25:04.173702  2844 slave.cpp:477] Slave hostname: 74ef606c4063  I0206 01:25:04.174816  2847 state.cpp:58] Recovering state from '/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw/meta'  I0206 01:25:04.175441  2847 status_update_manager.cpp:200] Recovering status update manager  I0206 01:25:04.175678  2858 containerizer.cpp:397] Recovering containerizer  I0206 01:25:04.177573  2858 provisioner.cpp:245] Provisioner recovery complete  I0206 01:25:04.178231  2847 slave.cpp:4496] Finished recovery  I0206 01:25:04.178834  2847 slave.cpp:4668] Querying resource estimator for oversubscribable resources  I0206 01:25:04.179405  2847 slave.cpp:796] New master detected at master@172.17.0.8:37843  I0206 01:25:04.179500  2847 slave.cpp:859] Authenticating with master master@172.17.0.8:37843  I0206 01:25:04.179525  2847 slave.cpp:864] Using default CRAM-MD5 authenticatee  I0206 01:25:04.179656  2858 status_update_manager.cpp:174] Pausing sending status updates  I0206 01:25:04.179798  2847 slave.cpp:832] Detecting new master  I0206 01:25:04.179891  2852 authenticatee.cpp:121] Creating new client SASL connection  I0206 01:25:04.179916  2847 slave.cpp:4682] Received oversubscribable resources  from the resource estimator  I0206 01:25:04.180286  2847 master.cpp:5523] Authenticating slave(9)@172.17.0.8:37843  I0206 01:25:04.180569  2847 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(32)@172.17.0.8:37843  I0206 01:25:04.181000  2847 authenticator.cpp:98] Creating new server SASL connection  I0206 01:25:04.181315  2847 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5  I0206 01:25:04.181387  2847 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'  I0206 01:25:04.181562  2847 authenticator.cpp:203] Received SASL authentication start  I0206 01:25:04.181648  2847 authenticator.cpp:325] Authentication requires more steps  I0206 01:25:04.181843  2847 authenticatee.cpp:258] Received SASL authentication step  I0206 01:25:04.182034  2853 authenticator.cpp:231] Received SASL authentication step  I0206 01:25:04.182071  2853 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '74ef606c4063' server FQDN: '74ef606c4063' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false   I0206 01:25:04.182093  2853 auxprop.cpp:179] Looking up auxiliary property '*userPassword'  I0206 01:25:04.182145  2853 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'  I0206 01:25:04.182173  2853 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '74ef606c4063' server FQDN: '74ef606c4063' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true   I0206 01:25:04.182185  2853 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true  I0206 01:25:04.182193  2853 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true  I0206 01:25:04.182211  2853 authenticator.cpp:317] Authentication success  I0206 01:25:04.182333  2849 authenticatee.cpp:298] Authentication success  I0206 01:25:04.182422  2853 master.cpp:5553] Successfully authenticated principal 'test-principal' at slave(9)@172.17.0.8:37843  I0206 01:25:04.182510  2853 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(32)@172.17.0.8:37843  I0206 01:25:04.182945  2849 slave.cpp:927] Successfully authenticated with master master@172.17.0.8:37843  I0206 01:25:04.183178  2849 slave.cpp:1321] Will retry registration in 9.87937ms if necessary  I0206 01:25:04.183466  2852 master.cpp:4237] Registering slave at slave(9)@172.17.0.8:37843 (74ef606c4063) with id 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0  I0206 01:25:04.184039  2845 registrar.cpp:439] Applied 1 operations in 89453ns; attempting to update the 'registry'  I0206 01:25:04.185288  2856 log.cpp:683] Attempting to append 339 bytes to the log  I0206 01:25:04.185672  2850 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3  I0206 01:25:04.186674  2846 replica.cpp:537] Replica received write request for position 3 from (392)@172.17.0.8:37843  I0206 01:25:04.195863  2856 slave.cpp:1321] Will retry registration in 11.038094ms if necessary  I0206 01:25:04.196233  2856 master.cpp:4225] Ignoring register slave message from slave(9)@172.17.0.8:37843 (74ef606c4063) as admission is already in progress  I0206 01:25:04.208094  2856 slave.cpp:1321] Will retry registration in 27.881223ms if necessary  I0206 01:25:04.208472  2856 master.cpp:4225] Ignoring register slave message from slave(9)@172.17.0.8:37843 (74ef606c4063) as admission is already in progress  I0206 01:25:04.216698  2846 leveldb.cpp:341] Persisting action (358 bytes) to leveldb took 29.961291ms  I0206 01:25:04.216789  2846 replica.cpp:712] Persisted action at 3  I0206 01:25:04.218246  2845 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0  I0206 01:25:04.237861  2846 slave.cpp:1321] Will retry registration in 1.006941ms if necessary  I0206 01:25:04.238221  2846 master.cpp:4225] Ignoring register slave message from slave(9)@172.17.0.8:37843 (74ef606c4063) as admission is already in progress  I0206 01:25:04.239858  2856 slave.cpp:1321] Will retry registration in 167.305686ms if necessary  I0206 01:25:04.240044  2856 master.cpp:4225] Ignoring register slave message from slave(9)@172.17.0.8:37843 (74ef606c4063) as admission is already in progress  I0206 01:25:04.241482  2845 leveldb.cpp:341] Persisting action (360 bytes) to leveldb took 23.193162ms  I0206 01:25:04.241524  2845 replica.cpp:712] Persisted action at 3  I0206 01:25:04.241557  2845 replica.cpp:697] Replica learned APPEND action at position 3  I0206 01:25:04.243746  2844 registrar.cpp:484] Successfully updated the 'registry' in 59.587072ms  I0206 01:25:04.244210  2857 log.cpp:702] Attempting to truncate the log to 3  I0206 01:25:04.244344  2845 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4  I0206 01:25:04.244597  2856 master.cpp:4305] Registered slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 at slave(9)@172.17.0.8:37843 (74ef606c4063) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]  I0206 01:25:04.244746  2843 slave.cpp:3436] Received ping from slave-observer(8)@172.17.0.8:37843  I0206 01:25:04.244976  2845 hierarchical.cpp:473] Added slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 (74ef606c4063) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )  I0206 01:25:04.245072  2843 slave.cpp:971] Registered with master master@172.17.0.8:37843; given slave ID 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0  I0206 01:25:04.245121  2843 fetcher.cpp:81] Clearing fetcher cache  I0206 01:25:04.245146  2845 hierarchical.cpp:1403] No resources available to allocate!  I0206 01:25:04.245178  2845 hierarchical.cpp:1116] Performed allocation for slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 in 159744ns  I0206 01:25:04.245465  2846 status_update_manager.cpp:181] Resuming sending status updates  I0206 01:25:04.245776  2843 slave.cpp:994] Checkpointing SlaveInfo to '/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw/meta/slaves/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0/slave.info'  I0206 01:25:04.245745  2846 replica.cpp:537] Replica received write request for position 4 from (393)@172.17.0.8:37843  I0206 01:25:04.246273  2843 slave.cpp:1030] Forwarding total oversubscribed resources   I0206 01:25:04.246507  2850 master.cpp:4646] Received update of slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 at slave(9)@172.17.0.8:37843 (74ef606c4063) with total oversubscribed resources   I0206 01:25:04.247180  2824 sched.cpp:222] Version: 0.28.0  I0206 01:25:04.247155  2850 hierarchical.cpp:531] Slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 (74ef606c4063) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: )  I0206 01:25:04.247357  2850 hierarchical.cpp:1403] No resources available to allocate!  I0206 01:25:04.247406  2850 hierarchical.cpp:1116] Performed allocation for slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 in 183250ns  I0206 01:25:04.247938  2854 sched.cpp:326] New master detected at master@172.17.0.8:37843  I0206 01:25:04.248157  2854 sched.cpp:382] Authenticating with master master@172.17.0.8:37843  I0206 01:25:04.248265  2854 sched.cpp:389] Using default CRAM-MD5 authenticatee  I0206 01:25:04.248769  2854 authenticatee.cpp:121] Creating new client SASL connection  I0206 01:25:04.249311  2854 master.cpp:5523] Authenticating scheduler-f50aad75-78d0-4d9f-b1a4-488d5ab932d6@172.17.0.8:37843  I0206 01:25:04.249646  2854 authenticator.cpp:413] Starting authentication sess...",1
"Remove markdown files from doxygen pages
The doxygen html pages corresponding to doc/* markdown files are redundant and have broken links. They don't serve any reasonable purpose in doxygen site.",1
"Update configuration.md with `--cgroups_net_cls_primary_handle` agent flag.
As part of the net_cls epic, we introduce an agent flag called `--cgroup_net_cls_primary_handle` . We need to update configuration.md with the corresponding help string. ",1
"Add a stub Nvidia GPU isolator.
We'll first wire up a skeleton Nvidia GPU isolator, which needs to be guarded by a configure flag due to the dependency on NVML.",3
"Add allocation metrics for ""gpus"" resources.
Allocation metrics are currently hard-coded to include only {{\[""cpus"", ""mem"", ""disk""\]}} resources. We'll need to add ""gpus"" to the list to start, possibly following up on the TODO to remove the hard-coding.    See:  https://github.com/apache/mesos/blob/0.27.0/src/master/metrics.cpp#L266-L269  https://github.com/apache/mesos/blob/0.27.0/src/slave/metrics.cpp#L123-L126  ",1
"Implement Nvidia GPU isolation w/o filesystem isolation enabled.
The Nvidia GPU isolator will need to use the device cgroup to restrict access to GPU resources, and will need to recover this information after agent failover. For now this will require that the operator specifies the GPU devices via a flag.    To handle filesystem isolation requires that we provide mechanisms for operators to inject volumes with the necessary libraries into all containers using GPU resources, we'll tackle this in a separate ticket.",5
"Support Nvidia GPUs with filesystem isolation enabled in mesos containerizer.
When filesystem isolation is enabled in the mesos containerizer, containers that use Nvidia GPU resources need access to GPU libraries residing on the host.    We'll need to provide a means for operators to inject the necessary volumes into *all* containers that use ""gpus"" resources.    See the nvidia-docker project for more details:  [nvidia-docker/tools/src/nvidia/volumes.go|https://github.com/NVIDIA/nvidia-docker/blob/fda10b2d27bf5578cc5337c23877f827e4d1ed77/tools/src/nvidia/volumes.go#L50-L103]",13
"Implement fault tolerance tests for the HTTP Scheduler API.
Currently, the HTTP V1 API does not have fault tolerance tests similar to the one in {{src/tests/fault_tolerance_tests.cpp}}.     For more information see MESOS-3355.",5
"Implement partition tests for the HTTP Scheduler API.
Currently, the HTTP V1 API does not have partition tests similar to the one in src/tests/partition_tests.cpp.    For more information see MESOS-3355.",5
"Tests will dereference stack allocated agent objects upon assertion/expectation failure.
Tests that use the {{StartSlave}} test helper are generally fragile when the test fails an assert/expect in the middle of the test.  This is because the {{StartSlave}} helper takes raw pointer arguments, which may be stack-allocated.    In case of an assert failure, the test immediately exits (destroying stack allocated objects) and proceeds onto test cleanup.  The test cleanup may dereference some of these destroyed objects, leading to a test crash like:  {code}  [18:27:36][Step 8/8] F0204 18:27:35.981302 23085 logging.cpp:64] RAW: Pure virtual method called  [18:27:36][Step 8/8]     @     0x7f7077055e1c  google::LogMessage::Fail()  [18:27:36][Step 8/8]     @     0x7f707705ba6f  google::RawLog__()  [18:27:36][Step 8/8]     @     0x7f70760f76c9  __cxa_pure_virtual  [18:27:36][Step 8/8]     @           0xa9423c  mesos::internal::tests::Cluster::Slaves::shutdown()  [18:27:36][Step 8/8]     @          0x1074e45  mesos::internal::tests::MesosTest::ShutdownSlaves()  [18:27:36][Step 8/8]     @          0x1074de4  mesos::internal::tests::MesosTest::Shutdown()  [18:27:36][Step 8/8]     @          0x1070ec7  mesos::internal::tests::MesosTest::TearDown()  {code}    The {{StartSlave}} helper should take {{shared_ptr}} arguments instead.  This also means that we can remove the {{Shutdown}} helper from most of these tests.",5
"Tests will dereference stack allocated master objects upon assertion/expectation failure.
Tests that use the {{StartMaster}} test helper are generally fragile when the test fails an assert/expect in the middle of the test.  This is because the {{StartMaster}} helper takes raw pointer arguments, which may be stack-allocated.    In case of an assert failure, the test immediately exits (destroying stack allocated objects) and proceeds onto test cleanup.  The test cleanup may dereference some of these destroyed objects, leading to a test crash like:  {code}  [18:27:36][Step 8/8] F0204 18:27:35.981302 23085 logging.cpp:64] RAW: Pure virtual method called  [18:27:36][Step 8/8]     @     0x7f7077055e1c  google::LogMessage::Fail()  [18:27:36][Step 8/8]     @     0x7f707705ba6f  google::RawLog__()  [18:27:36][Step 8/8]     @     0x7f70760f76c9  __cxa_pure_virtual  [18:27:36][Step 8/8]     @           0xa9423c  mesos::internal::tests::Cluster::Slaves::shutdown()  [18:27:36][Step 8/8]     @          0x1074e45  mesos::internal::tests::MesosTest::ShutdownSlaves()  [18:27:36][Step 8/8]     @          0x1074de4  mesos::internal::tests::MesosTest::Shutdown()  [18:27:36][Step 8/8]     @          0x1070ec7  mesos::internal::tests::MesosTest::TearDown()  {code}    The {{StartMaster}} helper should take {{shared_ptr}} arguments instead.  This also means that we can remove the {{Shutdown}} helper from most of these tests.",5
"Add parent hook to subprocess.
nan",3
"Docker process executor can die with agent unit on systemd.
nan",1
"Posix process executor can die with agent unit on systemd.
nan",1
"Logrotate container logger can die with agent unit on systemd.
nan",1
"Add LOG(INFO) in `cgroups/net_cls` for debugging allocation of net_cls handles.
We need to add LOG(INFO) during the prepare phase of `cgroups/net_cls` for debugging management of `net_cls` handles within the isolator. ",1
"Document net_cls isolator in docs/mesos-containerizer.md.
We need to add a section in the doc to describe how to use cgroups/net_cls isolator.",1
"Expose persistent volume information in HTTP endpoints
The per-slave {{reserved_resources}} information returned by {{/state}} does not seem to include information about persistent volumes. This makes it hard for operators to use the {{/destroy-volumes}} endpoint.",3
"Add common compression utility
We need GZIP uncompress utility for Appc image fetching functionality. The images are tar + gzip'ed and they needs to be first uncompressed so that we can compute sha 512 checksum on it.",2
"`cgroup_info` not being exposed in state.json when ComposingContainerizer is used.
The ComposingContainerizer currently does not have a `status` method. This results in no `ContainerStatus` being updated in the agent, when uses `ComposingContainerizer` to launch containers. This would specifically happen when the agent is launched with `--containerizer=docker,mesos`",1
"Status updates from executor can be forwarded out of order by the Agent.
Previously, all status update messages from the executor were forwarded by the agent to the master in the order that they had been received.     However, that seems to be no longer valid due to a recently introduced change in the agent:    {code}  // Before sending update, we need to retrieve the container status.    containerizer->status(executor->containerId)      .onAny(defer(self(),                   &Slave::_statusUpdate,                   update,                   pid,                   executor->id,                   lambda::_1));  {code}    This can sometimes lead to status updates being sent out of order depending on the order the {{Future}} is fulfilled from the call to {{status(...)}}.",1
"Linux filesystem isolator tests are flaky.
LinuxFilesystemIsolatorTest.ROOT_ImageInVolumeWithRootFilesystem sometimes fails on CentOS 7 with this kind of output:  {noformat}  ../../src/tests/containerizer/filesystem_isolator_tests.cpp:1054: Failure  Failed to wait 2mins for launch  {noformat}    LinuxFilesystemIsolatorTest.ROOT_MultipleContainers often has this output:  {noformat}  ../../src/tests/containerizer/filesystem_isolator_tests.cpp:1138: Failure  Failed to wait 1mins for launch1  {noformat}    Whether SSL is configured makes no difference.    This test may also fail on other platforms, but more rarely.    ",3
"Cannot disable systemd support
On certain platforms the systemd init system is available, but not used.  Not being able to disable the mesos systemd integration on these platforms makes it hard to operate using a different init / monit system.",1
"ROOT_DOCKER_Logs is flaky.
{noformat}  [18:06:25][Step 8/8] [ RUN      ] DockerContainerizerTest.ROOT_DOCKER_Logs  [18:06:25][Step 8/8] I0215 17:06:25.256103  1740 leveldb.cpp:174] Opened db in 6.548327ms  [18:06:25][Step 8/8] I0215 17:06:25.258002  1740 leveldb.cpp:181] Compacted db in 1.837816ms  [18:06:25][Step 8/8] I0215 17:06:25.258059  1740 leveldb.cpp:196] Created db iterator in 22044ns  [18:06:25][Step 8/8] I0215 17:06:25.258076  1740 leveldb.cpp:202] Seeked to beginning of db in 2347ns  [18:06:25][Step 8/8] I0215 17:06:25.258091  1740 leveldb.cpp:271] Iterated through 0 keys in the db in 571ns  [18:06:25][Step 8/8] I0215 17:06:25.258152  1740 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned  [18:06:25][Step 8/8] I0215 17:06:25.258936  1758 recover.cpp:447] Starting replica recovery  [18:06:25][Step 8/8] I0215 17:06:25.259177  1758 recover.cpp:473] Replica is in EMPTY status  [18:06:25][Step 8/8] I0215 17:06:25.260327  1757 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (13608)@172.30.2.239:39785  [18:06:25][Step 8/8] I0215 17:06:25.260545  1758 recover.cpp:193] Received a recover response from a replica in EMPTY status  [18:06:25][Step 8/8] I0215 17:06:25.261065  1757 master.cpp:376] Master 112363e2-c680-4946-8fee-d0626ed8b21e (ip-172-30-2-239.mesosphere.io) started on 172.30.2.239:39785  [18:06:25][Step 8/8] I0215 17:06:25.261209  1761 recover.cpp:564] Updating replica status to STARTING  [18:06:25][Step 8/8] I0215 17:06:25.261086  1757 master.cpp:378] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_http=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/HncLLj/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/tmp/HncLLj/master"" --zk_session_timeout=""10secs""  [18:06:25][Step 8/8] I0215 17:06:25.261446  1757 master.cpp:423] Master only allowing authenticated frameworks to register  [18:06:25][Step 8/8] I0215 17:06:25.261456  1757 master.cpp:428] Master only allowing authenticated slaves to register  [18:06:25][Step 8/8] I0215 17:06:25.261462  1757 credentials.hpp:35] Loading credentials for authentication from '/tmp/HncLLj/credentials'  [18:06:25][Step 8/8] I0215 17:06:25.261723  1757 master.cpp:468] Using default 'crammd5' authenticator  [18:06:25][Step 8/8] I0215 17:06:25.261855  1757 master.cpp:537] Using default 'basic' HTTP authenticator  [18:06:25][Step 8/8] I0215 17:06:25.262022  1757 master.cpp:571] Authorization enabled  [18:06:25][Step 8/8] I0215 17:06:25.262177  1755 hierarchical.cpp:144] Initialized hierarchical allocator process  [18:06:25][Step 8/8] I0215 17:06:25.262177  1758 whitelist_watcher.cpp:77] No whitelist given  [18:06:25][Step 8/8] I0215 17:06:25.262899  1760 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.517992ms  [18:06:25][Step 8/8] I0215 17:06:25.262924  1760 replica.cpp:320] Persisted replica status to STARTING  [18:06:25][Step 8/8] I0215 17:06:25.263144  1754 recover.cpp:473] Replica is in STARTING status  [18:06:25][Step 8/8] I0215 17:06:25.264010  1757 master.cpp:1712] The newly elected leader is master@172.30.2.239:39785 with id 112363e2-c680-4946-8fee-d0626ed8b21e  [18:06:25][Step 8/8] I0215 17:06:25.264044  1757 master.cpp:1725] Elected as the leading master!  [18:06:25][Step 8/8] I0215 17:06:25.264061  1757 master.cpp:1470] Recovering from registrar  [18:06:25][Step 8/8] I0215 17:06:25.264117  1760 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (13610)@172.30.2.239:39785  [18:06:25][Step 8/8] I0215 17:06:25.264197  1758 registrar.cpp:307] Recovering registrar  [18:06:25][Step 8/8] I0215 17:06:25.264827  1756 recover.cpp:193] Received a recover response from a replica in STARTING status  [18:06:25][Step 8/8] I0215 17:06:25.265219  1757 recover.cpp:564] Updating replica status to VOTING  [18:06:25][Step 8/8] I0215 17:06:25.267302  1754 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.887739ms  [18:06:25][Step 8/8] I0215 17:06:25.267326  1754 replica.cpp:320] Persisted replica status to VOTING  [18:06:25][Step 8/8] I0215 17:06:25.267453  1759 recover.cpp:578] Successfully joined the Paxos group  [18:06:25][Step 8/8] I0215 17:06:25.267632  1759 recover.cpp:462] Recover process terminated  [18:06:25][Step 8/8] I0215 17:06:25.268007  1757 log.cpp:659] Attempting to start the writer  [18:06:25][Step 8/8] I0215 17:06:25.269055  1759 replica.cpp:493] Replica received implicit promise request from (13611)@172.30.2.239:39785 with proposal 1  [18:06:25][Step 8/8] I0215 17:06:25.270488  1759 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.406068ms  [18:06:25][Step 8/8] I0215 17:06:25.270511  1759 replica.cpp:342] Persisted promised to 1  [18:06:25][Step 8/8] I0215 17:06:25.271078  1761 coordinator.cpp:238] Coordinator attempting to fill missing positions  [18:06:25][Step 8/8] I0215 17:06:25.272146  1756 replica.cpp:388] Replica received explicit promise request from (13612)@172.30.2.239:39785 for position 0 with proposal 2  [18:06:25][Step 8/8] I0215 17:06:25.273478  1756 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 1.297217ms  [18:06:25][Step 8/8] I0215 17:06:25.273500  1756 replica.cpp:712] Persisted action at 0  [18:06:25][Step 8/8] I0215 17:06:25.274355  1757 replica.cpp:537] Replica received write request for position 0 from (13613)@172.30.2.239:39785  [18:06:25][Step 8/8] I0215 17:06:25.274405  1757 leveldb.cpp:436] Reading position from leveldb took 25294ns  [18:06:25][Step 8/8] I0215 17:06:25.275800  1757 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 1.362978ms  [18:06:25][Step 8/8] I0215 17:06:25.275823  1757 replica.cpp:712] Persisted action at 0  [18:06:25][Step 8/8] I0215 17:06:25.276348  1755 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0  [18:06:25][Step 8/8] I0215 17:06:25.277765  1755 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.391531ms  [18:06:25][Step 8/8] I0215 17:06:25.277788  1755 replica.cpp:712] Persisted action at 0  [18:06:25][Step 8/8] I0215 17:06:25.277802  1755 replica.cpp:697] Replica learned NOP action at position 0  [18:06:25][Step 8/8] I0215 17:06:25.278336  1754 log.cpp:675] Writer started with ending position 0  [18:06:25][Step 8/8] I0215 17:06:25.279371  1755 leveldb.cpp:436] Reading position from leveldb took 29214ns  [18:06:25][Step 8/8] I0215 17:06:25.280272  1758 registrar.cpp:340] Successfully fetched the registry (0B) in 16.02688ms  [18:06:25][Step 8/8] I0215 17:06:25.280385  1758 registrar.cpp:439] Applied 1 operations in 31040ns; attempting to update the 'registry'  [18:06:25][Step 8/8] I0215 17:06:25.281054  1755 log.cpp:683] Attempting to append 210 bytes to the log  [18:06:25][Step 8/8] I0215 17:06:25.281165  1757 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1  [18:06:25][Step 8/8] I0215 17:06:25.281780  1757 replica.cpp:537] Replica received write request for position 1 from (13614)@172.30.2.239:39785  [18:06:25][Step 8/8] I0215 17:06:25.283159  1757 leveldb.cpp:341] Persisting action (229 bytes) to leveldb took 1.348041ms  [18:06:25][Step 8/8] I0215 17:06:25.283184  1757 replica.cpp:712] Persisted action at 1  [18:06:25][Step 8/8] I0215 17:06:25.283695  1759 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0  [18:06:25][Step 8/8] I0215 17:06:25.285059  1759 leveldb.cpp:341] Persisting action (231 bytes) to leveldb took 1.334577ms  [18:06:25][Step 8/8] I0215 17:06:25.285084  1759 replica.cpp:712] Persisted action at 1  [18:06:25][Step 8/8] I0215 17:06:25.285099  1759 replica.cpp:697] Replica learned APPEND action at position 1  [18:06:25][Step 8/8] I0215 17:06:25.285910  1758 registrar.cpp:484] Successfully updated the 'registry' in 5.46816ms  [18:06:25][Step 8/8] I0215 17:06:25.286043  1758 registrar.cpp:370] Successfully recovered registrar  [18:06:25][Step 8/8] I0215 17:06:25.286121  1755 log.cpp:702] Attempting to truncate the log to 1  [18:06:25][Step 8/8] I0215 17:06:25.286301  1756 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2  [18:06:25][Step 8/8] I0215 17:06:25.286478  1759 hierarchical.cpp:171] Skipping recovery of hierarchical allocator: nothing to recover  [18:06:25][Step 8/8] I0215 17:06:25.286476  1754 master.cpp:1522] Recovered 0 slaves from the Registry (171B) ; allowing 10mins for slaves to re-register  [18:06:25][Step 8/8] I0215 17:06:25.287137  1755 replica.cpp:537] Replica received write request for position 2 from (13615)@172.30.2.239:39785  [18:06:25][Step 8/8] I0215 17:06:25.289104  1755 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.938609ms  [18:06:25][Step 8/8] I0215 17:06:25.289127  1755 replica.cpp:712] Persisted action at 2  [18:06:25][Step 8/8] I0215 17:06:25.289667  1759 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0  [18:06:25][Step 8/8] I0215 17:06:25.290956  1759 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 1.256421ms  [18:06:25][Step 8/8] I0215 17:06:25.291007  1759 leveldb.cpp:399] Deleting ~1 keys from leveldb took 28064ns  [18:06:25][Step 8/8] I0215 17:06:25.291021  1759 replica.cpp:712] Persisted action at 2  [18:06:25][Step 8/8] I0215 17:06:25.291038  1759 replica.cpp:697] Replica learned TRUNCATE action at position 2  [18:06:25][Step 8/8] I0215 17:06:25.300550  1760 slave.cpp:193] Slave started on 393)@172.30.2.239:39785  [18:06:25][Step 8/8] I0215 17:06:25.300573  1760 slave.cpp:194] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/DockerContainerizerTest_ROOT_DOCKER_Logs_a4NS2N/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""https://auth.docker.io"" --docker_kill_orphans=""true"" --docker_puller_timeout=""60"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/DockerContainerizerTest_ROOT_DOCKER_Logs_a4NS2N/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mnt/teamcity/work/4240ba9ddd0997c3/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/DockerContainerizerTest_ROOT_DOCKER_Logs_a4NS2N""  [18:06:25][Step 8/8] I0215 17:06:25.300868  1760 credentials.hpp:83] Loading credential for authentication from '/tmp/DockerContainerizerTest_ROOT_DOCKER_Logs_a4NS2N/credential'  [18:06:25][Step 8/8] I0215 17:06:25.301030  1760 slave.cpp:324] Slave using credential for: test-principal  [18:06:25][Step 8/8] I0215 17:06:25.301180  1760 resources.cpp:576] Parsing resources as JSON failed: cpus:2;mem:1024;disk:1024;ports:[31000-32000]  [18:06:25][Step 8/8] Trying semicolon-delimited string format instead  [18:06:25][Step 8/8] I0215 17:06:25.301553  1760 slave.cpp:464] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]  [18:06:25][Step 8/8] I0215 17:06:25.301609  1760 slave.cpp:472] Slave attributes: [  ]  [18:06:25][Step 8/8] I0215 17:06:25.301620  1760 slave.cpp:477] Slave hostname: ip-172-30-2-239.mesosphere.io  [18:06:25][Step 8/8] I0215 17:06:25.302417  1757 state.cpp:58] Recovering state from '/tmp/DockerContainerizerTest_ROOT_DOCKER_Logs_a4NS2N/meta'  [18:06:25][Step 8/8] I0215 17:06:25.302515  1740 sched.cpp:222] Version: 0.28.0  [18:06:25][Step 8/8] I0215 17:06:25.302772  1755 status_update_manager.cpp:200] Recovering status update manager  [18:06:25][Step 8/8] I0215 17:06:25.302956  1758 docker.cpp:559] Recovering Docker containers  [18:06:25][Step 8/8] I0215 17:06:25.303050  1761 sched.cpp:326] New master detected at master@172.30.2.239:39785  [18:06:25][Step 8/8] I0215 17:06:25.303133  1754 slave.cpp:4565] Finished recovery  [18:06:25][Step 8/8] I0215 17:06:25.303154  1761 sched.cpp:382] Authenticating with master master@172.30.2.239:39785  [18:06:25][Step 8/8] I0215 17:06:25.303169  1761 sched.cpp:389] Using default CRAM-MD5 authenticatee  [18:06:25][Step 8/8] I0215 17:06:25.303364  1759 authenticatee.cpp:121] Creating new client SASL connection  [18:06:25][Step 8/8] I0215 17:06:25.303467  1754 slave.cpp:4737] Querying resource estimator for oversubscribable resources  [18:06:25][Step 8/8] I0215 17:06:25.303668  1756 master.cpp:5523] Authenticating scheduler-806c70e3-1cf6-418f-aa30-6bb26db42d18@172.30.2.239:39785  [18:06:25][Step 8/8] I0215 17:06:25.303707  1760 status_update_manager.cpp:174] Pausing sending status updates  [18:06:25][Step 8/8] I0215 17:06:25.303707  1754 slave.cpp:796] New master detected at master@172.30.2.239:39785  [18:06:25][Step 8/8] I0215 17:06:25.303767  1755 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(829)@172.30.2.239:39785  [18:06:25][Step 8/8] I0215 17:06:25.303791  1754 slave.cpp:859] Authenticating with master master@172.30.2.239:39785  [18:06:25][Step 8/8] I0215 17:06:25.303805  1754 slave.cpp:864] Using default CRAM-MD5 authenticatee  [18:06:25][Step 8/8] I0215 17:06:25.303956  1754 slave.cpp:832] Detecting new master  [18:06:25][Step 8/8] I0215 17:06:25.303971  1761 authenticatee.cpp:121] Creating new client SASL connection  [18:06:25][Step 8/8] I0215 17:06:25.303984  1760 authenticator.cpp:98] Creating new server SASL connection  [18:06:25][Step 8/8] I0215 17:06:25.304131  1754 slave.cpp:4751] Received oversubscribable resources  from the resource estimator  [18:06:25][Step 8/8] I0215 17:06:25.304275  1757 master.cpp:5523] Authenticating slave(393)@172.30.2.239:39785  [18:06:25][Step 8/8] I0215 17:06:25.304344  1754 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5  [18:06:25][Step 8/8] I0215 17:06:25.304369  1754 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'  [18:06:25][Step 8/8] I0215 17:06:25.304373  1761 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(830)@172.30.2.239:39785  [18:06:25][Step 8/8] I0215 17:06:25.304440  1757 authenticator.cpp:203] Received SASL authentication start  [18:06:25][Step 8/8] I0215 17:06:25.304491  1757 authenticator.cpp:325] Authentication requires more steps  [18:06:25][Step 8/8] I0215 17:06:25.304548  1754 authenticator.cpp:98] Creating new server SASL connection  [18:06:25][Step 8/8] I0215 17:06:25.304582  1761 authenticatee.cpp:258] Received SASL authentication step  [18:06:25][Step 8/8] I0215 17:06:25.304688  1761 authenticator.cpp:231] Received SASL authentication step  [18:06:25][Step 8/8] I0215 17:06:25.304714  1761 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-239.mesosphere.io' server FQDN: 'ip-172-30-2-239.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false   [18:06:25][Step 8/8] I0215 17:06:25.304723  1761 auxprop.cpp:179] Looking up auxiliary property '*userPassword'  [18:06:25][Step 8/8] I0215 17:06:25.304767  1761 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'  [18:06:25][Step 8/8] I0215 17:06:25.304805  1761 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-239.mesosphere.io' server FQDN: 'ip-172-30-2-239.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true   [18:06:25][Step 8/8] I0215 17:06:25.304817  1761 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true  [18:06:25][Step 8/8] I0215 17:06:25.304824  1761 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true  [18:06:25][Step 8/8] I0215 17:06:25.304836  1761 authenticator.cpp:317] Authentication success  [18:06:25][Step 8/8] I0215 17:06:25.304841  1758 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5  [18:06:25][Step 8/8] I0215 17:06:25.304870  1758 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'  [18:06:25][Step 8/8] I0215 17:06:25.304909  1757 authenticatee.cpp:298] Authentication success  [18:06:25][Step 8/8] I0215 17:06:25.304983  1756 authenticator.cpp:203] Received SASL authentication start  [18:06:25][Step 8/8] I0215 17:06:25.305033  1756 authenticator.cpp:325] Authentication requires more steps  [18:06:25][Step 8/8] I0215 17:06:25.305042  1759 master.cpp:5553] Successfully authenticated principal 'test-principal' at scheduler-806c70e3-1cf6-418f-aa30-6bb26db42d18@172.30.2.239:39785  [18:06:25][Step 8/8] I0215 17:06:25.305071  1755 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(829)@172.30.2.239:39785  [18:06:25][Step 8/8] I0215 17:06:25.305124  1756 authenticatee.cpp:258] Received SASL authentication step  [18:06:25][Step 8/8] I0215 17:06:25.305222  1758 sched.cpp:471] Successfully authenticated with master master@172.30.2.239:39785  [18:06:25][Step 8/8] I0215 17:06:25.305246  1758 sched.cpp:776] Sending SUBSCRIBE call to master@172.30.2.239:39785  [18:06:25][Step 8/8] I0215 17:06:25.305286  1760 authenticator.cpp:231] Received SASL authentication step  [18:06:25][Step 8/8] I0215 17:06:25.305310  1760 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-239.mesosphere.io' server FQDN: 'ip-172-30-2-239.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false   [18:06:25][Step 8/8] I0215 17:06:25.305318  1760 auxprop.cpp:179] Looking up auxiliary property '*userPassword'  [18:06:25][Step 8/8] I0215 17:06:25.305344  1760 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'  [18:06:25][Step 8/8] I0215 17:06:25.305363  1758 sched.cpp:809] Will retry registration in 1.888777185secs if necessary  [18:06:25][Step 8/8] I0215 17:06:25.305379  1760 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-239.mesosphere.io' server FQDN: 'ip-172-30-2-239.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true   [18:06:25][Step 8/8] I0215 17:06:25.305397  1760 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true  [18:06:25][Step 8/...",2
"Upgrade vendored Protobuf to 2.6.1
We currently vendor Protobuf 2.5.0. We should upgrade to Protobuf 2.6.1. This introduces various bugfixes, performance improvements, and at least one new feature we might want to eventually take advantage of ({{map}} data type). AFAIK there should be no backward compatibility concerns.",3
"Document docker runtime isolator.
Should include the following information:    *What features are currently supported in docker runtime isolator.  *How to use the docker runtime isolator (user manual).  *Compare the different semantics v.s. docker containerizer, and explain why.",2
"Create base docker image for test suite.
This should be widely used for unified containerizer testing. Should basically include:    *at least one layer.  *repositories.    For each layer:  *root file system as a layer tar ball.  *docker image json (manifest).  *docker version.",3
"Implement master failover tests for the scheduler library.
Currently, the scheduler library creates its own {{MasterDetector}} object internally. We would need to create a standalone detector and create new tests for testing that callbacks are invoked correctly in the event of a master failover.",3
"Implement reliable floating point for scalar resources
Design doc: https://docs.google.com/document/d/14qLxjZsfIpfynbx0USLJR0GELSq8hdZJUWw6kaY_DXc/edit?usp=sharing",5
"Design doc for v1 Operator API
We need to design how the v1 operator API (all the HTTP endpoints exposed by master/agent that are not for scheduler/executor interactions) looks and works.",8
"Reorganize 3rdparty directory
This issues is currently being discussed in the dev mailing list:  http://www.mail-archive.com/dev@mesos.apache.org/msg34349.html",5
"Add a HierarchicalAllocator benchmark with reservation labels.
With {{Labels}} being part of the {{ReservationInfo}}, we should ensure that we don't observe a significant performance degradation in the allocator.",3
"SlaveTest.StateEndpoint is flaky
{code}  [ RUN      ] SlaveTest.StateEndpoint  ../../src/tests/slave_tests.cpp:1220: Failure  Value of: state.values[""start_time""].as<JSON::Number>().as<int>()    Actual: 1458159086  Expected: static_cast<int>(Clock::now().secs())    Which is: 1458159085  [  FAILED  ] SlaveTest.StateEndpoint (193 ms)  {code}    Even though this test does {{Clock::pause()}} before starting the agent, there's a possibility that a numified-stringified double to not equal itself, even after rounding to the nearest int.",1
"Allow Reserve operations by a principal without `ReservationInfo.principal`
Currently, we require a framework or operator to specify `ReservationInfo.principal` when they reserve resources. This isn't necessary, however; we already know the principal and can fill in the field if it isn't set already.",2
"Document default value of ""offer_timeout""
There isn't a default value (i.e., offers do not timeout by default), but we should clarify this in {{flags.cpp}} and {{configuration.md}}.",1
"Make Stout configuration modular and consumable by downstream (e.g., libprocess and agent)
Stout configuration is replicated in at least 3 configuration files -- stout itself, libprocess, and agent. More will follow in the future.    We should make a StoutConfigure.cmake that can be included by any package downstream.",1
"Enable zlib on Windows.
nan",1
"Remove 'force' field from the Subscribe Call in v1 Scheduler API
We/I introduced the `force` field in SUBSCRIBE call to deal with scheduler partition cases. Having thought a bit more and discussing with few other folks ([~anandmazumdar], [~greggomann]), I think we can get away from not having that field in the v1 API. The obvious advantage of removing the field is that framework devs don't have to think about how/when to set the field (the current semantics are a bit confusing).    The new workflow when a master receives a SUBSCRIBE call is that master always accepts this call and closes any existing connection (after sending ERROR event) from the same scheduler (identified by framework id).      The expectation from schedulers is that they must close the old subscribe connection before resending a new SUBSCRIBE call.    Lets look at some tricky scenarios and see how this works and why it is safe.    1) Connection disconnection @ the scheduler but not @ the master       Scheduler sees the disconnection and sends a new SUBSCRIBE call. Master sends ERROR on the old connection (won't be received by the scheduler because the connection is already closed) and closes it.    2) Connection disconnection @ master but not @ scheduler    Scheduler realizes this from lack of HEARTBEAT events. It then closes its existing connection and sends a new SUBSCRIBE call. Master accepts the new SUBSCRIBE call. There is no old connection to close on the master as it is already closed.    3) Scheduler failover but no disconnection @ master    Newly elected scheduler sends a SUBSCRIBE call. Master sends ERROR event and closes the old connection (won't be received because the old scheduler failed over).    4) If Scheduler A got partitioned (but is alive and connected with master) and Scheduler B got elected as new leader.    When Scheduler B sends SUBSCRIBE, master sends ERROR and closes the connection from Scheduler A. Master accepts Scheduler B's connection. Typically Scheduler A aborts after receiving ERROR and gets restarted. After restart it won't become the leader because Scheduler B is already elected.    5) Scheduler sends SUBSCRIBE, times out, closes the SUBSCRIBE connection (A) and sends a new SUBSCRIBE (B). Master receives SUBSCRIBE (B) and then receives SUBSCRIBE (A) but doesn't see A's disconnection yet.    Master first accepts SUBSCRIBE (B). After it receives SUBSCRIBE (A), it sends ERROR to SUBSCRIBE (B) and closes that connection. When it accepts SUBSCRIBE (A) and tries to send SUBSCRIBED event the connection closure is detected. Scheduler retries the SUBSCRIBE connection after a backoff. I think this is a rare enough race for it to happen continuously in a loop.    ",5
"ReviewBot should not fail hard if there are circular dependencies in a review chain
Instead of failing hard, ReviewBot should post an error to the review that a circular dependency is detected.",2
"""make DESTDIR=<path> install"" broken
There is a missing '$(DESTDIR)' prefix in the install-data-hook that causes DESTDIR builds to be broken.",2
"Add allocator metric for number of completed allocation runs
nan",1
"Add allocator metric for number of offers each framework received
A counter for the number of allocations to a framework can be used to monitor allocation progress, e.g., when agents are added to a cluster, and as other frameworks are added or removed.    Currently, an offer by the hierarchical allocator to a framework consists of a list of resources on possibly many agents. Resources might be offered in order to satisfy outstanding quota or for fairness. To capture allocations on fine granularity we should not count the number of offers, but instead the pieces making up that offer, as such a metric would better resolve the effect of changes (e.g., adding/removing a framework).  ",2
"Add allocator metrics for total vs offered/allocated resources.
Exposing the current allocation breakdown as seen by the allocator will allow us to correlated the corresponding metrics in the master with what the allocator sees. We should expose at least allocated or available, and total.",2
"Expose allocation algorithm latency via a metric.
The allocation algorithm has grown to become fairly expensive, gaining visibility into its latency enables monitoring and alerting.    Similar allocator timing-related information is already exposed in the log, but should also be exposed via an endpoint.",1
"Add allocator metric for number of active offer filters
To diagnose scenarios where frameworks unexpectedly do not receive offers information on currently active filters are needed.",1
"Add allocator metric for currently satisfied quotas
We currently expose information on set quotas via dedicated quota endpoints. To diagnose allocator problems one additionally needs information about used quotas.",2
"Add allocator metric for currrent dominant shares of frameworks and roles
nan",5
"Document scheduler driver calls in framework development guide.
The interface examples are slightly out of sync with scheduler.hpp, most notably missing the new acceptOffers call.",2
"Update /frameworks to use jsonify
This should let us remove the duplicated code in {{http.cpp}} between {{model(Framework)}} and {{json(Full<Framework>)}}.",3
"DockerContainerizerTest.ROOT_DOCKER_LaunchWithPersistentVolumes fails on CentOS 6
This test passes consistently on other OS's, but fails consistently on CentOS 6.    Verbose logs from test failure:  {code}  [ RUN      ] DockerContainerizerTest.ROOT_DOCKER_LaunchWithPersistentVolumes  I0222 18:16:12.327957 26681 leveldb.cpp:174] Opened db in 7.466102ms  I0222 18:16:12.330528 26681 leveldb.cpp:181] Compacted db in 2.540139ms  I0222 18:16:12.330580 26681 leveldb.cpp:196] Created db iterator in 16908ns  I0222 18:16:12.330592 26681 leveldb.cpp:202] Seeked to beginning of db in 1403ns  I0222 18:16:12.330600 26681 leveldb.cpp:271] Iterated through 0 keys in the db in 315ns  I0222 18:16:12.330634 26681 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned  I0222 18:16:12.331082 26698 recover.cpp:447] Starting replica recovery  I0222 18:16:12.331289 26698 recover.cpp:473] Replica is in EMPTY status  I0222 18:16:12.332162 26703 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (13761)@172.30.2.148:35274  I0222 18:16:12.332701 26701 recover.cpp:193] Received a recover response from a replica in EMPTY status  I0222 18:16:12.333230 26699 recover.cpp:564] Updating replica status to STARTING  I0222 18:16:12.334102 26698 master.cpp:376] Master 652149b4-3932-4d8b-ba6f-8c9d9045be70 (ip-172-30-2-148.mesosphere.io) started on 172.30.2.148:35274  I0222 18:16:12.334116 26698 master.cpp:378] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_http=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/QEhLBS/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/tmp/QEhLBS/master"" --zk_session_timeout=""10secs""  I0222 18:16:12.334354 26698 master.cpp:423] Master only allowing authenticated frameworks to register  I0222 18:16:12.334363 26698 master.cpp:428] Master only allowing authenticated slaves to register  I0222 18:16:12.334369 26698 credentials.hpp:35] Loading credentials for authentication from '/tmp/QEhLBS/credentials'  I0222 18:16:12.335366 26698 master.cpp:468] Using default 'crammd5' authenticator  I0222 18:16:12.335492 26698 master.cpp:537] Using default 'basic' HTTP authenticator  I0222 18:16:12.335623 26698 master.cpp:571] Authorization enabled  I0222 18:16:12.335752 26703 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 2.314693ms  I0222 18:16:12.335769 26700 whitelist_watcher.cpp:77] No whitelist given  I0222 18:16:12.335778 26703 replica.cpp:320] Persisted replica status to STARTING  I0222 18:16:12.335821 26697 hierarchical.cpp:144] Initialized hierarchical allocator process  I0222 18:16:12.335965 26701 recover.cpp:473] Replica is in STARTING status  I0222 18:16:12.336771 26703 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (13763)@172.30.2.148:35274  I0222 18:16:12.337191 26696 recover.cpp:193] Received a recover response from a replica in STARTING status  I0222 18:16:12.337635 26700 recover.cpp:564] Updating replica status to VOTING  I0222 18:16:12.337671 26703 master.cpp:1712] The newly elected leader is master@172.30.2.148:35274 with id 652149b4-3932-4d8b-ba6f-8c9d9045be70  I0222 18:16:12.337698 26703 master.cpp:1725] Elected as the leading master!  I0222 18:16:12.337713 26703 master.cpp:1470] Recovering from registrar  I0222 18:16:12.337828 26696 registrar.cpp:307] Recovering registrar  I0222 18:16:12.339972 26702 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 2.06039ms  I0222 18:16:12.339994 26702 replica.cpp:320] Persisted replica status to VOTING  I0222 18:16:12.340082 26700 recover.cpp:578] Successfully joined the Paxos group  I0222 18:16:12.340267 26700 recover.cpp:462] Recover process terminated  I0222 18:16:12.340591 26699 log.cpp:659] Attempting to start the writer  I0222 18:16:12.341594 26698 replica.cpp:493] Replica received implicit promise request from (13764)@172.30.2.148:35274 with proposal 1  I0222 18:16:12.343598 26698 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.97941ms  I0222 18:16:12.343619 26698 replica.cpp:342] Persisted promised to 1  I0222 18:16:12.344182 26698 coordinator.cpp:238] Coordinator attempting to fill missing positions  I0222 18:16:12.345285 26702 replica.cpp:388] Replica received explicit promise request from (13765)@172.30.2.148:35274 for position 0 with proposal 2  I0222 18:16:12.347275 26702 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 1.960198ms  I0222 18:16:12.347296 26702 replica.cpp:712] Persisted action at 0  I0222 18:16:12.348201 26703 replica.cpp:537] Replica received write request for position 0 from (13766)@172.30.2.148:35274  I0222 18:16:12.348247 26703 leveldb.cpp:436] Reading position from leveldb took 21399ns  I0222 18:16:12.350667 26703 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 2.39166ms  I0222 18:16:12.350690 26703 replica.cpp:712] Persisted action at 0  I0222 18:16:12.351191 26696 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0  I0222 18:16:12.353152 26696 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.935798ms  I0222 18:16:12.353173 26696 replica.cpp:712] Persisted action at 0  I0222 18:16:12.353188 26696 replica.cpp:697] Replica learned NOP action at position 0  I0222 18:16:12.353639 26696 log.cpp:675] Writer started with ending position 0  I0222 18:16:12.354508 26697 leveldb.cpp:436] Reading position from leveldb took 25625ns  I0222 18:16:12.355274 26696 registrar.cpp:340] Successfully fetched the registry (0B) in 17.406976ms  I0222 18:16:12.355357 26696 registrar.cpp:439] Applied 1 operations in 20977ns; attempting to update the 'registry'  I0222 18:16:12.355929 26697 log.cpp:683] Attempting to append 210 bytes to the log  I0222 18:16:12.356032 26703 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1  I0222 18:16:12.356657 26698 replica.cpp:537] Replica received write request for position 1 from (13767)@172.30.2.148:35274  I0222 18:16:12.358566 26698 leveldb.cpp:341] Persisting action (229 bytes) to leveldb took 1.881945ms  I0222 18:16:12.358588 26698 replica.cpp:712] Persisted action at 1  I0222 18:16:12.359081 26697 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0  I0222 18:16:12.361002 26697 leveldb.cpp:341] Persisting action (231 bytes) to leveldb took 1.894331ms  I0222 18:16:12.361023 26697 replica.cpp:712] Persisted action at 1  I0222 18:16:12.361038 26697 replica.cpp:697] Replica learned APPEND action at position 1  I0222 18:16:12.361883 26697 registrar.cpp:484] Successfully updated the 'registry' in 6.482944ms  I0222 18:16:12.361981 26697 registrar.cpp:370] Successfully recovered registrar  I0222 18:16:12.362052 26701 log.cpp:702] Attempting to truncate the log to 1  I0222 18:16:12.362167 26703 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2  I0222 18:16:12.362421 26696 master.cpp:1522] Recovered 0 slaves from the Registry (171B) ; allowing 10mins for slaves to re-register  I0222 18:16:12.362447 26698 hierarchical.cpp:171] Skipping recovery of hierarchical allocator: nothing to recover  I0222 18:16:12.362911 26701 replica.cpp:537] Replica received write request for position 2 from (13768)@172.30.2.148:35274  I0222 18:16:12.364760 26701 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.819954ms  I0222 18:16:12.364783 26701 replica.cpp:712] Persisted action at 2  I0222 18:16:12.365384 26697 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0  I0222 18:16:12.367961 26697 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 2.55143ms  I0222 18:16:12.368015 26697 leveldb.cpp:399] Deleting ~1 keys from leveldb took 28196ns  I0222 18:16:12.368028 26697 replica.cpp:712] Persisted action at 2  I0222 18:16:12.368044 26697 replica.cpp:697] Replica learned TRUNCATE action at position 2  I0222 18:16:12.376824 26703 slave.cpp:193] Slave started on 396)@172.30.2.148:35274  I0222 18:16:12.376838 26703 slave.cpp:194] Flags at startup: --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""https://auth.docker.io"" --docker_kill_orphans=""true"" --docker_puller_timeout=""60"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mnt/teamcity/work/4240ba9ddd0997c3/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpu:2;mem:2048;disk(role1):2048"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1""  I0222 18:16:12.377109 26703 credentials.hpp:83] Loading credential for authentication from '/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/credential'  I0222 18:16:12.377300 26703 slave.cpp:324] Slave using credential for: test-principal  I0222 18:16:12.377439 26703 resources.cpp:576] Parsing resources as JSON failed: cpu:2;mem:2048;disk(role1):2048  Trying semicolon-delimited string format instead  I0222 18:16:12.377804 26703 slave.cpp:464] Slave resources: cpu(*):2; mem(*):2048; disk(role1):2048; cpus(*):8; ports(*):[31000-32000]  I0222 18:16:12.377881 26703 slave.cpp:472] Slave attributes: [  ]  I0222 18:16:12.377889 26703 slave.cpp:477] Slave hostname: ip-172-30-2-148.mesosphere.io  I0222 18:16:12.378779 26701 state.cpp:58] Recovering state from '/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/meta'  I0222 18:16:12.379092 26697 status_update_manager.cpp:200] Recovering status update manager  I0222 18:16:12.379156 26681 sched.cpp:222] Version: 0.28.0  I0222 18:16:12.379250 26697 docker.cpp:722] Recovering Docker containers  I0222 18:16:12.379421 26703 slave.cpp:4565] Finished recovery  I0222 18:16:12.379627 26700 sched.cpp:326] New master detected at master@172.30.2.148:35274  I0222 18:16:12.379735 26703 slave.cpp:4737] Querying resource estimator for oversubscribable resources  I0222 18:16:12.379765 26700 sched.cpp:382] Authenticating with master master@172.30.2.148:35274  I0222 18:16:12.379781 26700 sched.cpp:389] Using default CRAM-MD5 authenticatee  I0222 18:16:12.379964 26696 status_update_manager.cpp:174] Pausing sending status updates  I0222 18:16:12.379992 26702 authenticatee.cpp:121] Creating new client SASL connection  I0222 18:16:12.380030 26697 slave.cpp:796] New master detected at master@172.30.2.148:35274  I0222 18:16:12.380106 26697 slave.cpp:859] Authenticating with master master@172.30.2.148:35274  I0222 18:16:12.380127 26697 slave.cpp:864] Using default CRAM-MD5 authenticatee  I0222 18:16:12.380188 26699 master.cpp:5526] Authenticating scheduler-1850b1cd-3396-4479-b2f3-47ee6c3fa270@172.30.2.148:35274  I0222 18:16:12.380269 26700 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(832)@172.30.2.148:35274  I0222 18:16:12.380280 26698 authenticatee.cpp:121] Creating new client SASL connection  I0222 18:16:12.380307 26697 slave.cpp:832] Detecting new master  I0222 18:16:12.380450 26697 slave.cpp:4751] Received oversubscribable resources  from the resource estimator  I0222 18:16:12.380452 26699 master.cpp:5526] Authenticating slave(396)@172.30.2.148:35274  I0222 18:16:12.380506 26698 authenticator.cpp:98] Creating new server SASL connection  I0222 18:16:12.380540 26697 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(833)@172.30.2.148:35274  I0222 18:16:12.380635 26700 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5  I0222 18:16:12.380659 26700 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'  I0222 18:16:12.380762 26700 authenticator.cpp:203] Received SASL authentication start  I0222 18:16:12.380765 26701 authenticator.cpp:98] Creating new server SASL connection  I0222 18:16:12.380843 26700 authenticator.cpp:325] Authentication requires more steps  I0222 18:16:12.380911 26698 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5  I0222 18:16:12.380931 26702 authenticatee.cpp:258] Received SASL authentication step  I0222 18:16:12.380936 26698 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'  I0222 18:16:12.381036 26702 authenticator.cpp:231] Received SASL authentication step  I0222 18:16:12.381052 26698 authenticator.cpp:203] Received SASL authentication start  I0222 18:16:12.381062 26702 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-148' server FQDN: 'ip-172-30-2-148' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false   I0222 18:16:12.381072 26702 auxprop.cpp:179] Looking up auxiliary property '*userPassword'  I0222 18:16:12.381104 26702 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'  I0222 18:16:12.381104 26698 authenticator.cpp:325] Authentication requires more steps  I0222 18:16:12.381134 26702 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-148' server FQDN: 'ip-172-30-2-148' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true   I0222 18:16:12.381142 26702 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true  I0222 18:16:12.381147 26702 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true  I0222 18:16:12.381162 26702 authenticator.cpp:317] Authentication success  I0222 18:16:12.381184 26698 authenticatee.cpp:258] Received SASL authentication step  I0222 18:16:12.381247 26699 authenticatee.cpp:298] Authentication success  I0222 18:16:12.381283 26696 authenticator.cpp:231] Received SASL authentication step  I0222 18:16:12.381311 26696 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-148' server FQDN: 'ip-172-30-2-148' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false   I0222 18:16:12.381325 26696 auxprop.cpp:179] Looking up auxiliary property '*userPassword'  I0222 18:16:12.381319 26701 master.cpp:5556] Successfully authenticated principal 'test-principal' at scheduler-1850b1cd-3396-4479-b2f3-47ee6c3fa270@172.30.2.148:35274  I0222 18:16:12.381345 26700 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(832)@172.30.2.148:35274  I0222 18:16:12.381361 26696 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'  I0222 18:16:12.381397 26696 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-148' server FQDN: 'ip-172-30-2-148' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true   I0222 18:16:12.381413 26696 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true  I0222 18:16:12.381422 26696 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true  I0222 18:16:12.381441 26696 authenticator.cpp:317] Authentication success  I0222 18:16:12.381548 26698 sched.cpp:471] Successfully authenticated with master master@172.30.2.148:35274  I0222 18:16:12.381563 26698 sched.cpp:776] Sending SUBSCRIBE call to master@172.30.2.148:35274  I0222 18:16:12.381634 26700 authenticatee.cpp:298] Authentication success  I0222 18:16:12.381660 26698 sched.cpp:809] Will retry registration in 770.60771ms if necessary  I0222 18:16:12.381675 26697 master.cpp:5556] Successfully authenticated principal 'test-principal' at slave(396)@172.30.2.148:35274  I0222 18:16:12.381734 26702 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(833)@172.30.2.148:35274  I0222 18:16:12.381811 26697 master.cpp:2280] Received SUBSCRIBE call for framework 'default' at scheduler-1850b1cd-3396-4479-b2f3-47ee6c3fa270@172.30.2.148:35274  I0222 18:16:12.381882 26697 master.cpp:1751] Authorizing framework principal 'test-principal' to receive offers for role 'role1'  I0222 18:16:12.382004 26698 slave.cpp:927] Successfully authenticated with master master@172.30.2.148:35274  I0222 18:16:12.382123 26698 slave.cpp:1321] Will retry registration in 8.1941ms if necessary  I0222 18:16:12.382282 26701 master.cpp:4240] Registering slave at slave(396)@172.30.2.148:35274 (ip-172-30-2-148.mesosphere.io) with id 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0  I0222 18:16:12.382482 26701 master.cpp:2351] Subscribing framework default with checkpointing disabled and capabilities [  ]  I0222 18:16:12.382612 26703 registrar.cpp:439] Applied 1 operations in 46327ns; attempting to update the 'registry'  I0222 18:16:12.382829 26699 hierarchical.cpp:265] Added framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000  I0222 18:16:12.382910 26699 hierarchical.cpp:1434] No resources available to allocate!  I0222 18:16:12.382915 26701 sched.cpp:703] Framework registered with 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000  I0222 18:16:12.382936 26699 hierarchical.cpp:1529] No inverse offers to send out!  I0222 18:16:12.382953 26699 hierarchical.cpp:1127] Performed allocation for 0 slaves in 89949ns  I0222 18:16:12.382982 26701 sched.cpp:717] Scheduler::registered took 46498ns  I0222 18:16:12.383536 26698 log.cpp:683] Attempting to append 423 bytes to the log  I0222 18:16:12.383628 26699 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3  I0222 18:16:12.384196 26700 replica.cpp:537] Replica received write request for position 3 from (13775)@172.30.2.148:35274  I0222 18:16:12.386602 26700 leveldb.cpp:341] Persisting action (442 bytes) to leveldb took 2.377119ms  I0222 18:16:12.386625 26700 replica.cpp:712] Persisted action at 3  I0222 18:16:12.387104 26698 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0  I0222 18:16:12.389159 26698 leveldb.cpp:341] Persisting action (444 bytes) to leveldb took 2.032301ms  I0222 18:16:12.389181 26698 replica.cpp:712] Persisted action at 3  I0222 18:16:12.389196 26698 replica.cpp:697] Replica learned APPEND action at position 3  I0222 18:16:12.390281 26698 registrar.cpp:484] Suc...",2
"CMake: Add leveldb library to 3rdparty external builds.
nan",3
"ContainerLoggerTest.MesosContainerizerRecover cannot be executed in isolation
Some cleanup of spawned processes is missing in {{ContainerLoggerTest.MesosContainerizerRecover}} so that when the test is run in isolation the global teardown might find lingering processes.  {code}  [==========] Running 1 test from 1 test case.  [----------] Global test environment set-up.  [----------] 1 test from ContainerLoggerTest  [ RUN      ] ContainerLoggerTest.MesosContainerizerRecover  [       OK ] ContainerLoggerTest.MesosContainerizerRecover (13 ms)  [----------] 1 test from ContainerLoggerTest (13 ms total)    [----------] Global test environment tear-down  ../../src/tests/environment.cpp:728: Failure  Failed  Tests completed with child processes remaining:  -+- 7112 /SOME/PATH/src/mesos/build/src/.libs/mesos-tests --gtest_filter=ContainerLoggerTest.MesosContainerizerRecover   \--- 7130 (sh)  [==========] 1 test from 1 test case ran. (23 ms total)  [  PASSED  ] 1 test.  [  FAILED  ] 0 tests, listed below:     0 FAILED TESTS  {code}    Observered on OS X with clang-trunk and an unoptimized build.  ",1
"Add Appc image fetcher tests.
Mesos now has support for fetching Appc images. Add tests that verifies the new component.",3
"Move HTB out of containers
Currently we set a fixed HTB bandwidth in each of the container, which makes it impossible to share the link if idle. As the first step, we should move it out of the containers, into the qdisc hierarchy of the physical interface.",3
"Document: Mesos Executor expects all SSL_* environment variables to be set
I was trying to run Docker containers in a fully SSL-ized Mesos cluster but ran into problems because the executor was failing with a ""Failed to shutdown socket with fd 10: Transport endpoint is not connected"".    My understanding of why this is happening is because the executor was trying to report its status to Mesos slave over HTTPS, but doesnt have the appropriate certs/env setup inside the executor.    (Thanks to mslackbot/joseph for helping me figure this out on #mesos)    It turns out, the executor expects all SSL_* variables to be set inside `CommandInfo.environment` which gets picked up by the executor to successfully reports its status to the slave.    This part of __executor needing all the SSL_* variables to be set in its environment__ is missing in the Mesos SSL transitioning guide. I request you to please add this vital information to the doc.",2
"The ""executors"" field is exposed under a backwards incompatible schema.
In 0.26.0, the master's {{/state}} endpoint generated the following:    {code}  {    /* ... */    ""frameworks"": [      {        /* ... */        ""executors"": [          {            ""command"": {              ""argv"": [],              ""uris"": [],              ""value"": ""/Users/mpark/Projects/mesos/build/opt/src/long-lived-executor""            },            ""executor_id"": ""default"",            ""framework_id"": ""0ea528a9-64ba-417f-98ea-9c4b8d418db6-0000"",            ""name"": ""Long Lived Executor (C++)"",            ""resources"": {              ""cpus"": 0,              ""disk"": 0,              ""mem"": 0            },            ""slave_id"": ""8a513678-03a1-4cb5-9279-c3c0c591f1d8-S0""          }        ],        /* ... */      }    ]    /* ... */  }  {code}    In 0.27.1, the {{ExecutorInfo}} is mistakenly exposed in the raw protobuf schema:    {code}  {    /* ... */    ""frameworks"": [      {        /* ... */        ""executors"": [          {            ""command"": {              ""shell"": true,              ""value"": ""/Users/mpark/Projects/mesos/build/opt/src/long-lived-executor""            },            ""executor_id"": {              ""value"": ""default""            },            ""framework_id"": {              ""value"": ""368a5a49-480b-41f6-a13b-24a69c92a72e-0000""            },            ""name"": ""Long Lived Executor (C++)"",            ""slave_id"": ""8a513678-03a1-4cb5-9279-c3c0c591f1d8-S0"",            ""source"": ""cpp_long_lived_framework""          }        ],        /* ... */      }    ]    /* ... */  }  {code}    This is a backwards incompatible API change.",2
"Mesos containerizer should get uid/gids before pivot_root.
Currently, we call os::su(user) after pivot_root. This is problematic because /etc/passwd and /etc/group might be missing in container's root filesystem. We should instead, get the uid/gids before pivot_root, and call setuid/setgroups after pivot_root.",3
"Add a 'name' field into NetworkInfo.
This allows the framework writer to specify the name of the network they want their container to join.    Why not using 'groups'? That's because there might be multiple groups under a single network (e.g., admin vs. user, public vs. private, etc.).",1
"Add network/cni isolator for Mesos containerizer.
See the design doc for more context (MESOS-4742).    The isolator will interact with CNI plugins to create the network for the container to join.",8
"Expose metrics and gauges for fetcher cache usage and hit rate
To evaluate the fetcher cache and calibrate the value of the fetcher_cache_size flag, it would be useful to have metrics and gauges on agents that expose operational statistics like cache hit rate, occupied cache size, and time spent downloading resources that were not present.",2
"Add agent flags to allow operators to specify CNI plugin and config directories.
According to design doc, we plan to add the following flags:    “--network_cni_plugins_dir”  Location of the CNI plugin binaries. The “network/cni” isolator will find CNI plugins under this directory so that it can execute the plugins to add/delete container from the CNI networks. It is the operator’s responsibility to install the CNI plugin binaries in the specified directory.    “--network_cni_config_dir”  Location of the CNI network configuration files. For each network that containers launched in Mesos agent can connect to, the operator should install a network configuration file in JSON format in the specified directory.",2
"Setup proper DNS resolver for containers in network/cni isolator.
Please get more context from the design doc (MESOS-4742).    The CNI plugin will return the DNS information about the network. The network/cni isolator needs to properly setup /etc/resolv.conf for the container. We should consider the following cases:  1) container is using host filesystem  2) container is using a different filesystem  3) custom executor and command executor",5
"Add test mock for CNI plugins.
In order to test the network/cni isolator, we need to mock the behavior of an CNI plugin. One option is to write a mock script which acts as a CNI plugin. The isolator will talk to the mock script the same way it talks to an actual CNI plugin.    The mock script can just join the host network?",5
"The network/cni isolator should report assigned IP address. 
In order for service discovery to work in some cases, the network/cni isolator needs to report the assigned IP address through the isolator->status() interface.",3
"MasterMaintenanceTest.InverseOffers is flaky
[MESOS-4169] significantly sped up this test, but also surfaced some more flakiness.  This can be fixed in the same way as [MESOS-4059].    Verbose logs from ASF Centos7 build:  {code}  [ RUN      ] MasterMaintenanceTest.InverseOffers  I0224 22:35:53.714018  1948 leveldb.cpp:174] Opened db in 2.034387ms  I0224 22:35:53.714663  1948 leveldb.cpp:181] Compacted db in 608839ns  I0224 22:35:53.714709  1948 leveldb.cpp:196] Created db iterator in 19043ns  I0224 22:35:53.714844  1948 leveldb.cpp:202] Seeked to beginning of db in 2330ns  I0224 22:35:53.714956  1948 leveldb.cpp:271] Iterated through 0 keys in the db in 518ns  I0224 22:35:53.715092  1948 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned  I0224 22:35:53.715646  1968 recover.cpp:447] Starting replica recovery  I0224 22:35:53.715915  1981 recover.cpp:473] Replica is in EMPTY status  I0224 22:35:53.717067  1972 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (4533)@172.17.0.1:36678  I0224 22:35:53.717445  1981 recover.cpp:193] Received a recover response from a replica in EMPTY status  I0224 22:35:53.717888  1978 recover.cpp:564] Updating replica status to STARTING  I0224 22:35:53.718585  1979 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 525061ns  I0224 22:35:53.718618  1979 replica.cpp:320] Persisted replica status to STARTING  I0224 22:35:53.718827  1982 recover.cpp:473] Replica is in STARTING status  I0224 22:35:53.719728  1969 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (4534)@172.17.0.1:36678  I0224 22:35:53.719974  1971 recover.cpp:193] Received a recover response from a replica in STARTING status  I0224 22:35:53.720369  1970 recover.cpp:564] Updating replica status to VOTING  I0224 22:35:53.720789  1982 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 322308ns  I0224 22:35:53.720823  1982 replica.cpp:320] Persisted replica status to VOTING  I0224 22:35:53.720968  1982 recover.cpp:578] Successfully joined the Paxos group  I0224 22:35:53.721101  1982 recover.cpp:462] Recover process terminated  I0224 22:35:53.721698  1982 master.cpp:376] Master aab18b61-7811-4c43-a672-d1a63818c880 (4db5fa128d2d) started on 172.17.0.1:36678  I0224 22:35:53.721719  1982 master.cpp:378] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""false"" --authenticate_http=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/MjbcWP/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.28.0/_inst/share/mesos/webui"" --work_dir=""/tmp/MjbcWP/master"" --zk_session_timeout=""10secs""  I0224 22:35:53.722039  1982 master.cpp:425] Master allowing unauthenticated frameworks to register  I0224 22:35:53.722053  1982 master.cpp:428] Master only allowing authenticated slaves to register  I0224 22:35:53.722061  1982 credentials.hpp:35] Loading credentials for authentication from '/tmp/MjbcWP/credentials'  I0224 22:35:53.722394  1982 master.cpp:468] Using default 'crammd5' authenticator  I0224 22:35:53.722525  1982 master.cpp:537] Using default 'basic' HTTP authenticator  I0224 22:35:53.722661  1982 master.cpp:571] Authorization enabled  I0224 22:35:53.722813  1968 hierarchical.cpp:144] Initialized hierarchical allocator process  I0224 22:35:53.722846  1980 whitelist_watcher.cpp:77] No whitelist given  I0224 22:35:53.724957  1977 master.cpp:1712] The newly elected leader is master@172.17.0.1:36678 with id aab18b61-7811-4c43-a672-d1a63818c880  I0224 22:35:53.725000  1977 master.cpp:1725] Elected as the leading master!  I0224 22:35:53.725023  1977 master.cpp:1470] Recovering from registrar  I0224 22:35:53.725306  1967 registrar.cpp:307] Recovering registrar  I0224 22:35:53.725808  1977 log.cpp:659] Attempting to start the writer  I0224 22:35:53.727145  1973 replica.cpp:493] Replica received implicit promise request from (4536)@172.17.0.1:36678 with proposal 1  I0224 22:35:53.727728  1973 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 424560ns  I0224 22:35:53.727828  1973 replica.cpp:342] Persisted promised to 1  I0224 22:35:53.729080  1973 coordinator.cpp:238] Coordinator attempting to fill missing positions  I0224 22:35:53.731009  1979 replica.cpp:388] Replica received explicit promise request from (4537)@172.17.0.1:36678 for position 0 with proposal 2  I0224 22:35:53.731580  1979 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 478479ns  I0224 22:35:53.731613  1979 replica.cpp:712] Persisted action at 0  I0224 22:35:53.734354  1979 replica.cpp:537] Replica received write request for position 0 from (4538)@172.17.0.1:36678  I0224 22:35:53.734485  1979 leveldb.cpp:436] Reading position from leveldb took 60879ns  I0224 22:35:53.735877  1979 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 1.324061ms  I0224 22:35:53.735930  1979 replica.cpp:712] Persisted action at 0  I0224 22:35:53.737061  1970 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0  I0224 22:35:53.738881  1970 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.772814ms  I0224 22:35:53.738939  1970 replica.cpp:712] Persisted action at 0  I0224 22:35:53.738975  1970 replica.cpp:697] Replica learned NOP action at position 0  I0224 22:35:53.740136  1976 log.cpp:675] Writer started with ending position 0  I0224 22:35:53.741750  1976 leveldb.cpp:436] Reading position from leveldb took 74863ns  I0224 22:35:53.743479  1976 registrar.cpp:340] Successfully fetched the registry (0B) in 18.11968ms  I0224 22:35:53.743755  1976 registrar.cpp:439] Applied 1 operations in 56670ns; attempting to update the 'registry'  I0224 22:35:53.745604  1978 log.cpp:683] Attempting to append 170 bytes to the log  I0224 22:35:53.745905  1977 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1  I0224 22:35:53.746968  1981 replica.cpp:537] Replica received write request for position 1 from (4539)@172.17.0.1:36678  I0224 22:35:53.747480  1981 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 456947ns  I0224 22:35:53.747609  1981 replica.cpp:712] Persisted action at 1  I0224 22:35:53.750448  1981 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0  I0224 22:35:53.751158  1981 leveldb.cpp:341] Persisting action (191 bytes) to leveldb took 535163ns  I0224 22:35:53.751258  1981 replica.cpp:712] Persisted action at 1  I0224 22:35:53.751389  1981 replica.cpp:697] Replica learned APPEND action at position 1  I0224 22:35:53.753149  1979 registrar.cpp:484] Successfully updated the 'registry' in 9.228032ms  I0224 22:35:53.753324  1979 registrar.cpp:370] Successfully recovered registrar  I0224 22:35:53.753593  1979 log.cpp:702] Attempting to truncate the log to 1  I0224 22:35:53.753805  1979 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2  I0224 22:35:53.754055  1981 master.cpp:1522] Recovered 0 slaves from the Registry (131B) ; allowing 10mins for slaves to re-register  I0224 22:35:53.754349  1979 hierarchical.cpp:171] Skipping recovery of hierarchical allocator: nothing to recover  I0224 22:35:53.755764  1977 replica.cpp:537] Replica received write request for position 2 from (4540)@172.17.0.1:36678  I0224 22:35:53.756459  1977 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 488559ns  I0224 22:35:53.756561  1977 replica.cpp:712] Persisted action at 2  I0224 22:35:53.757932  1972 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0  I0224 22:35:53.758400  1972 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 343827ns  I0224 22:35:53.758539  1972 leveldb.cpp:399] Deleting ~1 keys from leveldb took 34231ns  I0224 22:35:53.758658  1972 replica.cpp:712] Persisted action at 2  I0224 22:35:53.758782  1972 replica.cpp:697] Replica learned TRUNCATE action at position 2  I0224 22:35:53.778059  1978 slave.cpp:193] Slave started on 115)@172.17.0.1:36678  I0224 22:35:53.778105  1978 slave.cpp:194] Flags at startup: --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname=""maintenance-host"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.28.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF""  I0224 22:35:53.778609  1978 credentials.hpp:83] Loading credential for authentication from '/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/credential'  I0224 22:35:53.779175  1978 slave.cpp:324] Slave using credential for: test-principal  I0224 22:35:53.779520  1978 resources.cpp:576] Parsing resources as JSON failed: cpus:2;mem:1024;disk:1024;ports:[31000-32000]  Trying semicolon-delimited string format instead  I0224 22:35:53.780192  1978 slave.cpp:464] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]  I0224 22:35:53.780362  1978 slave.cpp:472] Slave attributes: [  ]  I0224 22:35:53.780483  1978 slave.cpp:477] Slave hostname: maintenance-host  I0224 22:35:53.782126  1967 state.cpp:58] Recovering state from '/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/meta'  I0224 22:35:53.782892  1969 status_update_manager.cpp:200] Recovering status update manager  I0224 22:35:53.783242  1969 slave.cpp:4565] Finished recovery  I0224 22:35:53.784001  1969 slave.cpp:4737] Querying resource estimator for oversubscribable resources  I0224 22:35:53.784678  1969 slave.cpp:796] New master detected at master@172.17.0.1:36678  I0224 22:35:53.784874  1967 status_update_manager.cpp:174] Pausing sending status updates  I0224 22:35:53.784808  1969 slave.cpp:859] Authenticating with master master@172.17.0.1:36678  I0224 22:35:53.784945  1969 slave.cpp:864] Using default CRAM-MD5 authenticatee  I0224 22:35:53.785181  1969 slave.cpp:832] Detecting new master  I0224 22:35:53.785326  1969 slave.cpp:4751] Received oversubscribable resources  from the resource estimator  I0224 22:35:53.785557  1969 authenticatee.cpp:121] Creating new client SASL connection  I0224 22:35:53.786227  1969 master.cpp:5526] Authenticating slave(115)@172.17.0.1:36678  I0224 22:35:53.786492  1969 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(298)@172.17.0.1:36678  I0224 22:35:53.786962  1969 authenticator.cpp:98] Creating new server SASL connection  I0224 22:35:53.787274  1969 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5  I0224 22:35:53.787308  1969 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'  I0224 22:35:53.787400  1969 authenticator.cpp:203] Received SASL authentication start  I0224 22:35:53.787470  1969 authenticator.cpp:325] Authentication requires more steps  I0224 22:35:53.787884  1972 authenticatee.cpp:258] Received SASL authentication step  I0224 22:35:53.787992  1972 authenticator.cpp:231] Received SASL authentication step  I0224 22:35:53.788027  1972 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '4db5fa128d2d' server FQDN: '4db5fa128d2d' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false   I0224 22:35:53.788040  1972 auxprop.cpp:179] Looking up auxiliary property '*userPassword'  I0224 22:35:53.788090  1972 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'  I0224 22:35:53.788122  1972 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '4db5fa128d2d' server FQDN: '4db5fa128d2d' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true   I0224 22:35:53.788136  1972 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true  I0224 22:35:53.788146  1972 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true  I0224 22:35:53.788164  1972 authenticator.cpp:317] Authentication success  I0224 22:35:53.788331  1972 authenticatee.cpp:298] Authentication success  I0224 22:35:53.788439  1972 master.cpp:5556] Successfully authenticated principal 'test-principal' at slave(115)@172.17.0.1:36678  I0224 22:35:53.788529  1972 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(298)@172.17.0.1:36678  I0224 22:35:53.788988  1972 slave.cpp:927] Successfully authenticated with master master@172.17.0.1:36678  I0224 22:35:53.789139  1972 slave.cpp:1321] Will retry registration in 1.535786ms if necessary  I0224 22:35:53.789515  1972 master.cpp:4240] Registering slave at slave(115)@172.17.0.1:36678 (maintenance-host) with id aab18b61-7811-4c43-a672-d1a63818c880-S0  I0224 22:35:53.790577  1972 registrar.cpp:439] Applied 1 operations in 78745ns; attempting to update the 'registry'  I0224 22:35:53.791128  1971 process.cpp:3141] Handling HTTP event for process 'master' with path: '/master/maintenance/schedule'  I0224 22:35:53.791877  1971 http.cpp:501] HTTP POST for /master/maintenance/schedule from 172.17.0.1:45095  I0224 22:35:53.793313  1972 log.cpp:683] Attempting to append 343 bytes to the log  I0224 22:35:53.793586  1972 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3  I0224 22:35:53.794533  1971 replica.cpp:537] Replica received write request for position 3 from (4547)@172.17.0.1:36678  I0224 22:35:53.794862  1971 leveldb.cpp:341] Persisting action (362 bytes) to leveldb took 283614ns  I0224 22:35:53.794893  1971 replica.cpp:712] Persisted action at 3  I0224 22:35:53.796646  1979 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0  I0224 22:35:53.797102  1972 slave.cpp:1321] Will retry registration in 17.198963ms if necessary  I0224 22:35:53.797186  1979 leveldb.cpp:341] Persisting action (364 bytes) to leveldb took 498502ns  I0224 22:35:53.797230  1979 replica.cpp:712] Persisted action at 3  I0224 22:35:53.797260  1979 replica.cpp:697] Replica learned APPEND action at position 3  I0224 22:35:53.797417  1972 master.cpp:4228] Ignoring register slave message from slave(115)@172.17.0.1:36678 (maintenance-host) as admission is already in progress  I0224 22:35:53.799119  1978 registrar.cpp:484] Successfully updated the 'registry' in 8.45824ms  I0224 22:35:53.799613  1978 registrar.cpp:439] Applied 1 operations in 176193ns; attempting to update the 'registry'  I0224 22:35:53.800472  1972 master.cpp:4308] Registered slave aab18b61-7811-4c43-a672-d1a63818c880-S0 at slave(115)@172.17.0.1:36678 (maintenance-host) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]  I0224 22:35:53.800623  1978 log.cpp:702] Attempting to truncate the log to 3  I0224 22:35:53.801255  1969 hierarchical.cpp:473] Added slave aab18b61-7811-4c43-a672-d1a63818c880-S0 (maintenance-host) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )  I0224 22:35:53.801301  1978 slave.cpp:971] Registered with master master@172.17.0.1:36678; given slave ID aab18b61-7811-4c43-a672-d1a63818c880-S0  I0224 22:35:53.801331  1978 fetcher.cpp:81] Clearing fetcher cache  I0224 22:35:53.801431  1969 hierarchical.cpp:1434] No resources available to allocate!  I0224 22:35:53.801466  1969 hierarchical.cpp:1147] Performed allocation for slave aab18b61-7811-4c43-a672-d1a63818c880-S0 in 162751ns  I0224 22:35:53.801532  1969 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4  I0224 22:35:53.801867  1978 slave.cpp:994] Checkpointing SlaveInfo to '/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/meta/slaves/aab18b61-7811-4c43-a672-d1a63818c880-S0/slave.info'  I0224 22:35:53.801877  1969 status_update_manager.cpp:181] Resuming sending status updates  I0224 22:35:53.802898  1977 replica.cpp:537] Replica received write request for position 4 from (4548)@172.17.0.1:36678  I0224 22:35:53.803252  1978 slave.cpp:1030] Forwarding total oversubscribed resources   I0224 22:35:53.803640  1970 master.cpp:4649] Received update of slave aab18b61-7811-4c43-a672-d1a63818c880-S0 at slave(115)@172.17.0.1:36678 (maintenance-host) with total oversubscribed resources   I0224 22:35:53.803858  1977 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 912626ns  I0224 22:35:53.803889  1977 replica.cpp:712] Persisted action at 4  I0224 22:35:53.804144  1978 slave.cpp:3482] Received ping from slave-observer(117)@172.17.0.1:36678  I0224 22:35:53.804535  1971 hierarchical.cpp:531] Slave aab18b61-7811-4c43-a672-d1a63818c880-S0 (maintenance-host) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: )  I0224 22:35:53.804684  1971 hierarchical.cpp:1434] No resources available to allocate!  I0224 22:35:53.804714  1971 hierarchical.cpp:1147] Performed allocation for slave aab18b61-7811-4c43-a672-d1a63818c880-S0 in 131453ns  I0224 22:35:53.805541  1967 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0  I0224 22:35:53.805941  1967 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 366444ns  I0224 22:35:53.806015  1967 leveldb.cpp:399] Deleting ~2 keys from leveldb took 42808ns  I0224 22:35:53.806041  1967 replica.cpp:712] Persisted action at 4  I0224 22:35:53.806066  1967 replica.cpp:697] Replica learned TRUNCATE action at position 4  I0224 22:35:53.807355  1978 log.cpp:683] Attempting to append 465 bytes to the log  I0224 22:35:53.807551  1978 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 5  I0224 22:35:53.809638  1979 replica.cpp:537] Replica received write request for position 5 from (4549)@172.17.0.1:36678  I0224 22:35:53.810858  1979 leveldb.cpp:341] Persisting action (484 bytes) to leveldb took 1.167663ms  I0224 22:35:53.810904  1979 replica.cpp:712] Persisted action at 5  I0224 22:35:53.811997  1979 replica.cpp:691] Replica received learned notice for position 5 from @0.0.0.0:0  I0224 22:35:53...",1
"Document the network/cni isolator.
We need to document this isolator in mesos-containerizer.md (e.g., how to configure it, what's the pre-requisite, etc.)",3
"TaskInfo/ExecutorInfo should include fine-grained ownership/namespacing
We need a way to assign fine-grained ownership to tasks/executors so that multi-user frameworks can tell Mesos to associate the task with a user identity (rather than just the framework principal+role). Then, when an HTTP user requests to view the task's sandbox contents, or kill the task, or list all tasks, the authorizer can determine whether to allow/deny/filter the request based on finer-grained, user-level ownership.  Some systems may want TaskInfo.owner to represent a group rather than an individual user. That's fine as long as the framework sets the field to the group ID in such a way that a group-aware authorizer can interpret it.",2
"Libprocess metrics/snapshot endpoint rate limiting should be configurable.
Currently the {{/metrics/snapshot}} endpoint in libprocess has a [hard-coded|https://github.com/apache/mesos/blob/0.27.1/3rdparty/libprocess/include/process/metrics/metrics.hpp#L52] rate limit of 2 requests per second:    {code}    MetricsProcess()      : ProcessBase(""metrics""),        limiter(2, Seconds(1)) {}  {code}    This should be configurable via a libprocess environment variable so that users can control this when initializing libprocess.",2
"Add appc/runtime isolator for runtime isolation for appc images.
Appc image also contains runtime information like 'exec', 'env', 'workingDirectory' etc.  https://github.com/appc/spec/blob/master/spec/aci.md    Similar to docker images, we need to support a subset of them (mainly 'exec', 'env' and 'workingDirectory').",13
"Remove `user` and `rootfs` flags in Windows launcher.
nan",2
"Executor env variables should not be leaked to the command task.
Currently, command task inherits the env variables of the command executor. This is less ideal because the command executor environment variables include some Mesos internal env variables like MESOS_XXX and LIBPROCESS_XXX. Also, this behavior does not match what Docker containerizer does. We should construct the env variables from scratch for the command task, rather than relying on inheriting the env variables from the command executor.",3
"Disable rate limiting of the global metrics endpoint for mesos-tests execution
Once we can optionally disable rate limiting in the global metrics endpoint with MESOS-4776 we should disable the rate limiting during the execution of mesos-tests.    * rate limiting makes it cumbersome to repeatedly hit the endpoint since one would not want to interfere with the rate limiting  * rate limiting might incur additional wait time which might slown down tests",3
"SlaveTest.MetricsSlaveLaunchErrors test relies on implicit blocking behavior hitting the global metrics endpoint
The test attempts to observe a change in the {{slave/container_launch_errors}} metric, but does not wait for the triggering action to take place. Currently the test passes since hitting the endpoint blocks for some rate limit-related time which provides under many circumstances enough wait time for the action to take place. ",1
"Reorganize ACL subject/object descriptions.
The authorization documentation would benefit from a reorganization of the ACL subject/object descriptions. Instead of simple lists of the available subjects and objects, it would be nice to see a table showing which subject and object is used with each action.",5
"HTTP endpoint docs should use shorter paths
My understanding is that the recommended path for the v1 scheduler API is {{/api/v1/scheduler}}, but the HTTP endpoint [docs|http://mesos.apache.org/documentation/latest/endpoints/] for this endpoint list the path as {{/master/api/v1/scheduler}}; the filename of the doc page is also in the {{master}} subdirectory.    Similarly, we document the master state endpoint as {{/master/state}}, whereas the preferred name is now just {{/state}}, and so on for most of the other endpoints. Unlike we the V1 API, we might want to consider backward compatibility and document both forms -- not sure. But certainly it seems like we should encourage people to use the shorter paths, not the longer ones.",2
"Revert external linkage of symbols in master/constants.hpp
src/master/constants.hpp contains:    {code}  // TODO(bmahler): It appears there may be a bug with gcc-4.1.2 in which the  // duration constants were not being initialized when having static linkage.  // This issue did not manifest in newer gcc's. Specifically, 4.2.1 was ok.  // So we've moved these to have external linkage but perhaps in the future  // we can revert this.  {code}    From commit 232a23b2a2e11f4e905b834aa2a11afe5bf6438a. We should investigate whether this is still necessary on supported compilers; it likely is not.",1
"Add documentation around using the docker containerizer on CentOS 6.
Support for persistent volumes was added to the docker containerizer in [MESOS-3413].  However, this does not work on CentOS 6.    On CentOS 6, the same {{docker run -v ...}} operation does not perform a recursive bind, whereas on every other OS supported by Mesos, docker does a recursive bind.    Docker has already [dropped support for CentOS 6|https://github.com/docker/docker/issues/14365], so we should add precautionary documentation in case anyone tries to use the docker containerizer on CentOS 6.",1
"Add a couple of registrar tests for /weights endpoint
nan",2
"Make existing scheduler library tests use the callback interface.
We need to migrate the existing tests in {{src/tests/scheduler_tests.cpp}} to use the new callback interface introduced in {{MESOS-3339}}. The changes to {{src/tests/master_maintenance_tests.cpp}} would be done when MESOS-4831 is resolved.    For an example see {{SchedulerTest.SchedulerFailover}} which already uses this new interface.",5
"Updated `createFrameworkInfo` for hierarchical_allocator_tests.cpp.
The function of {{createFrameworkInfo}} in hierarchical_allocator_tests.cpp should be updated by enabling caller can set a framework capability to create a framework which can use revocable resources.",1
"Update leveldb patch file to suport PowerPC LE
See: https://github.com/google/leveldb/releases/tag/v1.18 for improvements / bug fixes.  The motivation is that leveldb 1.18 has officially supported IBM Power (ppc64le), so this is needed by [MESOS-4312|https://issues.apache.org/jira/browse/MESOS-4312].    Update: Since someone updated leveldb to 1.4, so I only update the patch file to support PowerPC LE. Because I don't think upgrade 3rdparty library frequently is a good thing.",3
"Update vendored libev to 4.22
The motivation is that libev 4.22 has officially supported IBM Power (ppc64le), so this is needed by [MESOS-4312|https://issues.apache.org/jira/browse/MESOS-4312].",3
"Update ry-http-parser-1c3624a to nodejs/http-parser 2.6.1
See https://github.com/nodejs/http-parser/releases/tag/v2.6.1.  The motivation is that nodejs/http-parser 2.6.1 has officially supported IBM Power (ppc64le), so this is needed by [MESOS-4312|https://issues.apache.org/jira/browse/MESOS-4312].",3
"IOTest.BufferedRead writes to the current directory
libprocess's {{IOTest.BufferedRead}} writes to the current directory. This is bad for a number of reasons, e.g.,    * should the test fail data might be leaked to random locations,  * the test cannot be executed from a write-only directory, or  * executing the same test in parallel would race on the existence of the created file, and show bogus behavior.    The test should probably be executed from a temporary directory, e.g., via stout's {{TemporaryDirectoryTest}} fixture.",1
"ProvisionerDockerPullerTest.ROOT_INTERNET_CURL_ShellCommand fails.
{noformat}  [09:46:46] :	 [Step 11/11] [ RUN      ] ProvisionerDockerRegistryPullerTest.ROOT_INTERNET_CURL_ShellCommand  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.628413  1166 leveldb.cpp:174] Opened db in 4.242882ms  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.629926  1166 leveldb.cpp:181] Compacted db in 1.483621ms  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.629966  1166 leveldb.cpp:196] Created db iterator in 15498ns  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.629977  1166 leveldb.cpp:202] Seeked to beginning of db in 1405ns  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.629984  1166 leveldb.cpp:271] Iterated through 0 keys in the db in 239ns  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.630015  1166 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.630470  1183 recover.cpp:447] Starting replica recovery  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.630702  1180 recover.cpp:473] Replica is in EMPTY status  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.631767  1182 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (14567)@172.30.2.124:37431  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.632115  1183 recover.cpp:193] Received a recover response from a replica in EMPTY status  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.632450  1186 recover.cpp:564] Updating replica status to STARTING  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.633476  1186 master.cpp:375] Master 3fbb2fb0-4f18-498b-a440-9acbf6923a13 (ip-172-30-2-124.mesosphere.io) started on 172.30.2.124:37431  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.633491  1186 master.cpp:377] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_http=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/4UxXoW/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/tmp/4UxXoW/master"" --zk_session_timeout=""10secs""  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.633677  1186 master.cpp:422] Master only allowing authenticated frameworks to register  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.633685  1186 master.cpp:427] Master only allowing authenticated slaves to register  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.633692  1186 credentials.hpp:35] Loading credentials for authentication from '/tmp/4UxXoW/credentials'  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.633851  1183 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.191043ms  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.633873  1183 replica.cpp:320] Persisted replica status to STARTING  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.633894  1186 master.cpp:467] Using default 'crammd5' authenticator  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.634003  1186 master.cpp:536] Using default 'basic' HTTP authenticator  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.634062  1184 recover.cpp:473] Replica is in STARTING status  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.634109  1186 master.cpp:570] Authorization enabled  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.634249  1187 whitelist_watcher.cpp:77] No whitelist given  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.634255  1184 hierarchical.cpp:144] Initialized hierarchical allocator process  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.634884  1187 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (14569)@172.30.2.124:37431  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.635278  1181 recover.cpp:193] Received a recover response from a replica in STARTING status  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.635742  1187 recover.cpp:564] Updating replica status to VOTING  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.636391  1180 master.cpp:1711] The newly elected leader is master@172.30.2.124:37431 with id 3fbb2fb0-4f18-498b-a440-9acbf6923a13  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.636415  1180 master.cpp:1724] Elected as the leading master!  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.636430  1180 master.cpp:1469] Recovering from registrar  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.636554  1187 registrar.cpp:307] Recovering registrar  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.637111  1181 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.120322ms  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.637133  1181 replica.cpp:320] Persisted replica status to VOTING  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.637218  1186 recover.cpp:578] Successfully joined the Paxos group  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.637354  1186 recover.cpp:462] Recover process terminated  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.637715  1182 log.cpp:659] Attempting to start the writer  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.638617  1184 replica.cpp:493] Replica received implicit promise request from (14570)@172.30.2.124:37431 with proposal 1  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.639700  1184 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.057386ms  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.639722  1184 replica.cpp:342] Persisted promised to 1  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.640251  1184 coordinator.cpp:238] Coordinator attempting to fill missing positions  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.641274  1185 replica.cpp:388] Replica received explicit promise request from (14571)@172.30.2.124:37431 for position 0 with proposal 2  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.642371  1185 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 1.061574ms  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.642396  1185 replica.cpp:712] Persisted action at 0  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.643299  1186 replica.cpp:537] Replica received write request for position 0 from (14572)@172.30.2.124:37431  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.643349  1186 leveldb.cpp:436] Reading position from leveldb took 21735ns  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.644448  1186 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 1.06671ms  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.644469  1186 replica.cpp:712] Persisted action at 0  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.645077  1181 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.646174  1181 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.069097ms  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.646198  1181 replica.cpp:712] Persisted action at 0  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.646211  1181 replica.cpp:697] Replica learned NOP action at position 0  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.646716  1182 log.cpp:675] Writer started with ending position 0  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.647538  1183 leveldb.cpp:436] Reading position from leveldb took 21456ns  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.648298  1186 registrar.cpp:340] Successfully fetched the registry (0B) in 11.71072ms  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.648388  1186 registrar.cpp:439] Applied 1 operations in 21138ns; attempting to update the 'registry'  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.648947  1187 log.cpp:683] Attempting to append 210 bytes to the log  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.649050  1183 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.649655  1187 replica.cpp:537] Replica received write request for position 1 from (14573)@172.30.2.124:37431  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.650725  1187 leveldb.cpp:341] Persisting action (229 bytes) to leveldb took 1.041938ms  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.650748  1187 replica.cpp:712] Persisted action at 1  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.651198  1181 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.652312  1181 leveldb.cpp:341] Persisting action (231 bytes) to leveldb took 1.092268ms  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.652335  1181 replica.cpp:712] Persisted action at 1  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.652349  1181 replica.cpp:697] Replica learned APPEND action at position 1  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.653095  1187 registrar.cpp:484] Successfully updated the 'registry' in 4.664064ms  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.653236  1187 registrar.cpp:370] Successfully recovered registrar  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.653306  1181 log.cpp:702] Attempting to truncate the log to 1  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.653476  1184 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.653642  1183 master.cpp:1521] Recovered 0 slaves from the Registry (171B) ; allowing 10mins for slaves to re-register  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.653659  1181 hierarchical.cpp:171] Skipping recovery of hierarchical allocator: nothing to recover  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.654270  1181 replica.cpp:537] Replica received write request for position 2 from (14574)@172.30.2.124:37431  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.655357  1181 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.055267ms  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.655378  1181 replica.cpp:712] Persisted action at 2  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.655850  1184 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.657009  1184 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 1.137223ms  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.657059  1184 leveldb.cpp:399] Deleting ~1 keys from leveldb took 26459ns  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.657074  1184 replica.cpp:712] Persisted action at 2  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.657089  1184 replica.cpp:697] Replica learned TRUNCATE action at position 2  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.665710  1166 containerizer.cpp:149] Using isolation: docker/runtime,filesystem/linux  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.672399  1166 linux_launcher.cpp:101] Using /sys/fs/cgroup/freezer as the freezer hierarchy for the Linux launcher  [09:46:46]W:	 [Step 11/11] E0229 09:46:46.676822  1166 shell.hpp:93] Command 'hadoop version 2>&1' failed; this is the output:  [09:46:46]W:	 [Step 11/11] sh: hadoop: command not found  [09:46:46]W:	 [Step 11/11] E0229 09:46:46.676851  1166 fetcher.cpp:58] Failed to create URI fetcher plugin 'hadoop': Failed to create HDFS client: Failed to execute 'hadoop version 2>&1'; the command was either not found or exited with a non-zero exit status: 127  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.678383  1166 linux.cpp:81] Making '/tmp/ProvisionerDockerRegistryPullerTest_ROOT_INTERNET_CURL_ShellCommand_5BWCfv' a shared mount  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.687223  1180 slave.cpp:193] Slave started on 422)@172.30.2.124:37431  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.687248  1180 slave.cpp:194] Flags at startup: --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/ProvisionerDockerRegistryPullerTest_ROOT_INTERNET_CURL_ShellCommand_5BWCfv/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/ProvisionerDockerRegistryPullerTest_ROOT_INTERNET_CURL_ShellCommand_5BWCfv/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_providers=""docker"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""docker/runtime,filesystem/linux"" --launcher_dir=""/mnt/teamcity/work/4240ba9ddd0997c3/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/ProvisionerDockerRegistryPullerTest_ROOT_INTERNET_CURL_ShellCommand_5BWCfv""  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.687531  1180 credentials.hpp:83] Loading credential for authentication from '/tmp/ProvisionerDockerRegistryPullerTest_ROOT_INTERNET_CURL_ShellCommand_5BWCfv/credential'  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.687666  1180 slave.cpp:324] Slave using credential for: test-principal  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.687798  1180 resources.cpp:572] Parsing resources as JSON failed: cpus:2;mem:1024;disk:1024;ports:[31000-32000]  [09:46:46]W:	 [Step 11/11] Trying semicolon-delimited string format instead  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.688151  1180 slave.cpp:464] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.688207  1180 slave.cpp:472] Slave attributes: [  ]  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.688217  1180 slave.cpp:477] Slave hostname: ip-172-30-2-124.mesosphere.io  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.689259  1187 state.cpp:58] Recovering state from '/tmp/ProvisionerDockerRegistryPullerTest_ROOT_INTERNET_CURL_ShellCommand_5BWCfv/meta'  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.689394  1166 sched.cpp:222] Version: 0.28.0  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.689497  1180 status_update_manager.cpp:200] Recovering status update manager  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.689798  1182 containerizer.cpp:407] Recovering containerizer  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.690021  1186 sched.cpp:326] New master detected at master@172.30.2.124:37431  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.690146  1186 sched.cpp:382] Authenticating with master master@172.30.2.124:37431  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.690162  1186 sched.cpp:389] Using default CRAM-MD5 authenticatee  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.690378  1181 authenticatee.cpp:121] Creating new client SASL connection  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.690688  1186 master.cpp:5540] Authenticating scheduler-52603476-875a-49a8-85d4-c98d102cdfab@172.30.2.124:37431  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.690801  1184 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(877)@172.30.2.124:37431  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.691025  1181 authenticator.cpp:98] Creating new server SASL connection  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.691314  1180 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.691339  1180 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.691437  1180 authenticator.cpp:203] Received SASL authentication start  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.691490  1180 authenticator.cpp:325] Authentication requires more steps  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.691581  1180 authenticatee.cpp:258] Received SASL authentication step  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.691684  1180 authenticator.cpp:231] Received SASL authentication step  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.691712  1180 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-124.mesosphere.io' server FQDN: 'ip-172-30-2-124.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false   [09:46:46]W:	 [Step 11/11] I0229 09:46:46.691726  1180 auxprop.cpp:179] Looking up auxiliary property '*userPassword'  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.691768  1180 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.691802  1180 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-124.mesosphere.io' server FQDN: 'ip-172-30-2-124.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true   [09:46:46]W:	 [Step 11/11] I0229 09:46:46.691817  1180 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.691829  1180 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.691848  1180 authenticator.cpp:317] Authentication success  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.691944  1186 authenticatee.cpp:298] Authentication success  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.692011  1185 master.cpp:5570] Successfully authenticated principal 'test-principal' at scheduler-52603476-875a-49a8-85d4-c98d102cdfab@172.30.2.124:37431  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.692056  1187 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(877)@172.30.2.124:37431  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.692308  1184 sched.cpp:471] Successfully authenticated with master master@172.30.2.124:37431  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.692325  1184 sched.cpp:776] Sending SUBSCRIBE call to master@172.30.2.124:37431  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.692399  1184 sched.cpp:809] Will retry registration in 954.231367ms if necessary  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.692505  1183 master.cpp:2279] Received SUBSCRIBE call for framework 'default' at scheduler-52603476-875a-49a8-85d4-c98d102cdfab@172.30.2.124:37431  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.692553  1183 master.cpp:1750] Authorizing framework principal 'test-principal' to receive offers for role '*'  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.692836  1184 master.cpp:2350] Subscribing framework default with checkpointing disabled and capabilities [  ]  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.692942  1183 metadata_manager.cpp:188] No images to load from disk. Docker provisioner image storage path '/tmp/mesos/store/docker/storedImages' does not exist  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.693208  1180 provisioner.cpp:245] Provisioner recovery complete  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.693295  1186 hierarchical.cpp:265] Added framework 3fbb2fb0-4f18-498b-a440-9acbf6923a13-0000  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.693357  1186 hierarchical.cpp:1437] No resources available to allocate!  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.693397  1186 hierarchical.cpp:1532] No inverse offers to send out!  [09:46:46]W:	 [Step 11/11] I0229 09:46:46.693424  1186 ...",3
"Implement base tests for unified container using local puller.
Using command line executor to test shell commands with local docker images.",2
"Remove internal usage of deprecated *.json endpoints.
We still use the deprecated *.json internally (UI, tests, documentation). ",3
"Add end to end testing for Appc images.
Add tests that covers integration test of the Appc provisioner feature with mesos containerizer.   ",3
"Add documentation for Appc image discovery.
Add documentation for the Appc image discovery feature that covers:    - Use case  - Implementation detail (Simple discovery).",3
"Need to set `EXPOSED` ports from docker images into `ContainerConfig`
Most docker images have an `EXPOSE` command associated with them. This tells the container run-time the TCP ports that the micro-service ""wishes"" to expose to the outside world.     With the `Unified containerizer` project since `MesosContainerizer` is going to natively support docker images it is imperative that the Mesos container run time have a mechanism to expose ports listed in a Docker image. The first step to achieve this is to extract this information from the `Docker` image and set in the `ContainerConfig` . The `ContainerConfig` can then be used to pass this information to any isolator (for e.g. `network/cni` isolator) that will install port forwarding rules to expose the desired ports.",1
"Introduce a port field in `ImageManifest` in order to set exposed ports for a container.
Networking isolators such as `network/cni` need to learn about ports that a container wishes to be exposed to the outside world. This can be achieved by adding a field to the `ImageManifest` protobuf and allowing the `ImageProvisioner` to set these fields to inform the isolator of the ports that the container wishes to be exposed. ",1
"Add support for local image fetching in Appc provisioner.
Currently Appc image provisioner supports http(s) fetching. It would be valuable to add support for local file path(URI) based  fetching.",2
"Implement port forwarding in `network/cni` isolator
Most docker and appc images wish to expose ports that micro-services are listening on, to the outside world. When containers are running on bridged (or ptp) networking this can be achieved by installing port forwarding rules on the agent (using iptables). This can be done in the `network/cni` isolator.     The reason we would like this functionality to be implemented in the `network/cni` isolator, and not a CNI plugin, is that the specifications currently do not support specifying port forwarding rules. Further, to install these rules the isolator needs two pieces of information, the exposed ports and the IP address associated with the container. Bother are available to the isolator.",2
"""filesystem/linux"" isolator does not unmount orphaned persistent volumes
A persistent volume can be orphaned when:  # A framework registers with checkpointing enabled.  # The framework starts a task + a persistent volume.  # The agent exits.  The task continues to run.  # Something wipes the agent's {{meta}} directory.  This removes the checkpointed framework info from the agent.  # The agent comes back and recovers.  The framework for the task is not found, so the task is considered orphaned now.    The agent currently does not unmount the persistent volume, saying (with {{GLOG_v=1}})   {code}  I0229 23:55:42.078940  5635 linux.cpp:711] Ignoring cleanup request for unknown container: a35189d3-85d5-4d02-b568-67f675b6dc97  {code}    Test implemented here: https://reviews.apache.org/r/44122/",2
"Master's slave reregister logic does not update version field
The master's logic for reregistering a slave does not update the version field if the slave re-registers with a new version.",1
"Remove `grace_period_seconds` field from Shutdown event v1 protobuf.
There are two ways in which a shutdown of executor can be triggered:  1. If it receives an explicit `Shutdown` message from the agent.  2. If the recovery timeout period has elapsed, and the executor still hasn’t been able to (re-)connect with the agent.    Currently, the executor library relies on the field `grace_period_seconds` having a default value of 5 seconds to handle the second scenario. https://github.com/apache/mesos/blob/master/src/executor/executor.cpp#L608    The driver used to trigger the grace period via a constant defined in src/slave/constants.cpp. https://github.com/apache/mesos/blob/master/src/exec/exec.cpp#L92    The agent may want to force a shorter shutdown grace period (e.g. oversubscription eviction may have shorter deadline) in the future. For now, we can just read the value via an environment variable.",3
"Bind docker runtime isolator with docker image provider.
If image provider is specified as `docker` but docker/runtime is not set, it would be not meaningful, because of no executables. A check should be added to make sure docker runtime isolator is on if using docker as image provider.",1
"DockerContainerizerTest.ROOT_DOCKER_RecoverOrphanedPersistentVolumes exits when the /tmp directory is bind-mounted
If the {{/tmp}} directory (where Mesos tests create temporary directories) is a bind mount, the test suite will exit here:  {code}  [ RUN      ] DockerContainerizerTest.ROOT_DOCKER_RecoverOrphanedPersistentVolumes  I0226 03:17:26.722806  1097 leveldb.cpp:174] Opened db in 12.587676ms  I0226 03:17:26.723496  1097 leveldb.cpp:181] Compacted db in 636999ns  I0226 03:17:26.723536  1097 leveldb.cpp:196] Created db iterator in 18271ns  I0226 03:17:26.723547  1097 leveldb.cpp:202] Seeked to beginning of db in 1555ns  I0226 03:17:26.723554  1097 leveldb.cpp:271] Iterated through 0 keys in the db in 363ns  I0226 03:17:26.723593  1097 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned  I0226 03:17:26.724128  1117 recover.cpp:447] Starting replica recovery  I0226 03:17:26.724367  1117 recover.cpp:473] Replica is in EMPTY status  I0226 03:17:26.725237  1117 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (13810)@172.30.2.151:51934  I0226 03:17:26.725744  1114 recover.cpp:193] Received a recover response from a replica in EMPTY status  I0226 03:17:26.726356  1111 master.cpp:376] Master 5cc57c0e-f1ad-4107-893f-420ed1a1db1a (ip-172-30-2-151.mesosphere.io) started on 172.30.2.151:51934  I0226 03:17:26.726369  1118 recover.cpp:564] Updating replica status to STARTING  I0226 03:17:26.726378  1111 master.cpp:378] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_http=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/djHTVQ/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/tmp/djHTVQ/master"" --zk_session_timeout=""10secs""  I0226 03:17:26.726605  1111 master.cpp:423] Master only allowing authenticated frameworks to register  I0226 03:17:26.726616  1111 master.cpp:428] Master only allowing authenticated slaves to register  I0226 03:17:26.726632  1111 credentials.hpp:35] Loading credentials for authentication from '/tmp/djHTVQ/credentials'  I0226 03:17:26.726860  1111 master.cpp:468] Using default 'crammd5' authenticator  I0226 03:17:26.726977  1111 master.cpp:537] Using default 'basic' HTTP authenticator  I0226 03:17:26.727092  1111 master.cpp:571] Authorization enabled  I0226 03:17:26.727243  1118 hierarchical.cpp:144] Initialized hierarchical allocator process  I0226 03:17:26.727285  1116 whitelist_watcher.cpp:77] No whitelist given  I0226 03:17:26.728852  1114 master.cpp:1712] The newly elected leader is master@172.30.2.151:51934 with id 5cc57c0e-f1ad-4107-893f-420ed1a1db1a  I0226 03:17:26.728876  1114 master.cpp:1725] Elected as the leading master!  I0226 03:17:26.728891  1114 master.cpp:1470] Recovering from registrar  I0226 03:17:26.728977  1117 registrar.cpp:307] Recovering registrar  I0226 03:17:26.731503  1112 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 4.977811ms  I0226 03:17:26.731539  1112 replica.cpp:320] Persisted replica status to STARTING  I0226 03:17:26.731711  1111 recover.cpp:473] Replica is in STARTING status  I0226 03:17:26.732501  1114 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (13812)@172.30.2.151:51934  I0226 03:17:26.732862  1111 recover.cpp:193] Received a recover response from a replica in STARTING status  I0226 03:17:26.733264  1117 recover.cpp:564] Updating replica status to VOTING  I0226 03:17:26.733836  1118 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 388246ns  I0226 03:17:26.733855  1118 replica.cpp:320] Persisted replica status to VOTING  I0226 03:17:26.733979  1113 recover.cpp:578] Successfully joined the Paxos group  I0226 03:17:26.734149  1113 recover.cpp:462] Recover process terminated  I0226 03:17:26.734478  1111 log.cpp:659] Attempting to start the writer  I0226 03:17:26.735523  1114 replica.cpp:493] Replica received implicit promise request from (13813)@172.30.2.151:51934 with proposal 1  I0226 03:17:26.736130  1114 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 576451ns  I0226 03:17:26.736150  1114 replica.cpp:342] Persisted promised to 1  I0226 03:17:26.736709  1115 coordinator.cpp:238] Coordinator attempting to fill missing positions  I0226 03:17:26.737771  1114 replica.cpp:388] Replica received explicit promise request from (13814)@172.30.2.151:51934 for position 0 with proposal 2  I0226 03:17:26.738386  1114 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 583184ns  I0226 03:17:26.738404  1114 replica.cpp:712] Persisted action at 0  I0226 03:17:26.739312  1118 replica.cpp:537] Replica received write request for position 0 from (13815)@172.30.2.151:51934  I0226 03:17:26.739367  1118 leveldb.cpp:436] Reading position from leveldb took 26157ns  I0226 03:17:26.740638  1118 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 1.238477ms  I0226 03:17:26.740669  1118 replica.cpp:712] Persisted action at 0  I0226 03:17:26.741158  1118 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0  I0226 03:17:26.742878  1118 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.697254ms  I0226 03:17:26.742902  1118 replica.cpp:712] Persisted action at 0  I0226 03:17:26.742916  1118 replica.cpp:697] Replica learned NOP action at position 0  I0226 03:17:26.743393  1117 log.cpp:675] Writer started with ending position 0  I0226 03:17:26.744370  1112 leveldb.cpp:436] Reading position from leveldb took 34329ns  I0226 03:17:26.745240  1117 registrar.cpp:340] Successfully fetched the registry (0B) in 16.21888ms  I0226 03:17:26.745350  1117 registrar.cpp:439] Applied 1 operations in 30460ns; attempting to update the 'registry'  I0226 03:17:26.746016  1111 log.cpp:683] Attempting to append 210 bytes to the log  I0226 03:17:26.746119  1116 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1  I0226 03:17:26.746798  1114 replica.cpp:537] Replica received write request for position 1 from (13816)@172.30.2.151:51934  I0226 03:17:26.747251  1114 leveldb.cpp:341] Persisting action (229 bytes) to leveldb took 411333ns  I0226 03:17:26.747269  1114 replica.cpp:712] Persisted action at 1  I0226 03:17:26.747808  1113 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0  I0226 03:17:26.749511  1113 leveldb.cpp:341] Persisting action (231 bytes) to leveldb took 1.673488ms  I0226 03:17:26.749534  1113 replica.cpp:712] Persisted action at 1  I0226 03:17:26.749550  1113 replica.cpp:697] Replica learned APPEND action at position 1  I0226 03:17:26.750422  1111 registrar.cpp:484] Successfully updated the 'registry' in 5.021952ms  I0226 03:17:26.750560  1111 registrar.cpp:370] Successfully recovered registrar  I0226 03:17:26.750635  1112 log.cpp:702] Attempting to truncate the log to 1  I0226 03:17:26.750751  1113 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2  I0226 03:17:26.751096  1116 master.cpp:1522] Recovered 0 slaves from the Registry (171B) ; allowing 10mins for slaves to re-register  I0226 03:17:26.751126  1111 hierarchical.cpp:171] Skipping recovery of hierarchical allocator: nothing to recover  I0226 03:17:26.751561  1118 replica.cpp:537] Replica received write request for position 2 from (13817)@172.30.2.151:51934  I0226 03:17:26.751999  1118 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 406823ns  I0226 03:17:26.752018  1118 replica.cpp:712] Persisted action at 2  I0226 03:17:26.752521  1113 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0  I0226 03:17:26.754161  1113 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 1.614888ms  I0226 03:17:26.754210  1113 leveldb.cpp:399] Deleting ~1 keys from leveldb took 26384ns  I0226 03:17:26.754225  1113 replica.cpp:712] Persisted action at 2  I0226 03:17:26.754240  1113 replica.cpp:697] Replica learned TRUNCATE action at position 2  I0226 03:17:26.765103  1115 slave.cpp:193] Slave started on 399)@172.30.2.151:51934  I0226 03:17:26.765130  1115 slave.cpp:194] Flags at startup: --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/DockerContainerizerTest_ROOT_DOCKER_RecoverOrphanedPersistentVolumes_aJOesP/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/DockerContainerizerTest_ROOT_DOCKER_RecoverOrphanedPersistentVolumes_aJOesP/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mnt/teamcity/work/4240ba9ddd0997c3/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpu:2;mem:2048;disk(role1):2048"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/DockerContainerizerTest_ROOT_DOCKER_RecoverOrphanedPersistentVolumes_aJOesP""  I0226 03:17:26.765403  1115 credentials.hpp:83] Loading credential for authentication from '/tmp/DockerContainerizerTest_ROOT_DOCKER_RecoverOrphanedPersistentVolumes_aJOesP/credential'  I0226 03:17:26.765573  1115 slave.cpp:324] Slave using credential for: test-principal  I0226 03:17:26.765733  1115 resources.cpp:576] Parsing resources as JSON failed: cpu:2;mem:2048;disk(role1):2048  Trying semicolon-delimited string format instead  I0226 03:17:26.766185  1115 slave.cpp:464] Slave resources: cpu(*):2; mem(*):2048; disk(role1):2048; cpus(*):8; ports(*):[31000-32000]  I0226 03:17:26.766242  1115 slave.cpp:472] Slave attributes: [  ]  I0226 03:17:26.766250  1115 slave.cpp:477] Slave hostname: ip-172-30-2-151.mesosphere.io  I0226 03:17:26.767325  1097 sched.cpp:222] Version: 0.28.0  I0226 03:17:26.767390  1111 state.cpp:58] Recovering state from '/tmp/DockerContainerizerTest_ROOT_DOCKER_RecoverOrphanedPersistentVolumes_aJOesP/meta'  I0226 03:17:26.767603  1115 status_update_manager.cpp:200] Recovering status update manager  I0226 03:17:26.767865  1113 docker.cpp:726] Recovering Docker containers  I0226 03:17:26.767971  1111 sched.cpp:326] New master detected at master@172.30.2.151:51934  I0226 03:17:26.768045  1111 sched.cpp:382] Authenticating with master master@172.30.2.151:51934  I0226 03:17:26.768059  1111 sched.cpp:389] Using default CRAM-MD5 authenticatee  I0226 03:17:26.768070  1118 slave.cpp:4565] Finished recovery  I0226 03:17:26.768273  1112 authenticatee.cpp:121] Creating new client SASL connection  I0226 03:17:26.768435  1118 slave.cpp:4737] Querying resource estimator for oversubscribable resources  I0226 03:17:26.768565  1111 master.cpp:5526] Authenticating scheduler-c59020d6-385e-48a3-8a10-9e5c3f1dbd92@172.30.2.151:51934  I0226 03:17:26.768661  1118 slave.cpp:796] New master detected at master@172.30.2.151:51934  I0226 03:17:26.768659  1115 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(839)@172.30.2.151:51934  I0226 03:17:26.768679  1113 status_update_manager.cpp:174] Pausing sending status updates  I0226 03:17:26.768728  1118 slave.cpp:859] Authenticating with master master@172.30.2.151:51934  I0226 03:17:26.768743  1118 slave.cpp:864] Using default CRAM-MD5 authenticatee  I0226 03:17:26.768865  1118 slave.cpp:832] Detecting new master  I0226 03:17:26.768868  1112 authenticator.cpp:98] Creating new server SASL connection  I0226 03:17:26.768908  1114 authenticatee.cpp:121] Creating new client SASL connection  I0226 03:17:26.769003  1118 slave.cpp:4751] Received oversubscribable resources  from the resource estimator  I0226 03:17:26.769103  1115 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5  I0226 03:17:26.769131  1115 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'  I0226 03:17:26.769209  1116 master.cpp:5526] Authenticating slave(399)@172.30.2.151:51934  I0226 03:17:26.769253  1114 authenticator.cpp:203] Received SASL authentication start  I0226 03:17:26.769295  1115 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(840)@172.30.2.151:51934  I0226 03:17:26.769307  1114 authenticator.cpp:325] Authentication requires more steps  I0226 03:17:26.769403  1117 authenticatee.cpp:258] Received SASL authentication step  I0226 03:17:26.769495  1114 authenticator.cpp:98] Creating new server SASL connection  I0226 03:17:26.769531  1115 authenticator.cpp:231] Received SASL authentication step  I0226 03:17:26.769554  1115 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-151.mesosphere.io' server FQDN: 'ip-172-30-2-151.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false   I0226 03:17:26.769562  1115 auxprop.cpp:179] Looking up auxiliary property '*userPassword'  I0226 03:17:26.769608  1115 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'  I0226 03:17:26.769629  1115 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-151.mesosphere.io' server FQDN: 'ip-172-30-2-151.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true   I0226 03:17:26.769637  1115 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true  I0226 03:17:26.769642  1115 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true  I0226 03:17:26.769654  1115 authenticator.cpp:317] Authentication success  I0226 03:17:26.769728  1117 authenticatee.cpp:298] Authentication success  I0226 03:17:26.769769  1112 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5  I0226 03:17:26.769767  1118 master.cpp:5556] Successfully authenticated principal 'test-principal' at scheduler-c59020d6-385e-48a3-8a10-9e5c3f1dbd92@172.30.2.151:51934  I0226 03:17:26.769803  1112 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'  I0226 03:17:26.769798  1114 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(839)@172.30.2.151:51934  I0226 03:17:26.769881  1112 authenticator.cpp:203] Received SASL authentication start  I0226 03:17:26.769932  1112 authenticator.cpp:325] Authentication requires more steps  I0226 03:17:26.769981  1117 sched.cpp:471] Successfully authenticated with master master@172.30.2.151:51934  I0226 03:17:26.770004  1117 sched.cpp:776] Sending SUBSCRIBE call to master@172.30.2.151:51934  I0226 03:17:26.770064  1118 authenticatee.cpp:258] Received SASL authentication step  I0226 03:17:26.770102  1117 sched.cpp:809] Will retry registration in 1.937819802secs if necessary  I0226 03:17:26.770165  1115 authenticator.cpp:231] Received SASL authentication step  I0226 03:17:26.770193  1115 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-151.mesosphere.io' server FQDN: 'ip-172-30-2-151.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false   I0226 03:17:26.770207  1115 auxprop.cpp:179] Looking up auxiliary property '*userPassword'  I0226 03:17:26.770213  1116 master.cpp:2280] Received SUBSCRIBE call for framework 'default' at scheduler-c59020d6-385e-48a3-8a10-9e5c3f1dbd92@172.30.2.151:51934  I0226 03:17:26.770241  1115 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'  I0226 03:17:26.770274  1115 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-151.mesosphere.io' server FQDN: 'ip-172-30-2-151.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true   I0226 03:17:26.770277  1116 master.cpp:1751] Authorizing framework principal 'test-principal' to receive offers for role 'role1'  I0226 03:17:26.770298  1115 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true  I0226 03:17:26.770331  1115 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true  I0226 03:17:26.770349  1115 authenticator.cpp:317] Authentication success  I0226 03:17:26.770428  1118 authenticatee.cpp:298] Authentication success  I0226 03:17:26.770442  1116 master.cpp:5556] Successfully authenticated principal 'test-principal' at slave(399)@172.30.2.151:51934  I0226 03:17:26.770547  1116 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(840)@172.30.2.151:51934  I0226 03:17:26.770846  1116 master.cpp:2351] Subscribing framework default with checkpointing enabled and capabilities [  ]  I0226 03:17:26.770866  1118 slave.cpp:927] Successfully authenticated with master master@172.30.2.151:51934  I0226 03:17:26.770966  1118 slave.cpp:1321] Will retry registration in 1.453415ms if necessary  I0226 03:17:26.771225  1115 hierarchical.cpp:265] Added framework 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000  I0226 03:17:26.771275  1118 sched.cpp:703] Framework registered with 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000  I0226 03:17:26.771299  1115 hierarchical.cpp:1434] No resources available to allocate!  I0226 03:17:26.771328  1115 hierarchical.cpp:1529] No inverse offers to send out!  I0226 03:17:26.771344  1118 sched.cpp:717] Scheduler::registered took 50146ns  I0226 03:17:26.771356  1116 master.cpp:4240] Registering slave at slave(399)@172.30.2.151:51934 (ip-172-30-2-151.mesosphere.io) with id 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0  I0226 03:17:26.771348  1115 hierarchical.cpp:1127] Performed allocation for 0 slaves in 101438ns  I0226 03:17:26.771860  1114 registrar.cpp:439] Applied 1 operations in 59672ns; attempting to update the 'registry'  I0226 03:17:26.772645  1117 log.cpp:683] Attempting to append 423 bytes to the log  I0226 03:17:26.772758  1112 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3  I0226 03:17:26.773435  1117 replica.cpp:537] Replica received write request for position 3 from (13824)@172.30.2.151:51934  I0226 03:17:26.773586  1111 slave.cpp:1321] Will retry registration in 2.74261ms if necessary  I0226 03:17:26.773682  1115 master.cpp:4228] Ignoring register slave message from slave(399)@172.30.2.151:51934 (ip-172-30-2-151.mesosphere.io) as admission is already in progress  I0226 03:17:26.773937  1117 leveldb.cpp:341] Persisting action (442 bytes) to level...",2
"Poor allocator performance with labeled resources and/or persistent volumes
Modifying the {{HierarchicalAllocator_BENCHMARK_Test.ResourceLabels}} benchmark from https://reviews.apache.org/r/43686/ to use distinct labels between different slaves, performance regresses from ~2 seconds to ~3 minutes. The culprit seems to be the way in which the allocator merges together resources; reserved resource labels (or persistent volume IDs) inhibit merging, which causes performance to be much worse.",5
"Add 'file' fetcher plugin.
Add support for ""file"" based URI fetcher. This could be useful for container image provisioning from local file system.",2
"CgroupsAnyHierarchyWithFreezerTest.ROOT_CGROUPS_DestroyTracedProcess is flaky
Verbose logs:   {code}  [ RUN      ] CgroupsAnyHierarchyWithFreezerTest.ROOT_CGROUPS_DestroyTracedProcess  I0302 00:43:14.127846 11755 cgroups.cpp:2427] Freezing cgroup /sys/fs/cgroup/freezer/mesos_test  I0302 00:43:14.267411 11758 cgroups.cpp:1409] Successfully froze cgroup /sys/fs/cgroup/freezer/mesos_test after 139.46496ms  I0302 00:43:14.409395 11751 cgroups.cpp:2445] Thawing cgroup /sys/fs/cgroup/freezer/mesos_test  I0302 00:43:14.551304 11751 cgroups.cpp:1438] Successfullly thawed cgroup /sys/fs/cgroup/freezer/mesos_test after 141.811968ms  ../../src/tests/containerizer/cgroups_tests.cpp:949: Failure  Value of: ::waitpid(pid, &status, 0)    Actual: 23809  Expected: -1  ../../src/tests/containerizer/cgroups_tests.cpp:950: Failure  Value of: (*__errno_location ())    Actual: 0  Expected: 10  [  FAILED  ] CgroupsAnyHierarchyWithFreezerTest.ROOT_CGROUPS_DestroyTracedProcess (1055 ms)  {code}",2
"Fix rmdir for windows
This is due to a bug in MESOS-4415 that landed for 0.27.0.",1
"Move placement new processes into the freezer cgroup into a parent hook.
The Linux Launcher places new processes into the freezer cgroup.  This is currently done by a combination of childSetup function (blocking the new process until parent is done) and the parent (placing child process into the cgroup and then signaling child to continue).  ParentHooks support this behavior (blocking child until some work is done in the parent) in a much cleaner way.   ",3
"Remove internal usage of deprecated ShutdownFramework ACL
{{ShutdownFramework}} acl was deprecated a couple of versions ago in favor of the {{TeardownFramework}} message. Its deprecation cycle came with 0.27. That means we should remove the message and its references in the code base.",2
"Add authentication to master endpoints
Before we can add authorization around operator endpoints, we need to add authentication support, so that unauthenticated requests are denied when --authenticate_http is enabled, and so that the principal is passed into `route()`.",2
"Agent Authn Research Spike
Research the master authentication flags to see what changes will be necessary for agent http authentication.  Write up a 1-2 page summary/design doc.",2
"Add agent flags for HTTP authentication
Flags should be added to the agent to:  1. Enable HTTP authentication ({{--authenticate_http}})  2. Specify credentials ({{--http_credentials}})  3. Specify HTTP authenticators ({{--authenticators}})",2
"Add authentication to agent endpoints /state and /flags
The {{/state}} and {{/flags}} endpoints are installed in {{src/slave/slave.cpp}}, and thus are straightforward to make authenticated. Other agent endpoints require a bit more consideration, and are tracked in MESOS-4902.    For more information on agent endpoints, see http://mesos.apache.org/documentation/latest/endpoints/  or search for `route(` in the source code:  {code}  $ grep -rn ""route("" src/ |grep -v master |grep -v tests |grep -v json  src/version/version.cpp:75:  route(""/"", VERSION_HELP(), &VersionProcess::version);  src/files/files.cpp:150:  route(""/browse"",  src/files/files.cpp:153:  route(""/read"",  src/files/files.cpp:156:  route(""/download"",  src/files/files.cpp:159:  route(""/debug"",  src/slave/slave.cpp:580:  route(""/api/v1/executor"",  src/slave/slave.cpp:595:  route(""/state"",  src/slave/slave.cpp:601:  route(""/flags"",  src/slave/slave.cpp:607:  route(""/health"",  src/slave/monitor.cpp:100:    route(""/statistics"",  $ grep -rn ""route("" 3rdparty/ |grep -v tests |grep -v README |grep -v examples |grep -v help |grep -v ""process..pp""  3rdparty/libprocess/include/process/profiler.hpp:34:    route(""/start"", START_HELP(), &Profiler::start);  3rdparty/libprocess/include/process/profiler.hpp:35:    route(""/stop"", STOP_HELP(), &Profiler::stop);  3rdparty/libprocess/include/process/system.hpp:70:    route(""/stats.json"", statsHelp(), &System::stats);  3rdparty/libprocess/include/process/logging.hpp:44:    route(""/toggle"", TOGGLE_HELP(), &This::toggle);  {code}",3
"Update CHANGELOG with net_cls isolator
Need to update the CHANGELOG for 0.28 release.",1
"Make changes to executor v1 library around managing connections.
While implementing pipelining changes for the scheduler library (MESOS-3570), we noticed a couple of small bugs that we would like to fix in the executor library:    - Don't pass {{Connection}} objects to {{defer}} callbacks as they can sometimes lead to deadlocks.  - Minor cleanups around not accepting {{SUBSCRIBE}} call if one is currently in progress.  - Create a random UUID (connectionId) before we initiate a connection to the agent, as in some scenarios, we can accept connection attempts from stale connections.",3
"Add explicit upgrade instructions to the docs
The documentation currently contains per-version upgrade guidelines, which for recent releases only outlines the upgrade concerns for that version, without detailing explicit upgrade instructions.    We should add explicit upgrade instructions to the top of the upgrades documentation, which can be supplemented by the per-version concerns.    This is done within the upgrade docs for some early versions, with text like:    {code}  In order to upgrade a running cluster:    Install the new master binaries and restart the masters.  Upgrade the schedulers by linking the latest native library and mesos jar (if necessary).  Restart the schedulers.  Install the new slave binaries and restart the slaves.  Upgrade the executors by linking the latest native library and mesos jar (if necessary).  {code}    Instructions to this effect should be featured prominently in the doc.",1
"Add a script to install the Nvidia GDK on a host.
This script can be used to install the Nvidia GDK for Cuda 7.5 on a  mesos development machine. The purpose of the Nvidia GDK is to provide  all the necessary header files (nvml.h) and library files  (libnvidia-ml.so) necessary to build mesos with Nvidia GPU support.    If the machine on which Mesos is being compiled doesn't have any GPUs,  then libnvidia-ml.so consists only of stubs, allowing Mesos to build  and run, but not actually do anything useful under the hood. This  enables us to build a GPU-enabled mesos on a development machine  without GPUs and then deploy it to a production machine with GPUs and  be reasonably sure it will work.",2
"Add configure flags to build with Nvidia GPU support.
The configure flags can be used to enable Nvidia GPU support, as well as specify the installation directories of the nvml header and library files if not already installed in standard include/library paths on the system.    They will also be used to conditionally build support for Nvidia GPUs into Mesos.",2
"Add Nvidia GPU isolator tests.
We need to be able to run unit tests that verify GPU isolation, as well as run full blown tests that actually exercise the GPUs.    These tests should only build when the proper configure flags are set for enabling nvidia GPU support.",2
"Add flag to specify available Nvidia GPUs on an agent's command line.
In the initial GPU support we will not do auto-discovery of GPUs on an agent.  As such, an operator will need to specify a flag on the command line, listing all of the GPUs available on the system.",3
"Add GPUs as an explicit resource.
We will add ""gpus"" as an explicitly recognized resource in Mesos, akin to cpus, memory, ports, and disk.  In the containerizer, we will verify that the number of GPU resources passed in via the --resources flag matches the list of GPUs passed in via the --nvidia_gpus flag.  In the future we will add autodiscovery so this matching is unnecessary.  However, we will always have to pass ""gpus"" as a resource to make any GPU available on the system (unlike for cpus and memory, where the default is probed).",3
"PersistentVolumeTests do not need to set up ACLs.
The {{PersistentVolumeTest}} s have a custom helper for setting up ACLs in the {{master::Flags}}:  {code}  ACLs acls;      hashset<string> roles;        foreach (const FrameworkInfo& framework, frameworks) {        mesos::ACL::RegisterFramework* acl = acls.add_register_frameworks();        acl->mutable_principals()->add_values(framework.principal());        acl->mutable_roles()->add_values(framework.role());          roles.insert(framework.role());      }        flags.acls = acls;      flags.roles = strings::join("","", roles);  {code}    This is no longer necessary with implicit roles.",1
"Dump the contents of the sandbox when a test fails
[~bernd-mesos] added this logic for extra info about a rare flaky test:  https://github.com/apache/mesos/blob/d26baee1f377aedb148ad04cc004bb38b85ee4f6/src/tests/fetcher_cache_tests.cpp#L249-L259    This information is useful regardless of the test type and should be generalized for {{cluster::Slave}}.  i.e.   # When a {{cluster::Slave}} is destructed, it can detect if the test has failed.    # If so, navigate through its own {{work_dir}} and print sandboxes and/or other useful debugging info.  Also see the refactor in [MESOS-4634].",3
"Add documentation about container image support.
nan",5
"Mesos containerizer can't handle top level docker image like ""alpine"" (must use ""library/alpine"")
This can be demonstrated with the {{mesos-execute}} command:    # Docker containerizer with image {{alpine}}: success  {code}  sudo ./build/src/mesos-execute --docker_image=alpine --containerizer=docker --name=just-a-test --command=""sleep 1000"" --master=localhost:5050  {code}  # Mesos containerizer with image {{alpine}}: failure  {code}  sudo ./build/src/mesos-execute --docker_image=alpine --containerizer=mesos --name=just-a-test --command=""sleep 1000"" --master=localhost:5050  {code}  # Mesos containerizer with image {{library/alpine}}: success  {code}  sudo ./build/src/mesos-execute --docker_image=library/alpine --containerizer=mesos --name=just-a-test --command=""sleep 1000"" --master=localhost:5050  {code}    In the slave logs:    {code}  ea-4460-83  9c-838da86af34c-0007'  I0306 16:32:41.418269  3403 metadata_manager.cpp:159] Looking for image 'alpine:latest'  I0306 16:32:41.418699  3403 registry_puller.cpp:194] Pulling image 'alpine:latest' from 'docker-manifest://registry-1.docker.io:443alpine?latest#https' to '/tmp/mesos-test  /store/docker/staging/ka7MlQ'  E0306 16:32:43.098131  3400 slave.cpp:3773] Container '4bf9132d-9a57-4baa-a78c-e7164e93ace6' for executor 'just-a-test' of framework 4f055c6f-1bea-4460-839c-838da86af34c-0  007 failed to start: Collect failed: Unexpected HTTP response '401 Unauthorized  {code}    curl command executed:    {code}  $ sudo sysdig -A -p ""*%evt.time %proc.cmdline"" evt.type=execve and proc.name=curl                                                                   16:42:53.198998042 curl -s -S -L -D - https://registry-1.docker.io:443/v2/alpine/manifests/latest  16:42:53.784958541 curl -s -S -L -D - https://auth.docker.io/token?service=registry.docker.io&scope=repository:alpine:pull  16:42:54.294192024 curl -s -S -L -D - -H Authorization: Bearer eyJhbGciOiJFUzI1NiIsInR5cCI6IkpXVCIsIng1YyI6WyJNSUlDTHpDQ0FkU2dBd0lCQWdJQkFEQUtCZ2dxaGtqT1BRUURBakJHTVVRd1FnWURWUVFERXp0Uk5Gb3pPa2RYTjBrNldGUlFSRHBJVFRSUk9rOVVWRmc2TmtGRlF6cFNUVE5ET2tGU01rTTZUMFkzTnpwQ1ZrVkJPa2xHUlVrNlExazFTekFlRncweE5UQTJNalV4T1RVMU5EWmFGdzB4TmpBMk1qUXhPVFUxTkRaYU1FWXhSREJDQmdOVkJBTVRPMGhHU1UwNldGZFZWam8yUVZkSU9sWlpUVEk2TTFnMVREcFNWREkxT2s5VFNrbzZTMVExUmpwWVRsSklPbFJMTmtnNlMxUkxOanBCUVV0VU1Ga3dFd1lIS29aSXpqMENBUVlJS29aSXpqMERBUWNEUWdBRXl2UzIvdEI3T3JlMkVxcGRDeFdtS1NqV1N2VmJ2TWUrWGVFTUNVMDByQjI0akNiUVhreFdmOSs0MUxQMlZNQ29BK0RMRkIwVjBGZGdwajlOWU5rL2pxT0JzakNCcnpBT0JnTlZIUThCQWY4RUJBTUNBSUF3RHdZRFZSMGxCQWd3QmdZRVZSMGxBREJFQmdOVkhRNEVQUVE3U0VaSlRUcFlWMVZXT2paQlYwZzZWbGxOTWpveldEVk1PbEpVTWpVNlQxTktTanBMVkRWR09saE9Va2c2VkVzMlNEcExWRXMyT2tGQlMxUXdSZ1lEVlIwakJEOHdQWUE3VVRSYU16cEhWemRKT2xoVVVFUTZTRTAwVVRwUFZGUllPalpCUlVNNlVrMHpRenBCVWpKRE9rOUdOemM2UWxaRlFUcEpSa1ZKT2tOWk5Vc3dDZ1lJS29aSXpqMEVBd0lEU1FBd1JnSWhBTXZiT2h4cHhrTktqSDRhMFBNS0lFdXRmTjZtRDFvMWs4ZEJOVGxuWVFudkFpRUF0YVJGSGJSR2o4ZlVSSzZ4UVJHRURvQm1ZZ3dZelR3Z3BMaGJBZzNOUmFvPSJdfQ.eyJhY2Nlc3MiOltdLCJhdWQiOiJyZWdpc3RyeS5kb2NrZXIuaW8iLCJleHAiOjE0NTcyODI4NzQsImlhdCI6MTQ1NzI4MjU3NCwiaXNzIjoiYXV0aC5kb2NrZXIuaW8iLCJqdGkiOiJaOGtyNXZXNEJMWkNIRS1IcVJIaCIsIm5iZiI6MTQ1NzI4MjU3NCwic3ViIjoiIn0.C2wtJq_P-m0buPARhmQjDfh6ztIAhcvgN3tfWIZEClSgXlVQ_sAQXAALNZKwAQL2Chj7NpHX--0GW-aeL_28Aw https://registry-1.docker.io:443/v2/alpine/manifests/latest  {code}    Also got the same result with {{ubuntu}} docker image.",3
"Update glog patch to support PowerPC LE
This is a part of PowerPC LE porting",1
"Rescind all outstanding offers after changing some weights.
nan",2
"Add support for command and arguments to mesos-execute.
{{CommandInfo}} protobuf support two kinds of command:  {code}  // There are two ways to specify the command:    // 1) If 'shell == true', the command will be launched via shell    //		(i.e., /bin/sh -c 'value'). The 'value' specified will be    //		treated as the shell command. The 'arguments' will be ignored.    // 2) If 'shell == false', the command will be launched by passing    //		arguments to an executable. The 'value' specified will be    //		treated as the filename of the executable. The 'arguments'    //		will be treated as the arguments to the executable. This is    //		similar to how POSIX exec families launch processes (i.e.,    //		execlp(value, arguments(0), arguments(1), ...)).  {code}    The mesos-execute cannot handle 2) now, enabling 2) can help with testing and running one off tasks.",5
"Support mesos containerizer force_pull_image option.
Currently for unified containerizer, images that are already cached by metadata manager cannot be updated. User has to delete corresponding images in store if an update is need. We should support `force_pull_image` option for unified containerizer, to provide override option if existed.",3
"Default cmd is executed as an incorrect command.
When mesos containerizer launch a container using a docker image, which only container default Cmd. The executable command is is a incorrect sequence. For example:    If an image default entrypoint is null, cmd is ""sh"", user defines shell=false, value is none, and arguments as [-c, echo 'hello world']. The executable command is `[sh, -c, echo 'hello world', sh]`, which is incorrect. It should be `[sh, sh, -c, echo 'hello world']` instead.    This problem is only exposed for the case: sh=0, value=0, argv=1, entrypoint=0, cmd=1. ",2
"Implement runtime isolator tests.
There different cases in docker runtime isolator. Some special cases should be tested with unique test case, to verify the docker runtime isolator logic is correct.",5
"Add a '/containers' endpoint to the agent to list all the active containers.
This endpoint will be similar to /monitor/statistics.json endpoint, but it'll also contain the 'container_status' about the container (see ContainerStatus in mesos.proto). We'll eventually deprecate the /monitor/statistics.json endpoint.",8
"Add authentication to libprocess endpoints
In addition to the endpoints addressed by MESOS-4850 and MESOS-5152, the following endpoints would also benefit from HTTP authentication:  * {{/profiler/*}}  * {{/logging/toggle}}  * {{/metrics/snapshot}}    Adding HTTP authentication to these endpoints is a bit more complicated because they are defined at the libprocess level.    While working on MESOS-4850, it became apparent that since our tests use the same instance of libprocess for both master and agent, different default authentication realms must be used for master/agent so that HTTP authentication can be independently enabled/disabled for each.    We should establish a mechanism for making an endpoint authenticated that allows us to:  1) Install an endpoint like {{/files}}, whose code is shared by the master and agent, with different authentication realms for the master and agent  2) Avoid hard-coding a default authentication realm into libprocess, to permit the use of different authentication realms for the master and agent and to keep application-level concerns from leaking into libprocess    Another option would be to use a single default authentication realm and always enable or disable HTTP authentication for *both* the master and agent in tests. However, this wouldn't allow us to test scenarios where HTTP authentication is enabled on one but disabled on the other.",5
"Allow multiple loads of module manifests
The ModuleManager::load() is designed to be called exactly once during a process lifetime. This works well for Master/Agent environments. However, it can fail in Scheduler environments. For example, a single Scheduler binary might implement multiple scheduler drivers causing multiple calls to ModuleManager::load() leading to a failure.",3
"Tasks cannot be killed forcefully.
Currently there is no way for a scheduler to instruct the executor to kill a certain task immediately, skipping any possible timeouts and / or kill policies. This may be desirable in cases like, e.g., the kill policy is 10 minutes but something went wrong, so the scheduler decides to issue a forceful kill.",5
"Introduce kill policy for tasks.
A task may require some time to clean up or even a special mechanism to issue a kill request (currently it's a SIGTERM followed by SIGKILL). Introducing kill policies per task will help address these issue.",5
"Deprecate the --docker_stop_timeout agent flag.
Instead, a combination of {{executor_shutdown_grace_period}}  agent flag and optionally task kill policies should be used.",1
"Executor driver does not respect executor shutdown grace period.
Executor shutdown grace period, configured on the agent, is  propagated to executors via the `MESOS_EXECUTOR_SHUTDOWN_GRACE_PERIOD`  environment variable. The executor driver must use this timeout to delay  the hard shutdown of the related executor.",1
"LinuxFilesystemIsolatorTest.ROOT_MultipleContainers fails.
Observed on our CI:  {noformat}  [09:34:15] :	 [Step 11/11] [ RUN      ] LinuxFilesystemIsolatorTest.ROOT_MultipleContainers  [09:34:19]W:	 [Step 11/11] I0309 09:34:19.906719  2357 linux.cpp:81] Making '/tmp/MLVLnv' a shared mount  [09:34:19]W:	 [Step 11/11] I0309 09:34:19.923548  2357 linux_launcher.cpp:101] Using /sys/fs/cgroup/freezer as the freezer hierarchy for the Linux launcher  [09:34:19]W:	 [Step 11/11] I0309 09:34:19.924705  2376 containerizer.cpp:666] Starting container 'da610f7f-a709-4de8-94d3-74f4a520619b' for executor 'test_executor1' of framework ''  [09:34:19]W:	 [Step 11/11] I0309 09:34:19.925355  2371 provisioner.cpp:285] Provisioning image rootfs '/tmp/MLVLnv/provisioner/containers/da610f7f-a709-4de8-94d3-74f4a520619b/backends/copy/rootfses/0d7e047a-50f1-490b-bb58-00e9c49628d0' for container da610f7f-a709-4de8-94d3-74f4a520619b  [09:34:19]W:	 [Step 11/11] I0309 09:34:19.925881  2377 copy.cpp:127] Copying layer path '/tmp/MLVLnv/test_image1' to rootfs '/tmp/MLVLnv/provisioner/containers/da610f7f-a709-4de8-94d3-74f4a520619b/backends/copy/rootfses/0d7e047a-50f1-490b-bb58-00e9c49628d0'  [09:34:30]W:	 [Step 11/11] I0309 09:34:30.835127  2376 linux.cpp:355] Bind mounting work directory from '/tmp/MLVLnv/slaves/test_slave/frameworks/executors/test_executor1/runs/da610f7f-a709-4de8-94d3-74f4a520619b' to '/tmp/MLVLnv/provisioner/containers/da610f7f-a709-4de8-94d3-74f4a520619b/backends/copy/rootfses/0d7e047a-50f1-490b-bb58-00e9c49628d0/mnt/mesos/sandbox' for container da610f7f-a709-4de8-94d3-74f4a520619b  [09:34:30]W:	 [Step 11/11] I0309 09:34:30.835392  2376 linux.cpp:683] Changing the ownership of the persistent volume at '/tmp/MLVLnv/volumes/roles/test_role/persistent_volume_id' with uid 0 and gid 0  [09:34:30]W:	 [Step 11/11] I0309 09:34:30.840425  2376 linux.cpp:723] Mounting '/tmp/MLVLnv/volumes/roles/test_role/persistent_volume_id' to '/tmp/MLVLnv/provisioner/containers/da610f7f-a709-4de8-94d3-74f4a520619b/backends/copy/rootfses/0d7e047a-50f1-490b-bb58-00e9c49628d0/mnt/mesos/sandbox/volume' for persistent volume disk(test_role)[persistent_volume_id:volume]:32 of container da610f7f-a709-4de8-94d3-74f4a520619b  [09:34:30]W:	 [Step 11/11] I0309 09:34:30.843878  2374 linux_launcher.cpp:304] Cloning child process with flags = CLONE_NEWNS  [09:34:30]W:	 [Step 11/11] I0309 09:34:30.848302  2371 containerizer.cpp:666] Starting container 'fe4729c5-1e63-4cc6-a2e3-fe5006ffe087' for executor 'test_executor2' of framework ''  [09:34:30]W:	 [Step 11/11] I0309 09:34:30.848758  2371 containerizer.cpp:1392] Destroying container 'da610f7f-a709-4de8-94d3-74f4a520619b'  [09:34:30]W:	 [Step 11/11] I0309 09:34:30.848865  2373 provisioner.cpp:285] Provisioning image rootfs '/tmp/MLVLnv/provisioner/containers/fe4729c5-1e63-4cc6-a2e3-fe5006ffe087/backends/copy/rootfses/518b2464-43dd-47b0-9648-e78aedde6917' for container fe4729c5-1e63-4cc6-a2e3-fe5006ffe087  [09:34:30]W:	 [Step 11/11] I0309 09:34:30.849449  2375 copy.cpp:127] Copying layer path '/tmp/MLVLnv/test_image2' to rootfs '/tmp/MLVLnv/provisioner/containers/fe4729c5-1e63-4cc6-a2e3-fe5006ffe087/backends/copy/rootfses/518b2464-43dd-47b0-9648-e78aedde6917'  [09:34:30]W:	 [Step 11/11] I0309 09:34:30.854038  2374 cgroups.cpp:2427] Freezing cgroup /sys/fs/cgroup/freezer/mesos/da610f7f-a709-4de8-94d3-74f4a520619b  [09:34:30]W:	 [Step 11/11] I0309 09:34:30.856693  2372 cgroups.cpp:1409] Successfully froze cgroup /sys/fs/cgroup/freezer/mesos/da610f7f-a709-4de8-94d3-74f4a520619b after 2.608128ms  [09:34:30]W:	 [Step 11/11] I0309 09:34:30.859237  2377 cgroups.cpp:2445] Thawing cgroup /sys/fs/cgroup/freezer/mesos/da610f7f-a709-4de8-94d3-74f4a520619b  [09:34:30]W:	 [Step 11/11] I0309 09:34:30.861454  2377 cgroups.cpp:1438] Successfullly thawed cgroup /sys/fs/cgroup/freezer/mesos/da610f7f-a709-4de8-94d3-74f4a520619b after 2176us  [09:34:30]W:	 [Step 11/11] I0309 09:34:30.934608  2378 containerizer.cpp:1608] Executor for container 'da610f7f-a709-4de8-94d3-74f4a520619b' has exited  [09:34:30]W:	 [Step 11/11] I0309 09:34:30.937692  2372 linux.cpp:798] Unmounting volume '/tmp/MLVLnv/provisioner/containers/da610f7f-a709-4de8-94d3-74f4a520619b/backends/copy/rootfses/0d7e047a-50f1-490b-bb58-00e9c49628d0/mnt/mesos/sandbox/volume' for container da610f7f-a709-4de8-94d3-74f4a520619b  [09:34:30]W:	 [Step 11/11] I0309 09:34:30.937742  2372 linux.cpp:817] Unmounting sandbox/work directory '/tmp/MLVLnv/provisioner/containers/da610f7f-a709-4de8-94d3-74f4a520619b/backends/copy/rootfses/0d7e047a-50f1-490b-bb58-00e9c49628d0/mnt/mesos/sandbox' for container da610f7f-a709-4de8-94d3-74f4a520619b  [09:34:30]W:	 [Step 11/11] I0309 09:34:30.938129  2375 provisioner.cpp:330] Destroying container rootfs at '/tmp/MLVLnv/provisioner/containers/da610f7f-a709-4de8-94d3-74f4a520619b/backends/copy/rootfses/0d7e047a-50f1-490b-bb58-00e9c49628d0' for container da610f7f-a709-4de8-94d3-74f4a520619b  [09:34:45] :	 [Step 11/11] ../../src/tests/containerizer/filesystem_isolator_tests.cpp:1318: Failure  [09:34:45] :	 [Step 11/11] Failed to wait 15secs for wait1  [09:34:48] :	 [Step 11/11] [  FAILED  ] LinuxFilesystemIsolatorTest.ROOT_MultipleContainers (32341 ms)  {noformat}",3
"ProcessorManager delegate should be an Option<string>, not just a string.
Currently, the delegate field in the ProcessManager is just a string type. We check for 'existence' of a delegate by comparing (delegate != """"). Using an Option is the preferred method for things like this.",1
"Allow modules to express if they are multi-instantiable and thread safe.
A module might be instantiated multiple time (e.g., multiple schedulers in the same Java process instantiating an authenticator module) within the same process. The current mechanism doesn't provide a way through the module API to forbid multiple instantiations. It is up to the module to check and return error on prior instantiation.    Along similar lines, a module should be able to express thread-safety concerns. Typically, a module running in Master/Agent doesn't have to be concerned about thread safety if it uses libprocess API. However, we should investigate how it plays in the scheduler environment.",8
"Replace non-pod static variables in module/manager.[ch]pp with pod eqivalents.
nan",3
"Cache module manifests while loading in ModuleManager.
Since the module managers are allowed to load the same module multiple times, we should be caching the module manifests to avoid cases where the module tries to trick the module manager by changing `ModuleBase` fields before the next call to `ModuleManager::load`.",3
"Setup proper /etc/hostname, /etc/hosts and /etc/resolv.conf for containers in network/cni isolator.
The network/cni isolator needs to properly setup /etc/hostname and /etc/hosts for the container with a hostname (e.g., randomly generated) and the assigned IP returned by CNI plugin.  We should consider the following cases:  1) container is using host filesystem  2) container is using a different filesystem  3) custom executor and command executor",5
"Add a list parser for comma separated integers in flags.
Some flags require lists of integers to be passed in.  We should have an explicit parser for this instead of relying on ad hoc solutions.",2
"The flag parser for `hashmap<string, string>` should live in stout, not mesos.
The title says it all.",1
"Remove all '.get().' calls on Option / Try variables in the resources abstraction.
When possible, {{.get()}} calls should be replaced by {{->}} for {{Option}} / {{Try}} variables.  This ticket only proposes a blanket change for this in the resource abstraction files, not the code base as a whole.  This is in preparation for introducing the new GPU resource.  Without this change, I would need to use the old {{.get()}} calls.  Instead, I propose to fix the old code surrounding it so that consistency has me doing it the right way.  ",1
"Propose Design for Authorization based filtering for endpoints.
The design doc can be found here:  https://docs.google.com/document/d/1M27S7OTSfJ8afZCklOz00g_wcVrL32i9Lyl6g22GWeY",5
"Registrar HTTP Authentication.
Now that the master (and agents in progress) provide http authentication the registrar should do the same.     See http://mesos.apache.org/documentation/latest/endpoints/registrar/registry/",3
"Enable HELP to include authentication status of endpoint.
As we enable authentication for more and more endpoints we should document which endpoints support authentication and which ones don't.",2
"Investigate container security options for Mesos containerizer
We should investigate the following to improve the container security for Mesos containerizer and come up with a list of features that we want to support in MVP.    1) Capabilities  2) User namespace  3) Seccomp  4) SELinux  5) AppArmor    We should investigate what other container systems are doing regarding security:  1) [k8s| https://github.com/kubernetes/kubernetes/blob/master/pkg/api/v1/types.go#L2905]  2) [docker|https://docs.docker.com/engine/security/security/]  3) [oci|https://github.com/opencontainers/specs/blob/master/config.md]",5
"Support docker registry authentication
nan",5
"Support specifying per-container docker registry.
Currently, we only support a per agent flag to specify the docker registry. We should instead, allow people to specify the registry as part of the docker image name (like `docker pull` does).",3
