Issue,Storypoint
"Flag out the glowing edges of DECam CCDs
Pixels near the edges of the DECam CCDs are bigger/brighter and correcting them is not trivial. One way to move forward is to mask them out.      DESDM and CP mask 15 pixels on each edge.  The cut was later raised to 25 pixels, with the inner 10 pixels flagged as SUSPECT.  ",5
"Fix startup.py inside Docker container
qserv tag should be replace with qserv_latest",1
"Margaret's mgmt. activities in November
DMLT @ Princeton  Weekly DMLT and Standups  Local meetings  TPR  etc.",18
"Ops Planning and TOWG attendance - November
TOWG meetings, review service catalog as input to LOPT, review draft of operations WBS with Don and Athol",2
"L1 design specification and planning - November
With Steve, Jim, Don, and Jason, detailed design construction and planning for development of the Image Ingest and Processing System. Worked through LDM-230 and OCS design docs, input from discussions on Confluence. Made cleaner drawings (draft) and expanded Project plan. The plan is currently a ""to do"" list, without schedule or resource loading yet. ",10
"Facility Coordination Meeting and JCC meeting @ NCSA
Day 1 : Facility Coordination Day with Argonne, CC-IN2P3, NERSC, and NCSA  Day 2: Extended JCC meeting    Notes posted on Confluence: https://confluence.lsstcorp.org/pages/viewpage.action?spaceKey=JCC&title=Extended+JCC+meeting+--+2015-11-23 ",4
"Compilation errors from CLang (Apple LLVM 7.0) in XCode 7 on MacOSX
Compiling on MacOSX Yosemite with XCode 7, a number of files fail compilation.  ----  {{core/modules/util/EventThread.h,cc}} fails because {{uint}} is used as a data type.  This is non-standard (though some compilers support it), and should be replaced with {{unsigned int}}.  ----  {{core/modules/wbase/SendChannel.h,cc}} fails because {{#include <functional>}} is missing.  ----  {{core/modules/wsched/ChunkState.cc}} fails because {{#include <iostream>}} is missing.  ----  {{build/qmeta/qmetaLib_wrap.cc}} (generated by SWIG) fails with many errors because the {{typedef unsigned long int uint64_t}} included in {{qmetaLib.i}} conflicts with MacOSX's typedef of it as {{unsigned long long}}.",1
"shellcheck linting of lsstsw bash scripts
This issue is to recover a branch from DM-4113 that was not merged due to issues with installing shellcheck under travis.",1
"Create release notes docs template
Create a template for release notes/other release-type documentation in the new docs.",2
"Research simulation tools
We need to do simulations of DCR and other effects when determining appropriate mitigation techniques.  This will require settling on a simulation tool for doing this.    The obvious choices are: phosim, galsim, and a roll your own solution.  Look into which of these is the most reasonable choice and make a recommendation.",4
"Roll Qserv into SQuaRE release - part I
Improvements to codekit to support release process. ",4
"Revisit short and long term plans for butler
Revisit short and long term requirements and needs and capture it through stories.",5
"Review of storage quotes
Reviewed 4 products from 3 vendors with storage group. Prices and features comparison along with discussions on whether to integrate into condo or not. Also, discussed security considerations.    Preliminary vendor/product chosen. Follow up questions sent to vendor. Will review with storage before we are able to purchase.",2
"Update to Sizing Model
Removed 'memory effectiveness' factor as it was already included in compute efficiency  Updated capacity / tape to follow LTO roadmap  Updated bandwidth / tape drive to follow LTO roadmap  Changed ""tape!Number of new tapes purchased"" to round up to an even number for the archive site  Changed ""tape!Number of new tapes needed"" to round up for both sites  Changed ""tape!Number of tape drives needed based on bandwidth"" to round up for both sites  Fixed 'tape!tape bandwidth' colums to take into account mixed tape drive types  Updated 'tape!HPSS' to take into account retiring mover nodes    Also began work with Spectra Logic to further improve tape predictions.",2
"Power requirements and LSST footprint at NCSA
Finalized power requirements with the UofI engineer. I plan to distribute verification compute across multiple racks in order to reduce per-rack power requirements and reduce per-rack network port counts. This allows us to drop from 3x 60A for the verification cluster to 3x 30A which is the same for the other racks. This will result in some cost savings and simpler planning for future use of the racks.    Also, provided a plan for LSST's footprint in NPCF until 2032. We now have space reserved from the south side of Roger to the north side of Blue Waters. This space should be very visible from the room camera I believe. ",2
"Move butler from daf_persistence to daf_butler
nan",6
"Add initial butler support for remote GET
For Get:  If the mapper returns a URL:  retrieve the URL contents into a file  return the path to the file.    This will be optimized in subsequent stories by ""add read support for various transports to the afw object readers"". This is a degenerate case that will be used if the object does not have a reader for a given transport protocol.    Need to solve the cleanup problem of when to delete the file that was downloaded.    For Put:  serialize the object to a temporary file  transfer the file to the URL",10
"Fix docker workflow
Some issues where discovered while trying to package DM-2699 in Docker (for IN2P3 cluster deployment), they're fixed here.    - apt-get update times out: why?  - git clone then pull is too weak (if building the first clone fails, pull never occurs) => step merged  - eupspkg -er build creates lib/python in /qserv/stack/.../qserv/... and next install can't remove it for unknow reason => build and install merged.",2
"Base site additions to simulator
Started using login on the Nebula cluster - set up some instances and used snapshot facility. Detected an error with launching from volume.  Met with Chris Lindsey about issue.      Started coding replicator and Foreman functionality.   Began message taxonomy for comm between Foreman and Replicator.     Installed log rotate, Rabbitmq,  pip, pika and GCC on running instance. Successfully tested  rabbitmq with another instance which had pika libs installed.  Finished up prototyping of replicator and foreman code.    Installed Nova Client on foreman instance so it can start and stop replicator instances  as needed at runtime. Tested.    Began a project to simulate realistic camera data from LSST to be used for various timings of DMCS prototype code. This was also a concentrated effort to learn about the nature of astronomical data, how it is represented, its various types of data, and practice with the C and Python libraries for building FITS files. Two sample files were built: One additively built and converted 16bit DES data into scaled 18bit data, then placed that binary representation in a 32bit integer, and the other was additively built up from PhoSim data. Files were built to represent a single LSST camera raft with 9 HDUs - one for each CCD. Files were over 600Mg. The code used in these files has been saved and parts turned into scripts for generating other sample files.      Set up an environment for timing the imprint of these files as they are built and processed through the Base DMCS pipeline code. Initial timing was done on compression techniques.     Wrote result paper on FITS file generation and timing. Errors due primarily to my own ignorance of this type of file format and how the data was truly represented were encountered several times. This project was personally extremely helpful in beginning to learn the vast amount of domain knowledge needed to complete future pieces of the L1 Base site code package.      Added coding for setting up raw data to be moved and assembled into fits files in DMCS , simulating part of the Base DMCS data flow, into existing Base replicator/foreman prototype.    ",70
"Functional drawings, specification writing, and info gathering 
Initial meeting with Don about spec work   Began gathering architecture ideas for Base DMCS 4    Discussed requirements gathering for Base site network operations with  Steve P., Paul, and Don.   1    Began specification draft for base site, and integrated drafts into project wiki 4    More spec ideas and posted them on Dons pages under the OCS Bridge page.  4    DMCS Planning meetings begin in earnest.  16    Planned DMCS diagrams for L1 base site and NCSA site as a group and drew them in support of Margaret’s trip to DM meeting. These were refactored a couple of times and are in confluence.  8    Camera meeting: about 1",38
"Bug fix and improvement for DECam processing
- Bug fix in DecamMapper policy of fringe dataset  - Improve readme documentation about ingesting and processing raw data  - Bug fix on translating Community Pipeline's Bad Pixel Mask (BPM) --- Previously in DM-4191 I looked up the wrong table for the BPM bit definition.  - Flag the potentially problematic edge pixels as SUSPECT (DM-4515)  - Add data products for coaddition processing",6
"Local LSST Sec Meeting
nan",1
"Local LSST IaM Meeting
nan",1
"Security Playbook
nan",2
"Security Plan Renewal
nan",3
"Convert basic table functionalities to JS.
Task includes server-side json conversion, data modeling, and a simple React table for presentation.",20
"Table (JS): selection feature.
This task is composed of:  - converting java class SelectionInfo.  - reducing data into its table model state  - rendering SelectionInfo onto the TablePanel  - creating action and action creator",6
"Upgrade to the latest react-highcharts library
We need to upgrade from the early version of react-highcharts to the latest one, compatible with React 0.14.3. Just switching to the new library does not work, need to resolve issues.",6
"Table (JS): large table handling
This task is composed of:  - creating and adding a paging toolbar to TablePanel  - adding external data loading feature to TablePanel  - creating prefetch and background data fetching mechanism  - use websocket events for reporting background statuses    - requires new server-side code.    - depends on  DM-4578 - Integrate websocket messaging into flux",14
"Table (JS): sorting
This task is composed of:  - introduce sorting feature into TablePanel  - creating action and action creator  - reducing data into its table model state",4
"Histogram View of a Table
Combine HistogramOptions and Histogram widgets into a Histogram viewer:  - define histogram state tree, actions (getting/updating table statistics, getting/updating column data), and reducers  - write a smart widget, which shows options and histogram side-by-side",10
"Table (JS): filtering
This task is composed of:  - adding filter toolbar into TablePanel  - filter validation syntax  - creating action, action creator, and reducing data into its table model state  - -generating meta info for enumerated columns-  not sure if we wanted this.    Also, added actOn feature to FieldInput.",6
"Suggestion Box widget
We need to find or implement a suggestion box widget in JS. Currently, it is used to suggest table column names in XY plot and in some forms.",6
"Table (JS): table options
This task is composed of:  - adding table options panel to TablePanel.  - providing features:    - show/hide units in header    - show/hide columns, reset to defaults, etc    - page size",5
"JS expression parsing library
Since we are allowing column expressions we need a way to validate them on client side.",6
"Table (JS): text view
This task is composed of:  - adding text view option to TablePanel",2
"XY Plot view of the table (JS): define state tree
Define state tree, actions, and reducers for XY Plot view of the table.",4
"XY Scatter Plot (JS) 
Implement basic scatter plot widget using react-highcharts library",8
"XY Scatter Plot Options (JS)
nan",10
"Integrate websocket messaging into flux
This task is composed of:  - design and implement messaging concept into flux    Implementation thoughts:  - convert inbound messages into actions  - convert selected actions into outbound messages  - add message action reducer with the concept of a message consumer.    - consumer can be a predefined action creator or a function      - allow consumers to be added/removed into/from the system after bootstrap",10
"XY Plot view of a table (JS)
Implement smart widget which shows toolbar, plot options, and XY plot.",10
"XY Plot view of a table (JS) - Toolbar
Toolbar, which toggles plot options, selection and filter buttons    Extra:   - handling zoom from the toolbar rather than using built-in zoom  - ability to switch between histogram and scatter plot view",8
"XY Plot Viewer (JS) - density plot
Implement density plot using react-highchart library (Highchart's canvas-based heat map).",12
"XY Plot View of a table (JS) - selection support
Show/change selected/highlighted points. Ideally, this should be done without redrawing the whole plot. ",8
"SUIT: search returning images in a directory
- Create a sample search processor, which returns images in a given directory.  - It should be using an external python task  - Update search form configuration to use this search processor to return image metadata",2
"XY plot view of a table (JS) - density plot zoom support
Density plot zooming requires server call.",6
"XY Plot view of a table (JS) - density plot selection support (?)
density plot - how do we support selection?    (In current version we turn off selection support when the plot is density plot)",6
"GWT Conversion: Login
This task is composed of:  - adding user info into banner    - includes user name and links for login, logout, and profile.  - convert server-side code to return json  - use messaging to handle current user state.    - depends on DM-4578	Integrate websocket messaging into flux",4
"GWT Conversion: Search Panel
This task is composed of:  - converting server-side code to return json  - defining and loading search info data into application state    - loading should be implemented so that it can be from a server fetch or a client declaration  - creating action, action creator and reducing functions  - rendering SearchPanel from search info data    - attach SearchPanel to application:  depends on GWT Conversion: layout",12
"Create ctrl_platform_nebula package to exercise ctrl_orca orchestration on Nebula
We create a ctrl_platform_nebula package to support processing with the LSST framework as orchestrated by the ctrl_orca/ctrl_execute  packages within a HTCondor pool that resides on the Nebula OpenStack.",18
"GWT Conversion: basic layout for results
This task is composed of:  - creating a results container that handle the layout of its components  - define and load layout info into application state  - creating action, action creator, and reducing functions  - components include:    - vis toolbar    - last searched description    - layout options: tri-view.  side-by-side, single and popout can be added at a later time.    - tables, image plots, xy plots.  - depends on DM-4590: GWT Conversion: advance resizable layout panel",4
"GWT Conversion: advance resizable layout panel
Create an advance React component for layout.  Features should include:  - a set of predefined layouts  - resize strategies  - generic for reuse",6
"GWT conversion: System notifications
This task is composed of:  - adding notification panel to the application  - convert server-side code to use messaging for notifications  - use messaging on client-side to handle notifications  - creating action, action creator, and reducing functions  - depends on DM-4578	Integrate websocket messaging into flux  ",3
"GWT Conversion: History and routing
First pass at the implementation of history and routing.  Define a framework in which the application can be:  - bookmarked  - record state in history  - retore application from a url  ",4
"Remove deprecated versions of warpExposure and warpImage
afw.math supports two templated variants of warpExposure and warpImage, one that takes a warping control and the other which does not. The latter have been deprecated for a long time and are no longer used. I think it is time to remove them.",1
"Build docs.lsst.io Doc Index Page
Create an HTML landing page for all DM documentation/documents    - Software Documentation  - Developer Guides  - Requirement and Design Documentation  - Technotes  - Papers  - Presentations    The page will be implemented as a static site. The page build will be template driven, with content scraped from the metadata.yaml resources of technotes (among other sources).    Since this is the first SQuaRE web project, this ticket will also involve effort in establishing a CSS+HTML pattern library and Gulp-based development workflows. Long term, this investment will be returned with new dm.lsst.org, technote and Sphinx documentation web designs.",2
"sconsUtils tests should depend on shebang target
Some tests rely on code in the {{bin}} directory. Whilst these tests have been modified to use {{bin.src}} the general feeling is that the test code should be able to rely upon the {{shebang}} target having been executed before they are run.",1
"make codekit repos.yaml aware
Up to now codekit assumed the repo name is the same as the eups package name. FIx it by using repos.yaml. ",1
"Partition package should use the standard package layout
The partition package does not build on OS X El Capitan because the package is not laid out in the standard manner and whilst {{sconsUtils}} is used most of the default behaviors are over-ridden. This means that fixes implemented for DM-3200 do not migrate over to {{partition}}. I think the best approach would be to reorganize the package so that it does build in the normal way.",1
"Research Kerberos and LDAP replication options
IAM components, including the Kerberos KDC, need to be replicated between NCSA and Chile machine rooms. This may impact whether LSST can use NCSA's production Kerberos instance (if it supports selective replication) or needs a separate Kerberos instance that can be replicated outside NCSA. This task is to research and document the options, in consultation with NCSA Kerberos admins, and propose a Kerberos replication approach.",1
"Receive, verify and test network equipment
nan",4
"Install networking hardware into openstack and verify operation
nan",4
"Verify Network Emulator operation
nan",4
"Design network emulator integration into workflow
nan",2
"Deploy heirarchical queuing to test image precedence
Once we have a working workflow ready to move data between the ""base site"" and ""archive site"" (both at NCSA), deploy a base site exit router with stacked queuing to test prioritization of image traffic over various network conditions.",8
"Migrate Qserv code to stream-based logging
Migrate Qserv code from LOGF_* to LOGS_*.     While doing it, we will also revisit logging levels: in particular we are abusing INFO, most of what is now in INFO should be on DEBUG, in some places where INFO is used to cover unusual conditions, it should go to WARNING.     Further, we will unify how we initialize logging structures. Per discussions at 2015/12/09 Qserv meeting, we like best {code}LOG_LOGGER _log = LOG_GET(""lsst.qserv.<module>.<file>""){code} in anonymous namespace in cc files. Logging from .hh files is strongly discouraged.    This involves changing ~600 places.",10
"Send all chunk-queries to primary copy of the chunk
We are planning to distribute chunks / replicas across worker nodes such that each node will have a mix of primary copies for some chunks, and backup copies for some chunks. While doing shared scan, we are going to always rely on the primary chunks (e.g., all queries that need a given chunk should be sent to the same machine so that we read that chunks only once on one node). This story involves tweaking xrootd to ensure we don't send chunk-queries to nodes hosting non-primary copies.",5
"Design for butler support of multiple repositories
Work on design for Gregory/SUI's request:    We need to understand how put()/writing works when multiple repositories are made visible through a single Butler.  For get()/reading a single search order makes sense.  For put() it may be desirable to support alternative destinations (local disk, user workspace, Level 3 DB) or even multiple destinations for a single put().",6
"Explore coadd processing with DECam data with default config
Starting with raw DECam data, perform single frame processing and then try image coaddition with a few visits of images. ",6
"Provide input to CalibrateTask design work
Provide requirements and advice as input to the effort to redesign CalibrateTask (DM-3881).",10
"Create IDL pipeline workflow for DRP processing - processCcdDecam
For the verification datasets work we need the ability to run a dataset all the way through DRP processing that takes advantage of many cores (orchestration), keeps track of successful and failed dataIDs, creates individual log files, and creates helpful QA plots/metrics/webpages.  Nidever will use his PHOTRED IDL workflow and rewrite it for the stack.  The first step is processCcdDecam.",5
"Create IDL pipeline workflow for DRP processing - makeCoaddTempExp
For the verification datasets work we need the ability to run a dataset all the way through DRP processing that takes advantage of many cores (orchestration), keeps track of successful and failed dataIDs, creates individual log files, and creates helpful QA plots/metrics/webpages. Nidever will use his PHOTRED IDL workflow and rewrite it for the stack.  processCcdDecam is working.  The next step is makeCoaddTempExp.  ",4
"lsstsw should symlink afwdata or allow an envvar
To reduce disk usage, it is very handy to be able to make build/afwdata and stack/afwdata/BLAH be symlinks. build/ is easy: just make the symlink and then never have touch it unless you rm your whole stack. stack/afwdata/BLAH is harder: each time you rebuild something that depends on afwdata, it will install a new copy of afwdata, which you'll have to manually remove and declare your symlink with a new tag.    A couple of ways to make this more automatic:     * lsstsw checks whether build/afwdata is a symlink, and if so just makes the new stack ""install"" a symlink to the same directory.   * Check for some environment variable (e.g. AFWDATA_BASE_DIR or something) and if that exists, just make a symlink to it, or make a dummy eups table that points to that directory and don't put anything in stack at all.",5
"modernize afw code and reduce doxygen errors
I would like to make some simple modernizations afw code and reduce doxygen warnings as much as practical. The modernizations I had in mind were:  - Move doc strings from .cc files to .h files and standardize their format  - Use {{namespace lsst { namespace afw { ...}} in .cc files to make the code easier to read  - Eliminate all {{<Class>::Ptr}} and {{<Class>::ConstPtr}} typedefs (replacing with {{PTR(<Class>)}} and {{CONST_PTR(<Class>)}}).  - Make sure .py files import the appropriate packages from future and (where practical) pass the flake8 linter    Regarding doxygen warnings: I think moving the documentation to .h files will help in many cases. Some warnings may be impractical to fix, such as complaining about not documenting ""cls"" for python class methods.  ",6
"Implement mouse interaction with the drawing infrastructure
nan",8
"Migrate scisql and mysqlproxy to mariadb
MySQLproxy and SciSQL relies on MySQL, they should now move to MariaDB",4
"Add utility function to handle client-side download requests.
Create utility function to handle client-side download requests.  It needs to be done in a way that does not mess with history and current page state.",1
"Add workflow code to lsst-dm github
Since we have split the code for Super Task, all the workflow code should  go in a different repository",2
"L1-CONOPS
Contribute to L1-CONOPS document",10
"investigate replicating EUPS published packages
(This ticket is for work that has already been done, per internal discussion in SQRE, but accidentally without an open ticket)    https://github.com/lsst-sqre/lsyncd-eupspkg  https://github.com/lsst-sqre/sandbox-pkg",4
"Support sqlalchemy use with qserv
When one tries to connect to qserv using sqlalchemy there is an exception generated currently:  {noformat}  $ python -c 'import sqlalchemy; sqlalchemy.create_engine(""mysql+mysqldb://qsmaster@127.0.0.1:4040/test"").connect()'  /u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/engine/default.py:298: SAWarning: Exception attempting to detect unicode returns: InterfaceError(""(_mysql_exceptions.InterfaceError) (-1, 'error totally whack')"",)    ""detect unicode returns: %r"" % de)  Traceback (most recent call last):    File ""<string>"", line 1, in <module>    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 2018, in connect      return self._connection_cls(self, **kwargs)    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 72, in __init__      if connection is not None else engine.raw_connection()    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 2104, in raw_connection      self.pool.unique_connection, _connection)    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 2078, in _wrap_pool_connect      e, dialect, self)    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 1405, in _handle_dbapi_exception_noconnection      exc_info    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/util/compat.py"", line 199, in raise_from_cause      reraise(type(exception), exception, tb=exc_tb)    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 2074, in _wrap_pool_connect      return fn()    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/pool.py"", line 318, in unique_connection      return _ConnectionFairy._checkout(self)    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/pool.py"", line 713, in _checkout      fairy = _ConnectionRecord.checkout(pool)    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/pool.py"", line 480, in checkout      rec = pool._do_get()    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/pool.py"", line 1060, in _do_get      self._dec_overflow()    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/util/langhelpers.py"", line 60, in __exit__      compat.reraise(exc_type, exc_value, exc_tb)    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/pool.py"", line 1057, in _do_get      return self._create_connection()    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/pool.py"", line 323, in _create_connection      return _ConnectionRecord(self)    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/pool.py"", line 454, in __init__      exec_once(self.connection, self)    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/event/attr.py"", line 246, in exec_once      self(*args, **kw)    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/event/attr.py"", line 256, in __call__      fn(*args, **kw)    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/util/langhelpers.py"", line 1312, in go      return once_fn(*arg, **kw)    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/engine/strategies.py"", line 165, in first_connect      dialect.initialize(c)    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/dialects/mysql/base.py"", line 2626, in initialize      default.DefaultDialect.initialize(self, connection)    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/engine/default.py"", line 256, in initialize      self._check_unicode_description(connection):    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/engine/default.py"", line 343, in _check_unicode_description      ]).compile(dialect=self)    File ""/u2/salnikov/STACK/Linux64/mysqlpython/1.2.3.lsst1/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/cursors.py"", line 174, in execute      self.errorhandler(self, exc, value)    File ""/u2/salnikov/STACK/Linux64/mysqlpython/1.2.3.lsst1/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/connections.py"", line 36, in defaulterrorhandler      raise errorclass, errorvalue  sqlalchemy.exc.InterfaceError: (_mysql_exceptions.InterfaceError) (-1, 'error totally whack')  {noformat}    The reason for that is that sqlalchemy generate few SELECT queries to figure out unicode support by the engine, and those selects are passed to qserv which cannot parse them. Here is the list of SELECTs which appears in proxy log:  {code:sql}  SELECT CAST('test plain returns' AS CHAR(60)) AS anon_1  SELECT CAST('test unicode returns' AS CHAR(60)) AS anon_1  SELECT CAST('test collated returns' AS CHAR CHARACTER SET utf8) COLLATE utf8_bin AS anon_1  SELECT 'x' AS some_label  {code}",3
"Create and rename the sims_dustmaps repository to sims_maps
Create and rename the sims_dustmaps repository to sims_maps.    This is my plan after talking to [~jhoblitt]:    Add sims_maps to {{lsst_build/repos.yml}}. Change related dependencies and create ticket branches, run CI to confirm the changes.",2
"Update sims_dustmaps/sims_maps repository to use git-lfs
Update sims_dustmaps to use git-lfs.",1
"Convert GWT code to pure JavaScript (X16)
We plan to continue the GWT to JS conversion in Summer 16. The goal is to finish it.",100
"CI debugging
diagnosing build failures and refreshing build slaves",1
"Ci Deploy and Distribution Improvements part III
nan",60
"Port code style guidelines to new DM Developer Guide
Verbatim port of DM Coding style guidelines to Sphinx doc platform from Confluence.    - https://confluence.lsstcorp.org/display/LDMDG/DM+Coding+Style+Policy  - https://confluence.lsstcorp.org/display/LDMDG/Python+Coding+Standard  - https://confluence.lsstcorp.org/pages/viewpage.action?pageId=16908666 and contents    I’m unclear whether these pages should be included:    - https://confluence.lsstcorp.org/pages/viewpage.action?pageId=20283399 (C++ ‘using’)  - https://confluence.lsstcorp.org/pages/viewpage.action?pageId=20284190 (how to use C++ templates)  - https://confluence.lsstcorp.org/pages/viewpage.action?pageId=20283399 (C++11/14; which should seem to belong in the code style guide)    Any temptation to amend and update the style guideline content will be avoided.",5
"Port RFC/RFD/Decision Making Page to new docs
Port to new Sphinx docs: https://confluence.lsstcorp.org/display/LDMDG/Discussion+and+Decision+Making+Process?src=contextnavpagetreemode",3
"Read and extend SuperTask technical note
Read the current version of DMTN-002 and comment.    Write new sections describing the overall architecture and the expected role of SuperTask in the system.",3
"Review existing CmdLineTask instances' inputs and outputs
Review most or all existing DM CmdLineTask subclasses to understand their external inputs and outputs.  This will inform the design of the successor to the interim SuperTask.execute( DataRef ) interface.    The issue is that the single-DataRef interface supports only 1:1 input:output relationships, or N:1 relationships where a list of inputs is derivable from an output dataid.  This is believed to be insufficiently general.",6
"Improve sphgeom documentation
Per RFC-117, the sphgeom package needs decent overview documentation, linked from the top-level README.md. The doxygen output should also be reviewed.",2
"audit obs_subaru defaults and move them to lower-level code
The obs_subaru config overrides contain many useful settings that aren't actually specific to HSC or Suprimecam.  These should be moved down to the low-level defaults in the config classes themselves, so new obs_ packages don't have to copy these configurations explicitly.",2
"configure WebDAV with Kerberos/LDAP on lsst-auth1
Configure WebDAV for Kerberos authentication and LDAP authorization. Create example subdirectories where LDAP groups determine access (using .htaccess files):  * lsst: anyone in lsst group can read/write to this directory  * ncsa: anyone in all_ncsa_employe can write, anyone in lsst can read",6
"IAM process for managing L3 data access
Document a process for managing access to L3 Data Products.    On the wiki: https://confluence.lsstcorp.org/display/LAAIM/Managing+L3+Data+Access",1
"Prototype LSST User/Group Manager
nan",1
"Design Interfaces for Memory Management for Shared Scans
Part of the shared scans involve memory management - a system that will be used by Qserv that will manage memory allocation / pin chunks in memory. This story involves designing the API between Qserv and the memory management system.  ",8
"qserv/cfg has to be removed by ""scons -c""
qserv-meta.cong was still pointing on MySQL instead of MariaDB, even after running ""scons -c"". This error-prone behaviour should be fixed.",2
"work with database team to exercise all the APIs for data access (F16)
SUI will continue to work with database team to exercise all the APIs for data access. All known issues should be worked out in S16 cycle.",40
"Provide API for tabular data display using Firefly
We need to provide JavaScript API access to all the table displaying features to give user more control when using Firefly API to displaying table data in their own web page or to build customized web UI ",10
"Provide a prototype version of LSST web UI 
SUIT deployment at NCSA to access SDSS strip82 data processed by DM stack in 2013.  * Use the data access API, or TAP API  * Light curve for time series data  * Connection between the light curve data point and the image that the data point coming from  ",60
"Implementation of multiple repositories v1
nan",25
"Implementation of multiple repositories v2
nan",15
"Changed the implementation of HistogramProcessorTest due to the minor change about the algorithm in the HistogramProcessor
In Histogram, when the data points fall on the bin edges,  the following rules are used:  #  For each bin, it contains the data points fall inside the bin and the data point fall on the left edge.  For example, if binSize=2, the bin[0] is in the range of [0,2].  The data value 0 is in bin[0] .  #  For each bin, the data point falls on the right edge is not included in the number point count. For example if binSize=2, the bin[0] is having the range of [0,2].  The data value 0 is in bin[0] but the data value 2 is not in the bin[0].  # For the last bin, the data points fall inside the bin or fall on the left or right bin are counted as the number of bin points.    The last rule is newly introduced.      ",2
"Firefly visualization Java/JS code refactoring and bug fixes(F16) 
This epic will capture all the Java and JS code refactoring in Firefly, bug fixes, JS code optimization and performance enhancement. ",80
"Design worker scheduler for shared scans
nan",8
"Data Distrib proto (Jan)
nan",25
"Refactor ProcessCcdTask and sub-tasks
Based on conversation spurred by DM-3881 as discussed [on clo|https://community.lsst.org/t/requirements-for-overhauled-calibration-task/370], this ticket will refactor ProcessCcdTask to be easier to extend and instrument, easier to understand, and more modular.    The main work will be to break up ProcessCcdTask into it's component modules and, and reconfigured to meet the requirements as outlined by the clo discussion.",16
"distributed loader
nan",12
"Implement memory mgmt for shared scans
nan",12
"Implement worker scheduler for shared scans
nan",20
"Add initial butler support for remote PUT
For Get:  If the mapper returns a URL:  retrieve the URL contents into a file  return the path to the file.    This will be optimized in subsequent stories by ""add read support for various transports to the afw object readers"". This is a degenerate case that will be used if the object does not have a reader for a given transport protocol.    Need to solve the cleanup problem of when to delete the file that was downloaded.    For Put:  serialize the object to a temporary file  transfer the file to the URL",10
"Create ALERT framework for qserv
At the moment Qserv code will throw exception when something wrong/unusual happens. That is not always the best idea to do in a server code that is needs to run 24x7. If we don't throw exception and just log the issue, it might get unnoticed in the log files. So, it'd be useful to have some sort of alert framework where we could send alerts when something strange / unexpected happens in Qserv code and we are able to ""ignore it"" and continue running the server. It can be as naive as writing to a special place, or sending an email, or messaging the DBA etc. This story involves designing and implementing such framework. The sooner we do it the better so that we don't accumulate new code that is throwing exceptions where it should not.",10
"Revisit Qserv code that throws exceptions
We have ~500 places where we throw exceptions in qserv/core/modules/*/*. Revisit all of them and make sure we catch these exceptions properly.",15
"Promote IsrTask to command line task.
As pointed out by [~nidever] in DM-4635, it would be quite useful to have the IsrTask callable as a command line task without having to do all the other steps in ProcessCoaddTask.",4
"Promote CharacterizationTask to command line task
In refactoring the processCcd.py script, we'd like to make each component callable by command line as well.  This is to promote the image characterization task to a command line task.  A requirement will be that this task be able to run on data without IsrTask having been run (command line tasks should be insulated as much as possible from knowing about previous processing).",4
"Promote CalibrateTask to command line task
The task that takes care of measurement and calibration on characterized images will be promoted to a command line task.  As with the other command line tasks, it should be possible to run the calibration and measurement command line task on data without necessarily running IsrTask or CharacterizeTask.    Of course, this means the task will have to get a PSF from somewhere, see [clo|https://community.lsst.org/t/requirements-for-overhauled-calibration-task/370] for suggestions.",4
"Qserv integration tests fail on CentOS7 with gcc 4.8.5
The version of gcc that ships with CentOS7, {{gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-4)}}, appears to miscompile the qserv worker source in a way that makes it impossible to actually run queries. Installing {{devtoolset-3-toolchain}} and {{devtoolset-3-perftools}} via {{yum}} to get gcc 4.9 resolves the issue.",2
"qdisp/testQDisp fails with mariadb
Fabrice fried to build qserv with mariadb and it caused failure in one of the unit test: qdisp/testQDisp with the message:  {noformat}  pure virtual method called  terminate called without an active exception  {noformat}    Runnig it with GDB it' obvious that there is a problem with resource lifetime management in qdisp/testQDisp.cc. The problem is that XrdSsiSessionMock is destroyed sooner than other objects that use it.     One way to resolve this problem is to instantiate XrdSsiSessionMock earlier than other objects that use it (to reverse the order of destructors), possibly make it a global instance.    Big mystery here is how mariadb could trigger this interesting behavior and why did not we see this earlier.",1
"Rerun and create a repository for CFHT astrometry test.
Understand, re-rerun, and recreate clean version of [~boutigny] 's CFHT astrometry test for the astrometry RMS for two sample CFHT observations.  This test is on the NCSA machines in  /lsst8/boutigny/valid_cfht    Create a repository for this test with an eye toward it becoming integrated in a validation suite for the stack.        ",1
"Adapt CFHT astrometry test for DECam COSMOS field validation
Adapt the CFHT astrometry validation test to the DECam reprocessing effort.      Will focus on the repeat observations of the COSMOS field.  Goal is to just do a simple two-observation comparison.  Doing a full test of all of the observations will be a later story.  ",1
"Integrate astrometry test into SDSS demo lsst_dm_stack_demo
Incorporate the astrometry test as an optional component in lsst_dm_stack_demo.    This is chosen because lsst_dm_stack_demo currently serves as the very loose stack validation and understanding how to do astrometric repeatibility testing in this demo will help explore how it would make sense to put in a fuller CFHT validation test of the DM stack.",1
"Prototype a validation module of the stack using CFHT data.
Create a prototype standalone validation test of the astrometric performance of the stack on suitable CFHT data.  Module is called `validate_drp`     http://github.com/lsst/validate_drp_cfht    Decide how those data should be provided (testdata_cfht being one obvious possibility), and determine if obs_cfht tests and the tests for this validate_drp module should use the same test datasets.    This is prototyping for DM-2518.",1
"host identification info needs to be part of log message
The EventAppender needs to add host identification (host/process/id) information to the log message it transmits.   This was inadvertently left out.",3
"Edit testdata_cfht to pass obs_cfht unit tests
This ticket covers the first half of the issues in DM-2917.     {{testdata_cfht}} was left unedited while some past changes in {{obs_cfht}} {{MegacamMapper}} required coordinated changes.  The goal of this ticket is to simply pass the unit tests currently in {{obs_cfht}}.   ",1
"Fix documentation and restructure workflowTask
nan",2
"Improve documentation on pipe_base/supertask
nan",2
"Bad OpenBlas setting in miniconda/numpy causes very poor performance for running multiple processes
I have been running many processes of processCcdDecam.py on my new linux machine in Tucson (bambam).  To my surprise, running 40 processes at once gets very poor performance (~70 sec per process) compared to running a single process (~16 sec). I expected some performance hit because of larger overheads but not a factor of 4!    I ran it both on a spinning HDD and PCIe SSD but they both had the same problem.  I also tried running it on multiple visits versus multiple chips for a single visit (all accessing the same MEF FITS file) but this made no difference.  I tested it with various numbers of processes and found that the time per processes increases linearly with the number of processes running.      [~jmatt] has been helping me track this down.  We used some performance tools (htop, iotop, and perf) to figure out what was going on.  It was clear that the issue was not a RAM or I/O problem.  By watching htop while the 40 processes were running it became clear that once some of the processes hit ""deblending"" everything slowed down considerably and all cores were maxed out and showing lots of kernel traffic.  I also ran processCcdDecam.py with deblending turned off and the performance was much more reasonable (~24 sec. per process).    After more digging (with perftop), we found that there was a lot of swapping going on during the deblending step by ""openblas"".  This is a package that numpy uses for speeding up certain computationally intensive tasks using multithreading (e.g. linear algebra).  By default each openblas instance takes advantage of ALL cores on a machine.  So all 40 processes were trying to use all available cores and most of the time was spent swapping between all of these threads.    OpenBlas can be configured to use a more reasonable number of cores/threads, but the version that LSSTSW uses is installed by miniconda via a dependency of numpy and, as far as we could tell, it's not possible to configured NUM_THREADS for OpenBlas with miniconda.    We ended up compiling our own version of OpenBlas with NUM_THREADS = 6 (the maximum threads that OpenBlas uses) and the performance was great, 24 sec.    I'm not sure what the solution is for this but we probably don't want to go with miniconda for the default LSSTSW installation (uses currently done by bin/deploy).    JMatt might have comments to add.   ",4
"Track down reason for slow performance when running many jobs of processCcdDEcam on bambam
During the processing of the COSMOS data for the verification dataset work I ran many jobs of processCcdDecam.py on the new linux server, bambam.  The performance was very slow, 4x longer than running a single job at a time.  Figure out what is going on.",2
"Meetings Dec 2015
Verification dataset meetings, RFD meetings, DES Chicagoland meeting and preparation",6
"Other LOE -- Dec 2015
weekly LSST local grouop meetings, NCSA meetings (All-hands, software, etc), code review, other local meetings, postdoc meetings and tasks",6
"Vendor input on sizing predictions
Discussions about tape-pricing and disk-pricing predictions from Spectra and DDN respectively in order to improve our forecasting. This information needs to be incorporated into LDM-144.",2
"More preparation for FY16 hardware
More pricing iterations with several companies and incorporating that information into our final decision. More Q&A with storage companies re: comparable features. Compiled all storage option quotes into a spread sheet which now forms as a good comparison and helps LDM-144  forecasting.     Awaiting quotes for racks and PDUs. Now that rack size is known for the Chilean DC, this will serve us well for LDM-144 costs.     Power issues for FY16 hardware are settled and we are ready to schedule installation as soon as the hardware purchase contract is complete.    Tagging issues for FY15 hardware complete. Looking into pre FY15 tagging. Requested that LSST/Aura perform an inventory request to complete the circle and prove the process.    Working with Spectra / NetSource to create a sustainable tape condo that can serve LSST through 2030.",4
"Plan DM’s communication / documentation / information architecture strategy
Plan and write a technote outlining communication and documentation platforms from a DM perspective. The technote will specify    - how each platform is used  - what developments need to be done  - address integrations with LSST-wide communications projects  - address information architecture (generally, the ease of discovering the right information)",2
"File tickets for list of stack deficiencies and suggested upgrades
K-T suggested that I take my list of ""stack deficiencies and suggested improvements"" [https://confluence.lsstcorp.org/display/SQRE/Stack+Deficiencies+and+Suggested+Upgrades] on confluence and (with Tim J.'s help) create tickets for each item (as much as possible) so that the work could be scheduled.  ",3
"Continue learning about middleware
Learn more about orchestration, task execution, and logging.  ",3
"Implement zenodio.harvest
Harvest metadata about records in a Zenodo Community collection using the {{oai_datacite3}} format. See https://zenodo.org/dev    Part of the [zenodio|https://github.com/lsst-sqre/zenodio] Python package. This tool will be used by our technote and the documentation indexing platforms.",3
"Doxygen package fails to build with flex 2.6
To wit:    {code}  $ flex --version  flex 2.6.0    $ bash newinstall.sh    LSST Software Stack Builder  [...stuff...]  eups distrib: Failed to build doxygen-1.8.5.eupspkg: Command:  	source /Users/jds/Projects/Astronomy/LSST/stack/eups/bin/setups.sh; export EUPS_PATH=/Users/jds/Projects/Astronomy/LSST/stack; (/Users/jds/Projects/Astronomy/LSST/stack/EupsBuildDir/DarwinX86/doxygen-1.8.5/build.sh) >> /Users/jds/Projects/Astronomy/LSST/stack/EupsBuildDir/DarwinX86/doxygen-1.8.5/build.log 2>&1 4>/Users/jds/Projects/Astronomy/LSST/stack/EupsBuildDir/DarwinX86/doxygen-1.8.5/build.msg  exited with code 252    $ grep error /Users/jds/Projects/Astronomy/LSST/stack/EupsBuildDir/DarwinX86/doxygen-1.8.5/build.log  commentscan.l:1064:55: error: use of undeclared identifier 'yy_current_buffer'  commentscan.l:1126:58: error: use of undeclared identifier 'yy_current_buffer'  {code}    Builds fine using {{flex 2.5.35 Apple(flex-31)}}.",1
"HSC backport: Add functions to generate 'unpacked matches' in a Catalog
The qa analysis script under development (see DM-4393) calls to HSC {{hscPipeBase}}'s [matches.py|https://github.com/HyperSuprime-Cam/hscPipeBase/blob/master/python/hsc/pipe/base/matches.py] which adds functions to generate ""unpacked matches"" in a Catalog (and vice versa).  It will be ported into {{lsst.afw.table}}.    The port includes following HSC commits:  *Add functions to generate 'unpacked matches' in a Catalog.*  https://github.com/HyperSuprime-Cam/hscPipeBase/commit/210fcdc6e1d19219e2d9365adeefd9289b2e1186    *Adding check to prevent more obscure error.*  https://github.com/HyperSuprime-Cam/hscPipeBase/commit/344a96de741cd5aafb5e368f7fa59fa248305af5    *Some little error handling helps.*  https://github.com/HyperSuprime-Cam/hscPipeBase/commit/61cc053b873d42802581adff8cbbdb52a348879e  (from branch: {{stage-ncsa-3}})    *matches: add ArrayI to list of field types that require a size*  https://github.com/HyperSuprime-Cam/hscPipeBase/commit/d4ccd11d8afbcdd9cf0b35eba948cca4b5d09ba5  (from branch {{tickets/HSC-1228}})    Please also include a unittest.",3
"Adapt qa analysis script for LSST vs. HSC single visit processing comparison
The qa analysis script ported from HSC and adapted to LSST on DM-4393 currently performs qa on single visit processing by comparing outputs of a single run from the different measurement algorithms and comparing those with the astrometry/photemetry reference catalog.  Here we will add functionality to directly compare two different runs of the same dataset (requiring accommodations for two butlers).  Since the goal is to compare outputs from runs on the LSST vs. HSC stacks, this will require a mapping of the different schemas of the persisted source catalogs of the two stacks.",10
"Add labels to qa analysis plots for better interpretation
The plots output by the qa analysis script (see DM-4393) currently do not display any information regarding the selection/rejection criteria used in making the figures and computing the basic statistics.  This includes magnitude and clipping thresholds.  This information should be added to each plot such that the figures can be interpreted properly.",2
"lsst-build should support enabling Git LFS in an already-cloned repository
A repository which does not use Git LFS is created and described in {{repos.yaml}}. It runs through CI, and is cloned onto a Jenkins build slave. Subsequently, the repository configuration in {{repos.yaml}} is updated to enable LFS. The build system should notice this change and update the cloned repository on disk appropriately. Currently, it doesn't.",1
"afw fails to build on a machine with many cores
The afw package does not build reliably (if at all) on a linux box at UW (""magneto"", which has 32 cores and 128 Gb of RAM). The failure is that some unit tests fail with the following error:  {code}      OpenBLAS: pthread_creat error in blas_thread_init function. Error code:11  {code}    For the record, /usr/include/bits/local_lim.h contains this:  {code}  /* The number of threads per process.  */  #define _POSIX_THREAD_THREADS_MAX	64  /* We have no predefined limit on the number of threads.  */  #undef PTHREAD_THREADS_MAX  {code}    It appears that the build system is trying to use too many threads when building afw, which presumably means it is trying to use too many cores. According to [~mjuric] the package responsible for this is {{eupspkg}}, and it tries to use all available cores.    A workaround suggested by [~mjuric] is to set environment variable {{EUPSPKG_NJOBS}} to the max number of cores wanted. However, I suggest we fix our build system so that setting this variable is unnecessary. I suggest we hard-code an upper limit for now, though fancier logic is certainly possible.    A related request is to document the environment variables that control our build system. I searched for {{NJOBS}} on confluence and found nothing.",1
"Remove dead code from configuration procedure
 - remove scratch db?  - cleanup tmp/sql/*.sql filesi  - remove xrootd configuration script if useless?   - cleanup configuration script style (i.e. tmp/*.sh)  ",3
"Study if mysqlproxy can be compatible with mariaDB client
mysqlproxy is not compliant with mariaDB client: see https://mariadb.com/kb/en/mariadb/mariadb-vs-mysql-compatibility/#incompatibilities-between-mariadb-and-mysql-proxy    Nevertheless the trivial fix proposed (remove progress-report options) doesn't seems to work...  {code:bash}    mysql --host=127.0.0.1 --port=4040 --user=qsmaster --batch qservTest_case01_qserv    ERROR 1043 (08S01): Bad handshake  {code}",4
"Improve 'unit' tests using database
- Add mock database for it to work during unit tests or run it apart from unit tests?  - Fix testLocalInfile (read configuration file)  - Try all core/modules/sql/testSql*           ",5
" Improve  LOAD LOCAL INFILE management on czar side
Option ""mysql_options( m, MYSQL_OPT_LOCAL_INFILE, 0 );"" is added to all C++ sql client instance due to common sql interface, but is only required on master (for merging results) is it possible:   - to remove LOCAL keyword (on czar virtfile is on the same machine that mariadb server)  - or to set it in Qserv czar/master configuration  - or to set it in master MariaDB instance only?",4
"Update kernel on IN2P3 cluster
The ""Kernel Panic"" issue is non-blocking right-now for John due to machine automated reboot, but we have to solve it to target a stable production system.    With Yvan, we're converging on next update for the cluster:    - on my side I update Qserv metadata on ccqserv100 w.r.t. new Qserv metadata format, and then I test Docker+Qserv on ccqserv100->ccqserv124,  - then CC-IN2P3 team launches a upgrade of the kernel to kernel-ml ( ""mainline stable"" branch of The Linux Kernel Archives) on ccqserv100->ccqserv124, this could be done in January,  - I control Qserv behaviour is still the same than before,  - then Qserv developpers can use this cluster to see if ""Kernel Panic' issue is solved.    If it work we'll update to kernel-ml on ccqserv125->ccqserv149, if it doesn't, cc-in2p3 and Qserv developpers will have to find an other solution.    Regards,    Fabrice",5
"Audit and document obs_subaru scripts
{{obs_subaru}} has a {{bin.src}} directory containing a variety of miscellaneous scripts. Some of these may be actively useful; others could be useful, but require modernizing to work with the latest version of the LSST codebase; others are obsolete or duplicate functionality available elsewhere. Throughout, documentation is lacking.    Please audit this directory: remove the scripts which are useless and ensure the others are working and properly documented.",5
"Cyber security infrastructure document
This document details anticipated security infrastructure and roles needed for the operations of LSST at the base and summit observatory site.  ",1
"Bi-weekly LSST IaM meetings for December
Bi-weekly IaM meeting between NCSA and LSST for the month of December 2015.  Local coordinating meetings also included.",1
"Make deblender more robust against weird PSF dimensions
[~boutigny] reports two problems with PSF dimension calculations in the deblender that result in fatal errors, because earlier checks for bad dimensions intended to cause more graceful failures are incomplete.    The first appears to happen when the PSF dimensions are highly non-square, and the image width is smaller than 1.5x FWHM while the image height is more than 1.5x FWHM (or the opposite).  {code:hide-linenum}  measureCoaddSources FATAL: Failed on dataId={'filter': 'i', 'tract': 1, 'patch': '8,2'}:     File ""src/image/Image.cc"", line 92, in static typename lsst::afw::image::ImageBase<PixelT>::_view_t lsst::afw:  :image::ImageBase<PixelT>::_makeSubView(const lsst::afw::geom::Extent2I&, const lsst::afw::geom::Extent2I&, cons  t typename lsst::afw::image::detail::types_traits<PixelT, false>::view_t&) [with PixelT = double]      Box2I(Point2I(-2,2),Extent2I(11,11)) doesn't fit in image 7x15 {0}  lsst::pex::exceptions::LengthError: 'Box2I(Point2I(-2,2),Extent2I(11,11)) doesn't fit in image 7x15'    Traceback (most recent call last):    File ""/sps/lsst/Library/lsstsw/stack/Linux64/pipe_base/2015_10.0-3-g24e103a/python/lsst/pipe/base/cmdLineTask.py"", line 321, in __call__      result = task.run(dataRef, **kwargs)    File ""/sps/lsst/dev/lsstprod/clusters/my_packages/pipe_tasks/python/lsst/pipe/tasks/multiBand.py"", line 552, in run      self.deblend.run(exposure, sources, exposure.getPsf())    File ""/sps/lsst/Library/lsstsw/stack/Linux64/pipe_base/2015_10.0-3-g24e103a/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/sps/lsst/Library/lsstsw/stack/Linux64/meas_deblender/2015_10.0-3-ga5a97e7/python/lsst/meas/deblender/deblend.py"", line 231, in run      self.deblend(exposure, sources, psf)    File ""/sps/lsst/Library/lsstsw/stack/Linux64/pipe_base/2015_10.0-3-g24e103a/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/sps/lsst/Library/lsstsw/stack/Linux64/meas_deblender/2015_10.0-3-ga5a97e7/python/lsst/meas/deblender/deblend.py"", line 308, in deblend      clipStrayFluxFraction=self.config.clipStrayFluxFraction,    File ""/sps/lsst/Library/lsstsw/stack/Linux64/meas_deblender/2015_10.0-3-ga5a97e7/python/lsst/meas/deblender/baseline.py"", line 354, in deblend      psf, pk, sigma1, patchEdges)    File ""/sps/lsst/Library/lsstsw/stack/Linux64/meas_deblender/2015_10.0-3-ga5a97e7/python/lsst/meas/deblender/baseline.py"", line 1073, in _handle_flux_at_edge      psfim = psfim.Factory(psfim, Sbox, afwImage.PARENT, True)    File ""/sps/lsst/Library/lsstsw/stack/Linux64/afw/2015_10.0-8-g4057726/python/lsst/afw/image/imageLib.py"", line 4630, in Factory      return ImageD(*args)    File ""/sps/lsst/Library/lsstsw/stack/Linux64/afw/2015_10.0-8-g4057726/python/lsst/afw/image/imageLib.py"", line 4472, in __init__      this = _imageLib.new_ImageD(*args)  LengthError:     File ""src/image/Image.cc"", line 92, in static typename lsst::afw::image::ImageBase<PixelT>::_view_t lsst::afw::image::ImageBase<PixelT>::_makeSubView(const lsst::afw::geom::Extent2I&, const lsst::afw::geom::Extent2I&, const typename lsst::afw::image::detail::types_traits<PixelT, false>::view_t&) [with PixelT = double]      Box2I(Point2I(-2,2),Extent2I(11,11)) doesn't fit in image 7x15 {0}  lsst::pex::exceptions::LengthError: 'Box2I(Point2I(-2,2),Extent2I(11,11)) doesn't fit in image 7x15'  {code}    The second problem may occur when the overlap region between a PSF image and the data image it corresponds to is only 1 pixel in either dimension.  In any case, there's a gap in the graceful-failure logic that could let such a problem through, which would result in received error message:  {code:hide-linenum}  measureCoaddSources FATAL: Failed on dataId={'filter': 'i', 'tract': 1, 'patch': '8,1'}:   Traceback (most recent call last):    File ""/sps/lsst/Library/lsstsw/stack/Linux64/pipe_base/2015_10.0-3-g24e103a/python/lsst/pipe/base/cmdLineTask.  py"", line 321, in __call__      result = task.run(dataRef, **kwargs)    File ""/sps/lsst/dev/lsstprod/clusters/my_packages/pipe_tasks/python/lsst/pipe/tasks/multiBand.py"", line 552, i  n run      self.deblend.run(exposure, sources, exposure.getPsf())    File ""/sps/lsst/Library/lsstsw/stack/Linux64/pipe_base/2015_10.0-3-g24e103a/python/lsst/pipe/base/timer.py"", l  ine 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/sps/lsst/Library/lsstsw/stack/Linux64/meas_deblender/2015_10.0-3-ga5a97e7/python/lsst/meas/deblender/de  blend.py"", line 231, in run      self.deblend(exposure, sources, psf)    File ""/sps/lsst/Library/lsstsw/stack/Linux64/pipe_base/2015_10.0-3-g24e103a/python/lsst/pipe/base/timer.py"", l  ine 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/sps/lsst/Library/lsstsw/stack/Linux64/meas_deblender/2015_10.0-3-ga5a97e7/python/lsst/meas/deblender/de  blend.py"", line 308, in deblend      clipStrayFluxFraction=self.config.clipStrayFluxFraction,    File ""/sps/lsst/Library/lsstsw/stack/Linux64/meas_deblender/2015_10.0-3-ga5a97e7/python/lsst/meas/deblender/ba  seline.py"", line 312, in deblend      tinyFootprintSize=tinyFootprintSize)    File ""/sps/lsst/Library/lsstsw/stack/Linux64/meas_deblender/2015_10.0-3-ga5a97e7/python/lsst/meas/deblender/ba  seline.py"", line 575, in _fitPsfs      **kwargs)    File ""/sps/lsst/Library/lsstsw/stack/Linux64/meas_deblender/2015_10.0-3-ga5a97e7/python/lsst/meas/deblender/ba  seline.py"", line 752, in _fitPsf      sx1, sx2, sx3, sx4 = _overlap(xlo, xhi, px0+1, px1-1)    File ""/sps/lsst/Library/lsstsw/stack/Linux64/meas_deblender/2015_10.0-3-ga5a97e7/python/lsst/meas/deblender/ba  seline.py"", line 721, in _overlap      (xlo <= xhi)  and (xmin <= xmax))  AssertionError  {code}",2
"Demonstrate web authentication using CILogon and Globus
Configure mod_auth_oidc on lsst-auth1 with CILogon and Globus.",4
"CONOPS to  support design activities.
One foundational document  required for how NCSA Astronomy Core Services working methods that is missing is a concept of operations (CONOPS) for the L1 system.     Better late than never,  I wrote a L1 cops illustrating the uses of the L1 system, allowing for further specifications and context for all staff involved in the project. the document currently exists a a draft in google docs.  It's been review by Mario and KT.  This is more properly a systems engineering document, not at all sure where a final home for it belongs (or how it acquires status)",10
"Prepare  activity diagrams and backing conops  for LI provisioning and ARP, including satellite computing centers.
Prepared two longer con accompanied by (hand drawn activity diagrams).      One conops /activity  diagram describe the work at the archive center to provide and support the L1 services used by telescope operations.    the second activity diagram and conops respdes to Chuck Clavers' request to have materials that explain the relationship of the stiletto computing center at CCIN2P3 to Archive Center at NCSA.    Both are DRAFT; and coops seem to be systems engineering documents, and where to deliver a blessed version and who is responsible for this is unclear to me. ",5
"Beth Willman visit
self explanatory",2
"Review gartner materials relating ITIL, Devops and related topic
Read Gartner materials related to ITIL, devops  IT organization in preparation for more detailed thinking about Data Operations.    One major category of thought is ""Mode 1 and Mode 2"" type organizations. Mode ! is the current typical controlled environment the strength is when something precious needs to be managed.  For LSST this might be the data release production, which is baselined to be 9 months on a unique resource that the project procured.  Mode 2 is ""doves"" which is best used for nimble, fall fast software.  An example e testing algorithms.",3
"Management for December
includes attendance at the NSF CI for facilities workshop (2d).  Hiring,  hiring related presentation at U of I  ACI.   Management of group and effort distributed at NCSA.  Oversight of some legacy projects (Chileand data center, etc)",9
"Connecting table with histogram viewer
Create a demo, which takes a URL and shows a table with a histogram viewer connected to it.",6
"Cleanup location of anonymous namespaces
we place anonymous namespace in two ways: (a), INSIDE lsst::qserv::<module> namespace, or (b) BEFORE. This story involves cleaning it up - move them to before lsst::qserv::<module>",1
"Add mysql connection to QueryContext 
We need access to database schema for various reasons (analyzing queries, checking authorization, for queries like ""show create table"" and others).",3
"Implement globally unique queryId
nan",7
"Support human-friendly Thread ID in logging messages
Per discussion 1/6/2016, it'd be nice to have a function that generates user-friendly threadId  on Linux to simplify debugging.",4
"create multi image viewer
* port of MultiDataView.java  * support grids, rows, finger chart type grid  * support paging with table data sets  * support DatasetInfoConverter port  * The components should be able to display any group of data  * Critical for Firefly Viewer",12
"Port Data  set info converter achitechture
defines various image data types, how to get them, groupings, artifacts.   I am not quite happy with how we did in in GWT so the design needs to be improved.  Must be less complex.",8
"L1 Concept of Operations (December work)
Assist in developing a ConOps for the L1 system. This is the first step to making a detailed design and plan for construction.  	- Revised/cleaned up/added to L1 ConOps  	- Meeting to clarify calibration products production use case requirements  	- Discussions about operational use cases, processes, functions of L1 system",10
"Ops Planning - December
- LOPT and TOWG meetings  - Beth Willman 2-day visit; discussions about operations, proposal timeline and deliverables    - Prepared FTE estimates for IT roles  	- Reviewed ITIL roles and clarified work descriptions  	- Met with NCSA ICI leads to get input on FTE estimates for various roles  - Timing diagrams  	- Worked on first draft of cycle diagram showing 24 hours of operations at NCSA",7
"Margaret's mgmt. activities in December
- Meetings: security, IdM, DMLT, supertask coordination, standups, etc.  - Staffing  	- ARI meeting and preparation  	- Reviewed resumes, discussed staffing plan  - TPR, invoice breakouts, milestones  - Discussed EV process and MIS tool design for internal management  - etc.  ",13
"Exploration of In-memory database packages used in time critical applications
Begin evaluation of potential in-memory data storage tools - selecting memcached and redis to start. - 4    With the intent  to gain familiarity with these tools, procurred introductory volume on redis and began writing prototype python code to prototype lists, hashes, and lists of hashes. Sketched out and implemented base python class with virtual save method, then wrote child classes for replicators, replicator health, replicator jobs etc. and tested this code and implemented the save methods. - 4    Installed the above code on a nebula instance that acts as a job manager, then ran real job messages through the system and simulated task assignment and completion, using redis to track jobs. - 2    Exploring how a logging and visualization harness might be included so job activity could 1) be observed in realtime, and 2) so a session could be played back as an after action review to investigate errors, bottlenecks, cold restart behavior, etc. - 2    To finish out this story, redis replication must be included in the above prototype.",12
"CONOPS-V1
Evaluated first cut of CONOPS document. Formulated queries for clarifying specific questions regarding ldm-230 - 2",2
"Track and provide feedback on base site facitlity
nan",2
"Build network testbed
nan",4
"add CSS import and image import and clean up some existing jsx
nan",2
"Port W16 CModel improvements from HSC
Three significant changes were made to CModel in [HSC-1339|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1339]. They were described by [~jbosch] in a [post to {{hsc_software}}|http://jeeves.astro.princeton.edu/pipermail/hsc_software/all/4568.html]. They include:    * Changing the method by which the initial approximation is determined;  * Changing the determination of the pixel region to use in fitting;  * A new prior on ellipticity and radius.    Please port these changes to LSST.    Also include the results of fixing the bug described in [HSC-1384|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1384].",4
"Week end 12/12/15
Support for lsst-dev cluster, OpenStack, and accounts  for week ending December 12, 2015.",2
"Week end 12/19/15
Support for lsst-dev cluster, OpenStack, and accounts  for week ending December 19, 2015.",2
"New equipment setup and configuration (week end 12/05/15)
* Setup IPMI on lsst-esxi3, lsst-esxi5, lsst-esxi6  ** Created ipmitools binary - Ubuntu 12.04 has the correct libraries for ESXi6  ** Installed on lsst-esxi3, lsst-esxi5, lsst-esxi6  ** Configuration worked well on lsst-esxi5, lsst-esxi6.  ** lsst-esxi3 is actually lsst-esxi4.  System will not respond to ipmitool commands - suggest waiting for scheduled outage and manually setting up ipmi.  * Debugged networking issues with new equipment  * Cleanup of crashplan archives for new lsst system  * Mac VMs  ** Working on figuring out Mac VM requirements and process with Josh Hobblitt  ** Attempted to install and configure puppet on Mac VMs - running into configuration issues  ",3
"New equipment setup and configuration (week end 12/12/15)
* Set up IPMI on lsst-test1, lsst-test2,…lsst-test9  * More research on using Puppet on Mac VMs – little progress  * Cleanup of NFS space in ITS  ",2
"New equipment setup and configuration (week end 12/19/15)
* Research on using puppet on Mac VMs  ** Considering using Vagrant to manage VirtualBox or Fusion Mac VMs  ** Tried to setup LDAP auth for Mac user auth  * Cleanup of NFS space in ITS",2
"Updates to the Sizing Model
Updated processor projections based upon Haswell and Skylake expectations. Added Shipping rates, Chilean and US power and cooling rates, updated memory pricing projections. Working with Spectra Logic on updating and improving tape predictions, library space and power requirements, upgrade options and mapping of bandwidth and capacity requirements to hardware (need to figure in fudge factors for latency of mounting tapes, latency of seeks times, maybe space for tape migrations, replace replacement tapes with updating pricing that includes tape replacement). Inclusion of that into the document will be in the next story.",3
"Contractual work, justifications, inventory for LSST hardware
Reviewing hardware purchase contracts, reviewing internal hardware budget justifications (and attending related meetings), working with purchasing on 'vendor specific' purchasing options, incorporating updated vendor-quoted pricing into expected hardware expenditures. ",2
"meas_extensions_shapeHSM seems to be broken
I have installed the meas_extensions_shapeHSM package together with galsim and tmv (I documented it at : https://github.com/DarkEnergyScienceCollaboration/ReprocessingTaskForce/wiki/Installing-the-LSST-DM-stack-and-the-related-packages#installing-meas_extensions_shapehsm) and tried to run it on CFHT cluster data.     My config file is the following:    {code:python}  import lsst.meas.extensions.shapeHSM  config.measurement.plugins.names |= [""ext_shapeHSM_HsmShapeRegauss"", ""ext_shapeHSM_HsmMoments"",                                      ""ext_shapeHSM_HsmPsfMoments""]  config.measurement.plugins['ext_shapeHSM_HsmShapeRegauss'].deblendNChild=''  config.measurement.slots.shape = ""ext_shapeHSM_HsmMoments""  {code}    When I run measCoaddSources.py, I get the following error :    {code}  Traceback (most recent call last):    File ""/sps/lsst/Library/lsstsw/stack/Linux64/pipe_tasks/2015_10.0-10-g1170fd0/bin/measureCoaddSources.py"", line 3, in <module>      MeasureMergedCoaddSourcesTask.parseAndRun()    File ""/sps/lsst/Library/lsstsw/stack/Linux64/pipe_base/2015_10.0-3-g24e103a/python/lsst/pipe/base/cmdLineTask.py"", line 444, in parseAndRun      resultList = taskRunner.run(parsedCmd)    File ""/sps/lsst/Library/lsstsw/stack/Linux64/pipe_base/2015_10.0-3-g24e103a/python/lsst/pipe/base/cmdLineTask.py"", line 192, in run      if self.precall(parsedCmd):    File ""/sps/lsst/Library/lsstsw/stack/Linux64/pipe_base/2015_10.0-3-g24e103a/python/lsst/pipe/base/cmdLineTask.py"", line 279, in precall      task = self.makeTask(parsedCmd=parsedCmd)    File ""/sps/lsst/Library/lsstsw/stack/Linux64/pipe_base/2015_10.0-3-g24e103a/python/lsst/pipe/base/cmdLineTask.py"", line 363, in makeTask      return self.TaskClass(config=self.config, log=self.log, butler=butler)    File ""/sps/lsst/Library/lsstsw/stack/Linux64/pipe_tasks/2015_10.0-10-g1170fd0/python/lsst/pipe/tasks/multiBand.py"", line 530, in __init__      self.makeSubtask(""measurement"", schema=self.schema, algMetadata=self.algMetadata)    File ""/sps/lsst/Library/lsstsw/stack/Linux64/pipe_base/2015_10.0-3-g24e103a/python/lsst/pipe/base/task.py"", line 255, in makeSubtask      subtask = configurableField.apply(name=name, parentTask=self, **keyArgs)    File ""/sps/lsst/Library/lsstsw/stack/Linux64/pex_config/2015_10.0-1-gc006da1/python/lsst/pex/config/configurableField.py"", line 77, in apply      return self.target(*args, config=self.value, **kw)    File ""/sps/lsst/dev/lsstprod/clusters/my_packages/meas_base/python/lsst/meas/base/sfm.py"", line 247, in __init__      self.initializePlugins(schema=self.schema)    File ""/sps/lsst/dev/lsstprod/clusters/my_packages/meas_base/python/lsst/meas/base/baseMeasurement.py"", line 298, in initializePlugins      self.plugins[name] = PluginClass(config, name, metadata=self.algMetadata, **kwds)    File ""/sps/lsst/dev/lsstprod/clusters/my_packages/meas_base/python/lsst/meas/base/wrappers.py"", line 15, in __init__      self.cpp = self.factory(config, name, schema, metadata)    File ""/sps/lsst/dev/lsstprod/clusters/my_packages/meas_base/python/lsst/meas/base/wrappers.py"", line 223, in factory      return AlgClass(config.makeControl(), name, schema)    File ""/sps/lsst/dev/lsstprod/clusters/my_packages/meas_extensions_shapeHSM/python/lsst/meas/extensions/shapeHSM/hsmLib.py"", line 964, in __init__      def __init__(self, *args, **kwargs): raise AttributeError(""No constructor defined - class is abstract"")  AttributeError: No constructor defined - class is abstract  {code}",1
"MariaDB does not work together with mysql-proxy
We have switched to MAriaDB but there is one issue that complicates things - mysql client from mariadb fails to connect to mysql-proxy with an error:  {noformat}  ERROR 1043 (08S01): Bad handshake  {noformat}  so Fabrice had to find a workaround for our setup to use client from mysqlclient package instead. This workaround is not perfect and it complicates other things. Would be nice to make things work transparently for mariadb.    ",2
"JIRA project for the publication board
The LSST Publication Board requests a JIRA project for managing its workload.       ",2
"Rename temporarily mariadb client
MariaDB client isn't compliant with mysqlproxy and eups doesn't allow to override it with regular mysql client, so it will be rename, so that MYSQLCLIENT_DIR reference in qserv table file can be removed (indeed, it brokes qserv_distrib setup, but not qserv setup, ...)",1
"Consulting in December
nan",3
"Update provenance in baseline schema
Current provenance schema in baseline (cat/sql) is very old and no longer reflect latest thinking. This story involves bringing cat/sql up to data and replacing existing prv_* tables with tables we came up with in the epic.",2
"Packge mysqlproxy 0.8.5
See https://mariadb.atlassian.net/browse/MDEV-9389",2
"FITS Visualizer porting: Mouse Readout: part 2: flux value
Call the server when mouse pauses, include the flux value in the readout. The should also include support for 3 color.",4
"FITS Visualizer porting: Mouse Readout: part 3: Lock by click & 3 color support
add toggle button that make the mouse readout lock to last position click on.  It will not longer update on move but by click  Include: 3 Color Support",8
"S17 Refactor MySQL Connection in Qserv
nan",38
"F17 Setup Qserv and ImgServ with PanSTARRS data
Once the PanSTARRS data becomes public, we should load it to qserv. This epic involves partitioning data, loading to qserv and making it ready for analysis by friendly scientists (and for our internal testing). We should also make the panstarrs images available through imgserv, The work involves setting up webserv instance and configuring imgserv for panstarrs.",24
"Refactor prototype docs into “Developer Guide” and Science Pipelines doc projects
Refactor [lsst_stack_docs|https://github.com/lsst-sqre/lsst_stack_docs] into two doc projects    - LSST DM Developer Guide that will be published to {{developer.lsst.io}}, and  - LSST Science Pipelines that will be published to {{pipelines.lsst.io}}",3
"Write Zoom Options Popup
Write the simple zoom options popup that is show when the user clicks zoom too fast or the zoom level exceeds  the maximum size.      activate this popup from visualize/ui/ZoomButton.jsx",2
"DetectCoaddSourcesTask.scaleVariance gets wrong result
DetectCoaddSourcesTask.scaleVariance is used to adjust the variance plane in the coadd to match the observed variance in the image plane (necessary after warping because we've lost variance into covariance). The current implementation produces the wrong scaling in cases where the image has strongly variable variance (e.g., 10 inputs contributed to half the image, but only 1 input contributed to the other half) because it calculates the variance of the image and the mean of the variance separately so that clipping can affect different pixels.    Getting this scaling very wrong can make us dig into the dirt when detecting objects, with drastic implications for the resultant catalog.    This is a port of [HSC-1357|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1357] and [HSC-1383|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1383].",1
"Rotate Popup
nan",4
"Update the ground truth values in the lsst_dm_demo to reflect new defaults in deblending
In DM-4410 default configuration options were changed such that footprints are now grown in the detection task, and the deblender is run by default. This breaks the lsst_dm_demo, as now the results of processing are slightly different. The short term solution as part of DM-4410 was to run the demo with the defaults overridden to be what they were prior to DM-4410. In the long term the values used in the compare script should be updated to reflect what would be generated with running processCcd with the stack defaults. ",1
"Some wcs keywords need to be removed from the metadata of raw DECam data
Header keys such as PVi_j left in the raw metadata confuse the making of wcs in later processing steps.   For example, when {{calexp}} is read in {{makeDiscreteSkyMap.py}}, {{makeCoaddTempExp.py}}, and so on, this message appears:  {code:java}  makeWcs: Interpreting RA---TAN-SIP/DEC--TAN-SIP + PVi_j as TPV  makeWcs WARNING: Stripping PVi_j keys from projection RA---TPV-SIP/DEC--TPV-SIP  {code}  These {{calexp}} are created by running {{processCcd.py}} on raw data, and are mis-interpreted as TPV.  ",7
"Test stack with mariadbclient
Now that we switched Qserv to mariadb, it'd be good to switch the rest of the stack. This story involves trying out if things still work if we switch mysqlclient to mariadbclient.",2
"Add Shared Scan Table Information to CSS 
Some information should be added to CSS to indicate if a table should be locked in memory for shared scans and the effect the table is likely to have on the time it takes to complete a query.",4
"Package mariadbclient
There are some very low level modules that depend on mysqlclient (for example daf_persistence). It'd be too harsh to make them depend on mariadb, so we should package mariadb client.",1
"X16 Fine-tune Shared Scans
Fine tune shared scans code, in particular take advantage of unique queryId.",29
"Provenance Prototyping
Build a proof-of-concept provenance prototype for a selected pipeline, perhaps HCS.",50
"Create validation_data set for DECam validation test
Create a `validation_data_decam` to provide a few images for DECam validation tests.    Use the COSMOS field data as currently available on NCSA being processed by [~nidever].    Select just a few images for now.",2
"Planning for Software Documentation Deployment Service
Write initial draft of [SQR-006|http://sqr-006.lsst.io] that specifies how the documentation deployment service will work.",4
"Read and understand `ci_hsc` and plan relationship with `validate_drp`
Read through and run the `ci_hsc` tests and plan for how this module and efforts should relate to `validate_drp`.    a. Add capabilities to `validate_drp` to run the tests in `ci_hsc`.  (/)  b. Compare frameworks. (/)  c. Plan for how such validation and continuous integration data sets should be constructed. (/)  ",2
"Improvement of raw data handling in DecamMapper
Two minor improvements with better coding practice:  - Be more specific copying FITS header keywords. Avoid potential problems if unwelcome keywords appear in the header in the future. Suggested in the discussions in DM-4133.   - Reuse {{isr.getDefectListFromMask}} for converting defects. A more efficient method that uses the FootprintSet constructor with a Mask and a threshold has just been adopted in DM-4800.     Processing is not changed effectively.  ",1
"HSC backport: Remove interpolated background before detection to reduce junk sources
This is a port of [HSC-1353|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1353] and [HSC-1360|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1360].    Descriptions from HSC:   {panel:title=HSC-1353}  We typically get a large number of junk detections around bright objects due to noise fluctuations in the elevated background. We can try to reduce the number of junk detections by adding an additional local background subtraction before object detection. We can then add this back in after detection of footprints and peaks.  {panel}  {panel:title=HSC-1360}  I forgot to set the useApprox=True for the background subtraction that runs before footprint and peak detection. This will then use the Chebyshev instead of the spline.  {panel}",1
"Code review
DM-4133, DM-4800, DM-4709, DM-4707, DM-4814",4
"Add Dropdowns to Vis toolbar
Add the dropdown to the vis tool bar",2
"Clean up div and css layout on FitsDownloadDialog
FitsDownload dialogs html and css is not quite right. Needs some clean up.",1
"makeDiscreteSkymap has a default dataset of 'raw'
The default dataset type for command line tasks is raw.  In the case MakeDiscreteSkyMapTask is asking the butler for calexp images.  This shouldn't be a problem, but in my case I have calexp images, but no raw images.  This causes the task to think there is no data to work on, so it exits.",1
"Understand async queries in Qserv
Try to understand, without doing actual implementation what is involved in  implementation of support for asynchronous queries in Qserv and possibly web interface. Should result in a roadmap for implementation at all levels.  ",10
"Adapt `validate_drp` to standard python and bin subdir sturcture
Move Python files into python/lsst namespace convention.  Decide on where {{validateCfht.py}} and {{validateDecam.py}} executables should live  Add package requirements to {{ups/validate_drp.table}}",1
"Finish Fits View Decoration: context toolbar, title, expand button, etc
nan",5
"Add error handling to PsfFitter in meas::modelfit
The {{ShapeletPsfApprox}} Task uses a class called {{PsfFitter}} which is not a {{SimpleAlgorithm}} and does not support error handling.  Add error handling to this class, and modify the Task definition in {{psf.py}} to call an {{errorHandler fail()}} function when the algorithm's {{optimizer.run()}} call fails.    Also, add unit tests",6
"Add bright object masks to pipeline outputs
Given per-patch inputs providing   {code}  id, B, V, R, ra, dec, radius    {code}  for each star to be masked, use this information to set:  * A bit in the mask plane for each affected pixel  * A flag in the source catalogues for each object that has a centroid lying within this mask area    This is a port of [HSC-1342|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1342] and [HSC-1381|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1381].",3
"Update configuration for Suprime-Cam
The {{obs_subaru}} configuration for Suprime-Cam needs updating to match recent changes in the stack.    Port of [HSC-1372|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1372].",1
"Preliminaries for LSST vs HSC pipeline comparison through coadd processing
This is the equivalent of DM-3942 but through coadd processing.    Relevant HSC tickets include:    * [HSC-1371|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1371]",1
"Allow slurm to request total CPUs rather than nodes*processors.
On some systems, we are asked to request a total number of tasks, rather than specify a combination of nodes and processors per node.    It also makes sense to use the SMP option this way.    This is a port of [HSC-1369|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1369].",2
"Fix logic for applying aperture corrections
With the current flow, the aperture corrections are being applied only after all the measurement plugins have run through, independent of their execution order.  This results in plugins whose measurements rely on aperture corrected fluxes (i.e. with execution order > APCORR_ORDER) being applied prior to the aperture correction, leading to erroneous results.  The only plugin currently affected by this is {{base_ClassificationExtendedness}}.    This ticket involves applying a temporary fix to ensure proper application and order of aperture corrections.  However, the problem highlights the fact that the current logic of how and when aperture corrections are applied should be reworked (on another ticket) to be less error-prone.",6
"Implement brighter-fatter correction
Please port the prototype Brighter-Fatter correction work by Will Coulton from HSC.    This covers [HSC-1189|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1189], [HSC-1332| https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1368], [HSC-1368|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1368]. Note also the stand alone commits [783b124|https://github.com/HyperSuprime-Cam/obs_subaru/commit/783b124b6813f5745ce1e444f61fb0114d055907] and (if this work is performed after DM-3373) [9fc5e78| https://github.com/HyperSuprime-Cam/obs_subaru/commit/9fc5e78247e7173e095255dba34e994f73a6bd1d].",4
"High-level overview of DRP processing
Create high-level overview of Data Release Production, probably as an annotated flowchart, for use in sizing model work and as a graphical table of contents for more detailed descriptions.",6
"Add sky objects
Please add ""sources"" corresponding to empty sky (ie, at positions where nothing else has been detected) and include them in multiband processing.    This is a port of [HSC-1336|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1336] and [HSC-1358|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1358]. ",4
"Use high S/N band as reference for multiband forced photometry
We are currently choosing the priority band as the reference band for forced photometry as long as it has a peak in the priority band regardless of the S/N.  Please change this to pick the highest S/N band as the reference band when the priority band S/N is sufficiently low.    This is a port of [HSC-1349|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1349].",1
"Don't write HeavyFootprints in forced photometry
There's no need to persist {{HeavyFootprint}}s while performing forced photometry since retrieving them is as simple as loading the _meas catalog.    This is a port of [HSC-1345|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1345].",1
"Add new blendedness metric
[HSC-1316|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1316] shifts the calculation of blendedness from {{meas_deblender}} to {{meas_algorithms}} and defines a new blendedness metric in the process. Please port it.",3
"Measure photometric repeatability and correctness of reported errors
1. Calculate and plot photometric variability across series of N images.  Compare to reported photometric errors.  Designed for N > 5.  2. Calculate and plot Delta flux / sigma_flux for multiple observations of stars in field.  This is related to 1. but is focused on N=2 to N=5.  3. Fit uncertainty distribution vs. magnitude to identify any floor in the photometric uncertainty and to check performance vs. photon counts.",5
"LDM-151 - comments from Jacek
I am reading your https://github.com/lsst/LDM-151/blob/draft/DM_Applications_Design.tex, and I have some minor comments suggestions. I am going to add comments to this story to capture it. Feel free to apply to ignore :)",1
"Factor out duplicate setIsPrimaryFlag from MeasureMergedCoaddSourcesTask and ProcessCoaddTask
{{MeasureMergedCoaddSourcesTask.setIsPrimaryFlag()}} and {{ProcessCoaddTask.setIsPrimaryFlag()}} are effectively the same code. Please split this out into a separate task which both of the above can call.    This is a (partial) port of [HSC-1112|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1112] and should include fixes from [HSC-1297|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1297].",2
"XY Plot action and reducers
Write action and reducers for XY Plot",6
"Implement zenodio.metadata to mediate Zenodo's API with local YAML metadata
[Zenodio|http://zenodio.lsst.io] is a Python package we’re building to interact with Zenodo. For our various doc/technote/publishing projects we want to use YAML files (embedded in a Git repository, for example) to maintain deposition metadata so that the upload process itself can be automated.    The {{zenodio.metadata}} sub package provides a Python representation of Zenodo metadata (but not File or Zenodo deposition metadata).    See DM-4725 for the upload API work, which consumes the metadata objects.",2
"Add __setitem__ for columns in afw.table
It's confusing to have to use an extra {{[:]}} to set a column in afw.table, and we can make that unnecessary if we override {{\_\_setitem\_\_}} as well as {{\_\_getitem\_\_}}.",2
"Replace killproc and pidofproc with kill and pidof
Running at NCSA on OpenStack revealed that our qserv-stop.sh and qserv-status.sh fail because of missing killproc and pidofproc. It looks like (see eg http://stackoverflow.com/questions/3013866/killproc-and-pidofproc-on-linux) these are not very portable and it is better to use kill and pidof.",1
"imagesDiffer doesn't handle overflow for unsigned integers
I'm seeing a test failure in afw's testTestMethods.py, apparently due to my numpy (1.8.2) treating images that differ by -1 as differing by 65535 in both {{numpy.allclose}} and array subtraction (which doesn't promote to an unsigned type).    Does this still cause problems in more recent versions of {{numpy}}?  If not, I imagine it's up to me to find a workaround for older versions if I want it fixed?    (assigning to [~rowen] for now, just because I know he originally wrote this test and I hope he might know more)",1
"Please provide ""getting started"" documentation on writing meas_base algorithms
{{meas_base}} provides a framework for writing measurement algorithms in a uniform way. However, documentation on exactly how this should be done is fragmentary:    * There's some basic documentation in [Doxygen|https://lsst-web.ncsa.illinois.edu/doxygen/x_masterDoxyDoc/meas_base.html] which provides a useful introduction, but doesn't discuss common idioms and helpers such as {{FlagHandler}}, {{SafeCentroidExtractor}} and transformations.  * The [design notes on Confluence|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=20284390] are not intended as documentation and aren't kept up-to-date as new features are added, but can still be a useful reference.  * [~jbosch] gave a [nice introduction|https://github.com/lsst-dm/Oct15_bootcamp/blob/measurement/measurement/measurement.pdf] at the October 2015 Bootcamp, but a set of slides is no substitute for proper documentation, and again there's no expectation that these will be kept up-to-date.    Please provide a centralized, maintained guide to writing {{meas_base}} plugins.",4
"Add point selection
click and highlight a point.  Is on when mouse readout ""Lock by Click"" is on. However, can me turned on externally by adding toolbar context menu options.",2
"Setup webserv for SUI
This story involves setting up a webserv in a VM (NCSA OpenStack) with a small data set: images and corresponding database catalog. We need to   * setup VM   * build the stack for webserv and qserv   * identify images to load   * run qserv in the VM   * run ingest to load the data to mysql (mysql will run on lsst10) and qserv (qserv will run directly in the VM)   * run two webservers - one with mysql backend, one with qserv backend   * open the port numbers for the IPAC team",10
"Port HSC background matching routines
HSC has its own implementation of background matching: see {{background.py}} in {{hscPipe}}. Please port it to the LSST stack.",4
"Filter mask planes propagated to coadds
Some mask planes -- {{CROSSTALK}}, {{NOT_DEBLENDED}} -- do not need to be propagated to coadds. Add an option to remove them.    This is a port of work performed on [HSC-1174|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1174] and [HSC-1294|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1294].",2
"scisql build scripts are buggy 
The scisql build script logic for MySQL/MariaDB version checking is broken on all platforms. There are also assumptions about shared library naming that do not hold on OS/X, which means that the deployment scripts are likely broken on all platforms other than Linux.",2
"Setup LSST stack for verification datasets work
Created a script to setup required LSST stack packages for bulge survey processing.  ",10
"Installing a reference catalog to use in bulge survey processing
Install Astrometry.net Index Files for 2MASS all sky catalog",5
"Setup orchestration environment at lsstdev for bulge survey processing
Follow instructions at   https://confluence.lsstcorp.org/display/DM/1.+Quick+Start+-+LSST+Cluster+Orchestration    to set up the required packages to run the lsst stack at lsst cluster.",10
"Debugging lsst.astrometry task in bulge survey processing
The DECam bulge survey is being processed as part of the verification data sets effort. During astrometry calibration task a large number of failures (affecting ~100 of 213 visits) have been found in calibrate.astrometry.matcher. We report here details of the investigation around this issue. Part of the task is to learn how to use the task built in debug.     More info:    https://confluence.lsstcorp.org/display/SQRE/Bulge+Survey+Processing#BulgeSurveyProcessing-Results",10
"Setup firefly example for image visualization 
Start from example provided by the firefly team    https://github.com/lsst/suit",10
"Test the matchOptimisticB astrometric matcher
The matchOptimisticB matcher fails on many visits of the bulge verification dataset.  This prompted a deeper investigation of the performance of the matcher.  Angelo and David developed a test script and discovered that the matcher works well with offsets of the two source catalogs of up to 80 arcsec, but fails beyond that.  This should be robust enough for nearly all datasets that the LSST stack will be used on.",3
"Write a firefly search processor that retrieves image paths from the butler
nan",5
"base has no readme
The base package does not have a readme file, so it's unclear what it's for. The package name is also somewhat unfortunate, being so generic, but at least with a readme it would be clearer how important it is (if it is, in fact, important).",1
"Compile list of DM simulation needs for Andy Connolly
Compile list of DM simulation needs over the next ~6 months to give to Andy Connolly (simulations lead).",3
"Diagnostic plot showing the number of process ccd failures in each visit as function of the density of sources.
Diagnostic plot showing the number of process ccd failures in each visit as function of the density of sources.    - Use the butler to iterate over the data ids, read the src catalog and count the number of sources per ccd.    - Use afw.display.ds9 to display the image and overlay the sources",5
"Propagate flags from individual visit measurements to coadd measurements
It is useful to be able to identify suitable PSF stars from a coadd catalogue. However, the PSF is not determined on the coadd, but from all the inputs. Add a mechanism for propagating flags from the input catalogues to the coadd catalogue indicating stars that were used for measuring the PSF.    Make the inclusion fraction threshold configurable so we can tweak it (so we only get stars that were consistently used for the PSF model; the threshold might be set it to 0 for ""or"", 1 for ""all"" and something in between for ""some"").    Make the task sufficiently general that it can be used for propagating arbitrary flags.    This is a port of work carried out on [HSC-1052|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1052] and (part of) [HSC-1293|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1293].",2
"Make coadd input catalogs contiguous
It's convenient if we can assume that coadd input catalogs are contiguous -- it simplifies the implementation of {{PropagateVisitFlagsTask}} (DM-4878), for example. Make it so.    This is port of work carried out on [HSC-1293|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1293].",1
"Test capabilities of python bokeh plotting library for making interactive plots - I
We are testing the bokeh library plot for interactive visualization in the web, the python API is atractive and allows rapid prototype which is good for SQuaRE build up its QA system.    Some examples are available in this repo, including a scatter plot with linked histograms in both axis which seems really useful.    https://github.com/lsst-sqre/bokeh-plots    A more complete demonstration of bokeh is available in this webminar:     https://continuum-analytics.wistia.com/medias/f6wp9dam91    ",20
"base_Variance plugin generates errors in lsst_dm_stack_demo
Since DM-4235 was merged, we see a bunch of messages along the lines of:  {code}  processCcd.measurement WARNING: Error in base_Variance.measure on record 427969358631797076: The center is outside the Footprint of the source record  {code}  in the output from {{lsst_dm_stack_demo}}. (See e.g. [here|https://ci.lsst.codes/job/stack-os-matrix/label=centos-6/7482/console#console-section-3]). It's not fatal, but the warnings are disconcerting and could be indicative of a deeper problem.",2
"S17 Improve Qserv Integration Tests
Integration tests need improvements, in particular, we want to run multi-node integration tests easily (possibly without docker), get rid of mono-node test. We should catch errors from individual tests.",15
"Improve/simplify multi-node tests
nan",10
"Script launch of HTCondor pool on Nebula OpenStack
The ability to automate the launch of htcondor pools of worker nodes  (e.g., with LSST software installed) on the Nebula OpenStack can be useful  in several ways for LSST DM.  On one hand, users can start up their own customized  pool should the standard pool available on other resources such as lsst-dev not  be suitable (e.g., not enough cores, not enough memory per core/slot,  customized software not installed on the systems, etc.)  Also, scripted launch  of a pool can be part of the development of a solution for offering  ""batch"" scheduling to the Nebula OpenStack, following an approach similar to e.g.,  CANFAR ( http://www.canfar.net/docs/batch/ , http://cloudscheduler.org)  whereby  a ""cloud scheduler"" is used in conjunction with an htcondor central manager  to provide batch access to the cloud (i.e., submitted jobs are placed into an htcondor queue,  that will execute on launched instances, when those instances join the working pool.)  ",24
"Refactor measurement afterburners into a new plugin system
Some of the operations we currently run as part of measurement (or would like to) share some features that make them a bit different from most plugin algorithms:   - They must be run after at least some other high-level plugins, and may be run after all of them.   - They do not require access to pixel data, as they derive their outputs entirely from other plugins' catalog outputs.   - They may require an aggregation stage of some sort to be run on the regular plugin output before they can be run.    Some examples include:   - Star/Galaxy classification (with training done after measurement and before classification).   - Applying aperture corrections (estimating the correction must be done first).   - BFD's P, Q, R statistics (requires a prior estimated from deep data).    We should move these algorithms to a new plugin system that's run by a new subtask, allowing these plugins to be run entirely separately from {{SingleFrameMeasurementTask}}.  This will simplify some of the currently contorted logic required to make S/G classification happen after aperture correction, while making room for hierarchical inference algorithms like BFD and Bayesian S/G classification in the future.    (We will not be able to support BFD immediately, as this will also require changes to our parallelization approach, but this will be a step in the right direction).    This work should *probably* be delayed until after the HSC merge and [~rowen]'s rewrite of {{ProcessCcdTask}} are complete, but it's conceivable that this refactoring could solve emergent problems there and be worth doing earlier as a result.",8
"Update git-lfs documentation to work with git-lfs 1.1.0+
The git-lfs client (1.1.0+) does not support empty username and passwords. To work around this, users can store the appropriate credentials directly with the credential helper.",3
"Update git-lfs repositories to point to the git-lfs documentation.
Update git-lfs repositories to point to the git-lfs documentation.    All documentation should be generic and point to:    http://developer.lsst.io/en/latest/tools/git_lfs.html  ",1
"Take DECam data with collimated beam projector
[~mfisherlevine] and [~rhl] will travel to CTIO to observe on the Blanco 4m telescope.",30
"Write tutorial describing remote IPython + ds9 on lsst-dev
[~mfisherlevine] recently figured out how to set up his system to run a remote IPython kernel on {{lsst-dev}} and interact with it from his laptop, including streaming image display from the remote system to a local instance of {{ds9}}.    He will write all this up so that others in the community can easily do the same.",2
"Ingest DECam/CBP data into LSST stack
[~mfisherlevine] will ingest the data taken in DM-4892 into the LSST stack. Initial experiments indicate problems with:    * Bias subtraction  * Flat fielding  * Bad pixel masks    These may already be remedied by work on {{obs_decam}}; if not, he will file stories and fix them.",3
"Prepare calibration products for analysing DECam data
Determine if existing bad pixel masks, flats, etc are adequate for analysing the DM-4893 DECam data, and, if not, provide alternatives.",3
"Qualitative exploration of the CBP/DECam data
Having got the CBP/DECam data loaded into the stack, explore the parameter space and understand data.    This should result in a series of stories describing more detailed analysis with quantitative results.",8
"Implement the simulation and testing framework for analyzing image differencing
We need to be able to test all aspects of image differencing.  This includes template generation, astrometric registration, and differencing.  We know that DCR will be an effect that will need to be mitigated so we will have to be able to simulate it and show how well various techniques deal with it.",50
"Implement simulations for testing image differencing.
Implement a suite of simulations tools for testing the image differencing techniques, specifically with an eye toward dealing with DCR.",15
"Use yaml configuration files to store camera-specific data ID and ref image information for validation testing.
Currently there is {{validateCfht.py}} and {{validateDecam.py}} as code.  These differ in just having {{defaultData}} functions that specify the dataIds to consider and the dataIds to use as a reference for comparison.    Storing the information necessary to create these sets of dataIds in separate data files, to be stored as YAML would  1. Improve the separation of code and data  2. Clarify the usage and necessary information to run on a new or different set of data  3. Make it easier to run different subsets easily by specifying a different input file    The proposals is that {{validateCfht.py}} and {{validateDecam.py}} would disappear from {{bin}} and be replaced by just {{validate_drp.py}}.  The examples in {{examples/runCfhtTest.sh}} and {{examples/runDecamTest.sh}} will be updated to show the new usage.  The README will also be updated.",1
"Expand button hide/show, delete button hide/show, display title options,
Expand button hide/show, delete button hide/show, display title options,  support pv.hideTitleDetail to control showing zoom level and rotation info (used by planck)  support external title bar (planck as well)  support checkbox on title bar (planck)  ",4
"Buffer overrun in wcslib causes stack corruption
The buffer 'msg' in wcsfix.c is used to report attempts by wcslib to re-format units found in fits files. It is allocated on the stack (in function 'unitfix') using a pre-processor macro defined size of 160 chars (set in wcserr.h). When attempting to run the function 'unitfix' in wcsfix, this buffer can overflow on some fits files (the raw files generated by HSC seem particularly prone to triggering this behavior) and results in the session being terminated on Ubuntu 14.04 as stack protection is turned on by default i.e. the stack crashes with a 'stack smashing detected' error. We have reported the bug to the creators of wcslib. As a temporary workaround, users affected by the bug should increase the default size of 'msg' by increasing WCSERR_MSG_LENGTH defined in wcserr.h      We are providing a small python example that demonstrates the problem. Run it as  python test.py <path to ci_hsc>/raw/<any fits file in this directory>    We are also providing a simple c program to demonstrate the bug. Compile it as  cc -fsanitize=address -g -I$WCSLIB_DIR/include/wcslib -o test test.c -L$WCSLIB_DIR/lib -lwcs (on Linux)  cc -fsanitize=address -g -L$WCSLIB_DIR/lib -lwcs -I$WCSLIB_DIR/include/wcslib -o test test.c (on Mac OS X)",2
"Investigate astrometry warnings from processing raw DECam data
This ticket includes efforts to troubleshoot and improve processing DECam raw data in Jan 2016.   Investigations of the warnings from solving astrometry led to DM-4805, DM-4859. This ticket also includes partial efforts in DM-4859.         For validation, I ran {{processCcd.py}} with two visits of raw Stripe 82 DECam data with and without the changes in DM-4859 (using the first camera geometry fix, which is different from the final fix).  The script {{validateDecam}} in {{validate_drp}} is used to check astrometric scatter of the sources between the two visits.  Output plots are in the attachments.  By fixing DM-4859, the median astrometric scatter (mag < 21) decreases from 31.3 mas to 26.1 mas.  The number of matches between two visits increases from 56768 to 71265.  Also attached are the ra/dec patches plots, using the {{showVisitSkyMap.py}} script from DM-4095. The two colors represent the two visits of calexp wcs.  Southern CCDs had bad astrometric solutions before DM-4859.  The validation results are consistent with the patches visualization.   ",12
"Cyber security infrastructure requirements
Documenting cyber security operational requirements by LSST, particularly at the obs. site.",2
"Security plan renewal
Continuing work on cyber sec. plan renewal.  DM moving along, PO slated next.",1
"DM security meeting
Security meeting/planning with LSST DM team at NCSA.",1
"LSST IaM meetings at NCSA
nan",1
"LSST IaM bi-weekly coordinating meeting
Meeting between NCSA CSD group, NCSA LSST DM group, and other LSST groups.",1
"Add option for object name resolution
For some object names resolved by NED, the position is not right. In this situation, it would be better to get the position from Simbad. Currently, Firefly offers two options: first NED, then Simbad; first Simbad, then NED. The third option to be added would be ""the best position according to the object type"".     It should check the object type returned by NED, making a decision whether to get position from Simbad instead; and vice versa. ",6
"Test obs_decam with processed data
Sometimes DECam-specific bugs only reveal in or affect the processed data. For example the bug of DM-4859 reveals in the {{postISRCCD}} products.  If the bugs are DECam-specific, some changes in {{obs_decam}} are likely needed.  It would be useful to have a more convenient way to test those changes. In this ticket I modify {{testdata_decam}} so that those data can be processed, and then allow wider options in the {{obs_decam}} unit tests.    I add {{testProcessCcd.py}} in {{obs_decam}} that runs {{processCcd.py}} with raw and calibration data in {{testdata_decam}}.  Besides a short sanity check, I add a test (testWcsPostIsr) that tests DM-4859. {{testWcsPostIsr}} fails without the DM-4859 fix, and passes with it.  ",3
"Porting encodeURL of the java FitsDownlaodDialog code to javascript 
When download an image,  the proper name needs to be resolved based on the URL and   the information about the image.  In Java code, it has the following three methods:  {code}   encodeUrl  makeFileName  makeTitleFileName  {code}    These method should be ported to javascript.  Thus, the javascript version of the FitsDownloadDialog will save the file in the same manner. ",2
"Test performance of vertical-partition joins in mysql
We are planning to vertically partition some tables (for example Object). We should make sure such joins across say 5, 10 or 20 tables are not a problem for mysql from performance standpoint. The testing involves creating a wide table (say 200 columns) and testing a speed of full scan, then slicing that table vertically into different number of columns and using join to assemble the pieces together.",4
"Support Multi image fits and controls
add toolbar: next, prev arrow buttons, title when multi image fits has image specific titles or title would be cube number.    Make sure the store will support multi images, add next, prev actions, etc",4
"Make obs_subaru build with OS X SIP
Because of OS X SIP, {{obs_subaru}} fails to build on os x 10.11. In the {{hsc/SConscript}} file, the library environment variables need properly set, and scripts need to be delayed until the shebang rewriting occurs. ",1
"want to see locations in trace when butler raises because multiple locations were found
daf_persistence 11.0-2-g56eb0a1+1 gives the unhelpful error message:    {code}  > RuntimeError: Unable to retrieve bias for {'category': 'A', 'taiObs': '2015-12-22', 'visit': 7292, 'site': 'S', 'dateObs': '2015-12-22', 'filter': 'PFS-M', 'field': 'DARK', 'spectrograph': 2, 'ccd': 5}: No unique lookup for ['calibDate', 'calibVersion'] from {'category': 'A', 'taiObs': '2015-12-22', 'visit': 7292, 'site': 'S', 'dateObs': '2015-12-22', 'filter': 'PFS-M', 'field': 'DARK', 'spectrograph': 2, 'ccd': 5}: 2 matches  {code}    (the old butler did this too).  The user wants to know what the 2 matches were -- it's user error, but the user needs help and  printing the first few options (nicely formatted) is very useful.  I think I did this on the HSC side.      The butler code in question is actually in butlerUtils/python/lsst/daf/butlerUtils/mapping.py and my post-doc gave me the wrong package.    It's in need():  {code}  >         if len(lookups) != 1:  >             raise RuntimeError, ""No unique lookup for %s from %s: %d matches"" % (newProps, newId, len(lookups))  {code}",1
"Make FlagHandler, SafeCentroidExtractor usable from Python
The {{meas_base}} framework includes {{SafeCentroidExtractor}}, a convenience routine for extracting a centroid from a source record, setting a consistent set of flags if that's not possible or if the centroid is in some way compromised. This consistent flag handling is made possible by the use of the {{FlagHandler}} class.    Unfortunately, {{FlagHandler}} is not meaningfully usable from Python, not least because it's impossible to define flags:  {code:python}  >>> import lsst.meas.base as measBase  >>> measBase.FlagDefinition(""flag"", ""doc"")  [...]  TypeError: __init__() takes exactly 1 argument (3 given)  >>> fd = measBase.FlagDefinition()  >>> fd.name = ""flag""  [...]  AttributeError: You cannot add attributes to <lsst.meas.base.baseLib.FlagDefinition; proxy of <Swig Object of type 'lsst::meas::base::FlagDefinition *' at 0x10a82b900> >  {code}    Looking further, even were we able to create {{FlagDefinitions}}, the {{FlagHandler}} is initialized with pointers to the beginning/end of a container of them, which seems like a stretch for Python code.    Please add Python support for these routines.",2
"Centroids fall outside Footprints
In DM-4882, we observed a number of centroids measured while running the {{lsst_dm_stack_demo}} routines fall outside their associated {{Footprints}}. This was seen with both the {{NaiveCentroid}} and the {{SdssCentroid}} centroiders.    For the purposes of DM-4882 we quieted the warnings arising from this, but we should investigate why this is happening and, if necessary, weed out small {{Footprints}} entirely.",8
"Fix intermittent testQdisp failure
The mocks used in the executive class don't mock cancellation correctly and doing so would require significant effort. When Executive::squash() is called, the mocks threads are already running but waiting on the _go barrier. squash() calls JobQuery::cancel() for each thread and cancel() calls markComplete() for the job because a QueryResource has not been aquirred from xrootd. Once all the jobs are cancelled and _go is set to true, the ex.join() command doesn't wait for the jobs to complete since markComplete() has already been called for all of the jobs. If any of the jobs take longer to complete than the main thread, they call markComplete for an Executive that no longer exists and cause the test to fail.",1
"Fix build of MariaDB on OS X El Capitan
The current MariaDB EUPS package does not build on OS X El Capitan because OS X no longer ships with OpenSSL developer files. MariaDB has a build option to use a bundled SSL library in preference to OpenSSL but the logic for automatically switching to this version breaks when the Anaconda OpenSSL libraries are present.",1
"Deploy 4 bare metal hosts for testing Base to Archive transfer implementation
James needs to test network communication methodologies in an environment that mimics the expected real-world conditions. In order to minimize the complications with debugging, using bare metal machines in the first phase is preferred.    We can use 4 of the machines bought off the 2015 purchase or purchase new machines just for this purpose.",4
"Qserv build fails on El Capitan with missing OpenSSL
Qserv does not build on OS X El Capitan due to the absence of OpenSSL include files. Apple now only ship the OpenSSL library (for backwards compatibility reasons). Qserv only uses SSL in two places to calculate digests (MD5 and SHA). This functionality is available in the Apple CommonCrypto library. Qserv digest code needs to be taught how to use CommonCrypto.",2
"Track kernel panic issue
The line that caused the kernel panic is in modules/mysql/MySqlConnection.cc line 151.  Currently the line is fine and is:          std::string const killSql = ""KILL QUERY "" + std::to_string(threadId);    This version of the line will occasionally cause the kernel panic (note the missing %1% that should be after KILL QUERY).          std::string killSql = boost::str(bo  ost::format(""KILL QUERY "") % threadId);  ",3
"Create a utility function do do spherical geometry averaging
I would like to calculate a correct average and RMS for a set of RA, Dec positions.    Neither [~jbosch] nor [~price] knew of an easy, simple function to do that that existed in the stack.  [~price] suggested:    {code}  mean = sum(afwGeom.Extent3D(coord.toVector()) for coord in coordList, afwGeom.Point3D(0, 0, 0))  mean /= len(coordList)  mean = afwCoord.IcrsCoord(mean)  {code}    That makes sense, but it's a bit unobvious (it's obvious how it works, but would likely never occur to someone that they should do it that way in the stack).    Pedantically it's also not the best way to do a mean while preserving precision, but I don't anticipate that to be an issue in practice.    Creating a function that did this would provide clarity.  I don't know where that function should live.    Note: I know how to do this in Astropy.  I'm intentionally not using astropy here.  But part of the astropy dependency discussion is likely ""how much are we otherwise rewriting in the LSST stack"".",1
"on-going support to Camera team in visualization at UIUC
Attend the weekly meeting and answer questions as needed",2
"Enable validateMatches in ci_hsc
{{python/lsst/ci/hsc/validate.py}} in {{ci_hsc}} [says|https://github.com/lsst/ci_hsc/blob/69c7a62f675b8fb4164065d2c8c1621e296e40ad/python/lsst/ci/hsc/validate.py#L78]:  {code:python}      def validateMatches(self, dataId):          # XXX lsst.meas.astrom.readMatches is gone!          return  {code}  {{readMatches}} (or its successor) should be back in place as of DM-3633. Please enable this test.",2
"multiple CVEs relevant to mariadb 10.1.9 and mysql
Multiple CVEs have been released this week for mysql & mariadb.  The current eups product for mariadb is bundling 10.1.9, which is affected.  Several of the CVEs do not yet provide details, which typically means they are ""really bad"".    https://github.com/lsst/mariadb/blob/master/upstream/mariadb-10.1.9.tar.gz    https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-0505  https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-0546  https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-0596  https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-0597  https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-0598  https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-0600  https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-0606  https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-0608  https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-0609  https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-0616  https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-2047",1
"Update scisql to v0.3.5
In order to update MariaDB to v10.1.10 {{scisql}} needs to also be updated to deal with the hard-coded version checking. For the current version we get this error with the latest MariaDB:  {code}  :::::  [2016-01-28T16:51:40.539306Z]     user_function(self)  :::::  [2016-01-28T16:51:40.539334Z]   File ""/home/build0/lsstsw/build/scisql/wscript"", line 63, in configure  :::::  [2016-01-28T16:51:40.539346Z]     ctx.check_mysql()  :::::  [2016-01-28T16:51:40.539392Z]   File ""/home/build0/lsstsw/build/scisql/.waf-1.6.11-30618c54883417962c38f5d395f83584/waflib/Configure.py"", line 221, in fun  :::::  [2016-01-28T16:51:40.539410Z]     return f(*k,**kw)  :::::  [2016-01-28T16:51:40.539432Z]   File ""tools/mysql_waf.py"", line 85, in check_mysql  :::::  [2016-01-28T16:51:40.539451Z]     (ok, msg) = mysqlversion.check(version)  :::::  [2016-01-28T16:51:40.539473Z]   File ""tools/mysqlversion.py"", line 74, in check  :::::  [2016-01-28T16:51:40.539514Z]     if not comparison_op(version_nums, constraint_nums):  :::::  [2016-01-28T16:51:40.539547Z] UnboundLocalError: local variable 'constraint_nums' referenced before assignment  Failed during rebuild of DM stack.  {code}",1
"IRSA developer mentoring effort
IRSA is contributing to the Firefly package development.  we need to put in time to mentor the developers. ",2
"IRSA developer mentoring effort
IRSA is contributing to Firefly development. We need to mentor the new developers.",2
"Fix type inference and return types makeMaskedImage et al
The {{makeMaskedImage}} function and cousins like {{makeExposure}} don't do the type inference they're supposed to do in C++, because they use the old {{typename Image<T>::Ptr}} approach instead of {{PTR(Image<T>)}}.    They also return raw pointers, which is dangerous.  They should be converted to return shared_ptrs.  Note that this will have to include adjusting or removing Swig code (probably {{%newobject}} statements) that deal with taking ownership of the raw pointers.",1
"Switch to MemManReal in the worker scheduler
First iteration of worker scheduler uses a skeleton of the memory manager that doesn't actually look at any of the tables, files, or memory. The scheduler needs to be switched to MemManReal. ",15
"butler should transparently allow files to be compressed or not
see the conversation on c.l.o. at [https://community.lsst.org/t/how-does-the-butler-support-compression/502].    The summary is, when the mapper returns e.g. a non compressed file name e.g. {{foo.fits}}, that file may be compressed and the filename may reflect this e.g. in reality it might be named {{foo.fits.gz}}. On a posix system some component of the butler framework should discover this and transform the filename to the correct filename and pass that to the deserializer.    TBD if the list of allowed extensions is hard coded someplace (in a mapper subclass?) or specified another way, perhaps by the policy (could be for dataset type or globally).",6
"Schemas for QA information
This ticket is to capture preliminary design work we are doing for storage of QA system information, which we are working with the Database team on.     As well as prior experience, Jacek has made us aware of the sdqa tables in the schema:    https://lsst-web.ncsa.illinois.edu/schema/index.php?sVer=baseline    and also plan on mining pipeQA for quantities of interest.     Once we have a draft, there will be an RFD for soliciting further input. ",10
"afw Wcs object copying does not copy exactly
A probable bug in WCSLIB is causing {{wcscopy}} to create copies of {{Wcs}} objects which are not the same as the object that was copied. In some cases when this object is passed to {{wcsset}} it fails, as the {{Wcs}} object contains impossible values.    This has behaviour is non-deterministic (failure is only seen occasionally). The error has only been observed on OSX, but we do not believe it to be operating system dependent (except insofar as different systems and compilers produce different memory layouts and hence different failure modes). This reliably causes {{ci_hsc}} to fail when running on a Mac.    Relevant lines in {{afw}} are {{image/Wcs.cc:140}} and the {{Wcs}} copy constructor in {{image/Wcs.cc:468}}    Additionally a bug has been found in {{image/Wcs.cc}} on line 485 where the flag property should be set on an element, and not on the object itself, ie {{_wcsInfo[i]->flag = -1;}}.",6
"Please improve the documentation for TransformTask and derivatives
While working on DM-4629 (overhauling {{ProcessCcdTask}}) [~rowen] stumbled over {{TransformTask}}, which he wasn't previously familiar with. Existing Doxygen documentation covers what this task does, but lacks context as to why it's useful. Please provide a high-level overview of what the intention is here.",2
"Improve MySQL proxy code and add unit tests
QServ's proxy needs some cleanup:    1. Standardize passing of q and qU parameters to methods  2. Comment removal may have never worked  3. Whitespace translation is likely buggy  4. A few unit tests verifying routing of queries would be nice  ",4
"Build MVP of ltd-keeper web app covering ltd-mason interface
This ticket is to create an MVP of the ltd-keeper web app (RESTful API) that tracks versions of LSST the Docs’ published software documentation. Specifically this ticket will implement the RESTful endpoints needed by ltd-mason. See [SQR-006|http://sqr-006.lsst.io] for design information.    [SQR-006|http://sqr-006.lsst.io] will be updated in this ticket as the design is clarified in implementation.",14
"Add S3/Route53 project provisioning capabilities to ltd-keeper
An **authenticated** user should be able to provision (and likewise, delete) an entire published software documentation project via ltd-keeper’s RESTful API. This includes creating an S3 bucket in SQuaRE’s AWS account and setting up Route 53 DNS. The user should also be able to delete a project. This ticket will add AWS affordances to ltd-keeper. DM-4950 will be responsible for hooking this functionality into the methods that service API calls.",3
"delegate argument parsing to CmdLineTask instances
Command-line argument parsing of data IDs for {{CmdLineTask}} s is currently defined at the class level, which means that we cannot make data ID definitions dependent on task configuration.  That in turn requires custom {{processCcd}} scripts for cameras that start processing at a level other than ""raw"" (SDSS, DECam with community pipeline ISR, possibly CFHT).    Instead, we should let {{CmdLineTask}} *instances* setup command-line parsing; after a {{CmdLineTask}} is constructed, it will have access to its final configuration tree, and can better choose how to parse its ID arguments.    I've assigned this to Process Middleware for now, since that's where it lives in the codebase, but it may make more sense to give this to [~rowen], [~price], or [~jbosch], just because we've already got enough familiarity with the code in question that we could do it quickly.  I'll leave that up to [~swinbank], [~krughoff], and [~mgelman2] to decide.",2
"Update pyfits
The final version of {{pyfits}} has just been released. This ticket covers updating to that version. This will be helpful in determining whether the migration to {{astropy.io.fits}} will be straightforward or complicated.",1
"Adapt SRD-based measurements of astrometric performance for validate_drp
Adapt the SRD-based specifications for calculation of astrometric performance.  Follow the examples for AM1, AM2 as presented at    https://confluence.lsstcorp.org/pages/viewpage.action?pageId=41785659    and detailed in DM-3057, DM-3064",4
"Generate JSON output from validate_drp for inclusion in a test harness
Generate JSON output from validate_drp for inclusion in a test harness.    Generate a file that summarizes the key metrics calculated by `validate_drp`.      Develop naming conventions that will make it easy to plug into the eventual harness being developed as part of DM-2050.",2
"Hard copy support- saving regions
This ticket will only do the region saving.    The scope has change somewhat since region saving will talk a little longer and making the png requires some server side work. DM-6139",10
"ci_hsc fails to execute tasks from with SCons on OSX 10.11/SIP
The {{ci_hsc}} package executes a number of command line tasks directly from SCons based on {{Command}} directives in a {{SConstruct}} file. On an OSX 10.11 system with SIP enabled, there are two distinct problems which prevent the necessary environment being propagated to the tasks:  * -The {{scons}} executable starts with a {{#!/usr/bin/env python}}. Running through {{/usr/bin/env}} strips {{DYLD_LIBRARY_PATH}} from the environment.- (duplicates DM-4954)  * SCons executes command using the [{{sh}} shell on posix systems|https://bitbucket.org/scons/scons/src/09e1f0326b7678d1248dab88b28b456fd7d6fb54/src/engine/SCons/Platform/posix.py?at=default&fileviewer=file-view-default#posix.py-105]. By default, that means {{/bin/sh}} on a Mac, which, again, will strip {{DYLD_LIBRARY_PATH}}.    Please make it possible to run {{ci_hsc}} on such a system.",1
"LSST vs. HSC stack comparison: PSF estimation
In order to determine the cause of the output differences between single frame processing runs of the same data using the LSST vs. HSC stacks (see figures attached to DM-4730), a detailed look at some of the image characterization steps is required.  This ticket involves a detailed investigation of the initial PSF estimation including:  {panel: title=LSST vs. HSC stack runs:}  - a comparison of the initial object detection (will likely involve looking at the initial background estimate as well as the specific assignment of footprints)  - which objects are selected as PSF candidates  - the initial PSF model (as a function of position)  {panel}",5
"Obs_Subaru camera mapper has wrong deep_assembleCoadd_config
When lsst switched to using SafeClipAssembleCoaddTask, the camera mapper for hsc was not updated accordingly. This causes ci_hsc to fail when it attempts to verify the config class type for the deep_coadd. Camera mapper should be updated accordingly",1
"January Operation Support Related Tasks
Account cleanup process for existing infrastructure (Identify accounts, assign sponsors)    Reconcile inventory between NCSA and Aura (on going). Mock request was generated by Aura for dry run audit. Several machines have been found not included in inventory. Task to be completed in February.",3
"Investigate Roger as fallover for Nebula
Investigate Roger OpenStack as fallback for Nebula during outages. Internally, this required technical and coordination meetings. Externally, this required interfacing with Square in order to facilitate a proper evaluation.     This task is ongoing. ",2
"January AAA Tasks
Attended local AAA meetings and reviewed documentation. ",1
"January Tasks
Security meeting with Paul, Bill, Eyrich to review goals and coordinate efforts.   Initial draft of the procurement plan. Waiting for hardware contract.   Updates to internal FY16 cost estimate spreadsheet and planning (not LDM-144).  Updating expected expenditures based on update quotes.   Meetings and discussions with OBFS with respect to new vendors within MHEC and procurement approval processing for FY16 components.",4
"January Tasks
Technology and pricing updates to LDM-144. Explanation update in LDM-143.    Meetings with multiple vendors re: longer term technology forecasts.",15
"January Tasks
Mtg w/ IN2P3 re: ITIL implementation experiences.     Mtg w/ IN2P3 re: tape recall ordering",1
"Jason January Tasks
Activities this month include: IT sys admin meetings, LSST internal project meetings, conducting, coordinating, discussing interviews. Meeting with candidates. ICI coordination meeting (Randy). Discussion of work-to-be-done with onboarded teammates. Relaying task prioritization to IT for LSST-related activities. ",4
"DM Power Requirements
Further discussions about power requirements at the Chilean DC.",2
"Investigate logging, monitoring and metrics technologies and architecture
Investigate technologies and architectures to use with panopticon, our logging system. Perform preliminary research and evaluations into ELK (Elasticsearch, Logstash and Kibana), extensions to ELK and other alternatives.",24
"Meetings, Jan 2016
verfication dataset meetings, TechTalk, RFD, local middleware-related meetings, etc",2
"LOE, Jan 2016
LSST local group meetings, postdoc meeting, other local meetings, etc",2
"Reconsider high detection threshold in CharacterizeImageTask
[~price] makes the [reasonable recommendation|https://community.lsst.org/t/why-was-detection-includethresholdmultiplier-10-for-old-processccdtask/500/6] that we consider providing PSF estimation with the N brightest sources in the image, rather than only detecting bright sources.    [~rowen] reasonably believes that this is beyond the scope of DM-4692, hence this new issue.    There should be very little new code needed here, but it may involve quite a bit of experimentation and validation.",8
"upstream patches/deps from conda-lsst
Where ever possible, missing dep information and patches from conda-lsst should be upstreamed.  The patches have already been observed to cause builds to fail due to upstream changes.",3
"Finish data distribution prototype (March)
nan",3
"Prepare for auth session at JTM
Prepare for JTM session with a working title of “How Authentication/Authorization technology can be used to implement and enforce data access rights and operational processes for LSST"".  Prepare a final title and agenda for the session. Tuesday from 3:30pm - 5:00pm.",5
"Save algorithm metadata in multiband.py
The various {{Tasks}} in {{multiband.py}} do not attach the {{self.algMetadata}} instance attribute to their output tables before writing them out, so we aren't actually saving information like which radii were used for apertures.    We should also make sure this feature is maintained in the processCcd.py rewrite.",3
"work flow of light curve visulizaiton
Generate a description document of work flow that a scientist would go through in order to do time series research, visualize the light curve.",4
"review of dependency on the third party packages
We need to periodically review the status of the third party software packages that Firefly depends on. Making a plan to do upgrade if needed.   package.json lists out the dependencies Firefly has on the third party software. The attached file was last modified 2016-02-09.    package.json_version lists the current version of the third party packages, major changes were indicated by (M). The attached file was created on 2016-02-29.     bq.      ""babel""     : ""5.8.34"",                           6.5.2 (M)      ""history""   : ""1.17.0"",                           2.0.0 (M)      ""icepick""   : ""0.2.0"",                            1.1.0 (M)                ""react-highcharts"": ""5.0.6"",                      7.0.0 (M)      ""react-redux"": ""3.1.2"",                           4.4.0 (M)      ""react-split-pane"": ""0.1.22"",                     2.0.1 (M)      ""redux-thunk"": ""0.1.0"",                           1.0.3 (M)      ""redux-logger"": ""1.0.9"",                          2.6.1 (M)      ""validator"" : ""4.5.0"",                            5.1.0 (M)      ""chai"": ""^2.3.0"",                                 3.5.0 (M)      ""esprima-fb"": ""^14001.1.0-dev-harmony-fb"",        15001.1001.0-dev-harmony-fb (M)      ""babel-eslint""      : ""^4.1.3"",                   5.0.0 (M)      ""babel-loader""      : ""^5.3.2"",                   6.2.4 (M)      ""babel-plugin-react-transform"": ""^1.1.0"",         2.0.0 (M)      ""babel-runtime""     : ""^5.8.20"",                  6.6.0 (M)      ""eslint""            : ""^1.10.3"",                  2.2.0 (M)      ""eslint-config-airbnb"": ""0.1.0"",                  6.0.2 (M) works with eslint 2.2.0      ""eslint-plugin-react"": ""^3.5.1"",                  4.1.0 (M)  works with eslint 2.2.0      ""extract-text-webpack-plugin"": ""^0.8.0"",          1.0.1 (M)      ""html-webpack-plugin"": ""^1.6.1"",                  2.9.0 (M)      ""karma-sinon-chai"": ""^0.3.0"",                     1.2.0 (M)      ""redux-devtools""    : ""^2.1.2"",                   3.3.1 (M)      ""webpack"": ""^1.8.2""                               1.12.14, 2.1.0 beta4 (M)          ",2
"Design single-sign-on authentication system for webserv
Outline a design of the authentication system (based on components provided by NCSA) that will support single sign-on. Current thinking involves two tokens: application token to certify the app is legitimate and to determine which users it can represent, and user token",6
"Extend webserv API to pass security tokens
Extend the [API|https://confluence.lsstcorp.org/display/DM/AP] to pass security tokens.",8
"Update validate_drp for El Capitan
validate_drp does not work on El Capitan due to SIP (System Integrity Protection) stripping DYLD_LIBRARY_PATH from shell scripts. The simple fix is to add  {code}  export DYLD_LIBRARY_PATH=${LSST_LIBRARY_PATH}  {code}  near the top of the scripts.",1
"Benchmark dipole measurement (dipole fitting)
Benchmark dipole measurement (dipole fitting), compare speed directly to psf-fit (and/or galaxy measurement) task. Runtime should be comparable (~factor of two?) - if not understand why. Evaluate new implementation vs. current impl. Accuracy?",8
"Fix rotation for isr in obs_subaru
Approximately half of the HSC CCDs are rotated 180 deg with respect to the others.  Two others have 90 deg rotations and another two have 270 deg rotations (see [HSC CCD layout|http://www.naoj.org/Observing/Instruments/HSC/CCDPosition_20150804.png]) .  The raw images for the rotated CCDs thus need to be rotated to match the rotation of their associated calibration frames prior to applying the corrections.  This is accomplished by rotating the exposure using the *rotated* context manager function in {{obs_subaru}}'s *isr.py* and the *nQuarter* specification in the policy file for each CCD.  Currently, *rotated* uses {{afw}}'s *rotateImageBy90* (which apparently rotates in a counter-clockwise direction) to rotated the exposure by 4 - nQuarter turns.  This turns out to be the wrong rotation for the odd nQuarter CCDs as shown here:   !ccd100_nQuarter3.png|width=200!  top left = raw exposure as read in  top right = flatfield exposure as read in  bottom left = _incorrectly_ rotated raw exposure prior to flatfield correction",2
"Implement new dipole fitting algorithm as SimpleAlgorithm
Implement new dipole fitting algorithm as SimpleAlgorithm -- implement measure, fail methods, define flags",14
"Make ci_hsc resumable
if ci_hsc fails for any reason, (or is cancelled) it must start from the beginning of processing again. This is because of the use of functools.partial to generate dynamic function. These differ enough in their byte code that scons thinks each build has a new function definition passed to the env.command function. Using lambda would suffer from the same problem. This ticket should change how the function signature is calculated such that scons can be resumed.    This work does not prevent this from being used as a ci tool, as the .scons directory can be deleted which will force the whole SConstruct file to run again.",2
"Please trim config overrides in validate_drp
validate_drp will test more of our code if it uses default config parameters wherever possible. To that effect I would like to ask you to eliminate all config overrides that are not essential and document the reasons for the remaining overrides.    For DECam there are no overrides that are different than the defaults, so the file can simply be emptied (for now).    For CFHT there are many overrides that are different, and an important question is whether the overrides in this package are better for CFHT data than the overrides in obs_cfht; if so, please move them to obs_cfht.    As a heads up: the default star selector is changing from ""secondMoment"" to ""objectSize"" in DM-4692 and I hope to allow that in validate_drp, since it works better and is better supported.    Sorry for the incorrect component, but validate_drp is not yet a supported component in JIRA (see DM-5004)",1
"remove REUSE_DATAREPO in testCoadds in pipe_tasks
When the test fails and the output directory is written but not populated, subsequent test executions fail every time until the output directory is deleted or REUSE_DATAREPO is set to False. This is misleading for users who don't know about this hidden feature.    Furthermore, the REUSE_DATAREPO=False feature is broken; setting it False causes NameError: global name 'DATAREPO_ROOT' is not defined.    It would be better if the test cleaned up after itself (deleted all outputs) every time. If it's really important to reuse the outputs then the dir should be cleaned up in the case of failed writes and/or corruption.    ",1
"F16 Data Access Model Refresh
A refresh of the storage / IO model (LDM-141). The work involves understanding cost impact, and discussing the impact on science. ",33
"Resolve development issues by testing using WAN Emulator
A test plan draft was written and some short meetings were held regarding the use of the WAN Emulator. The manuals for the Apposite Netropy 40G emulator were retrieved and read. The test plan draft for three test projects is attached.",5
"Continued WBS planning
Finished out all necessary fields in the first cut at the WBS.  Split project into work phases,   Began drilling down into the milestones for each phase, with accurate estimation the goal.  Provided miscellaneous diagrams to capture expected functionality for various state transitions throughout the system.",10
"Convert Confluence DM Developer Guide to Sphinx (hack day) 
This is a hack day sprint to convert all remaining content on https://confluence.lsstcorp.org/display/LDMDG to reStructuredText content in the Sphinx project at https://github.com/lsst-sqre/dm_dev_guide and published at http://developer.lsst.io.    The top priority for this sprint is to port all content into reST and have it tracked by Git.    h2. Sprint ground rules    # Before the sprint, clone {{https://github.com/lsst-sqre/dm_dev_guide.git}} and {{pip install -r requirements.txt}} in a Python 2.7 environment so that you can locally build the docs ({{make html}}).  # Claim a page from the list below by putting your name on it. Put a checkmark on the page when you’ve merged it to the ticket branch (see below).  # See http://developer.lsst.io/en/latest/docs/rst_styleguide.html for guidance on writing our style of reStructuredText. Pay attention to the [heading hierarchy|http://developer.lsst.io/en/latest/docs/rst_styleguide.html#sections] and [labelling for internal links|http://developer.lsst.io/en/latest/docs/rst_styleguide.html#internal-links-to-labels].  # If you use Pandoc to do an initial content conversion, you still need to go through the content line-by-line to standardize the reStructuredText. I personally recommend copy-and-pasting-and-formatting instead of using Pandoc.  # Your Git commit messages should include the URL of the original content from Confluence.  # Merge your work onto the {{tickets/DM-5013}} ticket branch. Rebase your personal work branch before merging. JSick is responsible for merging this ticket branch to {{master}}.  # Put a note at the top of the confluence page with the new URL; root is {{http://developer.lsst.io/en/latest/}}.    h2. Planned Developer Guide Table of Contents    We’re improving the organization of DM’s Developer Guide; there isn’t a 1:1 mapping of Confluence pages to developer.lsst.io pages. Below is a proposed section organization and page structure. These sections can still be refactored based on discussion during the hack day.    h3. Getting Started — /getting-started/    * ✅ *Onboarding Checklist* (Confluence: [Getting Started in DM|https://confluence.lsstcorp.org/display/LDMDG/Getting+Started+in+DM]). I’d like this to eventually be a quick checklist of things a new developer should do. It should be both a list of accounts the dev needs to have created, and a list of important developer guide pages to read next. The NCSA-specific material should be spun out. [[~jsick]]  * *Communication Tools* (new + DM Confluence [Communication and Links|https://confluence.lsstcorp.org/display/DM/Communication+and+Links]). I see this as being an overview of what methods DM uses to communicate, and what method should be chosen for any circumstance.  * *Finding Code on GitHub* (new). This should point out all of the GitHub organizations that a developer might come across (DM and LSST-wide), and point out important repositories within each organization. Replaces the confluence page [LSST Code Repositories|https://confluence.lsstcorp.org/display/LDMDG/LSST+Code+Repositories]    h3. Processes — /processes/    * ✅ *Team Culture and Conduct Standards* (confluence)  * ✅ *DM Development Workflow with Git, GitHub, JIRA and Jenkins* (new & Confluence: [git development guidelines for LSST|https://confluence.lsstcorp.org/display/LDMDG/git+development+guidelines+for+LSST] + [Git Commit Best Practices|https://confluence.lsstcorp.org/display/LDMDG/Git+Commit+Best+Practices] + [DM Branching Policy|https://confluence.lsstcorp.org/display/LDMDG/DM+Branching+Policy])  * ✅ *Discussion and Decision Making Process* (new & [confluence|https://confluence.lsstcorp.org/display/LDMDG/Discussion+and+Decision+Making+Process])  * ✅ *DM Wiki Use* ([confluence|https://confluence.lsstcorp.org/display/LDMDG/DM+Wiki+Use]) [[~swinbank]]  * ✅ *Policy on Updating Doxygen* ([confluence|https://confluence.lsstcorp.org/display/LDMDG/Policy+on+Updating+Doxygen]); needs to be addressed with TCT. Inter-link with the developer workflow page. [[~jsick]] (we’re just re-pointing the Confluence page to the workflow document)  * ✅ *Transferring Code Between Packages* ([confluence|https://confluence.lsstcorp.org/display/LDMDG/Transferring+Code+Between+Packages]) [[~swinbank]]  * -*Policy on Changing a Baseline Requirement*- ([confluence|https://confluence.lsstcorp.org/display/LDMDG/Policy+on+Changing+a+Baseline+Requirement])  * ✅ *Project Planning for Software Development* ([confluence|https://confluence.lsstcorp.org/display/LDMDG/Project+Planning+for+Software+Development]) [[~swinbank]]  * ✅ *JIRA Agile Usage* ([confluence|https://confluence.lsstcorp.org/display/LDMDG/JIRA+Agile+Usage]) [[~swinbank]]  * -*Technical/Control Account Manager Guide*- ([confluence|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=21397653]) (Do not port; see discussion below.)  * *Licensing* (new) Need a centralized page to discuss license and copyright policies; include boilerplate statements.    h3. Coding Guides — /coding/    * ✅ *Introduction* and note on stringency language (confluence: [DM Coding Style Policy|https://confluence.lsstcorp.org/display/LDMDG/DM+Coding+Style+Policy])  * ✅ *DM Python Style Guide* (confluence: [Python Coding Standard|https://confluence.lsstcorp.org/display/LDMDG/Python+Coding+Standard])  * ✅ *DM C++ Style Guide* (confluence pages: [C++ Coding Standard|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=16908666] + [C++ General Recommendations|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=16908756] + [C++ Naming Conventions|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=16908685] + [C++ Files|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=16908674] + [C++ Statements|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=16908706] + [C++ Layout and Comments|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=16908737] + [Policy on use of C++11/14|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=20283399] + [On Using ‘Using’|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=20283856])  * Coding Style Linters (new; draft from confluence [C++ Coding Standards Compliance|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=20283861] and [Python Coding Standards Compliance|https://confluence.lsstcorp.org/display/LDMDG/Python+Coding+Standards+Compliance]  * ✅ *Using C++ Templates* ([confluence|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=20284190]); this page needs to severely edited or re-written, however.  * ✅ *Profiling* ([confluence|https://confluence.lsstcorp.org/display/LDMDG/Profiling|]). Also add a section ‘Using Valgrind with Python' (new) [[~jsick]]  * ✅ *Boost Usage* ([TRAC|https://dev.lsstcorp.org/trac/wiki/TCT/BoostUsageProposal]) [[~tjenness]]  * ✅ *Software Unit Test Policy* ([confluence|https://confluence.lsstcorp.org/display/LDMDG/Software+Unit+Test+Policy]) [[~swinbank]]  * ✅ *Unit Test Coverage Analysis* ([confluence|https://confluence.lsstcorp.org/display/LDMDG/Coverage+Analysis]) [[~swinbank]]  * ✅ *Unit Testing Private C++ Functions* ([trac|https://dev.lsstcorp.org/trac/wiki/UnitTestingPrivateFunctions]) [[~swinbank]]    h3. Writing Docs — /docs/    * *Introduction* (new): Overview of DM’s documentation needs; links resources on technical writing.  * *English Style Guide* (new): Supplement the [LSST Style Manual|https://www.lsstcorp.org/docushare/dsweb/Get/Document-13016/LSSTStyleManual.pdf] and provide English style guidance specific to DM. Capitalization of different heading levels; use of Chicago Manual of Style; a ‘this, not that’ table of spelling and word choices.  * ✅ *ReStructuredText Style Guide* (new)  * ✅ *Documenting Stack Packages* (new)  * ✅ *Documenting Python Code* (new)  * ✅ *Documenting C++ Code* (confluence, adapted from [Documentation Standards|https://confluence.lsstcorp.org/display/LDMDG/Documentation+Standards]); needs improvement  * ✅ *Writing Technotes* (new; port README from [lsst-technote-bootstrap|https://github.com/lsst-sqre/lsst-technote-bootstrap/blob/master/README.rst])    h3. Developer Tools — /tools/    * ✅ *Git Setup and Best Practices* (new)  * ✅ *Using Git Large File Storage (LFS) for Data Repositories* (new)  * ✅ *JIRA Work Management Recipes* (new)  * ✅ *Emacs Configuration* ([Confluence|https://confluence.lsstcorp.org/display/LDMDG/Emacs+Support+for+LSST+Development]). See DM-5045 for issue with Emacs config repo - [~jsick]  * ✅ *Vim Configuration* ([Confluence|https://confluence.lsstcorp.org/display/LDMDG/Config+for+VIM]) - [~jsick]    h3. Developer Services — /services/    * ✅ *NCSA Nebula OpenStack Guide* (Confluence: [User Guide|https://confluence.lsstcorp.org/display/LDMDG/NCSA+Nebula+OpenStack+User+Guide] + [Starting an Instance|https://confluence.lsstcorp.org/display/LDMDG/Introduction+to+Starting+a+Nebula+Instance] + [Using Snapshots|https://confluence.lsstcorp.org/display/LDMDG/Start+an+Instance+using+a+base+snapshot+with+the+LSST+Stack]. Add the [Vagrant instructions too from SQR-002|http://sqr-002.lsst.io]? [[~jsick]]  * ✅ *Using lsst-dev* (Confluence: [notes Getting Started|https://confluence.lsstcorp.org/display/LDMDG/Getting+Started+in+DM] + [Developer Tools at NCSA|https://confluence.lsstcorp.org/display/LDMDG/Developer+Tools+at+NCSA]  * ✅ *Using the Bulk Transfer Server at NCSA* ([confluence|https://confluence.lsstcorp.org/display/LDMDG/Using+the+Bulk+Transfer+Server+at+NCSA]) [[~jsick]]    h3. Build, Test, Release — /build-ci/    * *Eups for LSST Developers* (new) [[~swinbank]]  * ✅ *The LSST Software Build Tool* → ‘Using lsstsw and lsst-build' ([confluence|https://confluence.lsstcorp.org/display/LDMDG/The+LSST+Software+Build+Tool]); lsstsw and lsst-build documentation. [[~swinbank]]  * *Using DM’s Jenkins for Continuous Integration* (new) [~frossie]   * ✅ *Adding a New Package to the Build*([confluence|https://confluence.lsstcorp.org/display/LDMDG/Adding+a+new+package+to+the+build]) [[~swinbank]]  * ✅ *Distributing Third-Party Packages with Eups* ([confluence|https://confluence.lsstcorp.org/display/LDMDG/Distributing+third-party+packages+with+EUPS]) [[~swinbank]]  * ✅  *Triggering a Buildbot Build* ([confluence|https://confluence.lsstcorp.org/display/LDMDG/Triggering+a+Buildbot+Build]) [~frossie]  * ✅ *Buildbot Errors FAQ* ([confluence|https://confluence.lsstcorp.org/display/LDMDG/Buildbot+FAQ+on+Errors]) [~frossie]  * * Buildbot configuration ([confluence|https://confluence.lsstcorp.org/display/LDMDG/Buildbot+Configuration+and+Setup] [~frossie]    * *Creating a new DM Stack Release* ([confluence|https://confluence.lsstcorp.org/display/LDMDG/Creating+a+new+DM+Stack+Release]); though this page or a modern equivalent should probably belong with the software docs? [~frossie]    _A lot of work should go into this section._ Have something about Scons? Or maybe that belongs in the doc of each relevant software product.    h2. Leftover Confluence pages    h3. The following pages should be moved to a separate Confluence space run by NCSA:    * [NCSA Nebula OpenStack Issues|https://confluence.lsstcorp.org/display/LDMDG/NCSA+Nebula+OpenStack+Issues]  * [DM System Announcements|https://confluence.lsstcorp.org/display/LDMDG/DM+System+Announcements]  * [NCSA Development Servers|https://confluence.lsstcorp.org/display/LDMDG/DM+Development+Servers]    h3. The following pages are either not relevant, generally misplaced, or need to be updated/recalibrated:    * [Git Crash Course|https://confluence.lsstcorp.org/display/LDMDG/Git+Crash+Course]  * [Basic Git Operations|https://confluence.lsstcorp.org/display/LDMDG/Basic+Git+Operations]  * [Handling Git Push Problems|https://confluence.lsstcorp.org/display/LDMDG/Handling+Git+Push+Problems]  * [LSST Code Repositories|https://confluence.lsstcorp.org/display/LDMDG/LSST+Code+Repositories]; see the proposed “Finding Code on GitHub” page for a replacement.  * [Standards and Policies|https://confluence.lsstcorp.org/display/LDMDG/Standards+and+Policies]: this is a good TOC for the Confluence docs; but not longer needed for the new docs.  * [Documentation Guidelines|https://confluence.lsstcorp.org/display/LDMDG/Documentation+Guidelines]. Some of this could be re-purposed into an intro to the ‘Writing Documentation’ section; some of this should go in a ‘Processes' page.  * [DM Acknowledgements of Use|https://confluence.lsstcorp.org/display/LDMDG/DM+Acknowledgements+of+Use]: this probably belongs in documentation for the software projects that actually used this work.",5
"Set doRenorm default to False in AssembleCcdTask
Change the default value of {{AssembleCcdConfig.doRenorm}} to {{False}} for the reasons given in RFC-157 and to implement that RFC.",1
"Optionally report do-nothing config overrides
As discussion on DM-4692 and in various HipChat rooms, it's too easy for camera-level config override files to contain many options that don't actually change anything, because they simply override the defaults with the same default values.  To aid in tracking these down and removing them, we should have an option in which {{CmdLineTask}} s (delegating to {{pex_config}}) refuse or warn about overrides that have no effect.    We should probably not make failing on do-nothing overrides the default behavior, but we could consider making warning the default behavior.  Mostly, I think it's important just to be able to find such options when wanted.",4
"Modernize version check scripts in matplotlib and numpy packages
The version check scripts in the stub {{matplotlib}} and {{numpy}} eups packages use old Python conventions. They should be updated to work with 2.7+.",1
"FITS Visualizer porting: Expanded mode single - part 2
I split DM-4497 into two part so I can demonstrate code reviews. This part has paging controller & layout cleaned up.  This tickets is messy because it involves a lot of refactoring of the reducers.  Therefore I am going to end it and move the rest of the UI work to DM-5088.",4
"Modernize python code in Qserv scons package
The {{site_scons}} Python code is not using current project standards. For example, print is not a function, exceptions are not caught {{as e}}, {{map}} is called without storing the result and {{map/filter/lambda}} are used where list comprehensions would be clearer.    Most of these fixes are trivial with {{futurize}}.",1
"Adds, Moves, Change support for DNS, network, IP addressing, etc
nan",6
"define rack, pdu specifications and obtain pricing quotes
nan",3
"Base site and summit RFP
Working with Ron to create a RFP for the acquisition of network equipment for the summit and base. ",10
"Fix dependencies for eups-packaged sqlalchemy
Eups-packaged sqlalchemy lists {{mysqlclient}} as required dependency which is not really right. sqlalchemy does not directly depend on mysql client stuff, instead it determines at run time which python modules it needs to load depending on what exact driver client code is requesting (and {{mysqlclient}} does not actually provides python module so this dependency does not even make anything useful). So dependency on specific external package should be declared on client side and not in sqlalchemy, {{mysqlclient}} should be removed from sqlalchemy.table.",1
"eval NCSA vSphere/OSX support -- first attempt
nan",1
"Design interconnect for GPFS cluster prototype
nan",4
"Create physical and logical network diagrams for first phase of purchases
nan",6
"Tests fail on Qserv on OS X El Capitan because of SIP
OS X El Capitan introduced System Integrity Protection which leads to dangerous environment variables being stripped when executing trusted binaries. Since {{scons}} is launched using {{/usr/bin/env}} the tests that run do not get to see {{DYLD_LIBRARY_PATH}}. This causes them to fail.    The same fix that was applied to {{sconsUtils}} (copying the path information from {{LSST_LIBRARY_PATH}}) needs to be applied to the test execution code used by Qserv's private {{site_scons}} utility code.",2
"X16 Data Access and Database Documentation
Update the documentation for Data Access and Database - bring it up to date with the design. This includes LDM-135 (Database Design), and creating a new LDM document or DAX Design).",73
"DAX & DB Docs (Fritz, March)
* Document Data Distribution  * Create structure for DAX doc  * Bring over Provenance documentation from prov_prototype  * Update LDM-135 to reflect the updates to the storage/IO model  * Update LDM-152  * Fix LDM-135: 3.3.6.4 and 3.3.6.5 should be 3rd level, so 3.3.7 and 3.3.8  ",3
"DAX & DB Docs (John)
* Refresh shared scans design documentation (in LDM-135)  * Add info about query cancellation (in LDM-135)",10
"DAX & DB Docs (AndyS)
* Document db and table metadata  * Document async queries  * Document data loader",5
"DAX & DB Docs (Nate)
* Improve butler documentation",8
"DAX & DB Docs (Brian)
* Document webserv/imgserv/metaserv/dbserv",10
"DAX & DB Docs (Mike)
Document secondary index",5
"DAX & DB Docs (Serge)
* Document spatial indexing  * Document database ingest  * Refresh ""Stored Procedures and Function"" in LDM-135",6
"Load panstarrs data to qserv
nan",19
"Setup webserv with panstarrs data
nan",5
"update ""newinstall.sh"" nebula images & docker containers
[~hchiang2] is looking for nebula images newer than {{w_2015_45}} (from the exploratory work in DM-4326) and [~gdaues] is interested in images with a complete {{lsst_distrib}} install for orchestration testing.  New builds should incorporate the pending change to {{newinstall.sh}} that converts from {{anaconda}} to {{miniconda}}.",6
"SingleFrameVariancePlugin takes variance of entire image
{{SingleFrameVariancePlugin}} takes the median variance of the entire image, rather than within an aperture around the source of interest.  A {{Footprint}} is constructed with the aperture, but it is unused.    This means that this plugin takes an excessive amount of run time (255/400 sec in a recent run of processCcd on HSC {{visit=1248 ccd=49}} with DM-4692).",1
"Design replacement for A.net index files
We need a simple way to hold index files that will be easy to use and simple to set up.",2
"Some small things slipped through in winter 2016
Fix up things that slipped through or were delayed in winter 2016.  The individual things are small parts of larger epics and typically are the result of emergent work or increased scope.",30
"Implement Approx/Interp improvements
We are making due with the current approximation and interpolation scheme, but the two should be merged.  This must really be done after the HSC merge because of the difficulty of doing large refactoring before then.",30
"Assess priority of Aprox/Interp upgrades.
This is to assess the priority of a major approximation and interpolation refactor.",4
"RFC corrections for ISR.
Create a list of ISR requirements and have it RFCd.",9
"Assess the corrections that need to be imlemented
The stack can do many of the corrections needed.  Assess the status of the current algorithms and identify any deficiencies.",4
"Improve and implement crosstalk in ISR
-ISR needs the following correction algorithms: fringe, crosstalk, overscan (with discontinuity), and non-linearity.  Several of these are implemented in HSC and have been ported, but may need some work.-    As noted below this will now only be the crosstalk portion of the ISR upgrades.",10
"Design the refactoring for ProcessCcd
There is a significant design issue when refactoring a piece of this importance.  Carry out a design study to implement in DM-4692.",12
"Week end 1/09/16
Support for lsst-dev cluster, OpenStack, and accounts  for week ending January 9, 2016.",1
"Week end 1/16/16
Support for lsst-dev cluster, OpenStack, and accounts  for week ending January 16, 2016.",2
"Week end 1/23/16
Support for lsst-dev cluster, OpenStack, and accounts  for week ending January 23, 2016.",2
"Week end 1/30/16
Support for lsst-dev cluster, OpenStack, and accounts  for week ending January 23, 2016.",2
"New equipment setup and configuration (week end 1/23/16)
* Finished setting up Mac vSphere infrastructure with Paul",2
"New equipment setup and configuration (week end 1/30/16)
* Set up new lsst-dev7 as CentOS 7 server  * Continuing to set up IPMI on new test servers (working with Dell on issue with iDRAC license upgrade)",3
"Decommissioning old equipment (week end 1/16/16)
* Recovery of old LSST used equipment  ** Moved remaining surplussed last servers to wiping bench  ** Started wiping drives  ** Re-purposed 10 Dell 1950  ",2
"Decommissioning old equipment (week end 1/23/16)
* Complete the cleanup of last used NCSA systems",1
"Lenovo test server
* Mount Lenovo test server in LSST1 rack. Install fiber card and networking. Test PXE boot to 10G nic.  * Work on getting Lenovo to PXE boot to 10G card  * Booted satisfactorily to 1GB interface – loaded Centos 7  ** Abruptly ends after Menu with 10GB card  ",3
"PcaPsf can hit an assertion failure
This is bad for multiple reasons:  1. When multiprocessing, the assertion failure kills a single process, which prevents the final join of the multiple processes, so the job hangs forever.  2. The failure is not logged.  3. Hard assertions like this should only occur when we break the system integrity, which this does not (i.e., it's too big a hammer for the problem).    {code}  pprice@tiger-sumire:/tigress/pprice/dm-4692 $ eups list -s  afw                   tickets.DM-4692-gd8ad35cd96+1     b1901 setup  afwdata               2016_01.0         b1901 b1902 setup  astrometry_net        0.50.lsst2+5      b1901 b1902 setup  astrometry_net_data   sdss-dr9-fink-v5b         setup  base                  2016_01.0         b1901 b1902 setup  boost                 1.59.lsst5        b1901 b1902 setup  cfitsio               3360.lsst4        b1901 b1902 setup  coadd_chisquared      2016_01.0+6       b1901 setup  coadd_utils           2016_01.0+6       b1901 setup  daf_base              2016_01.0         b1901 b1902 setup  daf_butlerUtils       tickets.DM-4692-g048b33c50e+3     b1901 setup  daf_persistence       2016_01.0-1-gf47bb69+1    b1901 b1902 setup  display_ds9           2015_10.0+43      b1901 setup  doxygen               1.8.5.lsst1       b1901 b1902 setup  eigen                 3.2.5             b1901 b1902 setup  fftw                  3.3.4.lsst2       b1901 b1902 setup  geom                  10.0+50           b1901 b1902 setup  gsl                   1.16.lsst3        b1901 b1902 setup  ip_diffim             tickets.DM-4692-g543ea8fde5+3     b1901 setup  ip_isr                2016_01.0+6       b1901 setup  lsst_build            LOCAL:/tigress/pprice/lsstsw/lsst_build   setup  mariadbclient         master-gf2dee38289        b1901 b1902 setup  matplotlib            0.0.1+5           b1901 b1902 setup  meas_algorithms       tickets.DM-4692-g3d073a93d7+1     b1901 setup  meas_astrom           tickets.DM-4692-gbbf15418e6+1     b1901 setup  meas_base             LOCAL:/tigress/pprice/dm-4692/meas_base   setup  meas_deblender        2016_01.0+6       b1901 setup  minuit2               5.28.00.lsst2     b1901 b1902 setup  ndarray               10.1+58           b1901 b1902 setup  numpy                 0.0.1+5           b1901 b1902 setup  obs_subaru            LOCAL:/tigress/pprice/dm-4692/obs_subaru  setup  obs_test              tickets.DM-4692-g1533aee20f+1     b1901 setup  pex_config            2016_01.0         b1901 b1902 setup  pex_exceptions        2016_01.0         b1901 b1902 setup  pex_logging           2016_01.0         b1901 b1902 setup  pex_policy            2016_01.0         b1901 b1902 setup  pipe_base             2016_01.0+6       b1901 setup  pipe_tasks            LOCAL:/tigress/pprice/dm-4692/pipe_tasks  setup  psfex                 2016_01.0         b1901 b1902 setup  pyfits                3.4.0             b1901 b1902 setup  python                0.0.3             b1901 b1902 setup  python_d2to1          0.2.12            b1901 b1902 setup  pyyaml                3.11.lsst1        b1901 b1902 setup  scons                 2.3.5             b1901 b1902 setup  sconsUtils            2016_01.0         b1901 b1902 setup  skymap                2016_01.0+6       b1901 setup  skypix                10.0+347          b1901 setup  stsci_distutils       0.3.7-1-gb22a065  b1901 b1902 setup  swig                  3.0.2.lsst1       b1901 b1902 setup  utils                 2016_01.0         b1901 b1902 setup  wcslib                5.13.lsst1        b1901 b1902 setup  xpa                   2.1.15.lsst3      b1901 b1902 setup    pprice@tiger-sumire:/tigress/pprice/dm-4692 $ processCcd.py /tigress/HSC/HSC --rerun price/dm-4692 --rerun price/dm-4692 --id visit=1248 ccd=100 --clobber-config  /tigress/pprice/lsstsw/miniconda/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.    warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')  : Loading config overrride file '/tigress/pprice/dm-4692/obs_subaru/config/processCcd.py'  WARNING: Unable to use psfex: No module named extensions.psfex.psfexPsfDeterminer  Cannot import lsst.meas.extensions.photometryKron: disabling Kron measurements  Cannot enable shapeHSM (    File ""src/Utils.cc"", line 42, in std::string lsst::utils::getPackageDir(const std::string&)      Package meas_extensions_shapeHSM not found {0}  lsst::pex::exceptions::NotFoundError: 'Package meas_extensions_shapeHSM not found'  ): disabling HSM shape measurements  : Loading config overrride file '/tigress/pprice/dm-4692/obs_subaru/config/hsc/processCcd.py'  : input=/tigress/HSC/HSC  : calib=None  : output=/tigress/HSC/HSC/rerun/price/dm-4692  CameraMapper: Loading registry registry from /tigress/HSC/HSC/rerun/price/dm-4692/_parent/registry.sqlite3  CameraMapper: Loading calibRegistry registry from /tigress/HSC/HSC/CALIB/calibRegistry.sqlite3  processCcd: Processing {'taiObs': '2014-03-28', 'pointing': 817, 'visit': 1248, 'dateObs': '2014-03-28', 'filter': 'HSC-I', 'field': 'SSP_UDEEP_COSMOS', 'ccd': 100, 'expTime': 270.0}  processCcd.isr: Performing ISR on sensor {'taiObs': '2014-03-28', 'pointing': 817, 'visit': 1248, 'dateObs': '2014-03-28', 'filter': 'HSC-I', 'field': 'SSP_UDEEP_COSMOS', 'ccd': 100, 'expTime': 270.0}  processCcd.isr: Applying linearity corrections to Ccd 100  processCcd.isr.crosstalk: Applying crosstalk correction  processCcd.isr: Set 0 BAD pixels to 3147.74  processCcd.isr: Flattened sky level: 3847.800781 +/- 2114.507723  processCcd.isr: Measuring sky levels in 8x16 grids: 3884.324645  processCcd.isr: Sky flatness in 8x16 grids - pp: 15293.248379 rms: 1173.423587  processCcd.isr: Setting rough magnitude zero point: 34.678409  processCcd.charImage: Processing {'taiObs': '2014-03-28', 'pointing': 817, 'visit': 1248, 'dateObs': '2014-03-28', 'filter': 'HSC-I', 'field': 'SSP_UDEEP_COSMOS', 'ccd': 100, 'expTime': 270.0}  processCcd.charImage.repair: Identified 6044 cosmic rays.  processCcd.charImage.detectAndMeasure.detection: Detected 127 positive sources to 5 sigma.  processCcd.charImage.detectAndMeasure.detection: Resubtracting the background after object detection  processCcd.charImage.detectAndMeasure.measurement: Measuring 127 sources (127 parents, 0 children)   processCcd.charImage.measurePsf: Measuring PSF  /tigress/pprice/lsstsw/stack/Linux64/meas_algorithms/tickets.DM-4692-g3d073a93d7+1/python/lsst/meas/algorithms/objectSizeStarSelector.py:354: RuntimeWarning: invalid value encountered in less    bad = numpy.logical_or(bad, width < self._widthMin)  /tigress/pprice/lsstsw/stack/Linux64/meas_algorithms/tickets.DM-4692-g3d073a93d7+1/python/lsst/meas/algorithms/objectSizeStarSelector.py:355: RuntimeWarning: invalid value encountered in greater    bad = numpy.logical_or(bad, width > self._widthMax)  processCcd.charImage.measurePsf: PSF star selector found 6 candidates  meas.algorithms.psfDeterminer WARNING: You only have 3 eigen images (you asked for 4): reducing number of eigen components  meas.algorithms.psfDeterminer WARNING: You only have 1 eigen images (you asked for 3): reducing number of eigen components  meas.algorithms.psfDeterminer WARNING: You only have 1 eigen images (you asked for 2): reducing number of eigen components  python: /tigress/pprice/lsstsw/stack/Linux64/eigen/3.2.5/include/Eigen/src/Core/Redux.h:202: static Eigen::internal::redux_impl<Func, Derived, 3, 0>::Scalar Eigen::internal::redux_impl<Func, Derived, 3, 0>::run(const Derived&, const Func&) [with Func = Eigen::internal::scalar_max_op<double>; Derived = Eigen::CwiseUnaryOp<Eigen::internal::scalar_abs_op<double>, const Eigen::Matrix<double, -1, -1> >; Eigen::internal::redux_impl<Func, Derived, 3, 0>::Scalar = double]: Assertion `size && ""you are using an empty matrix""' failed.  Aborted  {code}    Note:  * This occurred while testing DM-4692.  The LOCAL pipe_tasks and obs_subaru are on that ticket branch.  The LOCAL meas_base is for the fix from DM-5050.  * One root cause of the bad PSF modeling may be bad rotations in the application of the calibs ([~lauren] is looking into that; don't know if there's a ticket number), but this should never happen regardless.",1
"Operations planning 
Draft Operations planning w.b.s.  Create additional activity Diagrams,  Draft DPPD Operations processing talk.",15
"January Management 
develop design and project management methods in conjunction with L1 design.  Deal with ARI labor component  General management of staff",15
"PropagateVisitFlags doesn't work with other pipeline components
{{PropagateVisitFlags}}, which was recently ported over from HSC on DM-4878, doesn't work due to some inconsistencies with earlier packages/tasks:   - The default fields to transfer have new names: ""calib_psfCandidate"" and ""calib_psfUsed""   - We're not currently transferring these fields from icSrc to src, so those fields aren't present in src anyway.  I propose we just match against icSrc for now, since it has all of the fields we're concerned with.   - It makes a call to {{afw.table.ExposureCatalog.subsetContaining(Point, Wcs, bool)}}, which apparently exists in C++ but not in Python; I'll look into seeing which HSC commits may have been missed in that port.",1
"Please add a package that includes obs_decam, obs_cfht and all validation_data datasets
It would be very helpful to have an lsstsw package that added all supported obs_* packages (certainly including obs_cfht and obs_decam, and I hope obs_subaru) and all validation_data_* packages. This could be something other than lsst_apps, but I'm not sure what to call it.",1
"Enable aperture correction on coadd processing
Aperture corrections are now coadded, so we can enable aperture corrections in measurements done on coadds.",1
"Add auto play,select which dialog, close button working,  to expanded mode
Add the auto play to expanded mode.  Add the choose which dialog to expanded mode. Make close button work.    I am breaking this up the expanded mode ticketa because the task is getting so big and ticket DM-5019 involved reducer refactoring.  Also the refactoring needs to get into the dev branch.",4
"Add task discovery on command line activator
I'll add a way to specify on the command line the path or the package to discover for CmdLineTask or SuperTasks",4
"Investigate alternative for networkx before RFC
I'll make sure I explored other alternatives before creating a RFC for adding networkx which by itself require other packages. This is needed for the pipe_flow_x work. I tried one stand-alone package before pygraphviz but then decided to migrate to networkx as it is more complete and allow other possible future features",4
"LDM documentation of butler basics & multiple repositories
nan",2
"Security plan renewal
Renewal of the LSST security plan.  Starts with DM.",3
"HSC backport: Set BAD mask for dead amps instead of SAT
This is a port of [HSC-1095|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1095] and a leftover commit from [HSC-1231|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1231]: [isr: don't perform overscan subtraction on bad amps|https://github.com/HyperSuprime-Cam/obs_subaru/commit/d6fe6cf5c4ecadebd5a344d163e1f1e60137c7e4] (noted in DM-3942).",3
"Redirect confluence based pages to new developer guide.
Delete and apply redirects to all migrated pages in old Confluence-based Developer Guide",1
"Make validateDrp a Task.
Make validateDrp a Task so   1. it can easily be run from the command line or programmatically.  2. it can import the standard command line arguments  3. it can be logged in the same way.    This eventually should fit into DM-2050, and DM-3859.",2
"Update validate_drp to use TransformTask to store calibrated measurements
Currently validate_drp uses some manual crude addition of calibration information and constructs new schemas to store this information.  This is essentially what TransformTask is meant for.  Using this would simplify the code, make it less fragile, and ideally eventually integrate more transparently with future calibration improvements or redefinitions of how zeropoints are tracked..    1. Learn how to use TransformTask.  Note DM-4948 is the doc task for this.  2. Adapt the code.  3. Verify unchanged results on existing validation_data_decam and validation_data_cfht.",2
"Add tests to validate_drp to verify SRD calculations and utility function behavior
The current validate_drp is woefully lacking in tests.    1. The key SRD metrics definitely need to have test cases that verify the calculation of these important metrics.  2. Overall the utility functions would benefit from testing.",4
"Polish IN2P3 cluster upgrade to CentOS7
What remains:    - problem with Docker 1.9.1+overlay+xfs => switch to Docker 1.10.1? Then switch back from devicemapper to overlay?  - problem with qserv uid: go back to 1000, instead of 1008?",4
"Docs for ltd-keeper
Create a documentation project within ltd-keeper that documents the RESTful API while it is being developed. This will allow the [SQR-006|http://sqr-006.lsst.io] technote to have a place to link to for detailed information.",1
"Fix --id examples in processCcd.py and friends to correctly show ""ccd=1^2"".
The required '^' convention for lists of things, e.g. {{ccd}}, {{filter}}, {{visit}} and such is surprising.  But, worse, the documentation is currently wrong in its examples and presents several {{ccd=1,2}}, {{patch=1,2}} examples.    * Fix the {{--id}} examples in {{pipe_tasks}} and other uses of processCcd.py in obs_* packages to correctly match the required syntax.    Here's the current list in {{pipe_tasks}}, but check other packages as well.    {code}  [serenity tasks] grep '[0-9],[0-9]' *.py | grep '""'  assembleCoadd.py:                               help=""data ID, e.g. --id tract=12345 patch=1,2"",  coaddBase.py:        parser.add_id_argument(""--id"", ""deepCoadd"", help=""data ID, e.g. --id tract=12345 patch=1,2"",  imageDifference.py:        parser.add_id_argument(""--id"", ""calexp"", help=""data ID, e.g. --id visit=12345 ccd=1,2"")  makeDiscreteSkyMap.py:            boxI = afwGeom.Box2I(afwGeom.Point2I(0,0), afwGeom.Extent2I(md.get(""NAXIS1""), md.get(""NAXIS2"")))  multiBand.py:        parser.add_id_argument(""--id"", ""deepCoadd"", help=""data ID, e.g. --id tract=12345 patch=1,2 filter=r"",  multiBand.py:                               help=""data ID, e.g. --id tract=12345 patch=1,2 filter=g^r^i"")  multiBand.py:        parser.add_id_argument(""--id"", ""deepCoadd"", help=""data ID, e.g. --id tract=12345 patch=1,2 filter=r"",  processCoadd.py:        parser.add_id_argument(""--id"", ""deepCoadd"", help=""data ID, e.g. --id tract=12345 patch=1,2"",  {code}",1
"Rewrite integration test queries with spatial constraint returning empty results
Some queries in the integration test suite return empty results, here's how to catch them:  {code:bash}  # this should be done for alll tests cases  egrep ""^0$"" ~/qserv-run/2016_02/tmp/qservTest_case02/outputs/mysql/*  # empty results files have also to be tracked  {code}    There parameters should be fixed to query a region containing data (use select * on object).",5
"Add scans for DRx-1 to the model
Per RFC-134 we need to support scans for DRx-1. This story involves building this into the model, costing it, and changing the baseline.",6
"Add scans for DRP-produced Dia* tables to the model
Per RFC-133, we need to support scans on DiaObject table, possibly Dia*Source tables as well. This story involves adding it to the model, costing it and adding it to the baseline.",6
"new conda 'mkl' dependent packages break meas_base tests
Continuum release/rebuilt a number of packages last friday to depend on the the Intel MKL library.     https://www.continuum.io/blog/developer-blog/anaconda-25-release-now-mkl-optimizations    There are [new feature named] versions that continue to use openblas but the MKL versions appear to be installed by default.  This causes at least multiple {{meas_base}} tests to fail.After extensive testing, I have confirmed that the meas_base tests do not fail with the equivalent 'nomkl' package.  In addition, mkl is closed source software that requires you to accept and download a license file or it is time-bombed to stop working after a trial period.      {code:java}      docker-centos-7: [ 36/36 ]  meas_base 2015_10.0-9-g6daf04b+7 ...      docker-centos-7:      docker-centos-7: ***** error: from /opt/lsst/software/stack/EupsBuildDir/Linux64/meas_base-2015_10.0-9-g6daf04b+7/build.log:      docker-centos-7: tests/sincPhotSums.py      docker-centos-7:      docker-centos-7: tests/measureSources.py      docker-centos-7:      docker-centos-7: tests/testApertureFlux.py      docker-centos-7:      docker-centos-7: tests/testJacobian.py      docker-centos-7:      docker-centos-7: tests/testScaledApertureFlux.py      docker-centos-7:      docker-centos-7: The following tests failed:      docker-centos-7: /opt/lsst/software/stack/EupsBuildDir/Linux64/meas_base-2015_10.0-9-g6daf04b+7/meas_base-2015_10.0-9-g6daf04b+7/tests/.tests/sincPhotSums.py.failed      docker-centos-7: /opt/lsst/software/stack/EupsBuildDir/Linux64/meas_base-2015_10.0-9-g6daf04b+7/meas_base-2015_10.0-9-g6daf04b+7/tests/.tests/measureSources.py.failed      docker-centos-7: /opt/lsst/software/stack/EupsBuildDir/Linux64/meas_base-2015_10.0-9-g6daf04b+7/meas_base-2015_10.0-9-g6daf04b+7/tests/.tests/testApertureFlux.py.failed      docker-centos-7: /opt/lsst/software/stack/EupsBuildDir/Linux64/meas_base-2015_10.0-9-g6daf04b+7/meas_base-2015_10.0-9-g6daf04b+7/tests/.tests/testJacobian.py.failed      docker-centos-7: /opt/lsst/software/stack/EupsBuildDir/Linux64/meas_base-2015_10.0-9-g6daf04b+7/meas_base-2015_10.0-9-g6daf04b+7/tests/.tests/testScaledApertureFlux.py.failed      docker-centos-7: 5 tests failed  {code}  (the exact cause of the test failures was not investigated as this should not have happened)    This change has also broken the ability to import an existing conda env from 2016-02-05 or earlier that uses scipy due to some sort of package version resolution problem.  Explicit declaring it as the scipy package without mkl fixes the resolution problem.    There is a new 'nomkl' package, when installed, any subsequent package installations will default to versions without mkl.  However, this does not fix any already installed packages.    I am traumatized by the lack of reproducible  build envs even within a few days of each other.  After discussion in the Tucson office, I'm going to pin the lsstsw and newinstall.sh conda package versions with a commitment from square to update them on a monthly basis.  I already have a test version of lsstsw/bin/deploy that defaults to a bundled package but with a option flag to use bleeding edge.  ",4
"newinstall.sh fails with ""eups: command not found""
[~jgates] has reported the following output when running {{newinstall.sh}} on el6.    {code:java}  Installing EUPS (v2.0.1)... done.  setup: No module named utils  Installing Miniconda2 Python Distribution ...   newinstall.sh: line 277: eups: command not found  {code}    Clearly, a command failure which should have been fatal was ignored.  ",2
"Fix effective coordinates for defects in obs_subaru
The defects as defined in {{obs_subaru}} (in the {{hsc/defects/20NN-NN-NN/defects.dat}} files) are defined in a coordinate system where pixel (0, 0) is the lower left pixel.  However, the LSST stack does not use this interpretation, preferring to maintain the coordinate system tied to the electronics.  As such, the defect positions are being misinterpreted for the rotated CCDs in HSC (see [HSC CCD layout|http://www.naoj.org/Observing/Instruments/HSC/CCDPosition_20150804.png]).  This needs to be remedied.",2
"Offset in gaussian-psf in ci_hsc
I'm seeing what looks like an aperture correction problem in psf-gaussian on {{ci_hsc}} coadds.  This gets in the way of our ability to do star/galaxy classification, and suggests potentially more serious problems elsewhere.  ",2
"S17 Data Access and Database Documentation
Update the documentation for Data Access and Database",65
"S17 Data Access and Database Documentation
Update the documentation for Data Access and Database",65
"FY17 Data Access Model Refresh
A refresh of the storage / IO model (LDM-141). The work involves understanding cost impact, and discussing the impact on science.",26
"S18 Data Access and Database Documentation
Update the documentation for Data Access and Database.",65
"S18 Data Access and Database Documentation
Update the documentation for Data Access and Database",65
"FY20 Data Access and Database Documentation
Update the documentation for Data Access and Database.",100
"FY18 Data Access Model Refresh
A refresh of the storage / IO model (LDM-141). The work involves understanding cost impact, and discussing the impact on science.",26
"FY19 Data Access Model Refresh
A refresh of the storage / IO model (LDM-141). The work involves understanding cost impact, and discussing the impact on science.",26
"FY18 Data Access Model Refresh
A refresh of the storage / IO model (LDM-141). The work involves understanding cost impact, and discussing the impact on science.",26
"FY19 Data Access and Database Documentation
Update the documentation for Data Access and Database.",100
"Add intelligence to `validate_drp` so it does ""A Reasonable Thing"" on an unknown output repo
validate_drp current takes as input both a repository and a configuration file.  The configuration file contains information to construct the list of dataIds to analyze.    However, these dataIds could be extracted from the repo itself, in cases where the desired is to analyze the entire repo.      1.  Add a function that loads the set of dataIds from the repo. (/)  2.  Select reasonable defaults for the additional parameters specified in the config file. (/)  3.  Design how to handle multiple filters. (/)",5
"Add multiple-filter capabilities to `validate_drp`
Design and refactor `validate_drp` to produce results for multiple filters.    1. Decide on the syntax for the YAML configuration file that denotes the multiple filters.  E.g., which visit goes with what filter? (/)  2. Organize the running of multiple filters in `validate.run` to sequentially generate statistics and plots for each filter. (/)  3. Add a filter designation to the default output prefix. (/)    Note: matching objects *across* filters is out-of-scope for this ticket.",1
"LOAD DATA LOCAL does not work with mariadb
After we un-messed mariadb-mysqlclient we see errors now when trying to run integration tests:  {noformat}    File ""/usr/local/home/salnikov/dm-yyy/lib/python/lsst/qserv/wmgr/client.py"", line 683, in _request      raise ServerError(exc.response.status_code, exc.response.text)  ServerError: Server returned error: 500 (body: ""{""exception"": ""OperationalError"", ""message"": ""(_mysql_exceptions.OperationalError) (1148, 'The used command is not allowed with this MariaDB version') [SQL: 'LOAD DATA LOCAL INFILE %(file)s INTO TABLE qservTest_case01_mysql.LeapSeconds FIELDS TERMINATED BY %(delimiter)s ENCLOSED BY %(enclose)s                          ESCAPED BY %(escape)s LINES TERMINATED BY %(terminate)s'] [parameters: {'terminate': u'\\n', 'delimiter': u'\\t', 'enclose': u'', 'file': '/home/salnikov/qserv-run/2016_02/tmp/tmpWeAj6u/tabledata.dat', 'escape': u'\\\\'}]""}"")  2016-02-10 14:17:40,836 - lsst.qserv.admin.commons - CRITICAL - Error code returned by command : qserv-data-loader.py -v --config=/usr/local/home/salnikov/testdata-repo/datasets/case01/data/common.cfg --host=127.0.0.1 --port=5012 --secret=/home/salnikov/qserv-run/2016_02/etc/wmgr.secret --delete-tables --chunks-dir=/home/salnikov/qserv-run/2016_02/tmp/qserv_data_loader/LeapSeconds --no-css --skip-partition --one-table qservTest_case01_mysql LeapSeconds /usr/local/home/salnikov/testdata-repo/datasets/case01/data/LeapSeconds.schema /usr/local/home/salnikov/testdata-repo/datasets/case01/data/LeapSeconds.tsv.gz  {noformat}    It looks like mariadb client by default disables LOCAL option for data loading and it needs to be explicitly enabled.  ",1
"Adapt all HSC calibration data to LSST camera geometry
In the [HSC CCD layout|http://www.naoj.org/Observing/Instruments/HSC/CCDPosition_20150804.png], approximately half of the HSC CCDs are rotated 180 deg with respect to the others, two others have 90 deg rotations and another two have 270 deg rotations.  The HSC camera geometry defined a coordinate system where pixel (0, 0) is always the lower-left corner.  However, the new camera geometry in the LSST stack does not use this interpretation, preferring to maintain the coordinate system tied to the electronics.  As such, accommodations have had to be made for the rotated CCDs on {{obs_subaru}}.  See DM-4998 and DM-5107 in particular for details.  The need for these accommodations, and the accommodations themselves, should be removed.  This entails a re-ingestion of the HSC calibration data files (BIAS, DARK, FLAT, etc.) as well as a redefinition of the defects files in {{obs_subaru}}.",4
"qserv fails when it mixes mariadb and mariadbclient directories
When I tried to run qserv-configure after installing qserv 2016_01-7-gbd0349f I got this error:  {noformat}  2016-02-10 16:03:16,915 - lsst.qserv.admin.commons - CRITICAL - Error code returned by command : /home/salnikov/qserv-run/2016_02/tmp/configure/mysql.sh  {noformat}    Running script configure/mysql.sh:  {noformat}  $ sh -x /home/salnikov/qserv-run/2016_02/tmp/configure/mysql.sh    + echo '-- Installing mysql database files.'  -- Installing mysql database files.  + /u2/salnikov/STACK/Linux64/mariadbclient/10.1.11/scripts/mysql_install_db --basedir=/u2/salnikov/STACK/Linux64/mariadbclient/10.1.11 --defaults-file=/home/salnikov/qserv-run/2016_02/etc/my.cnf --user=salnikov  + echo 'ERROR : mysql_install_db failed, exiting'  ERROR : mysql_install_db failed, exiting  + exit 1  {noformat}    and     {noformat}  $ /u2/salnikov/STACK/Linux64/mariadbclient/10.1.11/scripts/mysql_install_db --basedir=/u2/salnikov/STACK/Linux64/mariadbclient/10.1.11 --defaults-file=/home/salnikov/qserv-run/2016_02/etc/my.cnf --user=salnikov    FATAL ERROR: Could not find mysqld    The following directories were searched:        /u2/salnikov/STACK/Linux64/mariadbclient/10.1.11/libexec      /u2/salnikov/STACK/Linux64/mariadbclient/10.1.11/sbin      /u2/salnikov/STACK/Linux64/mariadbclient/10.1.11/bin  {noformat}    So it looks for mysqld in mariadbclient, the same directory as mysql_install_db script, mysql_install_db should be actually running from mariadb.  ",1
"FY18 Centralize access to database servers
We will have multiple services: L1 live database, multiple DR databases, calibration databases, EFD etc. It'd be nice if users would not have to know which server / which port / which dialect (plain mysql or qserv etc) to use. Instead, it'd be good to have a single entry point that redirects to the right place.",60
"Cost adding the support for Object / DiaObject joins in Qserv
Per RFC-133, we should support Object / DiaObject joins. That requires changes to query analyzer (and possibly elsewhere), currently we only support self-joins on objectId for director table. We'd need to either make DiaObject a director table and allow director-director joins, or allow director-child joins. This story involves costing how much effort it will be to implement it (and making a straw-man proposal how to implement it)",2
"Create InputField for generic use cases.
Create a composable, validating InputField so it can use outside of the form/submit use-case.",2
"B-F correction breaks non-HSC custom ISR, ci_hsc
The addition of brighter-fatter correction on DM-4837 breaks obs_cfht's custom ISR, since it slightly changes an internal ISR API by addding an argument that isn't expected by the obs_cfht version.  It also breaks ci_hsc, since the B-F kernel file isn't included in the calibrations packaged there.  ",1
"make the fits statistics call work with JSON
nan",1
"obs_subaru install with eups distrib fails
Thus:  {code}  $ eups distrib install -t w_2016_06 obs_subaru  ...    [ 52/52 ]  obs_subaru 5.0.0.1-60-ge4efae7+2 ...    ***** error: from /Users/jds/Projects/Astronomy/LSST/stack/EupsBuildDir/DarwinX86/obs_subaru-5.0.0.1-60-ge4efae7+2/build.log:  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""tests/hscRepository.py"", line 91, in setUp      self.repoPath = createDataRepository(""lsst.obs.hsc.HscMapper"", rawPath)    File ""tests/hscRepository.py"", line 63, in createDataRepository      check_call([ingest_cmd, repoPath] + glob(os.path.join(inputPath, ""*.fits.gz"")))    File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/subprocess.py"", line 540, in check_call      raise CalledProcessError(retcode, cmd)  CalledProcessError: Command '['/Users/jds/Projects/Astronomy/LSST/stack/EupsBuildDir/DarwinX86/obs_subaru-5.0.0.1-60-ge4efae7+2/obs_subaru-5.0.0.1-60-ge4efae7+2/bin/hscIngestImages.py', '/var/folders/jp/lqz3n0m17nqft7bwtw3b8n380000gp/T/tmptUSKuf', '/Users/jds/Projects/Astronomy/LSST/stack/DarwinX86/testdata_subaru/master-gf9ba9abdbe/hsc/raw/HSCA90402512.fits.gz']' returned non-zero exit status 1    ----------------------------------------------------------------------  Ran 8 tests in 9.928s    FAILED (errors=7)  The following tests failed:  /Users/jds/Projects/Astronomy/LSST/stack/EupsBuildDir/DarwinX86/obs_subaru-5.0.0.1-60-ge4efae7+2/obs_subaru-5.0.0.1-60-ge4efae7+2/tests/.tests/hscRepository.py.failed  1 tests failed  scons: *** [checkTestStatus] Error 1  scons: building terminated because of errors.  + exit -4  {code}  Please fix it.",1
"Make meas_simastrom a stack package
Currently the simastrom code is sitting outside LSST which makes it not very visible and does not get built regularly making it sensitive to bitrot.  Before we can really continue to gather requirements and develop the system, we need to bring it inside the fence.    This should make the package located [here|https://github.com/lsst-france/meas_simastrom] buildable and usable in the LSST system with a minimum of external dependencies.  By usable, I mean that it should be callable as a task.  This does not need to solve the problem of persistence.",10
"Make ci_hsc buildable by Jenkins
1. Make sure {{ci_hsc}} is buildable by {{lsstsw}} / {{lsst_build}}  (/)  2. Add {{ci_hsc}} to lsstsw/etc/repos.yaml so that one can request that Jenkins builds it.  (/)  3. Verify that the test in {{ci_hsc}} fails on known broken tags and passes on known successful tags. (/)    No dependencies will be added to {{lsst_sims}} or {{lsst_distib}}.  This is meant to provide the ability to request that Jenkins do these builds and to fail if something has broken them.    This will later be expanded to new packages {{ci_cfht}}, {{ci_decam}}, and {{ci_sim}}.    The key goal is to make sure one hasn't broken obs_ packages in their butler interface or in their processCcd    Additional Notes and Thoughts from HipChat Discussion  [~ktl]  Sounds good to me; we might have an ""lsst_ci"" top-level metapackage depending on all of them which is what Jenkins would run regularly.     If the goal is to test obs_ packages, then my first instinct would be to put that in the obs_ package.  Longer term goal to test the stack with different precursor datasets.  If this is testing obs_ packages on a slower cadence than the built-in tests, it's OK for that to be a separate package.    [~jbosch]  Eventually, I think we need to run a CI dataset for each camera, then run some camera generic tests on each of those, then run some camera-specific tests on each of those.  So we don't want to go too far down a road in which all tests are camera-specific, but maybe we don't have a choice until we have some better unifying framework for them.  I've certainly been putting some checks in {{ci_hsc}} that would be valid for all other cameras, if we had a CI package for them that went through to coadd processing.",2
"Increase key_buffer_size
I just looked at my qserv-run/etc/my.cnf and I don't see us setting key_buffer_size there. Looking at mysqld run as part of qserv I can see it is set to 128 MB. That is pretty low given we are planning to do lots of joins. Please add an entry in my.cnf that sets it to something higher with a comment that ""~20% of available RAM is recommended"".",1
"on-going support to Camera team in visualization (Feb. 2016) 
Attend the weekly meeting and answer questions as needed.  Help with the Python and JS debug ",4
"document adding git-lfs repos to CI
nan",1
"Update apr and apr_util
{{apr}} and {{apr-util}} are outdated and lagging behind the versions on RHEL6. They should be updated as agreed in RFC-76.",1
"Move luaxmlrpc to lsst-dm/legacy-
We no longer need luaxmlrpc because we run czar inside proxy. We should move it to lsst-dm/legacy-, and remove mentioning it in readme.",1
"DM Power Requirements Justification
The power requirements for the base site appeared to have increased greatly since LSE-239 or LDM-144 v140. Significant effort was spent digging through LDM-144 for precise rack counts, rack weights, rack power. Further time was spent on the analysis of why the power requirement is greater then expected. This involved analyzing swing space power requirements, max swing space needed, investigation into what LSE-239 refers to 'expansion' (turns out to be alert processing), attributing alert processing power requirements to the base (LDM-144 contributes to archive site but contingency is still in place for base site operations), comparing peak and steady state power needs. Also discussions around reinforcing the floors for greater rack weights.  ",6
"Jason Feb Tasks
Weeks 1&2 - Interviews, Team mtgs, uptime institute tier discussions: 1.5 pts  Weeks 3&4 - Team mtgs, ICI meetings, set/prioritize IT goals 4 pts",6
"Jason Feb Educational Activities
Learning DM stack deployment and layout, reading on redesign of butler 1.5",2
"Avoid merge table (i.e. result_m) creation on czar side
When launching a query which require an aggregation/merge, Qserv first creates a result_m table to collect chunk query results and then a result table. On the other hand, for a query which doesn't require a merge, only result table is created.    If merge query was send to mysql-proxy right after query parsing (like it is currently done with ""order by"" clause only, this would be then generalized to all merge queries), creation of result_m table could be avoided. This would lead to simpler C++ code, and aggregation would be performed at the same time that returning result, which may lead to better performance. Queries wich requires or not aggregation step would be processed exactly the same way on the C++ side (store results of chunk queries), and mysql-proxy would release lock on result table when running aggregation/merge query (here, one can consider that simply concatening results of query would also be a kind of aggregation).    Please not that removal of result_m table would also free some space on master, which is a bottleneck.    [~jbecla], I propose you to plan this interesting feature for next sprint, feel free to postpone it. I think that intersection with ""Query coverage"" story might not be empty.",10
"Provide usable repos in {{validation_data_*}} packages.
Re-interpreted ticket:  1. Provide already-initialized repositories in the `validation_data_cfht`, `validation_data_decam`, and `validation_data_hsc` packages alongside the raw data.  The goal is to allow both easy quick-start analyses as well as comparisons of output steps from processCcd.py and friends at each step of the processing. (/)  2. Add (Cfht,Decam,HSC).list files to provide for easy processing of the available dataIds in the example data. (/)  3. Update README files to explain available data.  (/)    [Original request:]  In validation_drp when I run examples/runXTest.sh I find that any data I had saved in CFHT or DECam is lost, even if I have carefully renamed it. This is very dangerous and I lost a lot of work due to it. At a bare minimum please do NOT touch any directories not named ""input"" or ""output"".    Lower priority requests that I hope you will consider:  - Have the the input repo be entirely contained in the validation_data_X packages, ready to use ""as is"". That would simplify the use of those packages by other code. It would also simplify validate_drp, and it would just leave the output repo to generate (which already has a link back to the input repo).  - Have runXTest.sh accept a single argument: the path to the output. (The path to the input is not necessary if you implement the first suggestion).",3
"IN2P3 cluster worker nodes failed to start due to Innodb error
Next error happens when starting mariadb on worker (with existing data from 35TB dataset, which were generated by mysql):  {code:bash}  2016-02-13 22:02:36 139632684558144 [Note] InnoDB: Completed initialization of buffer pool  2016-02-13 22:02:36 139632684558144 [ERROR] InnoDB: auto-extending data file ./ibdata1 is of a different size 640 pages (rounded down to MB) than specified in the .cnf file: initial 768 pages, max 0 (relevant if non-zero) pages!  2016-02-13 22:02:36 139632684558144 [ERROR] InnoDB: Could not open or create the system tablespace. If you tried to add new data files to the system tablespace, and it failed here, you should now edit innodb_data_file_path in my.cnf back to what it was, and remove the new ibdata files InnoDB created in this failed attempt. InnoDB only wrote those files full of zeros, but did not yet use them in any way. But be careful: do not remove old data files which contain your precious data!  2016-02-13 22:02:36 139632684558144 [ERROR] Plugin 'InnoDB' init function returned error.  2016-02-13 22:02:36 139632684558144 [ERROR] Plugin 'InnoDB' registration as a STORAGE ENGINE failed.  2016-02-13 22:02:36 139632684558144 [Note] Plugin 'FEEDBACK' is disabled.  2016-02-13 22:02:36 139632684558144 [ERROR] Unknown/unsupported storage engine: InnoDB  2016-02-13 22:02:36 139632684558144 [ERROR] Aborting  {code}",4
"Create an easy place to add tests to ci_hsc
Create a single file where tests for validating source can be added. The tests will be duck typed to a class method and be registered to the corresponding validation class with a decorator.",1
"Code review, Feb 2016
DM3733,DM4825",2
"Meetings, Feb 2016
verification dataset meetings",1
"Process a tiny set of raw DECam Stripe 82 data
Process some DECam data to gain familiarity with process execution and learn to debug issues",8
"Continue learning middleware
Ramp up with the middleware status and development. Look into packages pipe_base, pex_config, pex_logging. ",16
"LOE, Feb 2016
Local LSST meetings, postdoc meetings, NCSA All hand meetings, RFDs, NCSA software meeting, astronomy events, workshops, travel to JTM, other local meetings. ",10
"Please document MemoryTestCase
{{lsst.utils.tests.MemoryTestCase}} is used extensively throughout our test suite, but it is lacking in documentation and it's not clear under what circumstances its use is required or encouraged. Please add appropriate documentation to the [Software Unit Test Policy |http://developer.lsst.io/en/latest/coding/unit_test_policy.html].    See also [this thread on clo|https://community.lsst.org/t/what-is-the-policy-for-using-lsst-utils-tests-memorytestcase].",1
"Record CCD, visit of input catalog in `validate_drp`
1. Record the CCD and `visit` of the individual source in the catalog so that it is available for later analysis.  3. Update `analyzeData` to use these newly available CCD and `visit` information in the catalog.  ",1
"HSC backport: Support a full background model when detecting cosmic rays
This is a port of the following two standalone HSC commits:    [Support a full background model when detecting cosmic rays|https://github.com/HyperSuprime-Cam/pipe_tasks/commit/3bae328e0fff4b2a02267e97cc1e53b5bbe431cb]  {code}  If there are strong gradients (e.g. M31's nucleus) we need to do more than  treat the background as a constant.  However, this requires making a copy  of the data so the background-is-a-constant model is preserved as a special  case  {code}  [Fixed cosmicRay() in RepairTask for the case background is subtracted.|https://github.com/HyperSuprime-Cam/pipe_tasks/commit/2cdb7c606270d84c7a05baf9949ff5724463fa6b]  {code}  When the background is subtracted with finer binsize, new exposure  will be created and cosmic rays will be detected on that exposure.  But the image of that exposure was not properly returned back.  {code}  ",1
"Audit the LSST and HSC codebases for differences
We've already merged a lot of code from HSC to LSST, and are optimistic that we've captured most of the big ticket items. However, we need to perform a thorough comparison of the codebases to check there's nothing we're missing. Please do that, and file tickets in the DM-3560 and DM-3568 epics to describe outstanding work.",4
"Modify System layout to support expanded views
Each of the visualizers needs to expand to full screen.  We need to modify our current layout system so each and expand and collapse so that the old view is restored. The system needs to be flexible enough so an 'expanded version' of the component can be used.",4
"Tests in daf_persistence should skip properly
Some of the tests in {{daf_persistence}} have a couple of problems that cause difficulties with modern test frameworks:  # unittest is not being used at all in some cases  # Skipping is done with a print and a {{sys.exit}}    They need to be modernized.",2
"Mouse Readout: part 1.5 - update flux server call to work in JS
nan",1
"Analyze catalog-comparison CmdLineTasks
Analyze the QA CmdLineTask collection being generated by [~lauren] sufficiently well to determine the interface requirements needed to represent them as Supertasks.    Does not include actually designing that interface.",6
"Standup Fastly infrastructure for LSST the Docs
LSST the Docs will use Fastly to serve docs out of an S3 bucket with well-formatted URLs thanks to routing at the Varnish layer. See https://www.hashicorp.com/blog/serving-static-sites-with-fastly.html for an overview of the desired setup and http://sqr-006.lsst.io for an overview of LSST the Docs. Specific outcomes are:    * Create S3 bucket for LTD  * Create Fastly account (may be a demo account pending negotiations with Fastly)  * Basic configurations for Fastly account  * Research pricing/configure a TLS certificate for *.lsst.io domains  * Set up base VCL configuration on Fastly.  ",3
"Fastly API interactions for LSST the Docs
Using Fastly’s API, have ltd-keeper setup new builds and editions:    - Add {{Surrogate-Key}} to headers of objects uploaded to S3 (happens on ltd-mason side)  - Configure Varnish to serve specific bucket directories as specific domains (DM-4951 has added Route 53 interactions to ltd-keeper)  - Purge content when editions switch or content is deleted.    DM-5167 is covering non-API driven work to configure fastly.    See https://www.hashicorp.com/blog/serving-static-sites-with-fastly.html for a write-up on serving static site via fastly. See also http://sqr-006.lsst.io for an overview of LSST the Docs.",8
"New XY  functions to be developed (F16)
There are several new functions requested by users",45
"Manipulating masks is confusing
A possible bug exists in afwImage.Exposure.getMaskedImage(). This function returns a copy of the Exposure's masked image, and not the actual maskedImage owned by the Exposure. This means that any changes made to the mask are done only on the copy, and are not reflected in the Exposure's maskImage. The intended behavior seems to be that a shallow copy be returned with pointers to all the original objects (such as the mask). This however does not seem to be the case, a deep copy is always made instead. Verify that the intended behavior is indeed happening. Steps to reproduce    {code:python}  coaddExposure = afwImage.ExposureF()  coaddExposure.getMaskedImage().getMask().addMaskPlane('TEST')  print(coaddExposure.getMaskedImage().getMask().getMaskPlaneDict().asdict())  m = coaddExposure.getMaskedImage().getMask()  print(m.getMaskPlaneDict().asdict())  m.removeAndClearMaskPlane('TEST')  print(m.getMaskPlaneDict().asdict())  print(coaddExposure.getMaskedImage().getMask().getMaskPlaneDict().asdict())  {code}     A second concern, though not necessarily a bug, is that adding and removing mask planes is confusing due to inconsistent manipulation of global state. For example:  {code}  In [1]: import lsst.afw.image as afwImage    # Create two separate Masks  In [2]: mask1 = afwImage.MaskU()  In [3]: mask2 = afwImage.MaskU()    # Neither Mask contains a ""TEST"" plane  In [4]: 'TEST' in mask1.getMaskPlaneDict()  Out[4]: False  In [5]: 'TEST' in mask2.getMaskPlaneDict()  Out[5]: False    # Adding a plane to one updates a shared list of planes, so it appears in the other  In [6]: mask1.addMaskPlane('TEST')  Out[6]: 9  In [7]: 'TEST' in mask1.getMaskPlaneDict()  Out[7]: True  In [8]: 'TEST' in mask2.getMaskPlaneDict()  Out[8]: True    # But deleting a plane from one affects only that particular Mask and not the global state  In [9]: mask1.removeAndClearMaskPlane('TEST')  In [10]: 'TEST' in mask1.getMaskPlaneDict()  Out[10]: False  In [11]: 'TEST' in mask2.getMaskPlaneDict()  Out[11]: True  {code}",4
"Add CSS information for shared scans to integration test data.
Some tables int the integration tests need to be flagged as needing to be locked in memory and given a scan rating.",1
"lsstsw deploy on OS X fails in miniconda install
Testing the fixes for the {{deploy}} script in DM-4359 it seems that the part of the script installing {{miniconda}} no longer works on OS X because the list of packages to be installed has been derived from a Linux system and not all the Linux packages have OS X equivalents. There needs to be a per-OS list of packages. The default OS X list seems to be:  {code}  # This file may be used to create an environment using:  # $ conda create --name <env> --file <this file>  # platform: osx-64  astropy=1.1.1=np110py27_0  conda=3.19.1=py27_0  conda-env=2.4.5=py27_0  cycler=0.9.0=py27_0  cython=0.23.4=py27_1  freetype=2.5.5=0  libpng=1.6.17=0  matplotlib=1.5.1=np110py27_0  nomkl=1.0=0  numpy=1.10.4=py27_nomkl_0  openssl=1.0.2f=0  pandas=0.17.1=np110py27_0  pip=8.0.2=py27_0  pycosat=0.6.1=py27_0  pycrypto=2.6.1=py27_0  pyparsing=2.0.3=py27_0  pyqt=4.11.4=py27_1  python=2.7.11=0  python-dateutil=2.4.2=py27_0  pytz=2015.7=py27_0  pyyaml=3.11=py27_1  qt=4.8.7=1  readline=6.2=2  requests=2.9.1=py27_0  scipy=0.17.0=np110py27_nomkl_0  setuptools=19.6.2=py27_0  sip=4.16.9=py27_0  six=1.10.0=py27_0  sqlalchemy=1.0.11=py27_0  sqlite=3.9.2=0  tk=8.5.18=0  wheel=0.29.0=py27_0  yaml=0.1.6=0  zlib=1.2.8=0  {code}",1
"miniconda2 eups package fails to install on OS X
The {{miniconda2}} eups package attempts to install the relevant conda packages by downloading a list from the {{lsstsw}} repository. This fails for the same reason that {{lsstsw}} fails in DM-5178 in that the list of packages is not OS-specific. This means that {{newinstall.sh}} does not work any more on OS X.",1
"update ""newinstall.sh"" nebula images & docker containers - w_2016_08
nan",1
"Hook up help system
We need to help system like we have in GWT.",8
"Implement Lock plot button on toolbar
* Write a button on the toolbar that monitors the active plot view's group and shows the locked or unlocked icons  * Add an action and reducer functions the will toggle the lock state of the group.",6
"Add Xrdssi plugin configuration file
Xrdssi plugin configuration file could be useful for sharedscan.  to pass plugin configuration file path to xrootd  http://xrootd.org/doc/dev42/xrd_config.htm#_Passing_Plug-In_Command (use -+xrdssi)  to get this argument from C++  http://xrootd.org/doc/dev42/ssi_reference.htm#_Toc431242001    then an add-hoc config file parser needs to be used (not to be xrootd dependant), json/yaml could be used.",6
"Set Qserv master in env variable for Docker containers
This would allow use of pre-configured container on all clusters, indeed the only parameter which currently change in cluster install is master fqdn.  See http://xrootd.org/doc/dev42/Syntax_config.htm  and  {code}  if defined ?~EXPORTPATH    set exportpath = $EXPORTPATH    else    set exportpath = /tmp    fi    all.export $exportpath    {code}",3
"Add fftools API: Image Viewer plus foundational work
nan",10
"Add fftools API: Table
nan",6
"Add fftools API: XYPlots and Histgram
nan",8
"Coverage, Coverage API, ImageMetaData API
nan",8
"Add remote (python) API support 
The python interface needs to be ported.  This involves the following:    * Modify FireflyClient.py  * Change all the API to work by dispatching remote actions. There is currently a dispatchRemoteAction method in  FireflyClient.py  * On the server side clean up file PushCommands.java, PushJob.java and ServerParams to remove the old api.  * Move the fftools/python to firefly/python  * clean up the test notebooks.  There are currently several, some don't work and should be removed.  Others should be clean test cases.  * Make sure the python support for RangeValues is consistent with the Java and JavaScript. I suspect it is not.  * Make sure events can be received into the python.  ",16
"attend the bi-weekly meeting authentication and authorization discussion
attend the bi-weekly meeting authentication and authorization discussion. provide input and feedback to IAM. ",4
"Deploy ltd-keeper as a Docker Container
ltd-keeper should be deployed as a Docker container as a best practice for maintainable cloud microservices.    This involves writing a Dockerfile committed to the ltd-keeper repo and demonstrating that the container can be stood up on Google Container Engine.    I plan on use data-containers attached to the service’s container to maintain the sqlite DB. This ticket should document how to operate ltd-keeper and apply updates to both the ltd-keeper app *and* DB migrations..    This ticket also involves initial overhead in researching Docker/Kubernetes.",12
"swift API availability?
The downtime announcement email for {{Nebula unavailable Feb 9-10}} mentioned a ""roadmap"" for swift.  I have checked and post maintenance, there is not a swift endpoint available in the catalog.  Is there a time line for availability?",1
"Test and robustify shapelet PSF approximations
The CModel code ported from HSC only works as well as the ShapeletPsfApproximation algorithm that runs before it, but we've switched on the LSST side to a more flexible algorithm that isn't as nearly as battle-tested as what's been running on the HSC side, and there are some concerning indications from [~pgee]'s work that it can be catastrophically slow on some reasonable PSFs.  On this issue, I'll run it on some real HSC data and try to improve it, even if that means reducing the flexibility back to what was on the HSC side in some ways.",8
"FITS Visualizer porting: Statistics - part 2 - drawing overlay & 3 color support
drawing overlay 3 Color Support",8
"instance I/O errors
The kernel dmesg for Instance {{bbfd7458-6dd6-4412-a8ba-8d417c3df56b}} has started reporting thousands of block I/O errors and these are starting to trickle up as a filesystem I/O errors.  I suspect this is likely a hypervisor I/O issue.    {code}  [687301.556430] Buffer I/O error on device dm-3, logical block 3768490  [687301.556433] Buffer I/O error on device dm-3, logical block 3768491  [687301.556436] Buffer I/O error on device dm-3, logical block 3768492  {code}    {code}  $ openstack server show bbfd7458-6dd6-4412-a8ba-8d417c3df56b  +--------------------------------------+-----------------------------------------------------------------------+  | Field                                | Value                                                                 |  +--------------------------------------+-----------------------------------------------------------------------+  | OS-DCF:diskConfig                    | MANUAL                                                                |  | OS-EXT-AZ:availability_zone          | nova                                                                  |  | OS-EXT-STS:power_state               | 1                                                                     |  | OS-EXT-STS:task_state                | None                                                                  |  | OS-EXT-STS:vm_state                  | active                                                                |  | OS-SRV-USG:launched_at               | 2016-02-11T23:36:25.000000                                            |  | OS-SRV-USG:terminated_at             | None                                                                  |  | accessIPv4                           |                                                                       |  | accessIPv6                           |                                                                       |  | addresses                            | LSST-net=172.16.1.115, 141.142.209.121                                |  | config_drive                         |                                                                       |  | created                              | 2016-02-11T23:36:12Z                                                  |  | flavor                               | m1.xlarge (5)                                                         |  | hostId                               | f7fbf308022d02f52e1111c91cf578d852784d290d0e0ddb0d69635c              |  | id                                   | bbfd7458-6dd6-4412-a8ba-8d417c3df56b                                  |  | image                                | centos-7-docker-20151116230205 (59a2a478-11ab-41c5-affc-29706d38d65a) |  | key_name                             | vagrant-generated-comshorc                                            |  | name                                 | el7-docker-jhoblitt                                                   |  | os-extended-volumes:volumes_attached | []                                                                    |  | progress                             | 0                                                                     |  | project_id                           | 8c1ba1e0b84d486fbe7a665c30030113                                      |  | properties                           |                                                                       |  | security_groups                      | [{'name': 'default'}, {'name': 'remote SSH'}]                         |  | status                               | ACTIVE                                                                |  | updated                              | 2016-02-11T23:36:25Z                                                  |  | user_id                              | 83bf259d1f0c4f458e03f9002f9b4008                                      |  +--------------------------------------+-----------------------------------------------------------------------+  {code}",1
"sph-partition does not support BLOB fields
Next command fails with BLOB field in input file:  {code:bash}  dev@clrinfoport09:~/src/qserv_testdata/datasets/case01/data$ sph-partition --out.dir /home/dev/qserv-run/git/tmp/qserv_data_loader/Object --part.prefix chunk --config-file /home/dev/src/qserv_testdata/datasets/case01/data/common.cfg --config-file /home/dev/src/qserv_testdata/datasets/case01/data/Object.cfg --in Object.tsv  CSV record contains an embedded line terminator, a trailing escape character, or a quoted field without a trailing quote character.  {code}  Note that the command works in input file is reduced to its first line.    Note that mysql import works fine:  {code:sql}  MariaDB [qservTest_case01_mysql]> LOAD DATA INFILE ""/tmp/Object.tsv"" INTO TABLE Object FIELDS TERMINATED BY '\t' OPTIONALLY ENCLOSED BY '""' LINES TERMINATED BY '\n';  {code}",15
"Remove LOGF macros from log package
We have removed all uses of LOGF macros from qserv and as far as I know no other clients use those macros. It's time to clean up log package itself from those macros.",1
"Add support for 3 Color
Most of this is done.  I just need to plot a few 3 color images and work out the bugs.",4
"Remove remaining LOGF macros from qserv
There are still few cases of LOGF macros in qserv, have to replace them all.",1
"replace Associations::CollectRefStars with LoadReferenceObjectsTask
AstroUtils.cc has code for loading USNO catalog which is used by Associations:CollectRefStars to build a reference list. We should replace this with  LoadReferenceObjectsTask from meas_algorithms to both make it more generic, and to remove problematic endian handling in AstroUtils.",10
"Please do not write garbage to the FITS EQUINOX
The equinox is not relevant when dealing with ICRS coordinates.    When {{afw}} manipulates {{Wcs}} objects, it simply doesn't bother initializing the {{equinox}} field of its {{_wcsInfo}} struct when dealing with an ICRS system.    When {{afw}} persists the {{Wcs}} to FITS, it blindly writes whatever happens to be in that uninitialized field to the FITS header. Thus, we end up with something like:  {code}  EQUINOX =      9.87654321E+107 / Equinox of coordinates  {code}  This should be no problem, since, per the [FITS standard|http://fits.gsfc.nasa.gov/standard30/fits_standard30aa.pdf] (page 30), the {{EQUINOX}} is ""not applicable"" if they {{RADESYS}} is {{ICRS}}. The reader should thus ignore this value.    However, [SAOimage DS9|http://ds9.si.edu] version 7.4.1 (the latest release at time of writing) does _not_ ignore the {{EQUINOX}}. Rather, it refuses to handle the WCS for the image. Note that version 7.3 of DS9 does not seem to have the same issue.    While this does seem to be a bug in DS9, it's easy enough to work around by simply not writing {{EQUINOX}}.",1
"Evaluate MariaDB GSSAPI Authentication Plugin
As a follow-on to DM-4315, deploy the new [Maria DB GSSAPI Authentication Plugin|https://mariadb.com/kb/en/mariadb/gssapi-authentication-plugin/] in the IAM testbed for Kerberos ticket-based authentication, to provide single sign-on access.",2
"Improve worker configuration files.
Configuration on the worker is done by setting environment variables in a script, which is lacking in flexibility, but there is a question of if it is worth changing to some form of text configuration file. The code that reads the configuration could be improved in either case.",10
"Run Qserv multinodes integration tests inside Travis
This aims at preparing integration of this procedure inside Jenkins CI",4
"Add configured requirements parameters.  Pass/Fail test.
1. Add pass/fail routine to report success/fail against metrics.  Do this for    * SRD  (/)    * Configured metrics  (/)    2. Add pass/fail reporting to running of `validate.drp.run`  (/)",4
"Add a ci_hsc daily build
Please add a daily build of `ci_hsc` to the Jenkins system.    This does not need to explicitly build `lsst_distrib` or `lsst_sims`.  The only product to list is `ci_hsc`.    In the slightly near future, I anticipate that this build will be replaced by a daily build of a metapackage `lsst_ci`.",5
"implement cycle change in DLP
Summer --> Fall, Winter --> Sprint, add X16",1
"X16 Object Characterization Bucket
Catch all epic for essential bugs and improvements in object characterization emerging during X16.",20
"Segfault in shapeHSM centroid extractor
[~boutigny] reports a segfault in {{meas_extenstions_shapeHSM}}. He provides the following backtrace:  {code}  Program received signal SIGSEGV, Segmentation fault.  0x00007fffe7043156 in lsst::afw::table::BaseRecord::getElement<lsst::afw::table::Flag> (this=0x21c8d60, key=...)  at include/lsst/afw/table/BaseRecord.h:61  61	typename Field<T>::Element * getElement(Key<T> const & key) {  (gdb) bt  #0 0x00007fffe7043156 in lsst::afw::table::BaseRecord::getElement<lsst::afw::table::Flag> (this=0x21c8d60, key=...)  at include/lsst/afw/table/BaseRecord.h:61  #1 0x00007fffdc8775f2 in set<lsst::afw::table::Flag, bool> (value=<synthetic pointer>, key=..., this=0x21c8d60)  at /home/boutigny/CFHT/lsstsw/stack/Linux64/afw/11.0-8-g38426eb/include/lsst/afw/table/BaseRecord.h:137  #2 setValue (value=true, i=0, record=..., this=0x1da2500) at include/lsst/meas/base/FlagHandler.h:73  #3 lsst::meas::base::SafeCentroidExtractor::operator() (this=<optimized out>, record=..., flags=...)  at src/InputUtilities.cc:134  #4 0x00007fffd03655c6 in lsst::meas::extensions::shapeHSM::HsmPsfMomentsAlgorithm::measure (this=0x1da2410,   source=..., exposure=...) at src/HsmMoments.cc:115  #5 0x00007fffd06708d5 in _wrap_HsmPsfMomentsAlgorithm_measure (args=0x7fffccc67b90)  at python/lsst/meas/extensions/shapeHSM/hsmLib_wrap.cc:14337  #6 0x00007ffff7aee37f in ext_do_call (nk=-859407472, na=<optimized out>, flags=<optimized out>,   pp_stack=0x7fffffff7d18, func=0x7fffd0c21878) at Python/ceval.c:4345  #7 PyEval_EvalFrameEx (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:2720  #8 0x00007ffff7aefdbe in PyEval_EvalCodeEx (co=0x7fffd0a9ceb0, globals=<optimized out>, locals=<optimized out>,   args=<optimized out>, argcount=3, kws=0x7fffccd43b08, kwcount=0, defs=0x0, defcount=0, closure=0x0)  at Python/ceval.c:3267  {code}    See the discussion at DM-4780.",2
"Implement background gradient fit in pre-sub. images for dipole fit
Add a linear background gradient fit to the integrated pre-subtraction and dipole fitter (for testing).  This will eventually be implemented in the measurement plugin.",2
"Implement a way to pass more than one exposure to a SingleFrameMeasurement (DipoleMeasurementTask)
nan",10
"Add background gradient fit to new dipole measurement task
nan",2
"lsstsw breakage with spaces in paths
There are still some issues relating to using {{lsstsw}} to build the stack when spaces are in the path to the {{$LSSTSW}} location. This is a fine thing to sort out on Rodeo Day...",1
"Base ""bright star"" cut on S/N instead of magnitudes
The astrometry histogram generated by validateDrp.py conflates astrometric and photometric calibration because it uses magnitude for brightness, and this relies on the accuracy of the photometric calibration. [~ctslater] suggests (and I agree) that brightness should be based on signal to noise ratio, thus making the astrometry histogram independent of photometric calibration.  ",2
"X16 Butler
Work on improving Butler:  * Refactor butler multiple repository support based on user feedback.   * Formalize butler configuration mechanism and define configuration persistence.   * RFC and implement support for repo selection based on version.   * Design and RFC mechanism for Butler to define output dataset type.   * R&D & stub implementation of Butler storage factorization (python type + file type + storage location)  * implement spatial lookups in butler  * minor bug fixing",49
"Modernize python in lsst_build
The python in {{lsst_build}} uses old-style print and exception handling. These should be updated to the current standard.",1
"Turn on bias-jump fix for all CCDs 
The overscan fix to handle bias jump in an amplifier done in DM-4366 introduced a new config parameter {{overscanBiasJumpBKP}}, and the fix is applied for CCDs on the backplanes specified in {{overscanBiasJumpBKP}}.  Previously, the default is to only fix CCDs on backplanes next to the focus chips. But [~mfisherlevine] also see the bias jump features in other CCDs.  It would make more sense to turn it on for all CCDs by default. ",1
"Prepare narrative description of Level 3 operations from the perspective of the SUIT
Also known as the ""Level 3 ConOps"" needed by NCSA.",10
"Provide comparison routines for comparing two repos of the same data
Adapt the HSC capabilities from DM-4730 as represented on pipe_tasks u/laurenm/DM-4730 (as prepared by [~lauren] and [~price])  into generally available {{pipe_tasks}} routines for comparison of two different repositories of the same data.  The intended use case is comparing two different algorithms or configurations on the same data and providing individual source-measurement to source-measurement comparisons for debugging new algorithms and understanding the behavior.",3
"Audit SuprimeCam policy and update to current standards
{{obs_subaru}}'s {{policy/SuprimecamMapper.paf}} contains a number of entries that look wrong (e.g. {{deep_forcedPhotCoadd_metadata}} should be {{deepCoadd_forced_config}}) or do not apply to LSST (e.g. {{stack_config}}) and doesn't contain a number of entries that might be expected (e.g. {{transformed_src}}).    Please ensure that this file is updated to comply with current expectations.",1
"rename meas_simastrom to jointcal and flatten namespace
Moving meas_simastrom from lsst_france/ to lsst/ also resulted in a name change per RFC-123, and a namespace flattening: it's not derived from meas; it's a task. This is the necessary first step in getting it integrated into the stack.",1
"Filtering from XY Plot table view (JS)
Allow to filter in a selected area from XY Plot.",4
"make floating point exception handling cross-platform (or remove it)
jointcal currently has a couple of trapfpe() functions that wrap feenableexcept, which doesn't exist on OSX. Were these an important part of error handling in meas_simastrom, or can I just remove them?",2
"plan to upgrade the third party packages
The following packages need to be reviewed and maybe upgraded.      ""babel""     : ""5.8.34"",                           6.5.2 (M)      ""history""   : ""1.17.0"",                           2.0.0 (M)      ""icepick""   : ""0.2.0"",                            1.1.0 (M)                ""react-highcharts"": ""5.0.6"",                      7.0.0 (M)      ""react-redux"": ""3.1.2"",                           4.4.0 (M)      ""react-split-pane"": ""0.1.22"",                     2.0.1 (M)      ""redux-thunk"": ""0.1.0"",                           1.0.3 (M)      ""redux-logger"": ""1.0.9"",                          2.6.1 (M)      ""validator"" : ""4.5.0"",                            5.1.0 (M)      ""chai"": ""^2.3.0"",                                 3.5.0 (M)      ""esprima-fb"": ""^14001.1.0-dev-harmony-fb"",        15001.1001.0-dev-harmony-fb (M)      ""babel-eslint""      : ""^4.1.3"",                   5.0.0 (M)      ""babel-loader""      : ""^5.3.2"",                   6.2.4 (M)      ""babel-plugin-react-transform"": ""^1.1.0"",         2.0.0 (M)      ""babel-runtime""     : ""^5.8.20"",                  6.6.0 (M)      ""eslint""            : ""^1.10.3"",                  2.2.0 (M)      ""eslint-config-airbnb"": ""0.1.0"",                  6.0.2 (M) works with eslint 2.2.0      ""eslint-plugin-react"": ""^3.5.1"",                  4.1.0 (M)  works with eslint 2.2.0      ""extract-text-webpack-plugin"": ""^0.8.0"",          1.0.1 (M)      ""html-webpack-plugin"": ""^1.6.1"",                  2.9.0 (M)      ""karma-sinon-chai"": ""^0.3.0"",                     1.2.0 (M)      ""redux-devtools""    : ""^2.1.2"",                   3.3.1 (M)      ""webpack"": ""^1.8.2""                               1.12.14, 2.1.0 beta4 (M)  ",2
"replace buildbot with jenkins job(s)
Removing buildbot and replacing it with jenkins would provide a number of benefits    * one less dashboard for developers to know about / interact with  * one less system for SQRE to maintain  * lessening the cost of refactoring the CI drivers scripts as synchronized updates to two CI system configurations would no longer be necessary    It should also be easy to go one step further and try to eliminate the need for developers to manually log into the {{lsstsw}} account on {{lsst-dev}} to publish eups distrib packages. ",3
"Attend JTM
Joint Technical Meeting 2/22-24, Santa Cruz",6
"arrays not properly transmitted
Sending a property set with an array as one of the entries only passes the last element of the array.",1
"Port HSC afw changesets to LSST
We identified in DM-5162 several changesets that still need to be ported from HSC to LSST:    * 2c12255372bde846ba0429b5b542960e57d169f0, 0aec617e0ea604cde85105de3dade279a4fe10df: Footprint::overlapsMask  * 76f3706f6688b23d5b0c71e66af3e94095a9f821: copyWithinFootprint: respect image size  * f49676d7f1348f9de8ca21ee633e0c25473251ae: Implemented Linear and ZScale transformations, HSC-1206, 08a7740ed756ee7b2c845b4ce6aed8d9a0f50d04  * c369a8ad53962aba950f7710210be4b23f45a523: utils: make Mosaic.nImage a property  * Maybe 0a2647a4f57addc3b3adb347da995fa0d36b43cb: Add display.utils function to plot the bboxes of inputs to a coadd in ds9  * 386a4b71d974d9e5672fe8690d0db3e56c9fb40f: Box2?.getCorners  * f3d3029c561b957069cbf280b62ea8e37447c068: Calib: add operator*= for scaling zeropoint  * 6c1845474f28f528a95190eeb88f095b11999078: Check in #3092 (iterable coord) directly on master  * f3d3029c561b957069cbf280b62ea8e37447c068: Calib: add operator*= for scaling zeropoint  * 5d1934cc9fe7d8c43aa8f9318a1ac9a3ce85e94e, 0d1ab12db604d5e42a5d72f028411a64294283ce: Footprint merging  * b8578746d69920bc1e1089cca4b4acb230f0e8d5: Interpolate: add support for ndarray  * 3de3339aa075f869d73a5bc66fc65dbee8ae3d16: Fix unit test fallout from Interpolate std::vector change  * 08a7740ed756ee7b2c845b4ce6aed8d9a0f50d04, f73544e15abd2760bf84794798cf4b84e97e938d: Added xSize, ySize, and rescaleFactor arguments to {make,write}RGB; HSC-1207  * 88d838b74898d9572bbc8c46121da029958c1c72: rgb: fix makeRGB so it can replace saturated pixels and produce an image  * 254d7248ea20d98de481d968f6503d1610b16ae7: Remove tests of writing rgb images to jpeg and tiff  * 3252a40ad55222b882acf14d2f7cf0f3fe80f585: Added MatchControl and implemented it for matchXY and MatchRaDec  * 81c6063a32883b748f3770b7124d74ced7b480f5: Implement and test includeMismatches option in MatchControl object  * fd4c0baec617155fac0816607a5acba88e8970f5: Add support for renaming without replacing the full field in SchemaMapper  * 79337bb6d1ee3a0b73bcd2b2d0ca506a44d3fa56: Handle empty Footprints when normalising  ",4
"Port HSC skymap, shapelet changesets to LSST
We identified in DM-5162 some changesets that still need to be ported from HSC to LSST:    * skymap:  ** f83f71718eac5307d575d3113ee3757a63a16de2: Set vertex list in ExplicitTractInfo.    * shapelet:  ** bb928df3fc2fafe5183e0d075da19994f0af4fc7: Let the value to normalize to be specified in [Multi]ShapeletFunction  ",1
"Port HSC meas_algorithms changesets to LSST
We identified in DM-5162 several changesets that still need to be ported from HSC to LSST:    * 1293a31c19c238ba2c2acd8f67ec1be742764b66: BinnedWcs  * 9f392b134502f6e4fbbd8759806b15f89a267e5a: detection: additional debugging plots for background  * ad74fe8595ec523d6269e36ec2db051534bf3e9a: Add initial.flags.pixel.interpolated.any to ObjectSizeStarSelectorConfig.badFlags  * 69f5db0eba69225cff917fa4c96a94dc8b765aa0, 4a0d59e191fc40d3091b56b20cf27ede4e0c23ab: Check for bad PSF measurements (HSC-1153)  * a54b1ac52678025d3317e8a379c2849d3eb567ba: pcaPsfDeterminer: catch case of no good PSF candidates in debugging  * c4fcab3251e6f41da2248d63fdf28c0bf80e30f8: Indent seems to be wrong for debug display  * 2a889c17d47c879dbb4345bafba6aed9869b5984, f3e42cc03ab8a4f1b28d9e0852619cbdbf3b7018: Make IdSpanCompar more deterministic  * f99eb46f484609673b45290eaaba47688d7b4a24: CR code has to take care of 'NO_DATA' mask  * 6f6b786bce8ca34bf4c67f75f965130dea027147: Handle small numbers of psfCandidates (HSC-1176)  * d744e6514feaf67b87068ac502bca677306f9fc2: tests: add test for MeasureApCorrTask  * 65f617089038fe19179fca4f959bf23ea061a6b8, 1b7e3cc48ed347b0afa31e81c821b38f87d18d64: Test case for measurement of negative objects    There are also a couple of issues that were identified in the DM-5162 review:  * Delete tests/config/MeasureSources.py --- mere configuration, old-style measurement  * testPsfDetermination has method 'xtestRejectBlends'",4
"Port HSC daf_butlerUtils changesets to LSST
We identified in DM-5162 several changesets that still need to be ported from HSC to LSST:    * daee24edba01b01a0412df7f9b4cf70be5b10860: CameraMapper: allow a default filter name to be provided  * e3fee95d6a1850dd2309d3ebe4e3ef3ffe38eef0: CameraMapper: normalize path names, and remove leading double slash  * 476b6ddccd9d0cceb2b89ca34bee7d0fdcd70694: preserve timestamps in cameraMapper.backup()  * b2491ef60e5e23afa7d9f0297f257e694aa1af35: Only attempt to update Wcs if it's available  * 9f62bcce588fa9abc8e1e44ff2f0275e5230f629: Registry: hold registry cache for a single thread only (HSC-1035)  * 412f03b95b7a5e82003ab33a61bd43adbf465188: Registry: use a pool of registries to avoid having too many open files",2
"Port HSC meas_extensions_simpleShape package to LSST
HSC uses a package, meas_extensions_simpleShape, which needs to be ported to LSST.  The package is used for basic shape measurements for determining focus, and also serves as a simple guide for writing measurement plugins.",3
"Port HSC meas_extensions_multiShapelet changesets to LSST
We identified in DM-5162 a few changesets that still need to be ported from HSC to LSST:    * bf5f753133ae4b41357f9789ff4763949ebb6ffb: FitPsf: reduce outerOrder to 1  * a54d6cbd41baf916fac2a1bb235a8502af14edfd: Provide explicit instantiations for the sake of clang 6.0 on os/x 10.9  * a53ac9e5cdb678a3f8ef633110d7d4cc5ac84f15: FitProfileAlgorithm: bail if the PSF flux failed  ",1
"Port HSC meas_deblender changesets to LSST
We identified in DM-5162 a few changesets that still need to be ported from HSC to LSST:    * a8cf6c22df14494d6dcf2d7354c695cba9506301: Clarify tiny footprint limit  * 624790aa63a38fb7a328ebc21abfd1b10503aa26: config: change default strayFluxRule  * db7d705de93b43a5f32f771c716b1c5c7368d124: consolidate failed peak logic and downgrade warning    We also identified a few differences that should be resolved:  * clipStrayFluxFraction defaults to 0.01 for LSST, 0.001 for HSC  * Stray file, src/Baseline.cc.orig, on LSST side  ",1
"Port HSC ip_isr changesets to LSST
We identified in DM-5162 some changesets that still need to be ported from HSC to LSST:    * f1cee734998f1faf86c02af42ea599b077847eeb: IsrTask: allow fallback to a different filter when loading calibrations  * 89cd629bb8e1a72a545176311b1ef659358d95af: saturationDetection: apply to overscan as well as image  ",1
"Port HSC pipe_tasks changesets to LSST
We identified in DM-5162 some changesets that still need to be ported from HSC to LSST:    * 31ab5f02f7722650ad0a0eb4e2f7f8b3e0073366, 0c9a4a06bfb34ed26c72109131ef9f4a8c8f237a: multiBand: save background-subtracted coadd as deepCoadd_calexp  * e99e140feafe28e6f034143e8ee2ae58e9a9358d: Rejig interface for DetectCoaddSourcesTask to provide non-dataRef-centric API  * 829ee0cdd605ed027af1fada4446b715d9a5180d: multiband: activate sky objects  * MeasureMergedCoaddSources.doMatchSources defaults to False  * ProcessImageConfig.doWriteHeavyFootprintsInSources defaults to False ?  * 56666e8feba6893ac95fd4982d3e0daf6baf2d34: WcsSelectImagesTask: catch imagePoly is None    We also noticed some differences:    * * CalibrateConfig.setDefaults doesn't call parent  * CalibrateTask.run isn't returning apCorrMap  * reserveFraction=-1 instead of 0.2  ",3
"Port HSC obs_subaru changesets to LSST
We identified in DM-5162 several changesets that still need to be ported from HSC to LSST:    * 8948917de4579e032c7bbb2c8316014446e3841b: config: add astrometry filter map for HSC narrow-band filters  * 69d35a890234e37c1142ddbeff43e62fe36e6c45: Set radius for flux.naive, adjust comment for flux.sinc  * 8ea54d10f5ae56f8b6f244bca76d5796ae015216: config: disable sigma clipping in coadd assembly  * 8d2f4a02d0d668fc82e853b633444d8e0fe80010: config: reduce coadd subregionSize  * e36bd1b4410812ca314f50c01f899d92acc0e7a5: config: set pixelScale for jacobian correction  * Remove processCcdOnsiteDb.py, processStack.py  * Rename stacker.py to coaddDriver.py or whatever Nate chooses in DM-3369  * 49e9f5dcf16490f6be6438b89b17911a0cd35fb2: Fixed obvious errors caused by introducing VignetteConfig  * 8948917de4579e032c7bbb2c8316014446e3841b: config: add astrometry filter map for HSC narrow-band filters  * daa43eeac46e8708de6f37feeb5d5d16a3caca11: HscMapper: set unit exposure time for dark  * 77ff7c89d56bed94bca4f320f839dbd20fbab641: Set BAD mask for dead amps instead of SAT    We also noticed the following need to be done:    * Forced photometry configuration (CCDs and Coadds)  * Sanitize config of OBS_SUBARU_DIR (use getPackageDir)  * multiband config files need ""root"" --> ""config""  * No astrometry in measureCoaddSources  * Narrow bands missing from priority list  * detectCoaddSources removed from multiband  * Move filterMap from config/processCcd.py into own file",5
"Add z-index for dialogs components
Some of the outside modules that we have brought in have a z-index.  We need to make sure that our dialog components stay on top of them.",2
"Docker-ready configuration system for LTD Keeper
To deploy LTD Keeper in a Docker container (DM-5194), it’s best practice to handle all configurations through environment variables. In DM-4950, LTD Keeper was configured through files for test and dev/deployment profiles. What we should do is continue to allow hard-coded configurations for test and dev environments, but have a third fully fledged configuration environment that’s driven entirely by environment variables.    The environment variables should allow fine grained configuration (for example, to turn off calls to individual external services for testing).    This should also resolve how to deal with Google Container Engine/Kubernetes auth flow works with environment variables, config files, and profiles.",1
"Add queue support
The ctrl_events package currently only supports ActiveMQ topics.  This change will add support for queues.    Additionally, there will be some minor code and assertion clean up as noted in DM-5279.",12
"ImageDifferenceTask: Refactor Image DifferenceTask
The original DM-3704 was to refactor all ImageDifference task. This issue was split into 3 tasks:  1) Split image difference task into two tasks (1) to generate an image difference, and (2) to run detection and measurement on it: processDiffim.py  2) Refactor the Image Difference portion  3) Refactor the processDiffim portion    This ticket refactors the new task that just generates and image difference. ",8
"Rewrite unit tests for new dipole measurement task
nan",8
"Make jointcal buildable under CI
Once jointcal is part of the stack, we need to get it under continuous integration, buildable by Jenkins, etc. There is only one unittest in the package currently, but at least getting that test built and run will catch a number of basic problems.    This requires having its dependencies (CHOLMOD from SuiteSparse) under CI as well.    As part of this, it would be good to have at least one ""integration test"" that runs jointcal as part of processCcd, to catch problems that appear when that interface changes.",8
"Document simple simulator
Document the simple simulator produced in DM-4899.  This will also involve some refactoring and adding unit tests to make it usable by others in the group.",8
"X16 Design Discussions
Design discussions within the team and with other teams to ensure the plan is complete and properly scoped",64
"Compare LSST and HSC pipelines through through single-frame processing
Following DM-2984, we are confident that the ISR performed by the LSST and HSC stacks is equivalent. Extend that work to cover the whole of single-frame processing ({{ProcessCcdTask}}).    There are two deliverables for this issue:  * A collections of plots and/or comparison scripts that run on the pipeline outputs that can be used to compare their quality in all the ways we think matter. This should probably be in a new repo. Some of these tests should compare the results of the two pipelines against each other (as that's easier to do), but others should be useful for tracking the quality of the pipeline even after we abandon the HSC fork.  * A set of new JIRA issues that capture the code transfers we think we will need to get the LSST pipeline up to the same level of quality as the HSC one.",10
"manage jenkins core + plugin versions
There have been a couple of issues that have arisen when deploying test instances vs updating an existing instance due to slight differences between plugin versions.  This would be avoided by putting all plugin versions under change control.    Including:  * The versions of all jenkins components need to be explicitly specified  * The stored job {{config.xml}}'s should be updated to reflect plugin version changes  * The hipchat notification configuration should be updated to fix breakage caused by the production core/plugin update earlier this week  ",5
"Get high volume test script working again at IN2P3 cluster
Currently runquerys.py script falls over when running high-volume tests:    * ""LV"": 75,  * ""FTSObj"": 3,  * ""FTSSrc"": 1,  * ""FTSFSrc"": 1,  * ""joinObjSrc"": 1,  * ""joinObjFSrc"": 1,  * ""nearN"": 1    We need this to be working again to validate recent work on schedulers and to support upcoming work on large results, etc.",10
"Test removal of response queuing on czar to see if this provides useful flow control
nan",3
"Enable automated publication of qserv-dev release
This would allow integration tests in CI not to break when some Qserv dependencies change. Indeed CI uses a Docker container which include qserv-dev to build current Qserv version.",4
"Additional vertical partitioning tests
Test potential improvements in many-vertical-shards test (20,50) run-times with query optimizer settings.",5
"Refine MemManReal implementation per design discussion w/ John
nan",10
"Implement unique query-id generation
There are currently two separate query IDs defined for queries in czar code:  - ""user query ID"" - defined in {{Czar::submitQuery()}}, used for constructing table names for result table and message table  - ""QMeta query ID"" - ID obtained from QMeta after registering the query (by {{UserQuerySelect::_qMetaRegister()}})    Currently user query ID is used by the rest of the czar code to track the processing of this query, QMeta ID is not used yet for anything except QMeta registration and updates.     QMeta ID will be used for async query identification and there is no actual reason to keep two IDs around, so we should replace user query ID with the QMeta-generated one everywhere. One minor issue is that currently message table name is built and table is locked before we register query in QMeta. Need to understand it and see if we can reverse that logic.",4
"DM Verificational Plan Document for CoDR
This epic captures work resulting from a Systems Engineering request for a document on the DM Verification plan.     In summary, this document must describe how are DM deliverables are going to be verified against the requirements?    George agrees that structuring this DM Verification Plan around the already drawn KPMs is the right thing to do.     SQuaRE will draft a document for internal DM circulation and eventually leading to a project-led Conceptual Design Review of the DM Verification Plan.     The skeleton plan is:  * Go through the KPMs  * List the method (== tools) by which is KPM will be measured      => Describe the requirements for that method      => Criteria of success    * State when it can be measured    * Describe the data necessary or planned for doing the measurements    Additionally, a process (most likely an end2end run) will be described that can verify that external to DM interfaces are being correctly met.     There is no requirement from the point of the CoDR for a resource-loaded plan leading to this work. That is expected to follow from a successful CoDr.           ",28
"Research alert production database design
nan",4
"Identify specs within VO stack which should be implemented by database team
nan",6
"Begin exploratory TAP implementation within dbserv
This is a quick coding foray, to try to shake loose unforseen implementation dependencies or speed-bumps with TAP integration.    Time-boxed at 4 points to fit into a single sprint with Brian's current resource loading -- this is intended to be only a clarifying start.",4
"Fix mariadb CI
patch package is missing in docker container used by travis-CI.",1
"Make Bright Object Masks compatible with all cameras
Currently all of the logic that goes into using bright object masks falls into obs_subaru and pipe_tasks. This ticket should move parts (such as the bright object mask class) out of obs_subaru, into a camera agnostic location. The work should also duplicate relevant camera configurations and parameter overrides in the other camera packages. Bright object masks were originally introduced in DM-4831",2
"MeasureApCorrTask should use slot_CalibFlux as default ref flux
{{MeasureApCorrTask}} uses ""base_CircularApertureFlux_17_0"" as its default reference flux. It should use ""slot_CalibFlux"" instead.    Also check obs_sdss packages for overrides that can be removed; obs_sdss certainly has one in {{config/processCcdTask.py}}",1
"Remove any redundant or unused datasets
Please remove any redundant or unused dataset names from policy files throughout the stack.",1
"estimateBackground should not make a deep copy of the exposure
Implement RFC-155: change {{estimateBackground}} as follows:  - Always subtract the background  - Modify the exposure in place  - Replace {{estimateBackground}} with the run method of a new task {{SubtractBackgroundTask}}  - Replace {{getBackground}} (which fits a background) with {{SubtractBackgroundTask.fitBackground}}",4
"Convert GWT code to pure JavaScript (X16, part2, basic)
Continue to work on the GWT code conversion to JavaScript.",100
"Create network monitoring dashboard for nebula sys admins
nan",4
"JTM meeting in Santa Cruz
Met with many individuals, had lots of good conversations. Helped bring me closer to the working of the project and provided a level-set for where activities are currently at. Attended LHN session for most of Wednesday. ",10
"Assist in OSX VM environment deployment
nan",2
"Add ExposureIdInfo class
Implement RFC-146: add ExposureIdInfo class to daf_butlerUtils    This will be implemented in daf_butlerUtils as part of DM-4692, with a unit test in obs_test because daf_butlerUtils has no camera mapper or camera repo in its test directory.",4
"Add usesMatches to star selectors
Implement RFC-126 add usesMatches to star selectors    This will be implemented as part of DM-4692",4
"a Catch all bug fix epic (X16)
A epic for reported bugs in this scycle",10
"GWT Conversion: Table results container
Create a result container for table data.  This task is composed of:  - create actions, action creators and reducing functions  - dynamically add/remove table from view  - support expanded mode  - TabPanel support for deleting tabs.",6
"Fix minor issues in docker procedure
- params.sh was missing at configuration  - startup.py wasn't importing correctly module ""utils""  - remove unused parameters in params.sh",1
"Planning for GPFS, etc.
* Gathered filesize statistics from existing NFS for planning GPFS  * Assisted with GPFS client setup on test servers  * Reviewed infrastructure changes for Jason",2
"Week end 2/07/16
Support for lsst-dev cluster, OpenStack, and accounts  for week ending February 7, 2016.",5
"Week end 2/14/16
Support for lsst-dev cluster, OpenStack, and accounts  for week ending February 14, 2016.",6
"Week end 2/21/16
Support for lsst-dev cluster, OpenStack, and accounts  for week ending February 21, 2016.",2
"Week end 2/28/16
Support for lsst-dev cluster, OpenStack, and accounts  for week ending February 28, 2016.  ",4
"Jason Feb Tasks
Procurement activities to prepare ""procurement plan activity 1"". PDU, rack, network selection and review. Refresh quotes for compute, storage, rack, pdu, electrical. Refresh, finalize, present (internally) and review design for FY16 infrastructure.    Draft and review of Procurement Plan Activity 1 document.",6
"New equipment setup and configuration (week end 2/07/16)
* Updated lsst-dev7 with few missing pieces after initial user testing  * Setup 3 of 8 lsst-test servers  * Confirmed IPMI setup on new test servers (working with Dell on issue with 1 iDRAC license upgrade)  ** Completed and verified IPMI setups  *** Installed licenses for lsst-test1 - lsst-test6  *** re-associated IPMI lsst-test1m last-tsst6m with the correct systems.  *** Installed CentOS7 on lsst-test1 - lsst-test6. (in progress)  * UPS setup  ** Setup table with location of systems in 3003 racks  ** Setup apcusbd on lsst-stor141, lsst-stor142, lsst-stor143, lsst-stor144, lsst20, lsst13",3
"Jason Mar Tasks
Week 1: Admin mtg, group mtg, ICI mtg, interview. 1pt  Week 2: Meetings, ICI task planning and prioritization 2pt  Weeks 3&4: Interviews, admin and group meetings, ARI SOW",6
"New equipment setup and configuration (week end 2/14/16)
* Still pushing at Dell to fix broken iDRAC license  * Added 5 systems to RSA OTP system  * Completed the setup of lsst-test1, lsst-test4, lsst-test5, lsst-test6  ** Reinstalled lsst-test1 to correct error in puppet install, Completed CentOS7 install, Installed puppet and ran puppet  ** Corrected network error on lsst-test4, Completed CentOS7 install, Installed puppet and ran puppet  ** Completed the setup of Installed lsst-test5, Completed CentOS7 install, Installed puppet and ran puppet  ** Installed lsst-test6, Completed CentOS7 install, Installed puppet and ran puppet",1
"Recover from accumulated technical debt
Through the S15 and W16 cycles the DRP group focused on merging functionality from HSC. To expedite this process, we accepted lower quality documentation and poorer test quality than would normally be required. We need to recover from this, and other, accumulated technical debt.",50
"Add tests for recent improvements to CModel
In DM-4768 we ported a number of improvements to CModel from HSC. However, these were not accompanied by test cases. Please add them.",3
"Get rid of ProcessCcdSdssTask and ProcessCcdDecamTask
Update {{ProcessCcdTask}} so that it can be used with different datasete types as appropriate for the ISR task. This will allow us to get rid of obs-specific variants {{ProcessCcdSdssTask}} and {{ProcessCcdDecamTask}}    The plan is to change {{ProcessCcdTask}} as follows:  - set {{doMakeDataRefList=False}} in the call to {{add_id_argument}}  - get the dataset type from the ISR task (default to ""raw"") and set it in data container  - make the dataRef list by calling {{makeDataRefList}} on the data container    Question for DECam folks: do you want two executable scripts for DECam (one that processes data from the community pipeline and one that performs ISR)? Or do you prefer one exectutable (in which case you switch between performing ISR and reading the output of the community pipeline output by retargeting the ISR task)? If you prefer one binary, then which should be the default: perform ISR or read the output of the community pipeline?",2
"Revise LSE-140 to account for recent changes to calibration instrumentation
Produce a revision of LSE-140, the DM - to - auxiliary instrumentation ICD, taking into account recent changes to the calibration instrumentation.",5
"Establish goals and create EA framework for LSE-140 update
Deliverable: together with [~pingraham], identify the changes needed and develop initial content in EA.",2
"Create change request for LSE-140
Deliverable: change request and document diffs for LSE-140",1
"Add Vis toolbar to expanded mode
nan",1
"Upgrade minuit2
Minuit2 5.34.14 came out in 2014. The current version in the stack is 5.28 from 2010. Minuit2 is annotated on the DM third party software page as being approved for 6-monthly uprev. Minuit2 is only used by AFW.    Release notes for 5.34.14:    * Several fixes and improvements have been put between this verion and the previous stand-alone one (5.28). Main new features is now the support for using the {{ROOT::Math::Minimizer}} interface via the class {{ROOT::Math::Minuit2Minimizer}} also in the standalone version. A new test has been added ({{test/MnSim/demoMinimizer.cxx}}) to show this new functionality  * Other major improvements is in the control of the error messages. One can now use the class {{MnPrint::SetLevel(int value)}} to control the output level. The same can be achieved by calling {{ROOT::Math::Minuit2Minimizer::SetPrintLevel}}.",1
"meas_algorithms uses packages that are not listed in table file
{{meas_algorithms}} directly uses the following packages not expressed in the table file:  * Minuit2  * daf_persistence  * daf_base  * pex_config  * pex_exceptions  * pex_policy  ",1
"Test consistency of Shear Measurements with different Psfs
DM-1136 was done with a single Psf, partly to avoid some of the problems we found with PsfShapeletApprox.  In this issue, I will look at consistency of the measurement for different Psfs.",8
"Test error estimation with bootstrap resampling
Test the error estimation code using bootstrap resampling.",6
"Move supertask code our from pipe_base
Create a new package {{pipe_supertask}} and move all supertask code and activator there. Will soon create a poll to pick a better name.",3
"Update DMTN-002 to reflect last changes
Need to update documentation with latest changes on {{pipe_base}}, {{pipe_supertask}} and {{pipe_flow}}",1
"Update {{pipe_flow}}
Update {{pipe_flow}} to change dependencies and examples to reflect migration to {{pipe_supertask}}",1
"Begin Image Select Panel/Dialog
This is the first stop in image select panel:    * Design basic panel  * Implement tabs: issa, 2mass, wise  * implement ability to replace a image",10
"Image Select panel: finish tabs
Implement     * dss, sdss, blank image tabs  * a reusable radius input field   * implement upload file tab  ( the reusable upload file component is done in DM-5584)  ",10
"Image Select Panel: 3 color support
Add 3 color support.  Basically take the panel an be able to repeat of red,green, and blue.  We might want to use a disclosure component.    What has been done: (copied from github commit message)  Add 3 colors support to image selection dropdown, disclosure component is used for r, g, b field group tabs.    For SizeInpuFiield component, fixed validation, add props 'showFeedback' for feedback display and add valid range in error message popup.  (this update is based on the following issues as SizeInputField is used in other panel     - Popup message should have the range are part of the message     - The feedback at the bottom should be optional, turned on by a property.     - When a value such as ‘111d’ is entered,  It does not validate correctly. )    Some Issues:   - should 'disclosure' component's status be kept?   - image creation doesn't work if any of r, g, b is disabled.",10
"Image Select Panel: Support add or modify of plot
previously the image select panel would only modify a plot.  Now give it the ability to add a plot.",8
"Enable CC-IN2P3/Qserv team communication in order to prepare for Pan-STARRS large scale tests
The goal of this ticket is to enable communication between CC-IN2P3 and Qserv team in order to prepare for Pan-STARRS data ingestion into Qserv. This data ingestion step is necessary for the large scale tests of Qserv foreseen for summer 2016.    Specifically, we need to understand:    - What is the size of the data set to be imported to CC-IN2P3?  - Where the Pan-STARRS data set to be imported is currently located?  - What mechanisms will the host of Pan-STARRS data make available to CC-IN2P3 for downloading the data set?  - Does the envisaged ingestion mechanism into Qserv requires that the data transit through the Qserv master server or will each Qserv worker be able to ingest its own chunk of data?  - After the ingestion process is finished, do we need to keep a copy of the ingested data out of Qserv?      Given the size of the dataset likely involved in this process, this project will probably require that we (both Qserv and CC-IN2P3 experts) set up specific mechanisms and equipment for efficient transport, storage and ingestion of these data. Timely planning and several testing campaigns seem necessary for this project to make progress.      ",8
"Change default value of MeasApCorrConfig.refFluxName to slot_CalibFlux
The default value of {{MeasApCorrConfig.refFluxName}} is presently ""base_CircularApertureFlux_17_0"". This should be changed to ""slot_CalibFlux"". That is what the slot is intended for. The slot usually points to ""base_CircularApertureFlux_17_0"", but {{obs_sdss}}, at least, overrides this.    Additional jobs:  - Update {{obs_sdss}} {{config/processCcd.py}} to remove the override for this value, since it will no longer be needed.  - Check for and remove unnecessary overrides in other obs_ packages",1
"Use modern TAP package declarations for all EUPS third party packages
In DM-4670 the TAP-ness of the packages was declared using a {{.tap_package}} file. The modern fix is to use a {{$TAP_PACKAGE}} environment variable in the {{eupspkg.cfg.sh}} file. This is how {{pyyaml}} was implemented.",1
"Create {{lsst_ci}} package as a continuous integration build target
Create an {{lsst_ci}} package to be built for the continuous integration testing.    Plan:  1. Create empty package that has dependencies on {{obs_cfht}}, {{obs_decam}}, {{obs_subaru}}, {{testdata_cfht}}, {{testdata_decam}}, {{testdata_subaru}}. (/)  2.  Ensure above builds. (/)  3.  Add {{obs_lsstSim}} and ensure that it builds. (/)    The following were moved to DM-5381:  [ [~tjenness] : How can I get strikethrough to work in the following list?]  3. Add dependencies on {{validation_data_cfht}} and {{validation_data_decam}}, and {{validate_drp}}.  4. Run CFHT, DECam quick examples in {{validate_drp}}.  5. Test for successful running of the above examples.  Fail and trigger Jenkins FAILURE message if these examples fail.  6. Check performance of CFHT, DECam runs against reference numbers.  Fail if there is a significant regression.  7. Decide how to include {{ci_hsc}}, which currently can take at least 30 minutes to process the image data.--",1
"butler planning for X16
nan",4
"Fix obs_* packages and ci tests broken by DM-4683
The butler changes in DM-4683, in particular the removal of {{.mapper}} from the interface exposed by a {{Butler}} object, broken {{obs_cfht}}, {{obs_decam}}, and {{ci_hsc}}.    This issue will fix those changes, and search for additional broken things.    This work is proceeding in conjunction with DM-5370 to test that the CI system, e.g. {{lsst_ci}}, is sensitive to these breakages and fixes.",1
"Add to baseline a dedicated replica of L1 database just for scans
Per RFC-133, users will sometimes need to do full table scan through L1 catalogs, and our baseline does not allow for full scans on the L1 catalog. It'd be good to maintain a replica of L1 for such scans. This story involves changing LDM-141 and adding hardware for the replica. ",4
"Remotely attend JTM 2016 sessions
SSIA.  The final hours of the final day were very valuable.",6
"L1 base messaging topology.
Make sequence diagram for Base site messaging and enumerate each message, then provide a narrative description of each message (including logical flow control if applicable) AND an example message payload for the message dictionary. 4  Meetings about message exchange style.  Meetings about using queue fanout for return messages from forwarders and distributors, or binding using routing keys. 4  Rapid prototyping of some of these ideas for evaluation. 4",12
"Create {{lsst_qa}} package as a daily build target for regression testing
1. Add dependencies on {{validation_data_cfht}} and {{validation_data_decam}}, and {{validation_data_hsc}}.  (/)  2. Add dependency on {{validate_drp}}.  (/)  3. Run CFHT, DECam quick examples in {{validate_drp}}.  (/)  4. Test for successful running of the above examples.  (/)  5. Implement in a testing framework.  6. Check performance of CFHT, DECam runs against reference numbers. Fail if there is a significant regression.  (/)  7. Include {{ci_hsc}}, which currently takes 1000 seconds  to process the image data.  8. Check performance of HSC runs against reference numbers.  Fail if there is a significant regression.",4
"Port SdssShape changes from HSC meas_algorithms to LSST meas_base
In porting {{meas_algorithm}} changes from HSC to LSST, modifications to the {{SdssShape}} algorithm were discovered. These changes should be transferred to LSST.",3
"calib_psfReserved is only defined when candidate reservation is activated
The schema should in general not be a function of whether particular features are enabled or disabled so that users can have confidence looking for columns.  However, {{MeasurePsfTask}} only creates the {{calib_psfReserved}} column when {{reserveFraction > 0}}.  This causes warnings when attempting to propagate flags from calibration catalogs to deep catalogs.",1
"Filter  editor
A dialog to edit all the filters on the data for table and XY plot.     AND, OR conditions?    implemented:  * display column's units and descriptions  * add single column filter with auto-correction  * add free-hand filters field with validation and auto-correction  * reset, clear filters as well.  * 'Column' is sticky... scrolling left/right will not affect it.    ",8
"catalog search panel
The search panel to do the catalog search",10
"GWT Conversion: Dropdown Container
Create drop down container to display search panel, catalog search panel, image search panel, etc.     ",4
"JavaScript loading/caching plan
We need to ensure that the latest version of the application(javascript) is loaded. Conditions: 1. once loaded, it should be cached by the browser. 2. name of the script has to be a static, so it can be referenced by api user. 3. it also has to load dependencies(gwt scripts) after the main script is loaded.  To do this, we created a tiny firefly_loader.js script whose role is to load the main script and then its dependencies. firefly_loader.js is configured to never cache so that the latest main script is always picked up. The main script is appended with a unique hash on every build.  This ensures that the browser will pick up the new script the very first time, and then cache it for future use. ",2
"Please stop leaving repoCfg.yaml files around
After a recent change to {{daf_persistence}} and possibly other packages I'm finding that many packages leave {{repoCfg.yaml}} files lying around after they run unit tests.    I'm not sure what is best to do about these files. If they are temporary, as I am guessing, then I think we need some way to clean them up when the tests that generated them have run. If they are intended to be permanent (which would be surprising for auto-generated files) then they should probably be committed?    I hope we can do better than adding them to .gitignore.",1
"Investigate boost compiler warnings and update boost to v1.60
As reported in comments in DM-1304 clang now triggers many warnings with Boost v1.59:  {code}  /Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/boost/1.59.lsst5/include/boost/archive/detail/check.hpp:148:5: warning: unused typedef 'STATIC_WARNING_LINE148' [-Wunused-local-typedef]      BOOST_STATIC_WARNING(typex::value);      ^  /Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/boost/1.59.lsst5/include/boost/serialization/static_warning.hpp:100:33: note: expanded from macro 'BOOST_STATIC_WARNING'  #define BOOST_STATIC_WARNING(B) BOOST_SERIALIZATION_BSW(B, __LINE__)                                  ^  /Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/boost/1.59.lsst5/include/boost/serialization/static_warning.hpp:99:7: note: expanded from macro 'BOOST_SERIALIZATION_BSW'      > BOOST_JOIN(STATIC_WARNING_LINE, L) BOOST_STATIC_ASSERT_UNUSED_ATTRIBUTE;         ^  /Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/boost/1.59.lsst5/include/boost/config/suffix.hpp:544:28: note: expanded from macro 'BOOST_JOIN'  #define BOOST_JOIN( X, Y ) BOOST_DO_JOIN( X, Y )                             ^  /Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/boost/1.59.lsst5/include/boost/config/suffix.hpp:545:31: note: expanded from macro 'BOOST_DO_JOIN'  #define BOOST_DO_JOIN( X, Y ) BOOST_DO_JOIN2(X,Y)                                ^  /Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/boost/1.59.lsst5/include/boost/config/suffix.hpp:546:32: note: expanded from macro 'BOOST_DO_JOIN2'  #define BOOST_DO_JOIN2( X, Y ) X##Y                                 ^  <scratch space>:25:1: note: expanded from here  STATIC_WARNING_LINE148  ^  {code}  v1.60 is the current version so we should see if these warnings have been fixed in that version.",2
"Prototype afw/AstroPy integration
Investigate and prototype options for integrating {{afw}} with AstroPy. In particular, this epic focuses on establishing how tightly, if at all, AstroPy and {{afw::table}} should be coupled.",25
"Transfer relevant HSC documentation to LSST
Audit the HSC [questions site|http://hsca.ipmu.jp:8080/questions//] and [Pipeline How To|http://hsca.ipmu.jp/hscsphinx/] documentation. Identify the parts which are still relevant to LSST. Transfer them to the LSST documentation.    First assumption is that questions go to [clo|http://community.lsst.org] in an appropriate category, while the howto documentation is incorporated into the developer or user guides as appropriate. Confirm this with SQuaRE before starting work.    The intention is to carry out most of this work as a group in a ""dockathon"" session.",20
"X16 Framework bucket
Catch all epic for emergent work in 02C.04.01.",20
"Support for DM replanning process
Throughout the X16 cycle we expect to have to assign effort to support the ongoing DM replanning process. This work takes two forms:  * Tasks assigned by the DMLT Working Groups;  * Face-to-face discussions with other parts of the DM project.    Both are captured in this epic.",25
"HSC port: data release verification
Throughout S15 and W16 we have worked to merge changes from the Hyper Suprime-Cam stack to LSST. This work is now broadly complete, but requires acceptance testing by HSC. In support of that, the LSST stack must be brought to a level at which it is capable of reproducing the January 2016 HSC data release. This work is a continuation of the effort undertaken in DM-3628. It will reach a successful conclusion when the HSC project undertakes future development based on the LSST stack.",50
"Cleanup jointcal
Before we start digging into jointcal, it'd be good to get the whitespace/oldpython/indentation/lint/etc. questions sorted out. This ticket is for that.",2
"Calibration Products Pipeline development in X16
Continued investigation and characterization of the DECam CBP data.",25
"Make cluster deployment scripts more generic and enable ccqserv100...124
These scripts will be improved (i.e. more genericity) and integrated inside Qserv code. Qserv will be deployed on ccqserv100 to ccqserv125",3
"Developer Guide Content & Maintenance Backlog Epic
General maintenance and original content for the DM Developer Guide (http://developer.lsst.io) based on needs during the cycle.",7
"LSST the Docs Production Deployment
In DM-1139 we developed LSST the Docs. LSST the Docs is described in [SQR-006|http://sqr-006.lsst.io]. This epic will focus on the deployment of LSST the Docs as a reliable production service for documentation builds and hosting.",16
"Re-enable CModel forced measurement on CCDs
Recent changes from the HSC side (DM-4768) were implemented in a hurry, and break CModel forced measurement when the reference WCS is different from the measurement WCS (as is the case with forced measurement on CCDs).  This was considered an acceptable temporarily, since forced CCD measurement is currently severely limited by our lack of deblending, but we'll need to fix it eventually.    The fix is trivial from an algorithmic standpoint but may require a bit of refactoring (at least changing some function signatures; maybe more).    This should include re-enabling the different-WCS complexity in testCModelPlugins.py,",2
"Require fields listed in icSourceFieldsToCopy to be present
{{CalibrateTask}} presently treats config field {{icSourceFieldsToCopy}} as a list of fields to copy *if present*. This was required because one of the standard fields to copy was usually missing. However, [~price] fixed that problem in DM-5385. Now we can raise an exception if any field listed is missing (though I propose to continue ignoring {{icSourceFieldsToCopy}} if isSourceCatalog is not provided).",1
"Rename datasets to utilize butler aliases
Now that the butler has alias features that can allow for some degree of dataset substitutability, we should consider renaming (or adding aliases) for our existing datasets to make the naming consistent and analysis code more generic.    This work should be proceeded by an RFC with a proposal for the new names and a migration plan.    It *might* make sense to defer this until the high-level pipeline descriptions are more mature and we can choose relatively future-proof names, but hopefully the alias features will also make migration easy enough that this doesn't matter a lot.",4
"Qserv do not return very same BLOB field than MySQL
Enabling query {{qserv_testdata/datasets/case01/queries/0007.2_fetchSourceByObjIdSelectBLOB.sql.FIXME}} will reveal this bug.    Qserv chunk table contains next BLOB:  {code:bash}  mysql --socket /home/dev/qserv-run/git/var/lib/mysql/mysql.sock --user=root --password=changeme qservTest_case01_qserv -e ""select blobField from Source_6630 where SourceId=29809239313746172;"" > 29809239313746172.chunk6630    vi 29809239313746172.chunk6630    blobField    ^D^B\0^W\0\0\0^B\0^K\0~B\0first_fieldsecond_field(�q.\\�  {code}    But Qserv returns:  {code}  ^D^B\0^W\0\0\0^B\0^K\0~B\0first_fieldsecond_field(�q.�  {code}    See DM-991 for additional informations.  ",8
"DecamIngestTask is mis-calling openRegistry
`DecamIngestTask` is mis-calling `lsst.pipe.tasks.RegistryTask`. Line 59:    {code}  with self.register.openRegistry(args.butler, create=args.create, dryrun=args.dryrun) as registry:  {code}  {{openRegistry}} is expecting a directory name, not a butler object for the first argument    Thanks to [~wmwood-vasey] for diagnosing this.",1
"Test new dipole fitting task on real data
Test new task on real data (which data, TBD); inspect results by eye and compare with existing DipoleMeasurementTask output. This is necessary prior to incorporation into the imageDifference command-line task.    This test may also indicate that further optimizations are necessary (DM-5721).",6
"Incorporate new DipoleFitTask into imageDifference command-line task alongside existing DipoleMeasurementTask
Incorporate the new task into the command-line task. The goal of this ticket is to implement DipoleFitTask along-side the existing DipoleMeasurementTask, eventually to replace it.    This is likely to have additional stories added, including testing, possibly as part of DM-5412.",6
"Create buildable SuiteSparse external package
To get jointcal to build in the stack, we need to satisfy the SuiteSparse dependency by creating an external package for SuiteSparse.    Assuming it builds cleanly, this should satisfy the remaining requirement of RFC-153, now that the licensing question has been answered there.",2
"generation of conda binary packages for DM software products
This epic covers work in generating binaries for stack releases. At this point we are persisting with the plan to produce conda binary packages for ease of use on the user side, though their reliable generation has so far resisted automation.    Conda binaries will be produced for the 12.0 Stack release. ",32
"Ci Deploy and Distribution Improvements part IV
This is a bucket epic for ongoing improvements to the CI system",8
"Release engineering Part Three
This epic covers testing and co-ordination work associated with making  engineering and official releases, and code to support them.      (FE:8, DN:6.5, JS:8)",23
"ci_hsc fails test requiring >95% of PSF stars to be stars on the coadd
Since the first week of March 2016, ci_hsc fails its test that requires that >95% of the PSF stars be identified as stars in the coadd.  I suspect this is related to the DM-4692 merge.    Here is a sample job that fails:  https://ci.lsst.codes/job/stack-os-matrix/9084/label=centos-6/console    The relevant snippet of the failure is:    {code}  [2016-03-10T17:12:06.667778Z] : Validating dataset measureCoaddSources_config for {'filter': 'HSC-R', 'tract': 0, 'patch': '5,4'}  [2016-03-10T17:12:06.697383Z] CameraMapper: Loading registry registry from /home/build0/lsstsw/build/ci_hsc/DATA/registry.sqlite3  [2016-03-10T17:12:06.697615Z] CameraMapper: Loading calibRegistry registry from /home/build0/lsstsw/build/ci_hsc/DATA/CALIB/calibRegistry.sqlite3  [2016-03-10T17:12:07.716310Z] CameraMapper: Loading registry registry from /home/build0/lsstsw/build/ci_hsc/DATA/registry.sqlite3  [2016-03-10T17:12:07.716443Z] CameraMapper: Loading calibRegistry registry from /home/build0/lsstsw/build/ci_hsc/DATA/CALIB/calibRegistry.sqlite3  [2016-03-10T17:12:08.663566Z] : measureCoaddSources_config exists: PASS  [2016-03-10T17:12:08.721051Z] : measureCoaddSources_config readable (<class 'lsst.pipe.tasks.multiBand.MeasureMergedCoaddSourcesConfig'>): PASS  [2016-03-10T17:12:08.721077Z] : Validating dataset measureCoaddSources_metadata for {'filter': 'HSC-R', 'tract': 0, 'patch': '5,4'}  [2016-03-10T17:12:08.721249Z] : measureCoaddSources_metadata exists: PASS  [2016-03-10T17:12:08.721663Z] : measureCoaddSources_metadata readable (<class 'lsst.daf.base.baseLib.PropertySet'>): PASS  [2016-03-10T17:12:08.721715Z] : Validating dataset deepCoadd_meas_schema for {'filter': 'HSC-R', 'tract': 0, 'patch': '5,4'}  [2016-03-10T17:12:08.721878Z] : deepCoadd_meas_schema exists: PASS  [2016-03-10T17:12:08.726703Z] : deepCoadd_meas_schema readable (<class 'lsst.afw.table.tableLib.SourceCatalog'>): PASS  [2016-03-10T17:12:08.726834Z] : Validating source output for {'filter': 'HSC-R', 'tract': 0, 'patch': '5,4'}  [2016-03-10T17:12:10.203469Z] : Number of sources (7595 > 100): PASS  [2016-03-10T17:12:10.204166Z] : calib_psfCandidate field exists in deepCoadd_meas catalog: PASS  [2016-03-10T17:12:10.204772Z] : calib_psfUsed field exists in deepCoadd_meas catalog: PASS  [2016-03-10T17:12:10.205468Z] : Aperture correction fields for base_PsfFlux are present.: PASS  [2016-03-10T17:12:10.206159Z] : Aperture correction fields for base_GaussianFlux are present.: PASS  [2016-03-10T17:12:10.207193Z]  FATAL: 95% of sources used to build the PSF are classified as stars on the coadd (0 > 0): FAIL  [2016-03-10T17:12:10.207455Z] scons: *** [.scons/measure-HSC-R] AssertionError : Failed test: 95% of sources used to build the PSF are classified as stars on the coadd (0 > 0)  [2016-03-10T17:12:10.207481Z] Traceback (most recent call last):  [2016-03-10T17:12:10.207525Z]   File ""/home/build0/lsstsw/stack/Linux64/scons/2.3.5/lib/scons/SCons/Action.py"", line 1063, in execute  [2016-03-10T17:12:10.207556Z]     result = self.execfunction(target=target, source=rsources, env=env)  [2016-03-10T17:12:10.207593Z]   File ""/home/build0/lsstsw/build/ci_hsc/python/lsst/ci/hsc/validate.py"", line 133, in scons  [2016-03-10T17:12:10.207611Z]     return self.run(*args, **kwargs)  [2016-03-10T17:12:10.207646Z]   File ""/home/build0/lsstsw/build/ci_hsc/python/lsst/ci/hsc/validate.py"", line 122, in run  [2016-03-10T17:12:10.207663Z]     self.validateSources(dataId)  [2016-03-10T17:12:10.207732Z]   File ""/home/build0/lsstsw/build/ci_hsc/python/lsst/ci/hsc/validate.py"", line 191, in validateSources  [2016-03-10T17:12:10.207749Z]     0.95*psfStars.sum()  [2016-03-10T17:12:10.207786Z]   File ""/home/build0/lsstsw/build/ci_hsc/python/lsst/ci/hsc/validate.py"", line 52, in assertGreater  [2016-03-10T17:12:10.207816Z]     self.assertTrue(description + "" (%d > %d)"" % (num1, num2), num1 > num2)  [2016-03-10T17:12:10.207853Z]   File ""/home/build0/lsstsw/build/ci_hsc/python/lsst/ci/hsc/validate.py"", line 43, in assertTrue  [2016-03-10T17:12:10.207877Z]     raise AssertionError(""Failed test: %s"" % description)  [2016-03-10T17:12:10.207919Z] AssertionError: Failed test: 95% of sources used to build the PSF are classified as stars on the coadd (0 > 0)  [2016-03-10T17:12:10.209935Z] scons: building terminated because of errors.  {code}    This is the test that fails    https://github.com/lsst/ci_hsc/blob/74303a818eb5049a2015b5e885df2781053748c9/python/lsst/ci/hsc/validate.py#L169  {code}  class MeasureValidation(Validation):      _datasets = [""measureCoaddSources_config"", ""measureCoaddSources_metadata"", ""deepCoadd_meas_schema""]      _sourceDataset = ""deepCoadd_meas""      _matchDataset = ""deepCoadd_srcMatch""        def validateSources(self, dataId):          catalog = Validation.validateSources(self, dataId)          self.assertTrue(""calib_psfCandidate field exists in deepCoadd_meas catalog"",                          ""calib_psfCandidate"" in catalog.schema)          self.assertTrue(""calib_psfUsed field exists in deepCoadd_meas catalog"",                          ""calib_psfUsed"" in catalog.schema)          self.checkApertureCorrections(catalog)          # Check that at least 95% of the stars we used to model the PSF end up classified as stars          # on the coadd.  We certainly need much more purity than that to build good PSF models, but          # this should verify that flag propagation, aperture correction, and extendendess are all          # running and configured reasonably (but it may not be sensitive enough to detect subtle          # bugs).          psfStars = catalog.get(""calib_psfUsed"")          extStars = catalog.get(""base_ClassificationExtendedness_value"") < 0.5          self.assertGreater(              ""95% of sources used to build the PSF are classified as stars on the coadd"",              numpy.logical_and(extStars, psfStars).sum(),              0.95*psfStars.sum()          )  {code}    Note that the assertion failure messages is a bit confusing.  It should say  ""Fewer than 95% of the sources used to build the PSF are classified as stars on the coadd.""",1
"Integration and test monitoring implementation Part I
Configure, develop and deploy an ELK system.    High level requirements:  * es.lsst.codes - An current version Elasticsearch cluster.  * collect.lsst.codes - A server with multiple services to collect and aggregate logging and messages.  * logging.lsst.codes - A server with Kibana hooked up between.  * Packer and ansible deploys to create artifacts and deploys on Nebula and AWS. Optionally Docker.  * Use ELK to monitor git-lfs and our CI system.        ",52
"Understand and test real space extension for ZOGY
The ZOGY algorithm (http://arxiv.org/abs/1601.02655) can be implemented as a real space extension of A&L.",43
"Documentation
We are reserving time this cycle for people to contribute to architecture and documentation efforts.",38
"Switch PropagateVisitFlags to use src instead of icSrc
On DM-5084 [~jbosch] switched PropagateVisitFlags to match against icSrc instead of src because we weren't yet matching `icSrc` to `src` in ProcessCcdTask.  That's now been done on DM-4692, so we can revert this.    After doing so, please verify with ci_hsc that this is working, as that's where the only test of this feature lives.",2
"Provide an easy way to set Coord fields of a source catalog
We sometimes need to set the coord fields of a source catalog, e.g. when fitting a new WCS or when studying an `icSrc` catalog (whose Coord field is not set). It would be nice to have a central, easily found way to do this. Right now we have the following as a static method of `TanSipWcsTask`, which works fine but is in a poor location:    {code}      def updateSourceCoords(wcs, sourceList):          """"""Update coords in a collection of sources, given a WCS          """"""          if len(sourceList) < 1:              return          schema = sourceList[1].schema          srcCoordKey = afwTable.CoordKey(schema[""coord""])          for src in sourceList:              src.set(srcCoordKey, wcs.pixelToSky(src.getCentroid()))  {code}    The other direction is also useful for reference catalogs, though from a practical standpoint the only user is probably `meas_astrom`. Even so, I suggest that this be made publicly available in the same way. Again, this is presently a static method of `FitTanSipWcsTask`:    {code}      def updateRefCentroids(wcs, refList):          """"""Update centroids in a collection of reference objects, given a WCS          """"""          if len(refList) < 1:              return          schema = refList[0].schema          coordKey = afwTable.CoordKey(schema[""coord""])          centroidKey = afwTable.Point2DKey(schema[""centroid""])          for refObj in refList:              refObj.set(centroidKey, wcs.skyToPixel(refObj.get(coordKey)))  {code}    I hope this can remain Python code, but admit that the extra speed of C++ might come in handy in some cases. In any case, once the function is in a central location we can implement it in C++ if we find the need.",2
"Participate in X16 DMLT Working Groups
Bosch, Lupton & Swinbank are members of [DMLT Working Groups|https://confluence.lsstcorp.org/display/DM/DMLT+Working+Groups] during X16. This epic captures work related to the activities of those groups.",45
"SingleFrameVariancePlugin can give numpy warnings
SingleFrameVariancePlugin can produce the following numpy warning, with no hint as to where the problem is coming from:  {code}  /Users/rowen/UW/LSST/lsstsw/miniconda/lib/python2.7/site-packages/numpy/core/_methods.py:59: RuntimeWarning: Mean of empty slice.    warnings.warn(""Mean of empty slice."", RuntimeWarning)  {code}  I tracked it down by adding the following code to the calling code:  {code}  import warnings  with warnings.catch_warnings():      warnings.filterwarnings('error')  {code}    It would be nice if the measurement plugin handled this situation more gracefully, such as turning the warning into an exception or testing for it and handling it.    One way to reproduce this problem is to run {{tests/testProcessCcd.py}} in {{pipe_tasks}}. However, it is commonly seen when running {{processCcd}} on other data, as well.",2
"ObjectSizeStarSelector can produce numpy warnings
`ObjectSizeStarSelector` can produce the following numpy warning:   {code}  RuntimeWarning: invalid value encountered in less  {code}  This occurs at the following point in the code:  {code}          for i in range(nCluster):              # Only compute func if some points are available; otherwise, default to NaN.              pointsInCluster = (clusterId == i)              if numpy.any(pointsInCluster):                  centers[i] = func(yvec[pointsInCluster])  {code}  where `func` has been assigned to `numpy.mean`. When I have seen this occur I have found that `dist` is an array of `nan`    I suggest that the star selector handle this situation more gracefully, e.g. by reporting an appropriate exception or handling the data in an appropriate way. If logging a message would be helpful, then please do that (and if RFC-154 is adopted, a log will be available).    One way to reproduce this is to run `tests/testProcessCcd.py` in `pipe_tasks`. However, I often see it when running `processCcd.py` on other data, as well.",2
"Tune and improve ngmix MCMC sampling
Improve the ngmix MCMC sampling plugin to get it working well on most sources.  This may require actually contacting Erin Sheldon and getting his help (but he's quite eager to help).",10
"Changes to galaxy_shear_experiments Python code
This ticket describes changes which were made to the test runner and analysis scripts during the Dec 2015 - Feb 2016 period.  Most of these changes were made as a part of moving to a large computing cluster, where both the units of work and the output file organization had to be changed to make parallelization possible.    The large number of tests run during this period and the need to more efficiently analyze and compare also introduced some changed to the analysis and plot modules.    Since these changes do not pertain to any single test (though many were done during Dm-1136), I have put them on a separate ticket.",5
"Add SFM plugin for ngmix fitting
Add an SFM pluggin for ngmix fitting using one of the simple fitters in ngmix/fitting.py.    This should depend on DM-5429 (or a suitably configured modelfit_ShapeletPsfApprox) for approximating the PSF as a sum of Gaussians.    Testing and tuning this algorithm to get it working well should be deferred to another issue.",10
"Ensure that new object loads are added to secondary index
L3 users might be generating new objectIds that are not in the DR Object table. If that is the case, ad-hoc L3 analysis would be triggering updates to secondary index.  It is possible that L3 users will be bringing data from other surveys and might partition it DR-way and cross match.  We need some mechanism to mark the secondary index dirty when something new gets added to the Object table, and trigger refresh.",10
"Provide a shared stack on lsst-dev & other relevant systems
Following the discussion in RFC-156, ensure that a documented, fast, easy to initialize shared stack is available for developers to use on shared systems, certainly to include {{lsst-dev}}.",3
"Create unit test for ip_isr fallbackfilter
DM-5287 introduced a configuration option that allows specifying a fallback filter in the event that getting a specific butler product fails. Currently there is no test for this functionality. One should be created which tests all the logical paths. This may involve just adapting or mimicking another test that already exists.",2
"Move tests/negative.py from meas_algorithms to meas_base
Porting code from HSC to LSST brought over a unit test into meas_algorithms for functionality that exists in meas_base in LSST. This is due to the refactoring of code into meas_base on the LSST some while ago. This unit test currently runs with code from meas_algorithms, which means it can not simply be moved, as meas_base comes before meas_algorithms in the build order. This work may involve rewriting the unit test to use different code, or evaluating if it is worth bringing that functionality to meas_base along with the test. The code in question is the detection task.",2
"Data Backbone ConOps
Data backbone first edit : 1pt (week 1)  Data backbone second edit : 1 pt (week 2)  Third edition : 1 pt week 3&4",3
"Astropy integration with LSST DM Software
Work covering the investigation of how to integrate Astropy into the LSST DM software stack.",30
"Compare Astropy and LSST functionality
This story will examine the overlap between Astropy and AFW and examine different approaches that could be taken to integrate Astropy into the DM software.",10
"Write Report on Astropy integration proposals
A report is to be written on the Astropy integration plan. This report will be in the form of an SPIE paper.",20
"Convert GWT code to pure JavaScript (X16, part3 visualization)
This Epic is for the remaining effort in Extra 2016 cycle related to Firefly visualization coed conversion from GWT to pure JavaScript. ",100
"Add scipy as a stack dependency
Adding scipy as a stack dependency is still a nebulous term to me.  David is going to follow up on how to do this exactly (it's already in conda_packages.txt).",2
"Write technical note describing galaxy shear fitting experiments
Through S15 (DM-1108) and W16 (DM-3561), [~pgee] has conducted a large-scale investigation into galaxy shear fitting. Please summarize the motivation, methodology and results of this study as a [technical note|http://sqr-000.lsst.io/en/master/].",8
"Familiarization with ngmix codebase
Download the ngmix codebase from https://github.com/esheldon/ngmix. Install it and its dependencies in the same environment as the LSST stack. Experiment with using it and understanding how it works",3
"Convert GWT code to pure JavaScript (F16)
The remaining work for converting GWT code to pure JavaScript",100
"Visualization algorithm related research (S17)
We have some algorithm related issues that need some research time. ",40
"inter team discussion (X16)
This epic is reserved for inter team discussion and supply/collect input to/from other teams.",6
"create support in Butler for multiple repositories
We need to be able to find repositories based on criteria such as version, validity date, etc.  This story is to provide support & proof of concept that demonstrates this.",12
"Implement experimental DCR correction
After the discussion about DCR, a few avenues for dealing with DCR were enumerated.  It was found that using imaging to model the DCR could be a very fruitful approach.  Nate Lust has suggested an algorithm that performs well in simplified, one-dimensional systems.    This epic is to extend this algorithm to 2-dimensions and add realistic SEDs, bandpasses, etc.  The result will be an implementation of the algorithm applied to the simulated data.  With measurements of how well it corrects for DCR in the context of image differencing.",54
"Adapt LTD Mason for Single-package doc builds on Travis CI
LTD Mason was originally intended to build docs for DM’s Eups-based packages from our Jenkins CI/CD servers. There is tremendous value in consolidating all of DM’s Sphinx-based documentation deployments to use LSST the Docs rather than Read the Docs. This ticket will design and implement adaptations to LTD Mason to build single repo doc projects (Technotes, Design Documents, the Developer Guide, and even generic software projects) from a Travis CI environment. Also includes a template {{.travis.yml}} and associated documentation to allow others to enable travis builds for their documentation.    We name Travis specifically because it is the easiest platform for implementing CI for generic open source projects.",4
"Update SQR-006 LSST the Docs technote to reflect deployment in DM-5404
This ticket will ensure that [SQR-006|http://sqr-006.lsst.io] reflects the LSST the Docs continuous delivery platform as it is deployed in the DM-5404 epic. (SQR-006 was initially written as a planning/design document).    This story should be closed only once the DM-5404 epic is ready to be closed.",4
"Add non-linearity correction to ISR task
Implement RFC-164    At the moment some preliminary code is on ticket branches, but this need to be redone once the RFC is finished.",6
"Don't restore the mask in CharacterizeImageTask.characterize
CharacterizeImageTask.characterize presently restores the mask from a deep copy for each iteration of the loop to compute PSF. This is unnecessary because repair and detection both clear the relevant mask planes before setting new values.",1
"Finish getting obs_decam ISR working with CBP data
Success criteria:    * Flats should be totally flat, i.e. bias jump problem fixed everywhere, amplifier levels fixed (both of these are currently hit & miss at the moment).  * CRs should be properly interpolated over for non-sky images (as this means no PSF estimate).   * Use un-binned images to confirm that bad pixel masks are correct everywhere.  ",10
"S17 Qserv Refactoring
Refactoring of Qserv as found necessary in F16. Specific tasks will be added during F16, and will include bug fixes, fixing major deficiencies discovered during F16, and keeping Qserv code up-to-date (latest compilers, supported OSes, security and alike). The scope of the work is limited by the number of story points assigned to this epic. ",23
"Develop C++ code for experimenting with Python binding
Produce a small C++ codebase that can be used for experimenting with the various technologies we can be used for exposing C++ to Python. It should enable us to experiment with as many of the potential pain points with these technologies as possible",3
"Wrap example C++ code with Cython
Take the example C++ codebase developed in DM-5470, and expose it to Python in the most idiomatic possible way using Cython. Produce a [technical note|http://sqr-000.lsst.io] describing how this was carried out and discussing any particular pain points either in implementation or results.",10
"Update meas_mosaic for compatibility with new single frame processing
Following [recent changes to single frame processing|https://community.lsst.org/t/backward-incompatible-changes-to-processccdtask-and-subtasks/581], {{icSrc}} no longer includes celestial coordinates and {{icMatch}} is no longer being written. {{meas_mosaic}} requires this information. Provide a work-around.",3
"Jenkins/ci_hsc failure: 'base_PixelFlags_flag_clipped' already present in schema
Since 15 March, the {{ci_hsc}} build in Jenkins has been failing as follows:    {code}  [2016-03-16T14:23:13.548928Z] Traceback (most recent call last):  [2016-03-16T14:23:13.548956Z]   File ""/home/build0/lsstsw/stack/Linux64/pipe_tasks/2016_01.0-23-gcf99090/bin/measureCoaddSources.py"", line 3, in <module>  [2016-03-16T14:23:13.548969Z]     MeasureMergedCoaddSourcesTask.parseAndRun()  [2016-03-16T14:23:13.548999Z]   File ""/home/build0/lsstsw/stack/Linux64/pipe_base/2016_01.0-6-g7751869/python/lsst/pipe/base/cmdLineTask.py"", line 450, in parseAndRun  [2016-03-16T14:23:13.549011Z]     resultList = taskRunner.run(parsedCmd)  [2016-03-16T14:23:13.549040Z]   File ""/home/build0/lsstsw/stack/Linux64/pipe_base/2016_01.0-6-g7751869/python/lsst/pipe/base/cmdLineTask.py"", line 192, in run  [2016-03-16T14:23:13.549048Z]     if self.precall(parsedCmd):  [2016-03-16T14:23:13.549076Z]   File ""/home/build0/lsstsw/stack/Linux64/pipe_base/2016_01.0-6-g7751869/python/lsst/pipe/base/cmdLineTask.py"", line 279, in precall  [2016-03-16T14:23:13.549087Z]     task = self.makeTask(parsedCmd=parsedCmd)  [2016-03-16T14:23:13.549115Z]   File ""/home/build0/lsstsw/stack/Linux64/pipe_base/2016_01.0-6-g7751869/python/lsst/pipe/base/cmdLineTask.py"", line 369, in makeTask  [2016-03-16T14:23:13.549132Z]     return self.TaskClass(config=self.config, log=self.log, butler=butler)  [2016-03-16T14:23:13.549160Z]   File ""/home/build0/lsstsw/stack/Linux64/pipe_tasks/2016_01.0-23-gcf99090/python/lsst/pipe/tasks/multiBand.py"", line 1008, in __init__  [2016-03-16T14:23:13.549179Z]     self.makeSubtask(""measurement"", schema=self.schema, algMetadata=self.algMetadata)  [2016-03-16T14:23:13.549206Z]   File ""/home/build0/lsstsw/stack/Linux64/pipe_base/2016_01.0-6-g7751869/python/lsst/pipe/base/task.py"", line 226, in makeSubtask  [2016-03-16T14:23:13.549846Z]     subtask = configurableField.apply(name=name, parentTask=self, **keyArgs)  [2016-03-16T14:23:13.549901Z]   File ""/home/build0/lsstsw/stack/Linux64/pex_config/2016_01.0+1/python/lsst/pex/config/configurableField.py"", line 77, in apply  [2016-03-16T14:23:13.549915Z]     return self.target(*args, config=self.value, **kw)  [2016-03-16T14:23:13.549943Z]   File ""/home/build0/lsstsw/stack/Linux64/meas_base/2016_01.0-12-gf26bc28+1/python/lsst/meas/base/sfm.py"", line 248, in __init__  [2016-03-16T14:23:13.549954Z]     self.initializePlugins(schema=self.schema)  [2016-03-16T14:23:13.549985Z]   File ""/home/build0/lsstsw/stack/Linux64/meas_base/2016_01.0-12-gf26bc28+1/python/lsst/meas/base/baseMeasurement.py"", line 298, in initializePlugins  [2016-03-16T14:23:13.550004Z]     self.plugins[name] = PluginClass(config, name, metadata=self.algMetadata, **kwds)  [2016-03-16T14:23:13.550032Z]   File ""/home/build0/lsstsw/stack/Linux64/meas_base/2016_01.0-12-gf26bc28+1/python/lsst/meas/base/wrappers.py"", line 15, in __init__  [2016-03-16T14:23:13.550616Z]     self.cpp = self.factory(config, name, schema, metadata)  [2016-03-16T14:23:13.550647Z]   File ""/home/build0/lsstsw/stack/Linux64/meas_base/2016_01.0-12-gf26bc28+1/python/lsst/meas/base/wrappers.py"", line 223, in factory  [2016-03-16T14:23:13.550660Z]     return AlgClass(config.makeControl(), name, schema)  [2016-03-16T14:23:13.550688Z]   File ""/home/build0/lsstsw/stack/Linux64/meas_base/2016_01.0-12-gf26bc28+1/python/lsst/meas/base/baseLib.py"", line 3401, in __init__  [2016-03-16T14:23:13.552891Z]     this = _baseLib.new_PixelFlagsAlgorithm(*args)  [2016-03-16T14:23:13.552924Z] lsst.pex.exceptions.wrappers.InvalidParameterError:   [2016-03-16T14:23:13.552967Z]   File ""src/table/Schema.cc"", line 563, in lsst::afw::table::Key<lsst::afw::table::Flag> lsst::afw::table::detail::SchemaImpl::addField(const lsst::afw::table::Field<lsst::afw::table::Flag>&, bool)  [2016-03-16T14:23:13.552986Z]     Field with name 'base_PixelFlags_flag_clipped' already present in schema. {0}  [2016-03-16T14:23:13.553012Z] lsst::pex::exceptions::InvalidParameterError: 'Field with name 'base_PixelFlags_flag_clipped' already present in schema.'  [2016-03-16T14:23:13.553014Z]   [2016-03-16T14:23:13.613484Z] scons: *** [.scons/measure] Error 1  [2016-03-16T14:23:13.617577Z] scons: building terminated because of errors.  {code}    Please fix it.",1
"Bugs in obs_subaru found by PyFlakes
I ran pyflakes on the code in obs_subaru and found a few bugs (beyond a few trivial ones that I am fixing as part of DM-5462)    {{ingest.py}} has undefined name {{day0}}    {{ccdTesting.py}} has at least three undefined variables: {{x}}, {{y}} and {{vig}} in the following:  {code}      ngood += pupilImage[y[good], x[good]].sum()    vig[i] = float(ngood)  {code}    {{crosstalkYagi.py}} has many undefined names, starting with {{makeList}}, {{estimateCoeffs}}",1
"Document investigation of logging, monitoring and metrics technologies and architecture
Finish technote SQR-007. Related to DM-4970",4
"Revise FlagHandler
 {{FlagHandler}} is ""unpolished ... and a bit dangerous to the unwary"" (DM-5247).  It could be improved by leveraging C++11 features, replacing the default constructor with something that defines the (required) general failure flag, and allowing flags to be added individually.    A potential starting point is [here|https://jira.lsstcorp.org/browse/DM-5247?focusedCommentId=45894&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-45894].",4
"Firefly support for Camera team visualization needs (X16)
Attend the weekly meeting with the camera team and UIUC development team, provide support in discussion and API usage. ",4
"Write script to derive and collate QA metrics from data repository of processed data
I wrote a python script using stack components to derive QA metrics and collate other QA-relevant information for a data repository of processed data.  This is currently output to a CSV file that can be loaded into a SQL database.",20
"Wrote script to print the names of all visits that overlap a patch
In order to finish the IDL workflow module for makeCoaddTempExp I needed a program to say which visits overlap a given path.  That's what this script does.",5
"Processing of COSMOS data - Part II
Continued work on processing and QA work on the COSMOS verification dataset.  Running processCcDecam, making diagnostic plots, and nvestigating the results.  Most recently I've  reprocessed the COSMOS data through processCcdDecam using SDSS as the astrometric and photometric reference catalog and am redoing the QA work on those results.",20
"Write software to match up and combine data for sources in a processed data repository
In order to fully check the outputs of the processed COSMOS data I needed to combine the information on sources from multiple visits.  This code (written in IDL for now) matching up sources astrometrically across different visits, combines all of the information on separate detections, and measures average quantities (phot and astrom) for unique sources.  The information is then output into four binary FITS files.",10
"Write presentation on verification datasets for AAS
Prepared and gave a talk at the NSF booth at the Florida AAS meeting on the progress of the verification datasets effort.",5
"Work on script to test the astrometric matcher
We encouraged astrometric matching problems for the Bulge verification dataset.  Therefore, I wrote a script that tests the matcher by systematically shifting the coordinates of one sets of the data to see if the matcher still works.  It worked well until ~80 arcsec.",5
"SdssMapper.paf has wrong python type for processCcd_config
[~npease] reports that {{Sdssmapper.paf}} has the wrong python data type for the dataset {{processCcd_config}}: it is {{lsst.obs.sdss.processCcdSdss.ProcessCcdSdssConfig}} instead of {{lsst.pipe.tasks.processCcd.ProcessCcdConfig}}",1
"Work on plan to test specific algorithmic components of the stack
After working on a script to test the astrometric matcher, I decided to put together a plan to run similar tests on our algorithmic code.  The rough plan is here:  https://confluence.lsstcorp.org/display/SQRE/Stack+Testing+Plan",2
"Work on putting together page of ""tips and tricks for using the stack""
Due to the incomplete state of the stack documentation and tutorials, I decided to write down various ""tips and tricks"" for using the stack as I learn them.  https://confluence.lsstcorp.org/display/SQRE/Tips+and+Tricks+for+using+the+Stack",2
"Revise operations concept for Observation Processing System
Turn the L1 ConOps document into appropriate sections of LDM-230, specifying automated operations sequences, how human intervention can occur, and processes to handle changes and updates.     (Story points are for KTL drafting and initial contributions)",2
"Field group updates
After some work we have realized that the following needs to be done to field groups:    * Tabs group should have a field group smart wrapper component  * field group needs to reinit on id change   * remove mixin, use Higher-Order Components instead  * support a function for a value, this function will return a value or a promise  * hidden fields - init field group with key/value object  * Sub-field groups? study only, unless it is easy to implement.  * maintain an option to keep unmount field value available  * determine if InitValue needs to be passed around  * passing fieldState around too much  * find reason for react warning every time popup is raised  * look at promise code make sure it is working the way we think  * if practical, remove all export default    FieldGroupConnector.  It is the high order component that replaces the mixin.   FieldGroupUtils.js:  (~line 33): The field value would be a function on the file upload case. Therefore the upload does not activate until validation. In the upload case the function would return a promise. However, It could return a value or an object with a value and a valid status. Now the value key of a field can contain a promise or function or primitive. The function can return a primitive, a promise, or an object with primitive and status.    fftools.js lines 102-158 you can see my experimenting with taking out the connector. It works fine and does eliminate one of the warning messages.    ",8
"improvement of the north/east arrow on image
make the compass sticky when scroll the image",1
"Develop operations concept for Batch Processing System
Develop a ConOps document that can be included as appropriate sections of LDM-230 describing the batch processing environment, specifying automated operations sequences, how human intervention can occur, and processes to handle changes and updates.    (Story points are for KTL drafting and initial contributions)",3
"Develop operations concept for Data Backbone
Develop a ConOps document that can be included as appropriate sections of LDM-230 describing the Data Backbone that contains, manages, and provides access to the Science Data Archive, specifying automated operations sequences, how human intervention can occur, and processes to handle changes and updates.    (Story points are for KTL drafting and initial contributions)",3
"Develop operations concept for Data Access Processing System
Develop a ConOps document that can be included as appropriate sections of LDM-230 describing the Data Access Processing System that manages L3 computing in and interfaces to the Data Access Center, specifying automated operations sequences, how human intervention can occur, and processes to handle changes and updates.    (Story points are for KTL drafting and initial contributions)",3
"Develop functional breakdown for Observation Processing System
Write sections that can be incorporated into LDM-148 describing the functional breakdown of the Observation Processing System, including, for each major element:  * overall function  * inputs, outputs, and control interfaces  * components used  * descriptions of functions to be performed    (Story points are for KTL drafting and initial contributions)",3
"Develop functional breakdown for Batch Processing System
Write sections that can be incorporated into LDM-148 describing the functional breakdown of the Batch Processing System, including, for each major element:  * overall function  * inputs, outputs, and control interfaces  * components used  * descriptions of functions to be performed    (Story points are for KTL drafting and initial contributions)",2
"Develop functional breakdown for Data Backbone
Write sections that can be incorporated into LDM-148 describing the functional breakdown of the Data Backbone, including, for each major element:  * overall function  * inputs, outputs, and control interfaces  * components used  * descriptions of functions to be performed    (Story points are for KTL drafting and initial contributions)",2
"Develop functional breakdown for Data Access Center Processing System
Write sections that can be incorporated into LDM-148 describing the functional breakdown of the Data Access Center Processing System, including, for each major element:  * overall function  * inputs, outputs, and control interfaces  * components used  * descriptions of functions to be performed    (Story points are for KTL drafting and initial contributions)",2
"Develop DPS-WG documents
Create documents needed to accomplish the goals of the DPS-WG.",24
"Coordinate completion of operations concepts
Coordinate the creation of a new version of LDM-230 incorporating DPS-WG-generated operations concepts.",2
"Coordinate completion of functional breakdowns
Coordinate the creation of a new version of LDM-148 incorporating DPS-WG-generated functional breakdowns.",2
"Solve the metadata sanitization problem
Applications need access to visit specific metadata: e.g. pointing, airmass, exposure length.  This information is typically carried around in a FITS header, but there are no conventions on spelling or even necessarily units of these metadata key, value pairs.  There needs to be a easy to use metadata sanitization process that allows data from many different systems to present a standardized interface to observation metadata to the algorithm code.",100
"Collect usage of header metadata
Collect a comprehensive set of exposure oriented metadata used by science code.  This should also include metadata that is not currently needed but that could be utilized in the future.  In practice, I suspect this will involve looking for all calls to PropertySet.get since that is how FITS header metadata is currently passed around.",5
"Implement single interface to sanitized exposure metadata
Currently metadata associated with exposures is accessed in a few different ways: through the Calib object, through the Wcs object, and through the metadata object.    In conjunction with DM-5502, which is to figure out what metadata is needed, this will provide a single interface to exposure oriented metadata.  One tricky thing is that the Calib object has some subset of the metadata we'll need in a sanitized form, but we won't want to have to remember where to look for metadata.  If we can't extend the Calib object to hold all sanitized metadata, we should create a new metadata object to store all sanitized metadata and remove the pieces from Calib that are currently there.",10
"Verification via precursor datasets
This epic covers covers timeboxed investigative activities into processing of precursor datasets with the LSST stack. ",26
"DAX & DB Docs (ABH)
* Add memman documentation (in LDM-135)  * Refresh XRDSSI documentation (in LDM-135)",6
"Design Discussions (AndyS, March)
nan",3
"QA Tasks & Supertasks
This epic covers stack-side work for the squash MVP (DM-5555)     (JS:8, MWV:14)",22
"alert production database next steps (April)
Place-holder for additional alert production database work after investigate design task completes.  We should split this into smaller stories for a total of 18 points this cycle.",7
"QA Tasks & Supertasks II
At this time this is epic is a bucket to keep track of backlog for validate_drp etc. ",22
"Design discussions (Brian, March)
nan",1
"Design Discussions (John, March)
nan",3
"Validate shared scan implementation on IN2P3 cluster
nan",9
"prepare Slack RFC
    https://jira.lsstcorp.org/browse/RFC-140",1
"Design Discussions (Fritz, March)
nan",3
"Design Discussions (Nate, March)
nan",3
"Create proposal & RFC for Butler API to define output dataset type
this story represents a spike to  1. ad-hoc gathering of requirements to create butler API that allows a task to define an output dataset type 2. do any proof-of-concept mock up needed 3. write an RFC & gather input  then  A. If there is major dissent, create another design spike story or  B. Close the RFC, and green light work on DM-4180",11
"SQuaRE documentation & design documents
This epic involves planning, design and usage documentation on SQuaRE products and services.      (JMP:4,FE:15,JS:6)",25
"SQuaSH design proposal document
  Document capturing situation as of beginning of X16 can be found at:    https://dmtn-016.lsst.io    Further extension is planned in F16 to cover X16 development as well as consequences of the LDM-151 rewrite.     ",10
"Python wrappers for sphgeom
This issue is a pre-req of DM-3472",15
"Weekly and monthly releases
  Some manual process at the rate of 1 SP / month is still involved in the releases until the automating publishing process is complete. ",6
"Add documentation to BinnedWcs
DM-5282 ported functionality from HSC to work in ""super-pixels"" which are the result of binning in wcs. This functionality was introduced in binnedWcs.(cc/h). This functionality needs proper doxygen documentation added.",1
"Documentation of Firefly functions and API (F16)
We are concentrating on the coding in X16. This epic will be capture the effort to write the document for using Firefly functions and API. ",40
"Change star selectors to return stars instead of PSF candidates
Implement RFC-154:  - Make star selectors tasks, but continue to use and prefer a registry  - Add an abstract base class for star selectors with the following methods:    - {{selectStars}} abstract method that takes a catalog of sources and returns a {{lsst.pipe.base.Struct}} containing a catalog of stars    - {{run}} concrete method that takes a catalog of sources and an optional name of a flag field, calls {{selectStars}} to select stars, then sets the flag field (if given) for stars    - {{makePsfCandidates}} make a list of psf candidates from a catalog of stars (does no selection, other than skipping stars that cannot be made into candidates, and logging the rejects)  ",4
"Add HTM indexing to sphgeom
To include Python wrappers, in support of DM-5052",10
"Design Discussions (Fritz, April)
nan",6
"Design Discussions (Fritz, May)
nan",6
"DAX & DB Docs (Fritz, April)
nan",8
"DAX & DB Docs (Fritz, May)
nan",8
"Finish data distribution prototype (April)
nan",6
"Finish data distribution prototype (May)
nan",6
"Design Discussions (John, April)
nan",6
"Design Discussions (John, May)
nan",6
"AFW rgb.py has undefined variable that breaks a test in some situations
The {{rgb.py}} test is failing for me with current AFW master:  {code}  tests/rgb.py  .E............  ======================================================================  ERROR: testMakeRGBResize (__main__.RgbTestCase)  Test the function that does it all, including rescaling  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""tests/rgb.py"", line 313, in testMakeRGBResize      with Tempfile(fileName, remove=True):  NameError: global name 'Tempfile' is not defined    ----------------------------------------------------------------------  Ran 16 tests in 7.296s    FAILED (errors=1)  {code}    {{Tempfile}} is definitely only used in line 313. It was introduced with commit c9864f49.    I'm not entirely sure how this is not picked up by Jenkins as the test will run if matplotlib and scipy are installed and Jenkins does have those.    ",1
"Design Discussions (AndyS, April)
nan",6
"Design Discussions (AndyS, May)
nan",6
"Alert production database next steps (May)
nan",7
"Design Discussions (Brian, April)
nan",2
"Design Discussions (Brian, May)
nan",2
"persistence improvements to butler config system
requirements:  - easy to know what to provide  - fails fast  - has a way to be backward compatible with persisted configs & existing scripts    existing issues to fix:  - currently config creation is verbose, difficult to read, and difficult to format properly  - has the butler class hierarchy baked into the format",10
"Design Discussions (Nate, April)
nan",6
"Design Discussions (Nate, May)
nan",6
"Add renderer option to js table
TablePanel and BasicTable now accept optional renderers.  For each column, you can set a custom renderer for the header, cell, or both.  Also, created several commonly used renderer for images, links, and input field.",2
"Z-scale stretch for image display
The z-scale stretch in current system is different from the one in OPS",8
"Assist in document investigation of logging, monitoring and metrics technologies and architecture
Assist with tech note SQR-007 and document investigation of logging, monitoring and metrics technologies and architecture.",2
"SQuaSH MVP
This is an epic that covers setting up a minimally viable QA environment for executing processing, calculating metrics, storing them and displaying them. This serves a dual purpose:    - It results in a limited but still useful production service that developers can take advantage of  - It allows us to assess our initial technology stack for suitability for further development.     The test case was picked to be a supertask version of the tests described in DM-4730 (informally ci_lauren) used during the merging of the HSC fork. This was meant to also allow us to use and give feedback on the supertask architecture. When it became obvious that we would not take delivery of that infrastructure in time for X16 work, we switched the test case to one of the KPMs calculated in validate_drp. This switch does not affect the engineering aims of this prototype. The MVP based on validate_drp can eventually be extended to service other types of KPM measurement for regression testing and release characterisation.        (JH:32, JMP:16, JS:32, MVW:14, AF:20)      Outcome: MVP stood up on squash.lsst.codes. Currently collecting AM1, AM2 and PA1 KPMs using validate_drp on validation_data_cfht. After a short period of evaluation of the performance of the toolchains in production we will proceed with more data, more metrics and more features for F16.     ",100
"SQuaRE ad-hoc developer requests
This is a bucket epic for requests from developers/ science users that come up mid-cycle and cannot wait until the new cycle, security vulnerabilities, critical bug fixes, etc.     (JH:16,JMP:8,FE:3)",27
"Present Supertask design to DMLT
Present the Supertask design to the November 2015 DMLT in-person meeting.    Covers preparation of a presentation and related discussions preceding and immediately following the meeting.",6
"Participate in October 2015 OCS-subsystems teleconference
Prepare for, attend, and follow up on the OCS-subsystems teleconference on October 8, 2015.",2
"Write tech note on modifications required to use py.test framework
Following the investigatory work into switching our Python test files to be compliant with pytest, whilst still using {{unittest}}, a tech note needs to be written explaining the required changes.",10
"Participate in November 2015 OCS-subsystems teleconference (LSE-70, LSE-209)
Prepare for, attend, and follow up on the OCS-subsystems teleconference on November 11, 2015.  This story covers work related to LSE-70 and LSE-209; LSE-74 work was also done under a separate epic.",4
"Participate in November 2015 OCS-subsystems teleconference (LSE-74)
Prepare for, attend, and follow up on the OCS-subsystems teleconference on November 11, 2015.  This story covers work related to LSE-74; LSE-70 and LSE-209 work was also done under a separate epic.",1
"Participate in December 2015 OCS-subsystems teleconference (LSE-70, LSE-209)
Prepare for, attend, and follow up on the OCS-subsystems teleconference on December 9, 2015. This story covers work related to LSE-70 and LSE-209; LSE-74 work was also done under a separate epic.",2
"Participate in December 2015 OCS-subsystems teleconference (LSE-74)
Prepare for, attend, and follow up on the OCS-subsystems teleconference on December 9, 2015. This story covers work related to LSE-74; LSE-70 and LSE-209 work was also done under a separate epic.",1
"Review of LSE-70 and LSE-209 drafts, September 2015
Arrange, prepare for, and attend a joint call with the Camera team to review the end-of-summer-2015 drafts of LSE-70 and LSE-209 from the OCS group.",3
"CCB review of LCR-567 (LSE-70) and LCR-568 (LSE-209)
Review the LSE-70 and LSE-209 drafts submitted with change requests LCR-567 and LCR-568 in January 2016.",2
"CCB review of LCR-603 (LSE-74)
Review LCR-603, ""LSE-74 document revision""",2
"LSE-70, LSE-209 refinements X16
There are open LCRs for cleanups to the versions of LSE-70 and LSE-209 approved by the CCB in February 2016.  An initial teleconference will be held on 30 March 2016 with the OCS group to discuss these.",4
"making PSF candidates should be simpler
The code to make PSF candidates is too complicated and repeated in too many places (even after DM-5532). Every time lsst.meas.algorithms.makePsfCandidate is called (except in a few tests) it is called as follows:  {code}              cand = measAlg.makePsfCandidate(source, mi)              if cand.getWidth() == 0:                  cand.setBorderWidth(borderWidth)                  cand.setWidth(kernelSize + 2*borderWidth)                  cand.setHeight(kernelSize + 2*borderWidth)                im = cand.getMaskedImage().getImage()              max = afwMath.makeStatistics(im, afwMath.MAX).getValue()              if not numpy.isfinite(max):                  continue  {code}    This should to be centralized somewhere. I suggest adding this code to {{meas.algorithms.makePsfCandidate}} itself (which could delegate some work to a private function, if desired).",2
"Add Error and Working feedback to FITS visualizer
* Add working message when plot is loading, (downloading..., plotting..., etc)  * Add error message when plot fails  * for multi-viewer remove and failed plot cells  * work out if image select panel should become visible again.  * Any thing else the is plotting feedback related",6
"Docgen draft from EA content for LSE-140
Create a docgen from the LSE-140 content in Enterprise Architect.",2
"SQuaRE Communication and Publication Platforms Document and Presentation
[SQR-011|http://sqr-011.lsst.io] documents the various communication and publishing platforms that SQuaRE operates on behalf of DM. This ticket will complete v1 of the document (DM-4721 created a time-boxed first draft) and also include work to present the document to LSST management.",4
"Support LCR-385
Support getting LCR-385 against LSE-78 through the CCB.",3
"Create a reusable upload file component
This  component will upload and validate the file as part of the input's validation process.  It will return a token generated by the server which will resolve to the uploaded file if the upload success.   ",4
"SQuaRE Communication and Publication Platforms Document and Presentation - Clone
This is a clone of DM-5581 tracking [~frossie]'s SPs",5
"Fix obs_decam butler level
There is a bug in {{obs_decam/policy/DecamMapper.paf}}, causing some butler features for the ""visit"" level or above working incorrectly. The {{hdu}} key is irrelevant for the visit level or above, but wasn't included in the policy file.    Because of this bug, the {{DemoTask}} in {{ctrl_pool}} (ctrlPoolDemo.py) runs incorrectly with DECam data. It incorrectly treats dataRef with different {{hdu}}s as they are from different visits, hence reads each ccd image multiple times (61 times for one visit with 61 hdu). Instead, each ccd image should be read once.        Besides fixing the policy file, I also added an optional test that only runs if {{testdata_decam}} is set up. The part with level=""visit"" in the test fails without the ticket changes in the policy.    (p.s. The raw data file in {{testdata_decam}} is modified and has only 2 hdus.) ",3
"Add lmfit package to the stack
The current implementation of the new {{DipoleFitTask}} for {{ip_diffim}} uses {{lmfit}} to perform parameter estimation (least-squares minimization). {{lmfit}} is essentially an API on top of {{scipy}}'s optimizer, adding functionality such as parameter boxing (constraints) and improved estimates of parameter uncertainties. It would be nice to include this small, pure-python package in the stack rather than investigating and re-implementing the optimization using {{scipy}} or {{minuit2}} (which are the two optimizers that I know of that are in the stack already).",4
"Fix afw build issues with recent clang
{{afw}} fails to build with recent versions of clang:    {code}  include/lsst/afw/image/MaskedImage.h:553:65: error: '_loc' is a protected member of 'lsst::afw::image::MaskedImage<unsigned short, unsigned short,        float>::MaskedImageLocatorBase<boost::gil::memory_based_2d_locator<boost::gil::memory_based_step_iterator<boost::gil::pixel<unsigned short,        boost::gil::layout<boost::mpl::vector1<boost::gil::gray_color_t>, boost::mpl::range_c<int, 0, 1> > > *> >,        boost::gil::memory_based_2d_locator<boost::gil::memory_based_step_iterator<boost::gil::pixel<unsigned short,        boost::gil::layout<boost::mpl::vector1<boost::gil::gray_color_t>, boost::mpl::range_c<int, 0, 1> > > *> >,        boost::gil::memory_based_2d_locator<boost::gil::memory_based_step_iterator<boost::gil::pixel<float,        boost::gil::layout<boost::mpl::vector1<boost::gil::gray_color_t>, boost::mpl::range_c<int, 0, 1> > > *> >, Reference>'                                       const_VarianceLocator(iter._loc.template get<2>())  {code}  and issues with statistics.i so far, more errors may turn up as these are cleared.    These problems are apparent with {{Apple LLVM version 7.3.0 (clang-703.0.29)}} (as shipped with the latest release of XCode, hence this now becoming an issue) and {{clang version 3.8.0 (branches/release_38 262722)}} (a recent release from LLVM; note that Apple uses its own versioning scheme). {{clang version 3.7.1 (tags/RELEASE_371/final)}} is not affected.",1
"Archive in a box v1 (F16)
Several times we were asked a question about Firefly: Great software. How could I use it for my data now?     We want to create a recipe and stubs of code (archive in a box) so others can take it, with minimal configuration changes and minimal customized data access code, to have a simple archive UI for their data. It will come with all the built-in images and catalogs access, all the image, catalog, and XY plot functions. ",40
"fix issue where butler repository search returns list for single item
Backwards compatible behavior is that when butler returns a single item, it is NOT in a list. A recent change (when the Repository class was added) broke this behavior.     Change it back so that if an operation in repository would return a list with a  single item, it pulls it from the list.    Note this is only related to the case where a repository's parentJoin field is set to 'outer' and since no one is using this yet (they should not be, anyway) then the point is moot.     ",1
"Fix qserv service timeout issue
After Qserv services have been running over ~couple of days, new queries fail and can also lead to a crash. Investigate and implement a solution.",5
"daf_persistence build failure on OSX
I see the following build failure in {{daf_persistence}} on OSX 10.11:  {code}  c++ -o python/lsst/daf/persistence/_persistenceLib.so -bundle -F/ -undefined suppress -flat_namespace -headerpad_max_install_names python/lsst/daf/persistence/persistenceLib_wrap.os -Llib -L/private/tmp/ssd/swinbank/shared_stack/DarwinX86/mariadbclient/10.1.11-2-gd04d8b7/lib -L/private/tmp/ssd/swinbank/shared_stack/DarwinX86/pex_policy/2016_01.0+4/lib -L/private/tmp/ssd/swinbank/shared_stack/DarwinX86/pex_logging/2016_01.0+4/lib -L/private/tmp/ssd/swinbank/shared_stack/DarwinX86/daf_base/2016_01.0+4/lib -L/private/tmp/ssd/swinbank/shared_stack/DarwinX86/utils/2016_01.0+4/lib -L/private/tmp/ssd/swinbank/shared_stack/DarwinX86/pex_exceptions/2016_01.0+3/lib -L/private/tmp/ssd/swinbank/shared_stack/DarwinX86/base/2016_01.0+3/lib -L/private/tmp/ssd/swinbank/shared_stack/DarwinX86/boost/1.59.lsst5/lib -L/tmp/ssd/swinbank/shared_stack/DarwinX86/miniconda2/3.19.0.lsst4/lib/python2.7/config -ldaf_persistence -lboost_serialization -lmysqlclient_r -lpex_policy -lpex_logging -lboost_filesystem -lboost_system -ldaf_base -lutils -lpex_exceptions -lbase -lboost_regex -lpthread -ldl -lpython2.7  ld: file not found: libz.1.dylib for architecture x86_64  clang: error: linker command failed with exit code 1 (use -v to see invocation)  scons: *** [python/lsst/daf/persistence/_persistenceLib.so] Error 1  scons: building terminated because of errors.  {code}    This happens with the current master ({{3484020}} at time of writing), but also with a recent weekly ({{3878625}}). ",1
"Remove obsolete install scripts from ~/src/qserv/admin/tools/
Internet-free install scripts are unused and should be removed with related documentation.",1
"runQueries.py fails on IN2P3 cluster
Launching runQueries.py produces some errors:  {code:bash}  fjammes@ccosvms0070:~/src/qserv/admin/tools/docker/deployment/in2p3 (tickets/DM-5402 *=)$ ./run-test-queries.sh  +--------------------+--------------------+  | ra                 | decl               |  +--------------------+--------------------+  | 29.308806347275485 | -86.30884046118973 |  +--------------------+--------------------+    real    1m20.725s  user    0m0.004s  sys     0m0.012s  Output directory: /afs/in2p3.fr/home/f/fjammes/runQueries_out  Exception in thread Thread-12:  Traceback (most recent call last):    File ""/usr/lib64/python2.7/threading.py"", line 811, in __bootstrap_inner      self.run()    File ""/usr/lib64/python2.7/threading.py"", line 764, in run      self.__target(*self.__args, **self.__kwargs)    File ""/afs/in2p3.fr/home/f/fjammes/src/qserv/admin/tools/docker/deployment/in2p3/runQueries.py"", line 185, in runQueries      db='LSST')    File ""/qserv/stack/Linux64/mysqlpython/1.2.3.lsst1/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/__init__.py"", line 81, in Connect      return Connection(*args, **kwargs)    File ""/qserv/stack/Linux64/mysqlpython/1.2.3.lsst1/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/connections.py"", line 187, in __init__      super(Connection, self).__init__(*args, **kwargs2)  OperationalError: (2013, ""Lost connection to MySQL server at 'reading authorization packet', system error: 0"")    Exception in thread Thread-23:  Traceback (most recent call last):    File ""/usr/lib64/python2.7/threading.py"", line 811, in __bootstrap_inner      self.run()    File ""/usr/lib64/python2.7/threading.py"", line 764, in run      self.__target(*self.__args, **self.__kwargs)    File ""/afs/in2p3.fr/home/f/fjammes/src/qserv/admin/tools/docker/deployment/in2p3/runQueries.py"", line 185, in runQueries      db='LSST')    File ""/qserv/stack/Linux64/mysqlpython/1.2.3.lsst1/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/__init__.py"", line 81, in Connect      return Connection(*args, **kwargs)    File ""/qserv/stack/Linux64/mysqlpython/1.2.3.lsst1/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/connections.py"", line 187, in __init__      super(Connection, self).__init__(*args, **kwargs2)  OperationalError: (1105, '(proxy) all backends are down')    Exception in thread Thread-16:  Traceback (most recent call last):    File ""/usr/lib64/python2.7/threading.py"", line 811, in __bootstrap_inner      self.run()    File ""/usr/lib64/python2.7/threading.py"", line 764, in run      self.__target(*self.__args, **self.__kwargs)    File ""/afs/in2p3.fr/home/f/fjammes/src/qserv/admin/tools/docker/deployment/in2p3/runQueries.py"", line 185, in runQueries      db='LSST')    File ""/qserv/stack/Linux64/mysqlpython/1.2.3.lsst1/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/__init__.py"", line 81, in Connect      return Connection(*args, **kwargs)    File ""/qserv/stack/Linux64/mysqlpython/1.2.3.lsst1/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/connections.py"", line 187, in __init__      super(Connection, self).__init__(*args, **kwargs2)  OperationalError: (2013, ""Lost connection to MySQL server at 'reading authorization packet', system error: 0"")    Exception in thread Thread-21:  Traceback (most recent call last):    File ""/usr/lib64/python2.7/threading.py"", line 811, in __bootstrap_inner      self.run()    File ""/usr/lib64/python2.7/threading.py"", line 764, in run      self.__target(*self.__args, **self.__kwargs)    File ""/afs/in2p3.fr/home/f/fjammes/src/qserv/admin/tools/docker/deployment/in2p3/runQueries.py"", line 185, in runQueries      db='LSST')    File ""/qserv/stack/Linux64/mysqlpython/1.2.3.lsst1/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/__init__.py"", line 81, in Connect      return Connection(*args, **kwargs)    File ""/qserv/stack/Linux64/mysqlpython/1.2.3.lsst1/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/connections.py"", line 187, in __init__      super(Connection, self).__init__(*args, **kwargs2)  OperationalError: (1105, '(proxy) all backends are down')    Exception in thread Thread-17:  Traceback (most recent call last):    File ""/usr/lib64/python2.7/threading.py"", line 811, in __bootstrap_inner      self.run()    File ""/usr/lib64/python2.7/threading.py"", line 764, in run      self.__target(*self.__args, **self.__kwargs)    File ""/afs/in2p3.fr/home/f/fjammes/src/qserv/admin/tools/docker/deployment/in2p3/runQueries.py"", line 185, in runQueries      db='LSST')    File ""/qserv/stack/Linux64/mysqlpython/1.2.3.lsst1/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/__init__.py"", line 81, in Connect      return Connection(*args, **kwargs)    File ""/qserv/stack/Linux64/mysqlpython/1.2.3.lsst1/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/connections.py"", line 187, in __init__      super(Connection, self).__init__(*args, **kwargs2)  OperationalError: (2013, ""Lost connection to MySQL server at 'reading authorization packet', system error: 0"")    Exception in thread Thread-19:  Traceback (most recent call last):    File ""/usr/lib64/python2.7/threading.py"", line 811, in __bootstrap_inner      self.run()    File ""/usr/lib64/python2.7/threading.py"", line 764, in run      self.__target(*self.__args, **self.__kwargs)    File ""/afs/in2p3.fr/home/f/fjammes/src/qserv/admin/tools/docker/deployment/in2p3/runQueries.py"", line 185, in runQueries      db='LSST')    File ""/qserv/stack/Linux64/mysqlpython/1.2.3.lsst1/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/__init__.py"", line 81, in Connect      return Connection(*args, **kwargs)    File ""/qserv/stack/Linux64/mysqlpython/1.2.3.lsst1/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/connections.py"", line 187, in __init__      super(Connection, self).__init__(*args, **kwargs2)  OperationalError: (2013, ""Lost connection to MySQL server at 'reading authorization packet', system error: 0"")    Exception in thread Thread-18:  Traceback (most recent call last):    File ""/usr/lib64/python2.7/threading.py"", line 811, in __bootstrap_inner      self.run()    File ""/usr/lib64/python2.7/threading.py"", line 764, in run      self.__target(*self.__args, **self.__kwargs)    File ""/afs/in2p3.fr/home/f/fjammes/src/qserv/admin/tools/docker/deployment/in2p3/runQueries.py"", line 185, in runQueries      db='LSST')    File ""/qserv/stack/Linux64/mysqlpython/1.2.3.lsst1/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/__init__.py"", line 81, in Connect      return Connection(*args, **kwargs)    File ""/qserv/stack/Linux64/mysqlpython/1.2.3.lsst1/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/connections.py"", line 187, in __init__      super(Connection, self).__init__(*args, **kwargs2)  OperationalError: (2013, ""Lost connection to MySQL server at 'reading authorization packet', system error: 0"")  {code}",4
"check & correct comparison operators in daf_persistence and daf_butlerUtils
per comments in DM-5593, an incorrect comparison operator was found, that used {{is}} instead of {{==}} in a string comparison (e.g. {{var is 'left'}} which is incorrect, it should be {{var == 'left'}}.  This needs to be corrected in {{Repository}} (see DM-5593 for details), and the rest of daf_persistence and daf_butlerUtils should be checked for correct use of is vs. ==.",1
"plan and RFC for ""data repository based on version""
create way using repo of repos to get only the posix root in the returned cfg, update example/test code.  write an RFC, review with KT  post RFC & gather feedback.  incorporate feedback and/or create another design story if needed.",12
"Investigate clang issues regarding friendship and protected members 
In DM-5590, we worked around a problem in which clang 3.8 refused to access protected members of a cousin class given a friend declaration in the base. To our best understanding at time of writing, the code is valid: it seems possible that this is a bug in clang.    Investigate what went wrong, produce a minimal test case, and (if appropriate) report this as an upstream bug.",3
"x16 Operations Planning in LOPT, TOWG, and DM replanning (ConOps development)
Develop ConOps for DM system, including Bulk Batch System, Data Backbone, L3 Hosting, etc. Develop use cases for TOWG. Continued planning for operations, focusing on Data Processing and Products directorate.    Don Petravick, Margaret Gelman, Hsin-Fang Chiang, Stephen Pietrowicz, Jaggi Yedetore, Paul Wefel  ",54
"x16 Joint Coordination Council
Coordination with CC-IN2P3.    Don Petravick, Jason Alt  ",7
"Design specification and requirements analysis of Bulk Batch System
Functional breakdown of Bulk Batch System, including L2 processing, calibration processing, etc. Detailed design, plan, and schedule.    Don Petravick, Margaret Gelman, Jason Alt, Hsin-Fang Chiang, Stephen Pietrowicz, Rob Kooper, Paul Wefel",68
"Design specification and requirements analysis of Data Backbone
Functional breakdown of Data Backbone. Detailed design, plan, and schedule.    Jason Alt, Don Petravick, Margaret Gelman, Paul Wefel",23
"x16 middlware/workflow package definition and development
Defining future middleware package to support science pipeline processing. Maintaining and adding to current middleware packages. Prototyping processing sequences with DECam data.    ",100
"x16 LSST Identity and Access Management Program development
nan",8
"Further requirements analysis of the L1 System
Additional design specification of parts of the L1 system that we haven't looked at in detail yet, such as EFD replication, Observatory Operations Server, Auxiliary Telescope processing, telemetry processing, and Commissioning support.    Margaret Gelman, Stephen Pietrowicz, James Parsons, Paul Wefel  ",20
"WAN emulation testing, project 1
Project 1 of WAN emulation. See https://confluence.lsstcorp.org/display/JP/WAN+Emulator+test+plan+-+January+2016 for details.    James Parsons, Paul Wefel  ",21
"L1 Basic Message Topology (x16)
Includes system status and message dictionary, main programs for all L1 entities, and message interaction between L1 entities.",95
"Network and Service Monitoring (Comfort Console)
This is the development and integration work required for architecting and building a Network monitoring center.",36
"Procure FY16 capabilities
Quotes, discussions with vendors, details specs of procurements.   ",42
"Deploy FY16 Cluster Services
nan",28
"Migrate to distributed filesystem
nan",20
"Deploy FY16 Verification Cluster
nan",36
"Deploy FY16 Object Store Discovery Infrastructure
Deploy infrastructure for FY16 object store evaluation.    Deliverable: Object Store discovery infrastructure  Staff: 3 NCSA ICI engineers (networking, storage, systems)  Effort: 20 days  Planned Start: 6/1/2016  Planned End: 6/30/2016",40
"x16 ISO Work
nan",20
"test run coaddDriver and multiBandDriver with DECam data
Preparation work to learn about ctrl_pool and pipe_drivers packages.      - Install ctrl_pool and pipe_drivers packages on my OSX desktop (no slurm).   - Run the ctrl_pool mpiexec example to verify if the mpi is working.  - Obtain a HSC data repo from ci_hsc as sanity checks.   - Construct a small DECam data repo, using raw Stripe82 data consisting of two visits, one band, one patch.  - Run ctrlPoolDemo.py with the HSC data repo and then the DECam data repo.  - Run the pipe_drivers scripts {{coaddDriver}} and {{multiBandDriver}} with the HSC data repo and then the DECam data repo.  - All with the default batch system SMP to run on a single machine.",12
"Add data products and config in obs_decam for multi-band processing
Add necessary data products and default config in order to run forcedPhotCcd, coaddDriverTask, and multiBandDriverTask with DECam data. ",3
"Python version checking in newinstall.sh is not quite right
There is a recent report on community where {{newinstall.sh}} reports that the python version is too old despite the user having a modern Anaconda python in their path.  In commit e6fc9ed2 the code was changed to check {{$PYTHON}} for version compatibility but that is not correct as the python that will be used for the actual build is the python in their path. {{$PYTHON}} is defined purely as the python to use for EUPS installation.    In the reported error {{$PYTHON}} was not set and their {{/usr/bin/python}} was too old. Confusingly the error message reporting the version problem actually reported the version information for the python in the path and not the {{$PYTHON}} python. The simple fix is to revert e6fc9ed2.    I already made significant comments on this topic in the original https://github.com/lsst/lsst/pull/19 but I really do have to insist on either reverting that PR or at least fixing the error messages to use a consistent python (I'd argue that this is still wrong but at least consistent). The current situation is at best confusing and at worst pointless and wrong.    The version test only makes sense if we are testing that the default python in the path is the correct version to build the stack. {{$PYTHON}} was originally designed to allow a different python to be used to build EUPS. Even that is no longer an issue as EUPS can work with Python >= 2.6 now.",1
"Make file upload show feedback when file is uploading
nan",2
"Build a tool to automatically run autopep8 on LSST Stack
Develop a lsst-autopep8 command in [sqre-codekit|https://github.com/lsst-sqre/sqre-codekit] that can run [autopep8|https://github.com/hhatto/autopep8] in an automated fashion across all of the LSST Stack repositories according to the PEP 8 exceptions determined in RFC-162.",1
"finish up afw.table to astropy.table view support
At an LSST/AstroPy summit hack session, we've put together a functional system for viewing afw.table objects as astropy.table objects on branch u/jbosch/astropy-tables of afw and https://github.com/astropy/astropy/pull/4740.    Before merging, we should add support for ""object"" columns for subclasses to hold e.g. Footprints in SourceCatalog, and add some documentation.  We may also want to add a convenience method to return an astropy.table.Table directly.",1
"use AstroPy-compliant strings for units in afw.table
With DM-5641, we'll soon be able to get astropy.table views into afw.table objects.  That will be a bit more useful if astropy can understand the unit strings we give it, and since we currently don't use those strings as anything more than textual information for humans, we might as well standardize on the terms they've already selected.",4
"add method to convert Property[Set,List] to nested dict
In interfacing with AstroPy it'd be useful to easily convert PropertySet and PropertyList to nested dict and OrderedDict (respectively), converting elements with multiple values to lists in the process.",2
"Add fine-grained authorization to ltd-keeper users
The initial MVP of ltd-keeper had all-or-nothing authentication; any user was effectively an admin user. It would be useful have fine grained roles that each API user could have (for example, one API user might be able to add a build, but not create an edition or product or add another user). The phases of this ticket at:    1. Design a set of roles that cover current functionality  2. Add these roles to the User DB model and user creation API  3. Authorize users against these roles in specific API calls",2
"Research & Design for object storage & transport factorization
Start research and design proposals & prototyping on the butler back end factorization problem; need to be able to configure butler to put and get different object types, different storage formats, and different storage locations. Do intermediate KT reviews.  Initial design and stubbed implementation to include the following: *ExposureF* to/from *Fits* file on *local filesystem* (Posix) *ExposureF* to/from *Memory* *SourceCatalog* to/from *database* *SourceCatalog* to/from *Fits* on *local filesystem* (Posix) *SourceCatalog* to/from *Memory*",9
"Finish stubs and write role description for butler back end factorization
nan",8
"visit AP team and work on processing DECam data
March 13-17, 2016. Work on various topics about processing DECam data:  - Improve documentations on processing raw DECam data, especially about the steps of ingesting calibration data  - Identify future work on improving processing raw data. Updates about Instrumental Signature Removal tasks.  - Learn how to run difference imaging pipeline with DECam data  - Try jointcal (Simultaneous Astrometry meas_simastrom package from IN2P3) with DECam data and identity necessary code changes for doing jointcal with DECam data  - Use the preliminary jointcal astrometry results to examine DECam data’s distortion  - Also more general discussions on data processing",8
"SUIT vision document
Writing down SUIT vision that the group has discussed and shaped in last year off and on.   SUIT will use it as guidance for system design.",4
"run jenkins for PRs on all EUPS products - part I
nan",1
"Implement RFC-167
Implement RFC-167 for adding esutil to the stack.  This will be done in the same way as proposed to add scipy.",2
"Reduce code duplication in StarSelectors
Both {{ObjectSizeStarSelector}} and {{SecondMomentStarSelector}} have logic to transform measured moments from pixel coordinates to TAN_PIXELS in order to remove optical distortion.  That's generically useful for any star selector that works on measured moments, and we shouldn't have to repeat it everywhere it is used.",2
"Improve Large Test Scale query script
This script is currently located in:   admin/tools/docker/deployment/in2p3/runQueries.py     Here's some improvments:    - use lsst/db instead of mysqlpython  - externalize queries and other parameters in a config file  - add an option to make script stop after a few queries (in order to have deterministic query results for Large Scale integration tests)  - any other minor improvments...",8
"Update tables of packages that depend on scipy
Now that the {{scipy}} package has been added (DM-5446), the table files of other packages to be fixed as soon as possible so that we have an idea of what is silently depending on {{scipy}}. These include {{afw}}, {{ip_diffim}}, {{meas_modelfit}}, {{mops_daymops}}, {{pipe_tasks}}, {{shapelet}} and {{sims_photUtils}}. Many of these are setupOptional that we should consider making mandatory. Some will be setupRequired.",2
"multiple dialog are not working well together
When several dialogs are up together.  The most recently click one should be one top. When table are in the dialogs such a fits header view. The scroll bars will go over other dialogs. This needs some though and work.  Another thing- when a message dialog is show because of a dialog error. It should center on the dialog.  Update- I don't think I will do the error centering now.  I am going to leave that and see if it is a real problem.",3
"Add motivated model fits to validate_drp  photometric and astrometric scatter/repeatability analysis and plots
Implement well-motivated theoretical fits to the astrometric and photometric performance measurements based on derivations from LSST Overview paper.  http://arxiv.org/pdf/0805.2366v4.pdf    Photometric errors described by  Eq. 5  sigma_rand^2 = (0.039 - gamma) * x + gamma * x^2  [mag^2]  where x = 10^(0.4*(m-m_5))    Eq. 4  sigma_1^2 = sigma_sys^2 + sigma_rand^2    Astrometric Errors   error = C * theta / SNR    Based on helpful comments from [~zivezic]    {quote}  I think eq. 5 from the overview paper (with gamma = 0.039 and m5 = 24.35; the former I assumed and the latter I got from the value of your  analytic fit that gives err=0.2 mag) would be a much better fit than the adopted function for mag < 21 (and it is derived from first principles).  Actually, if you fit for the systematic term (eq. 4) and gamma and m5, it would be a nice check whether there is any “weird” behavior in  analyzed data (and you get the limiting depth, m5, even if you don’t go all the way to the faint end).     Similarly, for the astrometric random errors, we’d expect        error = C * theta / SNR,    where theta is the seeing (or a fit parameter), SNR is the photometric SNR (i.e. 1/err in mag), and C ~ 1 (empirically, and 0.6 for the idealized maximum likelihood solution and gaussian seeing).   {quote}",5
"Config override fixes needed due to new star selector
As of DM-5532 a few config files need updating to not refer to star selector config fields as registries (not ones run by our normal CI, which is how I missed this).",2
"Organize HSC docs ""hackathon""
Liase with SQuaRE to determine the most effective way to transfer HSC docs to LSST. Organize a hackathon session for DRP developers at which we get this done. Bring doughnuts.",1
"Take part in HSC docs hackathon
Participate in HSC docs transfer hackathon.",2
"Take part in HSC docs hackathon
Participate in HSC docs transfer hackathon.  ",2
"Take part in HSC docs hackathon
Participate in HSC docs transfer hackathon.",2
"Take part in HSC docs hacakthon
Participate in HSC docs transfer hackathon.",2
"Take part in HSC docs hackathon
Participate in HSC docs transfer hackathon.",2
"Take part in HSC docs hackathon
Participate in HSC docs transfer hackathon.  ",2
"Take part in HSC docs hackathon
Participate in HSC docs transfer hackathon.  ",2
"Take part in HSC docs hackathon
Participate in HSC docs transfer hackathon.",2
"Prepare detailed L2 plan
At the scipi-wg meeting of 23 & 24 March 2017, [~jbosch] presented an overview of his plans for L2 processing. The next step is to refine those plans and prepare a more detailed ""deep-dive"" discussion of the L2 plans.",10
"Cannot enable shapeHSM because RegistryField fails validation
When running ci_hsc after setting-up the meas_extensions_shapeHSM, meas_extensions_photometryKron and dependencies using setup -v -r . in the respective cloned folders, I get  {code}  Cannot enable shapeHSM (RegistryField 'calibrate.detectAndMeasure.measurement.plugins' failed validation: Unknown key 'ext_shapeHSM_HsmMoments' in Registry/ConfigChoiceField  For more information read the Field definition at:    File ""/home/vish/code/lsst/lsstsw/stack/Linux64/pex_config/2016_01.0+3/python/lsst/pex/config/registry.py"", line 179, in __init__      ConfigChoiceField.__init__(self, doc, types, default, optional, multi)  And the Config definition at:    File ""/home/vish/code/lsst/lsstsw/stack/Linux64/meas_base/2016_01.0-13-g779ee14/python/lsst/meas/base/sfm.py"", line 109, in <module>      class SingleFrameMeasurementConfig(BaseMeasurementConfig):  ): disabling HSM shape measurements  {code}  Find out why this is happening and find a fix  ",1
"Wrap example C++ code with pybind11
Same as DM-5471 but using pybind11",10
"Wrap example code with cffi
As per DM-5471, but using cffi.",10
"Provide single-visit processing capability as required by HSC
In DM-3368, we provided a means of running multiple processCcd tasks across an exposure, but without performing global calibration etc as provided by HSC's ProcessExposureTask.    Please augment this with whatever additional capability is required to enable HSC data release processing.",2
"processCcd.py is failing on some CFHT u band images
processCcd.py is failing on some u band CFHT data, as reported by [~boutigny] on c.l.o: https://community.lsst.org/t/testing-dm-4692-the-new-processccdtask/507/24    See that posting for sample data to reproduce the problem.",8
"Accommodate pixel padding when unpersisting reference catalog matches
The reference object loader in {{meas_algorithm}}'s *loadReferenceObjects.py* grows the bbox by the config parameter pixelMargin:  doc = ""Padding to add to 4 all edges of the bounding box (pixels)"" . This is set to 50 by default but is not reflected by the radius parameter set in the metadata, so some matches may reside outside the circle searched within this radius. This increase needs to be reflected in the radius set in the metadata fed into {{joinMatchListWithCatalog()}}.  ",2
"Table performance on Firefly
Table seems to perform poorly on Firefox. Firefox gets into the complete refresh state only when the table is visible with charts only, fits view only or fits view and chart it does not happen    Helpful article: http://benchling.engineering/performance-engineering-with-react/    changelog:  - added react performance tools, React.addons.Perf  - fix some performance issues:    - skip render of selection boxes when not needed.    - skip rendering of xyplot options when not needed.    - skip wasted render called for table cell and headers.    - will do a more in depth investigation in another ticket.  - refactor table code and it's state.    - move all table related states into table_space.    - create sub reducers for each data domain    - rename and move functions to better describe what it's doing   - added 'title' to table.  - show mask while loading    ",6
"Table needs to fire another action when data completely loaded
When the data for a table is completely loaded fire another action such as TABLE_NEW_LOADED_DONE. This way the xyplots and the image overlays know to go fetch the data.    4/22/2026 from the pull request:  added new action TABLE_NEW_LOADED to table; fired when table is completely loaded.  added table error handling.  fix active table not updating after an active tab is removed.",2
"AP Emergent work -- F16
There is emergent work that comes up as a side effect of other work.  This epic will capture that effort in F16.",19
"Connect CatSim to StarFast simulation tool
CatSim can provide a fully realistic simulated catalog, which StarFast could use as an input for simulations. This ticket includes writing the code to connect to the CatSim database and updating the internal catalog format in StarFast to be compatible with CatSim.",4
"Write StarFast interface to ProcessCCD
The simulated images generated by StarFast need to be able to be run through the LSST stack, to test and make use of the existing measurement, fitting, stacking, and image differencing capabilities.   This includes writing or updating a simulations obs package, and determining and supplying the required metadata.",6
"Run StarFast simulated images through diffim
Determine the metadata and dependencies needed to fully process two images simulated with StarFast through diffim. ",2
"Implement simple 1D DCR correction on simulated data
Nate Lust wrote a simple DCR correction recipe that runs in 1D in an ipython notebook. This ticket is to re-write the notebook in python modules that can be run on StarFast simulated images prior to image differencing. For this ticket, the simulated images will be 2D, but DCR will be purely along the x or y pixel grid, allowing columns or rows of pixels to be treated separately in 1D.",8
"Add support for blank image
We need to add blank image support.",2
"Extend simple DCR correction to 2D
In DM-5695 a simple DCR correction was applied to simulated images in the case that the effect was purely along the pixel grid and could be reduced to 1D. This ticket extends that work to the general 2D case.  Possible approaches include resampling the ""science"" image to match the ""template"", or including neighboring pixels and computing their covariance. Ideally, multiple approaches will be implemented and tested.",6
"Add astrometric errors to StarFast
One concern with the proposed DCR correction is that it might fail in the presence of source position errors. This ticket is to add the capability to simulate a variety of types of position errors, such as atmospheric turbulence or an inaccurate WCS, to test the DCR implementation.",4
"Run many sky simulations through DCR correction to find edge cases
Once a complete DCR correction prototype is finished, we will want to run many different sky simulations from StarFast with different densities of sources, noise properties, airmasses, and astrometric errors to find the limitations and edge cases where it fails. There are likely to be several thousand simulations needed which will take an as-yet undefined number of CPU hours, but this ticket is for the work in setting up and analyzing the results from the run.",4
"Put ImageSelectPanel into dropdown
Currently the image select panel is in a dialog.  It also needs to be able to work in a dropdown.",4
"Create toy composite (AST/GWCS) model with supported components
To help us evaluate WCS options, we need to create a relatively complicated composite model in AST and GWCS, using a few models currently available within the existing packages. A minimal composite model to test these things would include:     * FITS linear transform   * ccd distortion   * optical model   * FITS TAN WCS    The middle steps do not need to be realistic models, just something that we can use to compare AST's and GWCS's respective interfaces and capabilities for creating the composite model, and test for differences in their results. We can then use this model to evaluate performance when run on different numbers of pixels.",4
"Create a new model in AST/GWCS to represent a complex distortion
Using lessons learned from DM-5701, create a more complex distortion model that cannot be represented from the basic models in GWCS or AST. A good example for this might be a rapidly varying sinusoidal tree-ring-like function that is not well represented by the standard polynomial basis functions. This will test our ability to extend each framework with new models that have not yet been decided on.    Once completed, we could plug this back into the composite model in DM-5701.",8
"Evaluate performance of AST/GWCS over a range of numbers of pixels
Once we have a composite distortion model from DM-5701, evaluate the performance of AST and GWCS over a range of numbers of pixels, likely from ~100 through full-CCD (4k^2).    As part of this process, we will try to determine whether there is a way to efficiently warp images/postage stamps using python-only models in GWCS and whether bottlenecks could be worked around via optimizations in cython.",8
"add cloudbees-folder support to puppet-jenkins 
nan",6
"Produce document describing DRP parallelization use cases
At various times in the past few months I've promised [~gpdf], [~petravick], [~kooper], and probably a few others a document describing the parallelization needs for DRP in greater detail.  My understanding of the plans for the eventual DRP probably good enough to do this well now, and is unlikely to improve further in the near future (as that will require algorithmic research).    This needs to be prioritized with my other responsibility for documents that describe the DRP system in other ways, most of which are oriented towards scientists and science pipelines developers.  The document on this ticket is essentially the description that would matter the most for the process control middleware team.",6
"UI Consistency
There is a need to go though the entire ui and document inconsistencies with the old UI.",4
"Firefly Result view architecture/component
The result view architecture needs to be written.    * A meeting needs to happen with David, Gregory, Xiuqin, Trey, and Tatiana to discuss this.  * Trey, Loi, and Tatiana should have a design meeting.    It should support some or all of the following ideas:    * A search defining a new results view type  * A search adding to an existing result view  * A search replacing the results - any cleanup needs to happen.  * Some sort of controller that know which view should show and which view can be shown   * New search panels easily added. Maybe the html file defines which are visible",10
"Verification Cluster, Object Store Procurement
Strategy design with pipeline and deployment teams. Discussions of service description and levels of support. Sufficient design to lead to procurement. Discussions with vendors. Quote selection. Budget tracking. Quote submission to finance. GCO follow up questions. OBFS follow up questions. Finance follow up questions. Overall tracking of purchase progression.",12
"JIRA fixes
This tracks SPs spent on JIRA requests. ",2
"Add table client-side sorting
Convert gwt's client-side sorting to javascript.",4
"make sure table can be resized properly
Test table to make sure it can be resized under a variety of layout.",2
"attend the weekly meeting with UIUC camera team
While Tatiana is the assignee of this ticket, Xiuqin and Gregory participate this weekly telecon semi-regularly to lend support. ",2
"attend the weekly meeting with UIUC camera team (May 2016)
Tatiana will attend the weekly meeting. Xiuqin and Gregory also attends when needed. ",2
"Create django project and initial dashboard app
This ticket captures the steps to create the django project for SQUASH, its configuration and the dashboard app http://sqr-009.lsst.io/en/latest/    The planned tasks are:        - Implement the ``Dataset``, ``Visit`` and ``Ccd`` tables in the django ORM layer, as a minimum set      of tables for the dashboard app      - Prototype home page and dashboard pages      ",5
"Config.loadFromStream suppresses NameError
Within a config override file being executed via {{Config.load}} or {{Config.loadFromStream}}, using a variable that hasn't been defined results in a {{NameError}} exception, but this is silently suppressed and the user has no idea the following overrides have not been executed.",1
"Fix the issues in the server side and the client side introduced by FitsHeaderViewer 's work
*  The testing data ""table_data.tbl"" in the testing tree was accidentally moved.  It should be added back so that IpactTableTest.java can run.    * The request in JsontableUtil was mistakenly moved out from the tableModel by the the line   * {code}  * if (request != null && request.getMeta().keySet().size()>1) {              tableModel.put(""request"", toJsonTableRequest(request));  }  {code}.  The meta can be null but the request is not null, the request should be put into the TableModel.     ",1
"Move Camera creation out of CameraMapper base class
With the new cameraGeom, it's considered desirable that each camera be able to define the serialization format for its static camera data.  Despite this, it's still the base CameraMapper that does loads it (at least for most cameras), going through a circuitous chain of policy files, obs_* package paths, and Python code.    It'd be vastly simpler for each mapper to simply build the Camera object and assign it to {{self.camera}} in its own {{\_\_init\_\_}} method (most would simply delegate all the work to {{afw.cameraGeom.makeCameraFromPath}}).  We could then remove all the camera entries from the PAF files and make it much easier to follow the logic.    Eventually, I think we need to be storing at least some components of the camera definition in the data repository (or something like a calibration repository associated with it), and that would require giving the mapper access to a partially-constructed butler when its time to build the camera.  But we can save that for another day.  ",2
"--clobber-config modifies input rerun
Using the {{--clobber-config}} option in a child butler repository can cause changes in the parent repository, as we try to rename files to back them up in the parent repository.    This is a critical bug because it can cause pipeline outputs to be unexpectedly modified.    It should be easy to fix, as it's just a matter of checking whether the files to be renamed backed up are in the output repository.    This was originally reported as https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1341  ",1
"Create and deploy common Ansible roles for ELK
Create roles and deploy to Ansible Galaxy.    These are common roles for cloud-init (and any other future cloud dependencies), java (openjdk-jdk) and an editors role.",3
"Create and deploy Elasticsearch and Kibana Ansible roles
Create roles and deploy to Ansible Galaxy.",3
"Create and deploy an ELK system
Create a Vagrant configuration and Ansible role to configure and combine Elasticsearch, Logstash and Kibana (ELK).",36
"Create and deploy Logstash, Fluentd and Riemann Ansible roles
Create roles and deploy to Ansible Galaxy.    Create a role to combine all the individual projects together.",3
"Create packer automation for ELK
Build packer automation to create machine images to use for the ELK system.",6
"Implement ingestion code for the QA results
The initial database model was implemented in DM-5728 and outputs of the QA analysis code are being produced by the work described in http://dmtn-008.lsst.io/en/latest/    In this ticket we plan to implement and API for listing and creating jobs, metrics and measurements so that a job or the QA analysis code can register this information in the dashboard app.",4
"Build parallel DCR simulator using GalSim
The result of DM-4899 was a simulation tool called StarFast that can quickly make simulated images with realistic Differential Chromatic Refraction. This ticket is to build an equivalent simulator using GalSim to check the accuracy of results and benchmark speed and memory usage. ",6
"SQUASH dashboard prototype design
SQUASH dashboard prototype design is described here    http://sqr-009.lsst.io/en/latest/",8
"Upgrade mpi4py to latest upstream
[mpi4py|https://bitbucket.org/mpi4py/] version 2.0 was released in October 2015 with a number of changes. We should upgrade. When upgrading, we should check whether it contains a proper fix for DM-5409 and, if not, file a bug report upstream.    This issue should not be addressed until we have proper test coverage on code which uses mpi4py (DM-3845).",1
"Integration of Django and bokeh server
nan",5
"XCode 7.3 can not link indirect dependencies that use @rpath
With XCode 7.3 on OS X we have difficulties resolving indirect dependencies when those dependencies are referenced using {{@rpath}}. This can be seen with Qserv:  {code}  Linking shared object build/libqserv_common.dylib  ld: file not found: @rpath/libboost_system.dylib for architecture x86_64  clang: error: linker command failed with exit code 1 (use -v to see invocation)  {code}  where {{libboost_system}} is being loaded via {{libboost_thread}}:  {code}  $ otool -L $BOOST_DIR/lib/libboost_thread.dylib  /Users/timj/work/lsstsw/stack/DarwinX86/boost/1.59.lsst5+fbf04ba888/lib/libboost_thread.dylib:  	@rpath/libboost_thread.dylib (compatibility version 0.0.0, current version 0.0.0)  	@rpath/libboost_system.dylib (compatibility version 0.0.0, current version 0.0.0)  	/usr/lib/libc++.1.dylib (compatibility version 1.0.0, current version 120.1.0)  	/usr/lib/libSystem.B.dylib (compatibility version 1.0.0, current version 1226.10.1)  {code}    This problem is also found when doing a {{conda}} build of the stack because in {{conda}} all shared libraries are modified on creation to reference other libraries via the {{@rpath}} mechanism.    This bug has been reported to Apple as [rdr://25313838|http://www.openradar.me/25313838] and a [Chromium bug report|https://bugs.chromium.org/p/chromium/issues/detail?id=597459] indicates that the fix is to simply ensure that {{-L}} directives include a trailing slash.  ",4
"Update Scons to v2.5.0
Scons 2.5.0 came out over the weekend. There were many fixes to the dependency determination code. The next version of Scons is intended to be 3.0 which will be the first version to support Python 3. Since we fully intend to switch to Python 3.0 in the summer it is prudent for us to ensuer that 2.5.0 works fine before switching to 3.0.0 so that we do not get confused as to why there is breakage in jumping straight to 3.0.0.",2
"FitsHeader's resize and sorting
DM-4494 has merged to the dev.  However, there are still two issues remained:  * Resize the popup with tabs does not work  * Sorting is depending on the BasicTable's sorting",1
"TabPanel needs a way to keep it state between renders
The TabPanel and CollapsiblePanel loses its state when it is re-rendered.  It is going to have to have a way keeps it state. Therefore it needs an option to take an ID and keep it state in the store.      Use case- tabs of tables then the image plot goes to expanded mode.  The table tabs gets reset to the first one.    ",4
"XYPlot needs to be expandable
Make XYPlot expandable",2
"XYPlot should support selecting columns from a table
There are should be a way to display all the information about table columns in a table and allow user to choose a column using this table.",10
"XYPlot: Optimize decimated plot aspect ratio
Currently decimation process assumes aspect ration 1. For decimated plots, the displayed area size (or user supplied value) needs to be used as an aspect ratio to approximate square bins.  - When aspect ratio changes, decimation process needs to be redone.  - To avoid server calls on resize, disallow flexible aspect ratio for decimated data.   ",6
"XYPlot: decimation options
User needs to be able to control number of bins and bin size.",3
"XYPlot: separate density plot from scatter plot
At the moment we display data as scatter plot, when the number of points does not exceed decimation limit, and as density plot when the number of points does exceed this limit.    Scatter plot and density plot should be separate charts. These are the reasons:  - User should be able to create density plot with any number of points  - Chart type and display might be different for density plot in the future  - Scatter plot look should not change when the number of points exceeds decimation limit  - Scatter plot should support errors in the future",5
"Remove unneeded imports in SConstruct
There's an outstanding pull request from an external contributor (Miguel de Val-Borro) [here|https://github.com/lsst/sconsUtils/pull/9] that makes some minor improvements to sconsUtils by cleaning up the imports. Somebody should review and (if appropriate) merge it. (Or, at least, reply to our community!)",1
"Implement spatial exposure selection task
Once DM-3472 lands, it will be possible to write an image selection task that uses the SQLite 3 database produced by {{IndexExposureTask}} (from [daf_ingest|https://github.com/lsst/daf_ingest]) to search for exposures overlapping a region (in particular, the spatial extent of a coadd patch). The {{_rtree_search}} method in {{test_index_exposure.py}} (also from daf_ingest) has an example of how to perform spatial queries quickly.    I was originally scheduled to do something in this space, but Paul mentioned that he had plans to refactor the image selection tasks already, and is much more familiar with the pipeline side of things than I am. Therefore, I'm handing off the implementation of the pipeline task mentioned in DM-3472 to him.",6
"Create custom basic coaddition code
Create script to do the following:  * Takes a list of DECam exposure numbers  * for each CCD, loads the corresponding calexps  * creates a naive pixel-by-pixel coadd of the underlying images  * Possibly either ANDs or ORs the masks (though perhaps not necessary)  * Either sums the expusure time info from the headers, or averages them, depending on whether the images were normalised to exposure times or not  * write the corresponding images out as coadded fits",1
"Coadd CPB exposures
Identify sets of DECam exposures from the CBP run and feed them to the coaddition script created in DM-5767.    This will need to be redone each time a reprocessing is done as the script will run on calexps. I will do it once now, and then again after DM-5465 is completed to satisfactory levels.",1
"Write spot visualisation snippets
Write some snippets to aide in the processing and visualisation of the CBP data/analysis.    Essentially, write some helper functions that you can throw sections of images at to help look at the shape of the CBP spots, as ds9 isn't great ideal this.    Some nice features would be:    A function that takes a list of images or arrays, and plots them side-by-side, which provides some intelligent options for the stretches, and optionally stretches each image as is best for it, or ties them all to be the same. This would be as 2D colour plots.    A function that takes part of an image and displays it as a colour-graded surface.    A function that takes part of an image and displays it as a 3D bar-chart (as in ROOT, but without using ROOT because there is already enough evil in the world)",2
"Investigate image processing for feature enhancement
Whilst looking at an individual spot from the CBP on DECam I noticed a weird feature, and upon further investigation, several more, though these were very hard to see.    This ticket is to investigate what image processing techniques will make these hard-to-see features pop out so that they can be examined more closely.",2
"Update config files
DM-46921 and DM-5348 changed ProcessCcd to the point where past config files are no longer valid as stuff has moved a lot (see https://community.lsst.org/t/backward-incompatible-changes-to-processccdtask-and-subtasks/581)    This ticket is to go through past configs and create a new config file to reproduce the reductions done, or at least make something sensible come out the end of processCcd",2
"Firefly API plan and decision
We need a plan for  all the Firefly APIs development in the new React/Redux based JS framework, including JS API and Python API.    - Backward compatibility  - Syntax format for JS API  - Syntax format for Python API  - Schedule     - convert the existing API first     - list of new ones to be added, when    ",2
"Change the TabPanel.jsx and TabPanel.css's properties to allow its children can be resizable
When an outside container is resizable (using css properties: resize: 'both', overflow: 'auto'...), in order for the child inside the container to be resizable, the child has to specify its height and width properties using percentage format (height: 90%, width:100%).   When the TabPanel is used, the table is put on TabPanel.  The table needs to access the size information of the outside container, ie,, the grandparent's width and height. The TabPanel has to pass the height and width to its child component.  Without specifying the height and width in the TabPanel, by default, the auto is used.  When the width (height) is auto, it allows to use the child's width (height).  However, the child replies on the parent to provide such information.  When this circular relations occur, the default size of the child is used.  That is why the table component forever has 75px when it was put in the TabPanel.  To be able to resize with the outside (root) contains all the ancestors of the component have to specify the width and height explicitly. ",1
"Document Configurable concept
The Configurable concept (a callable that takes a config as an argument) is a fairly important one in pex_config as the guts behind RegistryField and ConfigurableField, and it's mentioned several times in pex_config's documentation, but it doesn't seem to be directly documented itself.",1
"Assist schandra with ts_wep Luigi implementation
Assist [~schandra] with an initial implementation of his workflow using Luigi.",1
"Port Data set info converter, part2
Part 2 includes: * 3 color * clean up on plot fail * clean up image in general * better row highlighting * other types of data layout (a FC type view) * artifacts (maybe part 3) * When image is small, like zoom it down 1/16x, behavior of selecting an image is not consistent. Clicking on an edge will select one, but will not work on another.",10
"Include obs_cfht, obs_decam in lsst-dev shared stack
The shared stack on {{lsst-dev}} provided in DM-5435 does not contain the {{obs_cfht}} or {{obs_decam}} camera packages. Please add them.",1
"Port region serializer and data structures from GWT
The region serializer in: firefly/src/firefly/java/edu/caltech/ipac/util  * RegionFactory.java    Region container data structures files in : firefly/src/firefly/java/edu/caltech/ipac/util/dd    * ContainsOptions.java  * Global.java  * RegionFileElement.java  * RegParseException.java  * Region.java  * RegionAnnulus.java  * RegionBox.java  * RegionBoxAnnulus.java  * RegionCsys.java  * RegionDimension.java  * RegionEllipse.java  * RegionEllipseAnnulus.java  * RegionFont.java  * RegionLines.java  * RegionOptions.java  * RegionPoint.java  * RegionText.java  * RegionValue.java      Note - do not port CoordException, there are other ways to do this.",8
"Add support for SGE
Jean Coupon has requested support for SGE in ctrl_pool.",1
"Why is doSelectUnresolved an argument?
The {{run}} method in the {{PhotoCalTask}} has an argument that selects whether to use the extendedness parameter to select objects for photometric calibration.  This is a good idea, but it should be configurable, I think. ",1
"Support artifacts 
For now this is the artifacts for WISE images.   We should look at the possibilities to generalize this. ",8
"Convert  Mask support
nan",6
"Support image and drawing layer subgrouping
nan",8
"Add Python properties for getters and setters in afw::geom and shapelet
I'm adding properties via Swig %extend in much of afw::geom right now, because:   - I think we've all agreed this is something we want, even if we haven't agreed how much effort we want to put into it.   - I'm getting annoyed writing lots of parentheses for these getters and setters on DM-5197.   - I can get this done in a couple of hours on a weekend, so I don't need a T/CAM to give me permission to spend my own time on it :)  ",1
"Using 'CONSTANT' for background subtraction fails
Running processCcd (on a DECam file) with the following in the config file:    {code}  config.charImage.repair.cosmicray.background.algorithm='AKIMA_SPLINE'  config.charImage.background.algorithm='CONSTANT'  config.charImage.detectAndMeasure.detection.tempLocalBackground.algorithm='CONSTANT'  config.charImage.detectAndMeasure.detection.background.algorithm='CONSTANT'  config.calibrate.detectAndMeasure.detection.tempLocalBackground.algorithm='CONSTANT'  config.calibrate.detectAndMeasure.detection.background.algorithm='CONSTANT'  {code}    fails, and throws the following:    {code}  Traceback (most recent call last):    File ""/home/mfisherlevine/lsst/pipe_tasks/bin/processCcd.py"", line 25, in <module>      ProcessCcdTask.parseAndRun()    File ""/ssd/lsstsw/stack/Linux64/pipe_base/2016_01.0-6-g7751869+8/python/lsst/pipe/base/cmdLineTask.py"", line 450, in parseAndRun      resultList = taskRunner.run(parsedCmd)    File ""/ssd/lsstsw/stack/Linux64/pipe_base/2016_01.0-6-g7751869+8/python/lsst/pipe/base/cmdLineTask.py"", line 199, in run      resultList = mapFunc(self, targetList)    File ""/ssd/lsstsw/stack/Linux64/pipe_base/2016_01.0-6-g7751869+8/python/lsst/pipe/base/cmdLineTask.py"", line 324, in __call__      result = task.run(dataRef, **kwargs)    File ""/ssd/lsstsw/stack/Linux64/pipe_base/2016_01.0-6-g7751869+8/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/home/mfisherlevine/lsst/pipe_tasks/python/lsst/pipe/tasks/processCcd.py"", line 170, in run      doUnpersist = False,    File ""/ssd/lsstsw/stack/Linux64/pipe_base/2016_01.0-6-g7751869+8/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/home/mfisherlevine/lsst/pipe_tasks/python/lsst/pipe/tasks/characterizeImage.py"", line 298, in run      background = background,    File ""/ssd/lsstsw/stack/Linux64/pipe_base/2016_01.0-6-g7751869+8/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/home/mfisherlevine/lsst/pipe_tasks/python/lsst/pipe/tasks/characterizeImage.py"", line 356, in characterize      image -= estBg.getImageF()    File ""/home/mfisherlevine/lsst/afw/python/lsst/afw/math/mathLib.py"", line 5788, in getImageF      return _mathLib.Background_getImageF(self, *args)  lsst.pex.exceptions.wrappers.InvalidParameterError:     File ""src/math/Interpolate.cc"", line 61, in std::pair<std::vector<double>, std::vector<double> > lsst::afw::math::{anonymous}::recenter(const std::vector<double>&, const std::vector<double>&)      You must provide at least 1 point {0}    File ""src/math/BackgroundMI.cc"", line 196, in void lsst::afw::math::BackgroundMI::_setGridColumns(lsst::afw::math::Interpolate::Style, lsst::afw::math::UndersampleStyle, int, const std::vector<int>&) const      setting _gridcolumns {1}  lsst::pex::exceptions::InvalidParameterError: 'You must provide at least 1 point {0}; setting _gridcolumns {1}  {code}",2
"Pass butler to ref loader
The design of the indexed reference catalogs requires a butler to be sent to the loader.  This requires passing the butler down through the chain of subtasks from the parent command line task.  In this case, I believe only calibrateTask constructs sub-tasks that requires a reference catalog.    This will also require moving the loader and indexer to meas_astrom, otherwise it will introduce a circular dependency.",4
"Asinh stretch algorithm corerction
in DM-2634, the Asinh stretch algorithm  was implemented, but the behavior was not quite right. We need to figure out the issue and make it right. One possibility is that the understanding the relationship  of zero point  and black point, maximum point and white point. ",8
"TabPanel:  Tab titles need to shrink to accommodate a large number of tabs.
Should convert GWT's logic over to TabPanel.  - shrink title as needed.  - show full title on mouse over",4
"Cmake in mariadbclient finds wrong libz
When building mariadbclient, cmake identifies libz from a separate python installation than the one setup to run the stack. I have an anaconda installation on the disk, and a miniconda installation set up specifically for the lsst stack. During the building process CMake for some reason finds the alternate libz associated with that python installation.",1
"fetchUrl is not handling post requests correctly.
Parameters are not sent to the server when requests are posted via fetchUrl.",2
"Use aperture-corrected aperture flux in validate_drp
Shift from PsfFlux flux/magnitude to aperture-corrected aperture-based mag/flux measurements for calculating photometric repeatibility.",1
"Improve star/galaxy separation for validate_drp
Improve the star/galaxy separation for validate_drp.    Many of the LSST SRD KPMs are defined for bright, isolated stars.  There is clear evidence that galaxies are being included in current runs (they have significantly higher photometric scatter at the same mag|SNR).  Improved star/galaxy separation will help generate better numbers    Stretch goal:  Include additional informative plots about how well the `extendedness` value in the catalogs is successfully separating stars and galaxies.",1
"Add a paging bar to ImageMetaDataToolbarView
Add a paging bar similar to the one for table to the ImageMetaDataToolbarView.  This pages images instead of rows.",4
"Ensure that variance plane in calexps is unchanged HSC⟷LSST
Per discussion in HSC telecon 2016-04-19.",1
"Update imageDifferenceTask to cast template ids and use ObjectSizeStarSelector
A couple recent changes to the stack break imageDifferenceTask.     Requires updates to only a few lines.     While I'm updating it to reflect the star selector API, I'm also changing the default star selector from SecondMoment to ObjectSizeStarSelector (which I learned today is what the stack has been using by default for a while). ",1
"Incorporate Price suggestions to make `validate_drp` faster
Increase the loading and processing speed of {{validate_drp}} following suggestions by [~price]    1. Don't read in footprints  Pass {{flags=lsst.afw.table.SOURCE_IO_NO_FOOTPRINTS}} to {{butler.get}}    2. Work on speed of calculation of RMS and other expensive quantities.  Current suggestions:  a. {{calcRmsDistances}}  b. {{multiMatch}}  c. {{matchVisitComputeDistance}}  d. Consider boolean indexing  {code}     objById = {record.get(self.objectKey): record for record in self.reference}  to:     objById = dict(zip(self.reference[self.objectKey], self.reference))  {code}    Note that while this ticket will involve work to reduce the memory footprint of the processing, it will not cover work to re-architect things to enable efficient processing beyond the memory on one node.",2
"3 color and FITS header clean up
There are some issues with three color when not using all three bands (i.e. using on green and blue):  * Mouse readout is not labeled correctly  * FITS head popup does not come up    Other FITS header popup issues:  * If file size is too big then the text is wrapping  * On safari, the resizable indicator in on every cell  ",6
"Intermittent fault building ci_hsc through Jenkins
Occasionally (see e.g. [here|https://ci.lsst.codes/job/stack-os-matrix/label=centos-7/10437//console] and [here|https://ci.lsst.codes/job/stack-os-matrix/label=centos-6/9594//console]) the {{ci_hsc}} job in Jenkins fails, reporting:  {code}  RuntimeError: dictionary changed size during iteration  {code}  The fault seems to be intermittent. Please fix it.",3
"Afw fails unit test for convolve depending on compiler optimisation level
On OSX 10.11.4 with Apple LLVM version 7.3.0 (clang-703.0.29) afw fails {{test/convolve.py}} with the following error when either {{-O0}} or {{-O1}} is enabled but works fine for {{-O2}} and {{-O3}}.    {code:bash}  tests/convolve.py    .....FF/Users/pschella/Development/lsst/code/afw/python/lsst/afw/image/testUtils.py:283: RuntimeWarning: invalid value encountered in isnan    nan0 = np.isnan(filledArr0)  /Users/pschella/Development/lsst/lsstsw/miniconda/lib/python2.7/site-packages/numpy/lib/ufunclike.py:113: RuntimeWarning: invalid value encountered in isinf    nx.logical_and(nx.isinf(x), ~nx.signbit(x), y)  /Users/pschella/Development/lsst/lsstsw/miniconda/lib/python2.7/site-packages/numpy/lib/ufunclike.py:176: RuntimeWarning: invalid value encountered in isinf    nx.logical_and(nx.isinf(x), nx.signbit(x), y)  F.F...  ======================================================================  FAIL: testSpatiallyVaryingAnalyticConvolve (__main__.ConvolveTestCase)  Test in-place convolution with a spatially varying AnalyticKernel  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""tests/convolve.py"", line 437, in testSpatiallyVaryingAnalyticConvolve      rtol = rtol)    File ""tests/convolve.py"", line 290, in runStdTest      self.runBasicConvolveEdgeTest(kernel, kernelDescr)    File ""tests/convolve.py"", line 317, in runBasicConvolveEdgeTest      doVariance = True, rtol=0, atol=0, msg=msg)    File ""/Users/pschella/Development/lsst/code/afw/python/lsst/afw/image/testUtils.py"", line 201, in assertMaskedImagesNearlyEqual      testCase.fail(""%s: %s"" % (msg, ""; "".join(errStrList)))  AssertionError: basicConvolve(MaskedImage, kernel=Spatially Varying Gaussian Analytic Kernel using brute force) wrote to edge pixels: image planes differ: maxDiff=1.09176e+38 at position (73, 18); value=-1.09176e+38 vs. 2825.0; NaNs differ    ======================================================================  FAIL: testSpatiallyVaryingDeltaFunctionLinearCombination (__main__.ConvolveTestCase)  Test convolution with a spatially varying LinearCombinationKernel of delta function basis kernels.  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""tests/convolve.py"", line 556, in testSpatiallyVaryingDeltaFunctionLinearCombination      rtol = rtol)    File ""tests/convolve.py"", line 290, in runStdTest      self.runBasicConvolveEdgeTest(kernel, kernelDescr)    File ""tests/convolve.py"", line 317, in runBasicConvolveEdgeTest      doVariance = True, rtol=0, atol=0, msg=msg)    File ""/Users/pschella/Development/lsst/code/afw/python/lsst/afw/image/testUtils.py"", line 201, in assertMaskedImagesNearlyEqual      testCase.fail(""%s: %s"" % (msg, ""; "".join(errStrList)))  AssertionError: basicConvolve(MaskedImage, kernel=Spatially varying LinearCombinationKernel of delta function kernels using brute force) wrote to edge pixels: image planes differ: maxDiff=9.06659e+36 at position (75, 29); value=9.06659e+36 vs. 2865.0    ======================================================================  FAIL: testSpatiallyVaryingGaussianLinerCombination (__main__.ConvolveTestCase)  Test convolution with a spatially varying LinearCombinationKernel of two Gaussian basis kernels.  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""tests/convolve.py"", line 523, in testSpatiallyVaryingGaussianLinerCombination      rtol = rtol)    File ""tests/convolve.py"", line 290, in runStdTest      self.runBasicConvolveEdgeTest(kernel, kernelDescr)    File ""tests/convolve.py"", line 317, in runBasicConvolveEdgeTest      doVariance = True, rtol=0, atol=0, msg=msg)    File ""/Users/pschella/Development/lsst/code/afw/python/lsst/afw/image/testUtils.py"", line 201, in assertMaskedImagesNearlyEqual      testCase.fail(""%s: %s"" % (msg, ""; "".join(errStrList)))  AssertionError: basicConvolve(MaskedImage, kernel=Spatially Varying Gaussian Analytic Kernel with 3 basis kernels convolved using brute force) wrote to edge pixels: image planes differ: maxDiff=1.22472e+38 at position (74, 3); value=-1.22472e+38 vs. 2878.0; NaNs differ    ======================================================================  FAIL: testTicket873 (__main__.ConvolveTestCase)  Demonstrate ticket 873: convolution of a MaskedImage with a spatially varying  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""tests/convolve.py"", line 623, in testTicket873      rtol = rtol)    File ""tests/convolve.py"", line 290, in runStdTest      self.runBasicConvolveEdgeTest(kernel, kernelDescr)    File ""tests/convolve.py"", line 317, in runBasicConvolveEdgeTest      doVariance = True, rtol=0, atol=0, msg=msg)    File ""/Users/pschella/Development/lsst/code/afw/python/lsst/afw/image/testUtils.py"", line 201, in assertMaskedImagesNearlyEqual      testCase.fail(""%s: %s"" % (msg, ""; "".join(errStrList)))  AssertionError: basicConvolve(MaskedImage, kernel=Spatially varying LinearCombinationKernel of basis kernels with low covariance, using brute force) wrote to edge pixels: image planes differ: maxDiff=3.19374e+38 at position (1, 46); value=3.19374e+38 vs. 2774.0    ----------------------------------------------------------------------  Ran 13 tests in 43.252s    FAILED (failures=4)  The following tests failed:  /Users/pschella/Development/lsst/code/afw/tests/.tests/convolve.py.failed  1 tests failed  scons: *** [checkTestStatus] Error 1  scons: building terminated because of errors.  {code}",2
"ECL_B1950 coordinate was not defined correctly
The CoordSys.js defined ECL_B1950 incorrectly.  When I was testing WebGrid, the grid lines for  Ecliptic B1950 were not right.  Looked further, it was caused by wrong equinox value in its definition.",1
"Add CI tests for obs_lsstSim
I propose:    1. Create {{testdata_lsstSim}}.  This will be based on 12 images from the current Twinkles Run 1 (or pre-Run 1):  2 epochs each of 6 filters.  (/)    2. Add an optional test method to {{obs_lsstSim}} that runs if {{testdata_lsstSim}} has been declared.  This is the way these tests are set up for the other {{obs_*}} packages.    3. Add {{testdata_lsstSim}} dependency to {{lsst_ci}} (which already depends on {{obs_lsstSim}}).    This will then be run every time a full standard default Jenkins build is processed.",2
"Compare LSST and HSC pipelines through through multi-band coadd processing
Continue the work described in DM-5301 through the standard ""multi-band"" coadd processing workflow.    Performing an end-to-end comparison of the stacks will not be possible until {{meas_mosaic}} is fully operational on LSST (DM-2674). However, until that point is reached, comparisons are still possible by either:    * Shepherding data through {{meas_mosaic}} and coaddition on HSC, then performing further processing and measurement using the LSST stack;  * Omitting {{meas_mosaic}} from the workflow altogether and performing end-to-end comparisons of the stacks without mosaicking.    Obviously, neither of these will ultimately be adequate, but they should enable early identification of any major issues.",15
"Create outline of Level 3 ConOps
Create an outline of the sections of the Level 3 ConOps document",2
"Level 3 requirements flowdown
Document the flowdown of Level 3-related requirements from SRD, LSR, OSS, and DMSR.",3
"SUIT requirement flowdown
go through the original requirement of SUIT,  put them in the categories:  done, Tier1, Tier2    ",6
"LSE-140 post-CCB implementation
Following CCB approval of LSE-140, perform minor document work required for full implementation (application of standard cover page, change log, etc.).",2
"SUIT design diagramming
Prepare initial set of SysML diagrams of the SUIT's relationship to other system components.",4
"Prepare requirements and design for Fall 2016 SUIT deployments
Prepare functional and quantitative requirements and the SUIT-centric elements of design for the planned Fall 2016 SUIT deployments (SDSS Stripe 82 and WISE).",10
"Prepare a draft of the SUIT deployment timeline
Prepare a draft schedule, with some detail for 2016-2017, for deployments of the SUIT into (test) production, including the datasets that will be served.",2
"access to NCSA Nebular to setup servers for SUIT deployment
Get three hosts in NCSA nebular system to deploy the current Firefly application. The goal is workout the possible issues and identify the software needed to be installed for the hosts. Clarify which team is responsible to install what third-party software packages.",4
"Document pipe_drivers
Please provide a minimal level of documentation for {{pipe_drivers}}, to include:    * A {{doc}} directory with the usual content so that docstrings get generated by Doxygen;  * A package overview;  * All docstrings should be appropriate for parsing by Doxygen (ie, should start with {{""""""!}} where necessary).",2
"horizon console interface broken
It appears that at some point in the last few months the horizon console interface has stopped working.  I am still able to access the console log output via the API/CLI.",1
"instance limit low vs available cores
The LSST project is currently at 81/100 instances but there are over 200 cores unused.  Is it possible to increase the instance limit or are we being encouraged to use large instance flavors?",1
"unable to list nebula lsst project users
Currently, [with some difficulty] it is possible to discover the {{user_id}} that created an instance (might be possible for other resources as well) but it is not possible to map this back to a username / person.  This can make it difficult to 'self police' instances.    The administrative API endpoints are not publicly accessible and I doubt any end user has the appropriate permission. ",1
"automate deployment of qa dashboard server and database instance
Add a qa server + rds instance to the terraform configuration for the jenkins-demo sandbox for development purposes.  It may make sense to split this off to be an independent sandbox but that is very easy to do, if needed.",20
"ci_hsc fails with ""too many open files""
For example, with thanks to [~wmwood-vasey]:    {code}                ci_hsc: master-g78db638f21 .....................................................................................ERROR (207 sec).  *** error building product ci_hsc.  *** exit code = 2  *** log is in /Users/wmwv/lsstsw/build/ci_hsc/_build.log  *** last few lines:  :::::  [2016-04-25T19:25:59.824660Z]     jobs.run(postfunc = jobs_postfunc)  :::::  [2016-04-25T19:25:59.824699Z]   File ""/Users/wmwv/lsstsw/stack/DarwinX86/scons/2.3.5/lib/scons/SCons/Job.py"", line 113:  :::::  [2016-04-25T19:25:59.824709Z]     postfunc()  :::::  [2016-04-25T19:25:59.824752Z]   File ""/Users/wmwv/lsstsw/stack/DarwinX86/scons/2.3.5/lib/scons/SCons/Script/Main.py"", line 1294:  :::::  [2016-04-25T19:25:59.824767Z]     SCons.SConsign.write()  :::::  [2016-04-25T19:25:59.824808Z]   File ""/Users/wmwv/lsstsw/stack/DarwinX86/scons/2.3.5/lib/scons/SCons/SConsign.py"", line 109:  :::::  [2016-04-25T19:25:59.824816Z]     None  :::::  [2016-04-25T19:25:59.824869Z]   File ""/Users/wmwv/lsstsw/stack/DarwinX86/scons/2.3.5/lib/scons/SCons/dblite.py"", line 116:  :::::  [2016-04-25T19:25:59.824878Z]     None  :::::  [2016-04-25T19:25:59.935601Z] Exception IOError: (24, 'Too many open files', '.sconsign.tmp') in <bound method dblite.__del__ of <SCons.dblite.dblite object at 0x10dfe9c50>> ignored  {code}    Possibly only happens on OSX?",2
"libxml build issue with mpich on OS X
On OS X with Xcode installed {{mpich}} fails to build because it can not locate the libxml include files:    {code}  CC       topology-xml-libxml.lo   topology-xml-libxml.c:17:10: fatal error: 'libxml/parser.h' file not found   #include <libxml/parser.h>            ^   1 error generated.  {code}  with {{pkg-config}} 0.29.1 installed. The problem is that {{configure}} determines that {{libxml-2.0}} is available and is installed into {{/usr}} with a CFLAGS of {{-I/usr/include/libxml2}}. {{configure}} does not itself test whether those parameters are reasonable. With Xcode there are no files installed into {{/usr/include}} and {{clang}} knows to look in specific SDK locations. When {{mpich}} builds it assumes that {{libxml2}} can be found but fails to find it.    Strangely, {{pkg-config}} v0.28 does not seem to be able to find {{libxml-2.0}} so there is no issue.    One solution is to install the Command Line Tools but it might be more portable to attempt to disable {{libxml2}}.  ",2
"Investigate Jupyter internals, interactive widgets
In preparation for linking Jupyter notebooks with Firefly and other SUIT components, read Jupyter documentation. Learn how to build a sample widget or interactive dashboard in the Jupyter framework",2
"Investigate Ginga and Glueviz visualization tools
Ginga and Glue (glueviz) are community visualization tools in Python. Become familiar with the capabilities of both, thinking from the point of view of using Firefly for the display but using Python for many other things.",2
"Java array index out of bound error in VisSeverCommand.java
The class FileFluxCmdJson in VisServerCommand.java is calling   {code}              String[] res = VisServerOps.getFileFlux(fahAry, pt);  {code}    However, when the mouse is outside the image, the VisServerOps.getFileFlux(fahAry, pt) returns:  {code}  new String[]{PlotState.NO_CONTEXT}  {code}  It is fine for a single band.  However, for 2 or 3 bands, the for loop below caused the index out of bound error because res is an array of length=1 and the expected res is an array of length=no of bands.  {code}    JSONObject obj= new JSONObject();              obj.put(""JSON"", true);              obj.put(""success"", true);                int cnt=0;              JSONObject data= new JSONObject();              for(Band b : state.getBands()) {                  data.put(b.toString(), res[cnt++]);              }              data.put(""success"", true);  {code}    Thus,  res\[cnt++\] caused array index out of bound error.     To fix this issue, the for loop is changed as below:  {code}                        int cnt=0;              JSONObject data= new JSONObject();              Band[] bands = state.getBands();              for (int i=0; i<res.length; i++){                  data.put(bands[i].toString(), res[i]);              }              data.put(""success"", true);                JSONArray wrapperAry= new JSONArray();              obj.put(""data"", data);              wrapperAry.add(obj);  {code}    When the mouse is outside the image, the res returns a new String\[\]\{PlotState.NO_CONTEXT\}, it is added to the JSONObject only once.  ",1
"Make DipoleFitPlugin mask-safe
The DipoleFitPlugin does not correctly handle bad pixels and other masks/flags. Make it so it does so, and make tests to ensure it does so.",6
"LSST the Docs Production Fall 2016
DM-5404 introduced _LSST the Docs_ as a production platform for continuous documentation delivery. This Epic covers additional improvements to the platform, such as    - Implementation of a backup system for LSST the Docs’ DB    - Edition and build dashboards at the /v/ and /builds/ directories that help users find the appropriate version of the documentation site. These could be rendered with react from the API. This also serves as a ramping up exercise on UI elements that will be used on the SQuaSH dashboard and DocHub, so learning time has been rolled into the estimate",42
"Table: Add keyboard navigation
- Added arrow up/down to move between rows.  - Added page up/down to move between pages.    - Fixed table loading mask not showing  - Fixed PagingBar rendering more than it should  - Fixed annoying StandardView missing unique key warning",2
"Create obs_monocam
Make a package to hold the description of moncam.",10
"Minor tweaks to Cython and pybind11 tech notes
I'll be making some superficial changes to the text of DMTN-13 and DMTN-14 for grammar, while updating links to the python-cpp-challenge repo (which has just moved from my private GitHub to lsst-dm).",1
"Improve Qserv CI using multinode tests
Here's some tracks:    1. Run multinode integration tests during qserv_distrib CI build.  In order to do that we could create a qserv_testmultinodes repository containing a build script which would launch multinode tests (for example see travis.yml)  I can do this on my side but i'll require a recent version of Docker on the build machine.    2. I'll need your help to do next step:  Each time command below succeed:  {code:bash}  rebuild qserv_distrib  {code}  publish this build to eups web repository and docker hub by running:  {code:bash}  # bXXX is the build id and is available at the bottom of rebuild command standard output  publish -b bXXX -t qserv-dev qserv_distrib  # then create and publish to docker hub image ""qserv/qserv:dev""  # which embed current build products and is used for all Qserv deployment  # (bare-metal, openstack, travis, developper machines)  $QSERV_DIR/admin/tools/docker/2_build-dev-image.sh  {code}    As a TODO list, here's what could be done in an additional ticket, in the long term:    - run multinode integration test inside Jenkins instead of Travis?    * Travis/Gitub integration is very good, so I'm not sure this feature is still an active concern?    * on the other hand Travis has to download qserv/qserv:dev at each build, and if there's a timeout here, the build sometime fails. I don't know if this image can be cached in Travis free version?    * current procedure doesn't support yet tickets branch accross multiple repositories, (like qserv+xrootd+qserv_testdata for example). Do you think this feature would be easier to implement in Jenkins?",10
"Test lsst.log with pipeline tasks
Try to use {{lsst.log}} instead of {{lsst.pex.logging}} for a few science pipeline tasks, based on log {{u/ktlim/getLogger}} branch and DM-3532. Look into RFC-29.",10
"Literature research on image subtraction algorithms
We need to get a good understanding of where the image subtraction implementation in the stack currently stands. This first requires an up-to-date assessment of the literature, including Becker et al. (2012), and ZOGY (2016). Also, the ""preconvolution"" step.",8
