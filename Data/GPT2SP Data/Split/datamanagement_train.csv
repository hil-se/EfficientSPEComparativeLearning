Issue,Storypoint
"Transition git repositories to Stash
Transition gitolite-managed repositories to Atlassian Stash.",10
"Finalize DM mission statement
The proposed mission statement for the LSST Software Stack development: {quote} Enabling LSST science by creating a well documented, state-of-the-art, high-performance, scalable, multi-camera, open source, O/IR survey data processing and analysis system {quote} with the following rationale:  * well documented -- as otherwise it will be impossible to maintain or be usable for Level 3 * state-of-the-art -- because the quality of the instrument and needs of the science to be systematic limited require us to develop new algorithms an break new ground in a number of areas * high-performance -- because of the massive amount of data we need to process for a reasonable budget * scalable -- because we need it to run from between a single core (for developers, or Level 3 users) to tens of thousands of cores (for Data Release production) * multi-camera -- because we need to process precursor data for development purposes, and because our as-delivered camera won't be ideal. * open source -- because we want the community and future surveys to benefit from our efforts. ",2
"Open up LSST software mailing lists
We all benefit from making LSST software development as open as possible and conducive to outside volunteer contributions (*). One way to increase community involvement is to open up our development mailing lists to the public, analogous to the way other open source projects do. For example, we could have:  * software-devel@lsstcorp.org: the development mailing list, equivalent to current lsst-data * software-users@lsstcorp.org: the users mailing list, equivalent to current lsst-dm-stack-users mailing list (but it could possibly be replaced by StackOverflow/Confluence Questions) * lsst-dm@lsstcorp.org: internal, DM-staff only mailing list, for the *rare* discussions/notices that should go out to staff only.  (*) Though we don't rely on them for meeting the project specs (legally required disclaimer :) ).",1
"Transition to Confluence Questions
Open the Confluence Questions site, for interaction with the community (and to enable community self-help).  ",10
"Add derivatives-based optimizer to meas_multifit
See https://dev.lsstcorp.org/trac/ticket/3146  Story points estimate is for remaining work only (just the code review, which is still substantial).",10
"Release EUPS 1.3.0
Release EUPS 1.3.0 in RHL's github repository.",1
"Determine final URL/location for W'14 stack
Need to know where W'14 stack files are going to be housed.",1
"Update newinstall.sh to check for existence of git and python on users' machines
nan",1
"Confirm stack builds on OS X 10.8
nan",1
"Build Winter'14 release
Run lsst-build scripts and 'eups distrib create' to build the Winter'14 release.",1
"Update installation instructions
Update Confluence instructions on how to install the Winter'14 stack.  The instructions are at: https://confluence.lsstcorp.org/display/LSWUG/LSST+Software+User+Guide",1
"Modify gitolite permissions to allow issue/DM-NNNN branches
Use issue/DM-NNNN branches for issues tracked in JIRA, to differentiate them from tickets/NNNN branches that are still in Trac.",1
"Qserv configuration - detailed design
Detailed design covering how all Qserv components will be configured for runtime. ",3
"Node configuration and bootstrapping - detailed design
Design covering how all Qserv components will be configured for runtime.  Design how new Qserv nodes will be bootstrapped when we add them to the cluster, and how already added nodes will get updated after they were offline (crashed, turned off for maintenance etc) ",6
"Node bootstrapping - 1st prototype
nan",15
"Zookeeper-based CSS (v1)
nan",40
"switch back to throwing exceptions in css/Facede.cc
nan",1
"Rewrite xrootd-facing code
This task is related to DMTF-16570-09. It involves plugin in the new XRootD client (which has different interfaces, it is all async etc). We expect to also do a major cleanup on the code that is talking to XRootD while plugging in the new XRootD.",30
"Data Distribution Design v1
Need to come up with detailed design covering how we will deal with data distribution: managing multiple replicas, recovering from faults, adding new nodes to the cluster, registering new data from L2 ingest and user data (L3).",8
"Modify format of version numbers
The versions auto-generated by the new EUPS+buildbot look like this:  {code} ========== $ eups list .... pipe_tasks            7.3.2.0_5_g455c355d0f+070b2c1b35  b61 pyfits                3.1.2+9ef17db9b7  b61 b60 python                0.0.1             b61 b60 scisql                0.3+2b5a2f1b52    b61 b60 scons                 2.1.0+2b5a2f1b52  b61 b60 sconsUtils            6.2.0.0_11_gf38997df3e+759c3944a1         b61 sconsUtils            6.2.0.0_19_g755151c0a5+759c3944a1         b60 shapelet              7.3.1.0_1_g9331ee763c+0c72f294dd  b61 shapelet              7.3.1.0_1_g9331ee763c+e56ee84a68  b60 skymap                7.3.1.0_1_g64b750c066+db36490146  b60 skymap                7.3.1.0_1_ga6cd540cd3+493a438aa2  b61 skypix                6.1.0.0_1_g1157bf09ae+65137c93cd  b61 skypix                6.1.0.0_1_gea33592463+6039c04989  b60 .... ========== {code}  where the part before the plus sign is the output of git describe (slightly mangled), and the part after is the SHA1 of the sorted names+sha1s of the dependencies.  While this has the benefit that any two causally disconnected buildbots with the same inputs will build the same versions, many people have complained that they're plain ugly.  So here's an alternative proposal:  * If a tag exist on a commit, use <tag> as the version. * If there's no tag, use branchname-gSHA1ABBREV, where any illegal characters in branchname get turned into dots * If the package has dependencies, and a build of this package with different dependencies already exists, append a +N to the end. Keep the mapping of +N -> (dependency name, sha1s) in a special git repository. Given the source code and this git repo, two causally disconnected buildbots will again generate the same set of versions.  Example versions: * 7.10.2.1 * 7.10.2.1+5 * master-gdeadbeef * feature.dm-1234-gdeadbeef * feature.dm-1234-gdeadbeef+3 ",1
"Save a git-branch when a forced push is detected
Create a gitolite hook that will save a branch when a forced push is detected.  E.g., if we have a ticket: 'tickets/DM-AAAA' and someone rebases it and pushes  with '--force' before applying the update --- then the hook will branch off the old state into (say):  backups/tickets/DM-AAAA/NNNN where NNNN is a monotonically increasing number (per branch).",1
"from __future__ import division breaks division of Extent*
If one does: {{from __future__ import division}} the division operator on Extent types raises an exception.  How to repeat: I've tried this with v7_3 and master: {code:py} from __future__ import division import lsst.afw.geom as afwGeom npt = afwGeom.Extent2I(10,10)/2 {code} an exception is raised.  Removing the first line succeeds as expected.",4
"Measurement - Aperture Corrections
The current implementation of aperture corrections in meas_algorithms' CorrectFluxes class is broken, and should not be replicated in meas_base:  - it only works when the PSF model is correct  - it doesn't work when the aperture to correct to is larger than the PSF model image size  - it doesn't propagate the uncertainty in the aperture correction  We probably need to do this by estimating the aperture correction and its errors on single frames, using the PSF stars (not the PSF models), then attaching that information to the Psf object to be retrieved *and coadded* by CoaddPsf.  JK: In PMCS this would be 10% Bosch J 50% Krughoff S and 40% Owen R Breakdown: jbosch 10%; krughoff 50%; rowen 40%",47
"Publish Winter 2014 binaries
nan",3
"tests/testPsfDetermination.py has a broken test
In meas_algorithms tests/testPsfDetermination.py has a test testRejectBlends which does not operate as expected. When it calls pcaPsfDeterminer it results in no usable psf candidates BEFORE blends are rejected. Formerly this resulted in a numpy array named ""sizes"" containing one uninitialized value, which might raise an unexpected exception or raise the desired exception, depending on whether that value was negative or positive.    On tickets/DM-3117 I pushed a fix for the bug that caused the invalid ""sizes"" array, but the unit test is now reliably broken because no viable psf candidates raises the wrong exception and does not test blend rejection in any case. So on this same ticket I have commented out the bad test for now.",2
"Configure transition screens for DM agile workflow
Whenever an issue is transitioned on JIRA Agile board to 'Ready for Review', a screen should pop up to ask for a reviewer.  Whenever it's moved out of that state, another screen should ask for a new assignee.",1
"Make lsst-build reuse buildIDs if nothing's changed
All built packages are EUPS-tagged with build IDs (the bNNN EUPS tags). Without this change, new EUPS tags are declared even when nothing changed since the previous build (and EUPS' tags code doesn't scale well at this time).  This will be implemented by comparing the newly built manifest against ones stored in versiondb, and reusing the build IDs if a matching one is found.",1
"Write unit tests for lsst-build
Unit tests need to be written for lsst-build; they should've been written together with the code, but due to Winter'14 release fire drill they had to be postponed.",10
"clean up isr utility code
There is some commented code in isr.py.  This should be removed or updated so that it works.",2
"Improve naming of getters in AmpInfoTable
The names of the methods to get values from a record on AmpInfoCatalog are potentially confusing.    This is because the convention is to call the getters get[attributename].  We could change the method names in the AmpInfoCatalog, or add methods in the SWIG wrapper.",1
"Box2I(bbox.getMin(), bbox.getMax()) fails for an empty bbox
Empty Box2I cannot be round tripped: {code:py} from lsst.afw.geom import Box2I b1 = Box2I() b2 = Box2I(b1.getMin(), b1.getMax()) assert b2.isEmpty() {code}  It is confusing and surprising that this round tripping fails.  It is also a trap for the unwary because saving min and max is the logical way to store boxes in afw tables. Records can contain points but not extents and so it saves casting back and forth and simplifies and clarifies the code to save max instead of extent. Thus that is the path most users will take, and the problem can be a time bomb: it could be quite some time before somebody tries to store an empty box and finds that it does not get retrieved correctly.",1
"log4cxx-based logging prototype - v2 
This is continuation of DMTF-16570-16. Initial work (v1) was done in branch u/bchick/protolog. V2 will include comments sent by K-T and issues discussed at the Qserv meeting March 13  + Free functions vs. a log object need to be discussed more.  In particular, when metadata key/value pairs need to be attached, an object might make more sense.  Avoiding the getLogger() call when no logging is needed (due to threshold) can be significant.   + It's a security breach to use vsprintf() with any user-provided arguments.  Use vsnprintf() instead so that you can check for overflow. (Or use stringstream or boost::format.)   + In my prototype, I used a combination of a set of static log4cxx::LevelPtr variables with isEnabledFor(level) and a set of cpp macros to avoid the switch.   + The return value from getLogger() shouldn't need to be cast.  ----   + play with hierarchical names  + don't execute code for formatting if debug level is off  + use shorter threadId  + experiment with defining special python handler, intercept and redirect     to our log4cxx-based logging  ",15
"Replace PositionFunctor with some flavor of XYTransform
afw has a special functor PositionFunctor that acts like an XYTransform. Unless PositionFunctor does not need to be invertible, it makes sense to merge these, likely by replacing PositionFunctor with the transform from afw::image::XYTransformFromWcsPair (as suggested by Jim Bosch on Trac ticket #2214).",1
"Rework JOIN support, including Ref*Match tables
Add support to the Ref*Match tables. The relevant code in Qserv core (supporting joins) has already been written. This task is related to DMTF-1640-20",16
"Develop new master-worker result system 
Reimplement how results are returned from worker to the czar. Currently it relies on mysqldump, which is fairly inefficient. This is related to DMTF-1650-045",20
"Qserv: unit testing (controller module) 
Design and build toy prototype of a test framework for testing controller module. This might require a mock framework, as we want to be able to test things in isolation, without testing everything around the controller module at the same time. This is related to DMTF-16570-20.",10
"Qserv: unit testing (query execution) 
Design and build toy prototype of a test framework for testing query execution module. This might require a mock framework, as we want to be able to test things in isolation, without testing everything around the query execution module at the same time. This is related to DMTF-16570-21.",8
"Prepare for setting up new cluster at IN2P3 for continuous integration/testing
Once the hardware is available, setup the environment where we could easily run integration testing of different Qserv releases, including testing/comparing performance.  Integrate changes implemented in DM-1078  Add install script that exposes individual steps and allows modifications to the config file: newinstall, qserv-configure --prepare, then edit config file (or copy from somewhere), qserv-configure",4
"Migrate Qserv czar code to the new logging system
This includes switching Qserv to the new logging. Fine-tuning (what messages are printed deciding on error level) is covered in DM-685.",10
"catch exceptions from CSS
nan",1
"S15 Data ingest
Rework existing scripts used to load data into plain mysql. This involves mostly simplifying it, and pushing some functionality like converting types outside of the loading scripts.   JK: Refer to loading spreadsheet for PMCS assignments",10
"Revise design for Qserv front-end rearchitect 
Revisit the architecture. This includes proxy, down to XRootD client (mysqld, python, zookeeper). Capture all findings in new stories, add these stories to DM-1707",10
"Migrate away from using env variables in Qserv
Qserv is currently relying on many env variables. We should migrated away from that to the extend possible.",10
"Setup multi-node testbed
It'd be useful to test Qserv using Winter2014 or Summer2014 data set on a multi-node cluster, just to exercise all pieces of the software and double check we are not missing anything.",5
"W15 C++ geometry
Port the geometry related code used by Qserv (it is currently written in python) to C++, and switch Qserv to the C++ based version.  JK: Refer to loading spreadsheet for PMCS assignments",47
"Implement C++ geometry primitives for Qserv
nan",10
"Switch Qserv to C++ geometry primitives
nan",10
"Experiment with no-subchunking based approach
nan",10
"Qserv worker scheduler – code cleanup
The qserv worker scheduler code is a bit ugly.  The actual composable scheduler classes (FifoScheduler, ScanScheduler, GroupScheduler, BlendScheduler) might be pretty clean, but the interactions with the rest (wdb, wcontrol) may be harder to understand.  There should be a small amount of low-hanging fruit of code to clean up, but to make things more sensible and understandable may require some new abstractions and shuffling of logic to new/different classes.  Since this issue was opened, some refactoring work has been done as part of the new xrdssi port, so the organization may be somewhat cleaner now. Still, it's worth it to take a fresh look to evaluate the design/interactions to see how much can/should be reorganized.",6
"Implement new (async) XrootD client
(oops, earlier description was meant for DM-878. Sorry for the confusion. -danielw) (I also made a mess with the assignees. I'm sorry. -danielw)",30
"Reference Test Server using new XRootD
nan",5
"Switch to MariaDB
We should switch Qserv to the MariaDB Foundation based MySQL.",3
"Setup dev test environment
Setup whole Qserv environment, including installing data set, and validate it by running some simple queries. Suggest changes/improvements as appropriate.",8
"Refurbish existing configuration 
Refurbish existing configuration scripts to make them work with the new packaging/build system.",10
"meas_base plugins for CModel magnitudes
Create meas_base Plugins for single-frame and forced measurement that uses the model-fitting primitives in meas_multifit to implement SDSS-style CModel magnitudes, in which we fit an exp and dev model separately and then fit the linear combination with ellipse parameters held fixed.  An old-style plugin has already been implemented on the HSC fork, and should be used as a guide; this issue involves adapting that implementation to meas_base and potentially cleaning it up a bit.  Note that the HSC implementation cannot be transferred directly to the LSST side because the meas_algorithms APIs are slightly different on the two forks.",6
"refactor forced tasks into two tasks
After looking at it a bit more, I think we should refactor the current meas_base forced photometry task to separate the CmdLineTask from the Measurement task.  This will allow the forced measurement task to share a common base class with SingleFrameMeasurementTask (allowing us to move the callPlugin free functions into that base class), and give us better parallels with existing tasks:  - ProcesImageForcedTask (my proposed name for the base command-line task) will be more similar to ProcessImageTask.  We'll also have ProcessForcedCcdTask and ProcessForcedCoaddTask.  - ForcedMeasurementTask will be more similar to SingleFrameMeasurementTask.  In short, I think this will both clean up the ugliness in callPlugin and make the whole hierarchy easier for newcomers to understand.",6
"switch from '.' to '_' in afw::table fields
We've been mapping '.' to '_' in afw::table I/O, which unnecessarily complicates lots of things.  We'd like to switch to using '_' in the field names themselves, which requires ending this mapping in I/O, but we need to be backwards compatible.  So we'll add a version to the FITS headers, and continue the mapping if the version is not present or is less than some value.  Until we do this, the new field names being used in meas_base won't round-trip.",3
"Implement HSC camera in new camera framework
The HSC camera needs to be put in the new camera geometry.  I will implement is like we did lsstSim and sdss.  That is I will generate a repository with the camera config and ampInfo FITS files that can be unpersisted by the butler.  These changes should only require modifying obs_subaru.  My plan is to just use the policy file to populate the new geometry, but if there is more up to date information, I'll happily use that.",10
"Investigate compensation for Dcr
This is a continuation of the W14 image differencing work.  We have characterized the negative effects of Dcr on difference images, and now need to start working on compensation for these effects.  This work will also touch on the Wcs and Psf classes, which are probable consumers of this information.  Scope includes designing and implementing a class to describe the astrometric effects of Dcr, with consideration as to the other classes (Psf, Wcs) and tasks (ImageDifferencingTask) that may use it",60
"Design of Dcr Class
Describes the design process for implementation of a class to model the effects of Dcr.  Includes design itself, and the design review process.",20
"Implementation of Dcr Class
Core implementation of the class that represents the effects of Dcr.  This only includes the initial implementation.  We should realistically expect that this class will evolve as it encounters more use cases, no matter how thorough the design process is.  ",20
"Test and migrate to swig 3.0
-------- Original Message -------- Subject: [LSST-data] Swig 3.0 is out (with C++11 support) Date: Mon, 17 Mar 2014 08:26:05 -0400 From: Robert Lupton the Good <rhl@astro.princeton.edu> To: LSST Data <lsst-data@lsstcorp.org>  I tried a pre-release on os/x 10.7.5 and it failed some tests, but I haven't tried this version.  I had some discussion about this with William, but haven't had time to follow through.  							R   > Date: Sun, 16 Mar 2014 22:44:42 +0000 > From: William S Fulton <wsf@fultondesigns.co.uk> >  > *** ANNOUNCE: SWIG 3.0.0 (16 Mar 2014) *** >  > http://www.swig.org >  > We're pleased to announce SWIG-3.0.0, the latest SWIG release. >  > What is SWIG? > ============= >  > SWIG is a software development tool that reads C/C++ header files and > generates the wrapper code needed to make C and C++ code accessible > from other programming languages including Perl, Python, Tcl, Ruby, > PHP, C#, Go, Java, Lua, Scheme (Guile, MzScheme, CHICKEN), D, Ocaml, > Pike, Modula-3, Octave, R, Common Lisp (CLISP, Allegro CL, CFFI, UFFI). > SWIG can also export its parse tree in the form of XML and Lisp > s-expressions.  Major applications of SWIG include generation of > scripting language extension modules, rapid prototyping, testing, > and user interface development for large C/C++ systems. >  > Availability > ============ > The release is available for download on Sourceforge at >  >      http://prdownloads.sourceforge.net/swig/swig-3.0.0.tar.gz >  > A Windows version is also available at >  >      http://prdownloads.sourceforge.net/swig/swigwin-3.0.0.zip >  > Please report problems with this release to the swig-devel mailing list, > details at http://www.swig.org/mail.html. >  > Release Notes > ============= > SWIG-3.0.0 summary: > - This is a major new release focusing primarily on C++ improvements. > - C++11 support added. Please see documentation for details of supported >   features: http://www.swig.org/Doc3.0/CPlusPlus11.html > - Nested class support added. This has been taken full advantage of in >   Java and C#. Other languages can use the nested classes, but require >   further work for a more natural integration into the target language. >   We urge folk knowledgeable in the other target languages to step >   forward and help with this effort. > - Lua: improved metatables and support for %nspace. > - Go 1.3 support added. > - Python import improvements including relative imports. > - Python 3.3 support completed. > - Perl director support added. > - C# .NET 2 support is now the minimum. Generated using statements are >   replaced by fully qualified names. > - Bug fixes and improvements to the following languages: >   C#, Go, Guile, Java, Lua, Perl, PHP, Python, Octave, R, Ruby, Tcl > - Various other bug fixes and improvements affecting all languages. > - Note that this release contains some backwards incompatible changes >   in some languages. > - Full detailed release notes are in the changes file. ",2
"Board workflow modifications
On DM Software Development board, I propose we:  * Change ""Ready to Merge"" to ""Review Complete"". Per K-T:  {quote} That's what I thought ""Review Complete"" would be -- most of the work is done, but some fixups are needed before merging, and a re-review is not necessary.  {quote}  * Remove ""Ready for Review"". Instead, the developer should just drag the issue to ""In Review"" and assign it to a reviewer. If/when we re-instate the ""review master"", we may re-introduce ""Ready for Review""",1
"Setup the new Buildbot CI system
Setup the Buildbot 0.8.8 testbed for the DM environment.  This includes: (1) setting up slaves on the set of common OS on which the DM stack runs; (2) creating a new continuous integration slave using the new eupsPkg-based build and distribution support,  Definition of done: * Every git change of master should trigger a build of master * If a build failed, an e-mail will be sent to lsst-data (if the build succeeds, nothing happens) * Failures due to internal buildbot issues (e.g., config problems, transient system availability issues, timeouts, etc.) should go to the buildbot owner. * Allow user-triggered builds via web page (with specified refs to be built), with a common user (until we get LSSTC LDAP directory hooked up). It's understood that locking may not be fully implemented/fleshed out in this story. * It should be possible for a user on lsst-dev to easily setup the stack for either a failed or a successful build. ",100
"Move TCT-relevant  twiki documentation to Confluence
Congregate all the trac TCT-relevant documents (standards, policies, guidelines, meeting history) onto Confluence.",2
"Develop and then create the organizational structure for DM Confluence space
Before we start populating the DM Confluence space with active pages, we should define an overall organizational structure/taxonomy.",1
"Adding support for numpy scalar array types as arguments to SWIG wrapped methods
Building against the anaconda on lsst-dev causes construction of Point2I (and other point types) with numpy dtypes to fail with the standard exception: {code} NotImplementedError: Wrong number or type of arguments for overloaded function {code} I did not know that was not allowed.  If I am doing it others are as well.  I think this should be addressed in some way before W14 release.  How to repeat on lsst-dev: {code:sh} setenv EUPS_DIR ~lsstsw/stack/ source ~lsstsw/eups/bin/setups.csh setup afw setup anaconda echo ""import lsst.afw.geom as ag\nimport numpy\nprint ag.Point2I(5,5)\nval = numpy.int64(5)\nprint ag.Point2I(val,val)"" > test.py python test.py {code}  Using the old 7_3 stack you can do a similar test: {code:sh} source /lsst/DC3/stacks/default/loadLSST.csh setup -t v7_3 afw echo ""import lsst.afw.geom as ag\nimport numpy\nprint ag.Point2I(5,5)\nval = numpy.int64(5)\nprint ag.Point2I(val,val)"" > test.py python test.py {code}",3
"Improve handling errors occuring in AsyncQueryManager
AsyncQueryManager is initialized based on configuration file, if the configuration is invalid, an exception should be thrown (eg in _readConfig()) and gracefully handled upstream.",1
"clean up multiple aperture photometry code
I've been doing some minor work on the HSC-side ApertureFlux algorithm, and I wanted to record some concerns here (from both me and RHL) that should be addressed in the new meas_base version:  - We should consider merging ApertureFlux and EllipticalApertureFlux into the same algorithm (with a config field to choose whether to use elliptical apertures).  We could still register it twice, with a different default config value, and this should eliminate a lot of code duplication.  We could also consider having them inherit from a common base class (instead of having EllipticalApertureFlux inherit from ApertureFlux, as is done now).  - We should test that the threshold at which we switch from Sinc to naive apertures is obeyed exactly.  - We should create a flag for the failure mode in which an aperture cannot be measured because we go off the edge of the image, and test that it appears at the right point.  If possible, we should set this flag and measure what area we can within that aperture, instead of just bailing out.  - (Somewhat off-topic) We should consider having utility functions on SourceSlotConfig to set all slots to None for use in unit tests.",5
"More helpful location information for errors in duplicator/partitioner input
Currently, if the Qserv duplicator or partitioner encounters erroneous input (such as a formatting error, or missing columns), the error message it outputs does not include a mapping back to the location of the error, making it very hard/annoying indeed to fix that erroneous input.  Fabrice would (quite reasonably) like to see a filename and line number in the error message.  Unfortunately, producing a line number is complicated by the following facts:  - multiple threads may be reading sub-sections of the same input file in parallel  - sub-sections are not guaranteed to be read in order  - lines can have varying length  In other words, the code reading/parsing the input has no idea which line it is working on, and making that information available would involve either deferring error reports or extra synchronization (performance loss).  What we can easily (and should) do is to arrange for processing code to know the file and byte offset of the input text being processed; error messages should include both pieces of information.",2
"Eliminate dependence of query analysis on parser and antlr
I would like to write and compile query analyzer code completely independently of the parser and ANTLR (transitively). This doesn't seem to work right now. This is not currently possible.  This might take any where from a day to a week. (I'm not sure if we can finish anything in half a day, if you include the testing, review, feedback, and revision process, but perhaps unit testing will make that faster).  Updates to follow after the scope is estimated.  Dependencies to be broken: query --> parser, antlr (due to predicate depending on antlr nodes) qana --> parser, antlr ",8
"Investigate new XrdSsi interface
nan",10
"xrootd initialization should abort if mysql connection fails
Currently xrootd will happily start even if it can't connect to mysql, it will only print a message:  {code} Configration invalid: Unable to connect to MySQL with config: {code}  This can be easily overlooked, plus, it is a fatal error and xrootd initialization should be aborted.  while working on this, I propose to also improve validateMysql(). At the moment it connects to mysql and database in one call. If connection is fine, but the database does not exist, it will fail without telling user why it failed. This can be confusing. It'd better to connect to mysql without connecting to database, then do ""select_db"", and if that fails, inform the user that the database does not exist.  (transferred from trac ticket 3165)",1
"fix namespaces in all Qserv core modules
This was suggested by the code review for ticket 1945 (https://dev.lsstcorp.org/trac/wiki/SAT/CodeReviews/1945), pasted below:  common/src/*:  While it's not required by the coding standards, I'm a big proponent of using namespace scopes in .cc files, which usually save you from needing namespace aliases and will certainly save you from having prefix every declaration with qserv::.  At some point I'd recommend changing the header file extension from .hh to .h to match the rest of the LSST DM code, unless it's a big backwards compatibility issue.  (transferred from trac ticket 2528)",5
"Qserv should check for loaded spatial UDFs
Qserv should have a way of checking for the existence of spatial UDFs loaded in the worker MySQL instances.  Obviously, the worker must perform the physical check. However, the worker has no knowledge that spatial UDFs even exist, since the master is responsible for translating spatial spec into UDF call.  At the moment, the preferred way of checking would be some sort of administrative command that runs on all nodes. An alternative would be some sanity check that is run on a worker before an admin starts a worker. Or the master could devise a MySQL query to check for things and dispatch to all chunks (although this would not get full coverage when replica exist, while being redundant while workers host more than 1 chunk).  (transferred from trac ticket 1959)",2
"restarting mysqld breaks qserv
Restaring mysqld results in unusable qserv (even if the restart happens when qserv is completely idle). The error message is:  ERROR 2013 (HY000): Lost connection to MySQL server during query This happens most likely because qserv caches the connection, which becomes invalid when server is restarted. I am guessing the same will happen when there is a long period of inactivity (the connection times out).  (transferred from trac 2853)",1
"Centralize hardcoded constants
Some values in qserv need to become constant(e.g. chunkId column names, dirs, filenames). Some of these are configurable, others are hardcoded in non-obvious places in the code. When multiple places need this value, they really need to agree, and unfortunately, qserv doesn't have a well-known place for these constants yet.  Any (constant) value that is needed by different parts of the code needs to be managed in a way that is reasonably obvious to unfamiliar programmers.  List of values: * chunkId, subChunkId column names (master...indexing.py, app.py) * environment variable names * + others. This should actually be fairly simple to implement, once the right (?) design is conceived and worked-out.  (transferred from trac ticket #2405)",6
"Jira for Qserv
Jira setup for Qserv, includes things like adding new tasks, transferring tasks from trac, epic/story/task division, assigning story points, setting scrum board, just learning things and more...",8
"Come up with a standard to handle C++ Exceptions in Qserv (and the rest of DM?)
Currently CSS is using one class and relies on different error codes to differentiate between different type of exceptions, while other parts of Qserv core define an exception for each different error. It'd be good to standardize and use the same approach. ",4
"cleanup includes in Qserv core modules
Includes need cleanup: group into standard lib, boots and local, sort as appropriate etc. Also, unify forward declarations.",2
"CSS - surviving mysql and zookeeper glitches
CSS should gracefully recover from failures such as lost connection to mysqld or zookeeper. It is not clear if it would survive such glitches right now -- this needs to be tested and the code improved as necessary.",4
"Create a board (virtual or otherwise) with pictures and names of everyone in DM
I tried compiling a list of everyone who works or has worked on the DM code -- it was nearly impossible. We should have a (public) list both of current staff and our alumni (and where they are now).",1
"Re-think thread.cc and dispatcher.cc python interface
The mess of thread.cc and dispatcher.cc need to be re-thought and re-designed so that the interface is smaller and more obvious.  ",2
"Trim python importing by czar in app.py
Clean up the way modules are imported in qserv master, use relative import when appropriate instead of lsst.qserv.master.<package>   (migrated from Trac #2369)",2
"Libraries being built in lib64 on OpenSUSE, when EUPS tables assume lib
A report from Darko Jevremovic <darko@aob.rs>: {quote} Hi Mario,  I managed to build stack v8 on OpenSuse13.1  There were standard problems with lib/lib64 - namely system builds libraries in $PREFIX/lib64 and some programs are hard wired for $PREFIX/lib  if you could  change the last line of  mysqlclient-5.1.65+3/ups/eupspkg.cfg.sh  from  (cd $PREFIX/lib && ln -s mysql/* . )  to  ( cd $PREFIX && if [ ! -f ""lib"" ] ; then  ln -sf lib64 lib; fi &&cd $PREFIX/lib && ln -s mysql/* . )  or something along that line (am not sure whether the syntax would  work).  Also if you could add  in the same manner to ups/eupspkg.cfg.sh  ( cd $PREFIX && if [ ! -f ""lib"" ] ; then  ln -sf lib64 lib; fi)  for the following packages:  minuit2 gsl cfitsio wcslib {quote} ",1
"Take RAM into account when computing NCORES to use in installs
Darko Jevremovic reported he's had to switch off hyperthreading and manually override NCORES, MAKEFLAGS and SCONFLAGS because his 8-core machine had too little RAM to build afw with -j 8.  To fix this, eupspkg default build routines should take RAM into account when computing the level of build parallelism.  In the meantime, we should document the workaround (contact darko@aob.rs).",1
"Local lsst-build invocations should use a different build number prefix
Buildbot-invoked lsst-build installs packages in the stack with ""b#"" tags.  These are propagated to the distribution server by {{eups distrib create}}.  Local stacks maintained with lsst-build should use different tags so that they don't conflict with these distributed tags.",1
"S14 Enhancements in Qserv installation procedure
nan",15
"Cut Qserv release
It'd be very useful to have fully functioning Qserv release with the latest set of changes (build, packaging, CSS, Daniel's fixes etc) during the Hackathon week.",2
"Migrate std::lists to std::vectors
Suggested by Andy when reviewing DM-296, discussed at Qserv mtg 3/27.  std::list --> std::vector  * why? Default now is vector, iterating over vector is much more efficient than over list  * revisit on case by case bases, do not blindly replace  * preferred solution: typedef, and name it in a way that conveys the intent (e.g., might call it a ""container""), underneath use vector",8
"improve code that initializes shared_ptrs 
Reported by Andy when reviewing DM-296. Discussed at Qserv mtg 3/27.   boost::shared_ptr =(new T())"" --> boost::make_shared()",4
"removed dead code in stringUtil.h
Remove obsolete strToDoubleFunc (and more) in util/stringUtil.h.",1
"Make it easy to build and release point releases
nan",4
"Establish github mirror of LSST repositories
nan",4
"Transition to Stash
nan",10
"Enable gravatars
Could you enable gravatars for all our atlassian products (at least Jira + Agile; Confluence)  https://confluence.atlassian.com/display/AOD/Configuring+Gravatar+support",1
"Add cameraGeom overview to Doxygen documentation
The CameraGeom package needs an overview page (part of afw's main.dox) as part of the Doxygen documentation. I think it's up to Simon or me to add this.",2
"Install and tag multiple Qserv versions on the same distserver
Done in DM-366",1
"Simplify Co-add example in Software User Guide
The current example in the LSST Software User Guide for co-addition reflects the processes necessary to perform a DR production. While thorough, it only really works on the lsst cluster. The example should be simplified to work on a smaller subset of data, and on single-user machines.  Definition of Done: * Following the documentation, it will be possible for users to identify the SDSS data they need (the subset of files) * There will be instructions on how to download the necessary files to their local machine * There will be instructions on how to build the necessary repositories * There will be instructions on how to run the Co-Add+forced photometry tasks.  * Any issues requiring access rights to LSST machines or databases will be identified and issues created for later.",8
"Integration tests dataset should be packaged in eupspkg
A qserv-integration-tests package should be created : - it would allow to manage easily, in ups/qserv.table, tests version for a given Qserv version. - it would allow to install Qserv dependencies related to testing, like partition (and other data ingest code which may arrive.",3
"Refactor install/distribution procedures using lsst-sw
 Here's Andy Salnikov remark, during #3100 review : https://dev.lsstcorp.org/trac/ticket/3100#comment:18",5
"CSS performance optimizations (avoiding redundant checks)
Facade.cc: it seems worthwhile to think about how to tweak the implementation to reduce redundant calls. e.g., getChunkedTables() calls _cssI->exists() for the database for each contained table. It also calls exists() for each table, which it just retrieved. In reality, it only needs to call exists() once, for the db. So we are doing 1+3t reads rather than 1+t.  (This came up in the review of DM-56, the review comments are captured in DM-225) ",8
"integrate qserv_admin backend into czar or separate admind
client/qserv_admin_impl.py: I know the czar's Python code is ugly. But the concept of having more than one entry point for users is also ugly. I feel like the client should just wrap up stuff into JSON and make a REST call into the czar. Ideally the proxy would use REST, perhaps via: https://github.com/fperrad/lua-Spore/, which at least seems more thought-out than lua-xmlrpc.  (This came up in the review of DM-56, the review comments are captured in DM-225)",15
"Improve how CSS exceptions are handled
CSS has one class for all exceptions. The model adopted by Daniel (each type as a separate exception) is better, as we can catch individual exceptions  instead of (a) having to catch all css exceptions, (b) checking type and (c) rethrowing if it is not the type we want. To be able to catch all CSS exceptions, we can just introduce a new base class (that is new comparing to Daniel's version).",6
"improved how default values for CSS are handled
Need to improve how defaults are handled in qserv_admin. There seems to be some desire to warn when values are not set--how about setting defaults and just printing what configuration is being used? If this is something human-created, we should have reasonable defaults and not bother the user, unless no default is viable. I think we should only be strict on machine-generated input, where we would like to catch bugs as soon as possible.   (This came up in the review of DM-56, the review comments are captured in DM-225)",5
"fix testQueryAnalysis
5 tests fail in the testCppParser.",2
"Catch AttributeError problems in czar
I wonder if we could catch more exceptions in czar to simplify debugging. For example, if I change: {code} --- a/core/modules/czar/lsst/qserv/master/app.py +++ b/core/modules/czar/lsst/qserv/master/app.py @@ -418,7 +418,7 @@ class InbandQueryAction:                     self.constraints.size())          dominantDb = getDominantDb(self.sessionId)          dbStriping = getDbStriping(self.sessionId) -        if (dbStriping.stripes < 1) or (dbStriping.subStripes < 1): +        if (dbStriping.x < 1) or (dbStriping.y < 1): {code}  It will return to client a very cryptic error: {code} Qserv error: Unexpected error: (<type 'exceptions.AttributeError'>, AttributeError('x',), <traceback object at 0x969a02c>) {code}  with no other clues, traceback or information in the log. ",2
"S14 Improve error handling in all parts of Qserv, report sensible errors to users
Qserv needs to handle error gracefully. No matter what error occurs, it should try to automatically recover, and if it can't it should report a reasonable error to user. We should try to poke around and trigger various errors in random places in Qserv and watch what happens, how system fails and/or how it reports the error to user (then we should fix it...). I expect we will need to design a better error handling framework to achieve graceful error handling.",60
"loadLSST bug(s) for csh, ksh
A flaw in the v8.0 loadLSST scripts (and/or in eups/bin/setups) causes the following errors:     1) When using ksh:   {code}     $INSTALL_DIR/loadLSST.ksh     ksh: /path/to/INSTALL_DIR/eups/bin/setups.ksh: cannot open [No such file or directory]  {code}  And indeed, there is no eups/bin/setups.ksh file.     2) When attempting to run the installation demo (v7.2.0.0):  {code}     $> printenv SHELL     /bin/tcsh  {code}  [The same issue appears with csh, unsurprisingly.]  {code}     $> source /path/to/install_dir/loadLSST.csh     $> cd /path/to/demo     $> setup obs_sdss     $> ./bin/demo.sh     ./bin/demo.sh: line 7:  /volumes/d0/lsst/stack80/eups/*default*/bin/setups.sh: No such file or directory     ./bin/demo.sh: line 12: setup: command not found  {code}  After hand-editing the demo.sh script to omit the ""/default"" string from the offending line, the demo runs normally to completion.     Note that everything works fine for bash with v8.0, which is what I tested awhile back. ",1
"Add Versioning to SourceTable in lsst::afw::table
Add version to afw::table::SourceTable.  Persist that version number to fits file when the table is saved, and restore when the table is restored.  Tables created and saved to disk prior to this modification will have the version number 0, by default.  Tables created with the S14 version will have the version number 1.    This change is to enable a new version of slots and field naming conventions as needed by the Measurement Framework overhaul, at the same time allowing current clients of SourceTable to continue to function.  The work to define and persist the slots depending on the version will be on a separate issue.  Should not appear as an alterable member of the metadata, but should be saved with the metadata and reloaded when the file is reloaded.  getVersion and setVersion methods will be used to allow clients to alter this number.",1
"Create Command/Event Sender
Simulate the OCS and CCS (via the OCS) sending message to the Base DMCS.  Write a library to send commands via method calls, which will be used by commandable  entities and by the Base DMCS.  The method calls in this library will be used to simulate sending commands from the OCS.  This will initially be developed using DM messages, and later switched to use DDS.  Write a command line tool to send these messages.  Commands with no arguments: init, enable, disable, release, stop, abort, reset.  Commands with arguments: configure - arguments are: Set of computers, software and versions to be executed, parameters used to control that software.  Events with arguments: startIntegration, nextVisit.  Definition of done:  * A library that has method calls to each of these commands/events.  Each method call sends one message to the given Topic. * Command line tool that can send any of these commands/events to a commandable entity subscribed to a Topic. * Unit tests for each of the commands/events",6
"Build Base DMCS communications library
Write a library to be used by each commandable entity and the Base DMCS.   Methods in this library receiving commands from the simulated OCS. Specific command actions will be handled by each entity, and events are handled by the Base DMCS.  This will initially be developed using DM messages, and later switched to use the DDS.  The library will include:  * An object with methods for blocking receives, blocking receives with timeouts, and non-blocking receives.  Any commands received by these methods are given to another class to call appropriate action methods. * An abstract class implementing each of the following methods for commands: init, configure, enable, disable, release, stop, abort, reset. * An abstract class with methods for implementing the following methods for events:  startIntegration and nextVisit",6
"Build replicator
Replicator  On boot:  * Establishes connection with single, pre-configured distributor. * Checks connection with the network outage buffer, the Base raw image cache, and the tape archive.  On success, the replicator registers itself with the Orchestration Manager in the fully-operational pool.  From K-T: Note that what to check should eventually be dependent on what was specified in the configuration.   That way, if the tape library is down, we could still run using the network outage buffer or raw image cache as the redundancy source.  In operation:  * Receives a job with visit id, exposure sequence number within the visit and raft id. * Queries the Base EFD replica for information needed to process the image. * Subscribes to the Camera Data System (CDS) “startReadout” event and to the OCS telemetry topics. * Waits for a startReadout event * Request the cross-talk corrected exposure for the raft using the CDS client interface and block until it is available.  On receipt of crosstalk-corrected image:  * Verify integrity of image (hash or checksum) * Send image with telemetry to the distributor, compressing it if the configuration says to do so.  At the same time, the image is written to the network outage buffer and the raw image cache using the Data Access Client Framework, retrying until successful for a configured number of tries. All images that are written are tagged with the Archiver mode (by the replicator job).  * Request the raw image using the CDS client interface and block until it is available.  On receipt of the raw image:  * Verify integrity of image (hash or checksum) * Send image to the distributor and simultaneously write to the network outage buffer and the tape archive.  ALL image transmission statuses (successful and unsuccessful) are recorded to the Alert Production Database.  NOTE:  Image some calibration or engineering modes, there may only be raw-image data, not crosstalk-corrected image data.  The replicator job configuration will provide for this).  NOTE: Replicator jobs will need to detect they are not completing in the expected amount of time.  NOTE: Something (the Base DMCS?  The Event/Message Monitor?) needs to pay attention to the number of replicator re-registering themselves with the local-only pool and notify NetOps to investigate and resolve the issue.  On errors sending or keeping connected to the distributor, the replicator: * unregisters from the “fully-operational” pool * registers with the “local-only” pool  The replicator will continue to try and connect to the distributor, and if it is able to reestablish the connection: * unregisters from the “local-only” pool * registers with the “fully-operational” pool  Heartbeat messages between replicators and distributors will indicate when the replicators will be able to re-register with the fully-operational pool.",10
"Build distributor
Distributor  On boot:  * Checks the network, archive raw image cache and the tape archive.  After all have tested successfully, the distributor waits for a connection from its associated replicator.  In operation:  * On receipt of a visit id, exposure sequence number and raft id from its associated replicator, the distributor publishes them along with its network address to the Archive DMCS.  On receipt of crosstalk-corrected image and associated telemetry from its associated replicator:  * Verifies integrity using a hash or checksum * Writes to the raw image cache using the Data Access Client Framework * Decompresses (if necessary) * Separates into individual CCD-sized portions * Sends portions to the appropriate connected Workers  Note: Workers can connect to request a CCD-sized crosstalk-corrected image.  On receipt of raw image:  * Writes it to the tape system. * All images are tagged with the Archive mode.   Should include heartbeat monitoring of the replicator/distributor connection.",10
"Write Linux Standard Base - compliant init.d scripts
Qserv services init.d scripts have to rely on LSB, in order to work on multiple systems.  Remark : xrootd has to be launched as a background process (i.e. with a & at the end). But this always send of return code equal to 0, even if xrootd fails to start, a shell function wait_for_pid will be implemented in xrootd init.d scritps to solve that (inspired from mysqld init.d script).",5
"Simulate computation and production of VOEvents for worker batch job
Simulate jobs which do alert processing.  Produce VOEvents based on those results.",6
"finish adding aliases to afw::table::Schema
This issue picks up the partially-completed Trac Ticket #2351, which adds a string-substitution-based alias mechanism to afw::table::Schema. There are still some issues to sort out w.r.t. constness - in other respects, a Table or Catalog's schema cannot be changed once the Table has been constructed, but we do need to be able to change aliases after table construction.",6
"Use aliases in slots
nan",2
"Remove measurement code from meas_algorithms
nan",4
"add basic FunctorKeys
Add the basic FunctorKeys mechanism, and enough implementations to support Slots.",4
"Add FunctorKeys to replace compound field functionality
Add more FunctorKeys to replace the functionality in all current compound field types and meas_base result objects.  May involve moving some meas_base definitions to afw.",4
"Add FunctorKeys for common analysis tasks
Add FunctorKeys for simple, common, calculated fields, including:  - Magnitudes from fluxes  - Coords from Points, Points from Coords  - Ellipse conversions and radius/ellipticity extraction",4
"Integrate FunctorKeys with SchemaMapper
nan",10
"Remove support for compound field types
nan",2
"add live DS9-based debugging to measurement framework
The old measurement framework had a lot of live DS9-based display options.  We should ensure the new one has at least as many, and that it still works.    At some point, we should consider a different mechanism for enabling those displays and possibly other tools for displaying them, but that's out of scope for this issue.",6
"Make NoiseReplacer outputs reproduceable
We need a way to get back the noise-replaced Exposure as it was when a particular source was measurement, after the measurement has been run, without having to run noise-replacement on all the previous objects again.  There is already code in afw::math::Random to output its state as a string; I think we should probably just save this string in the output catalog.  This will require some API changes to allow the NoiseReplacer to modify the schema and set a field in the output records.",3
"Control log levels on a per-plugin basis
We should be able to control log levels so that certain plugins are run at one level while the rest are run at another (to allow a particular plugin being debugged to be more verbose).",4
"Add slot support for meas_base-style outputs
The slot mechanism in afwTable currently uses compound fields to save the 3 slot types:  flux, centroid, and shape.  Since the new measurement framework uses a flattened representation in the SourceTable where these types are saved as multiple scalar fields, the slot mechanism need to be altered to handle this new table type.  1.  An alternative to KeyTuple for storing the keys required by the slot 2.  Fixup get(Centroid, Flux, Shape) in SourceRecord to use correct keys. 3.  Fixup the single value getters (getX, getY, etc) to use the correct keys. 4.  Persist slot info to fits correctly, based on table version.",4
"add aperture-correction measurement code to the end of calibrate
At the end of CalibrateTask, we'll want to compute the PSF and aperture fluxes of the PSF stars, and send those to the PSF model to be stored and interpolated (using the featured added via DM-434).  We'll also need to run any other flux measurement algorithms that need to be tied to the PSF fluxes on these same stars; because these can be somewhat slow, we probably want to limit these measurements to only the PSF stars, rather than requiring all these algorithms to be run as part of calibrate.measurement.  The relationships between these fluxes and the PSF fluxes will be additional fields to be added to and interpolated by the PSF.  The HSC implementation of this work (as well as that of DM-436) was done on issue HSC-191: https://hsc-jira.astro.princeton.edu/jira/browse/HSC-191 There were changes to many packages, but the relevant ones for LSST are: https://github.com/HyperSuprime-Cam/afw/commit/057fb3c0581c512d5664f1883a72da950c9eae9d https://github.com/HyperSuprime-Cam/meas_algorithms/compare/HSC-3.0.0...u/jbosch/DM-191 https://github.com/HyperSuprime-Cam/pipe_tasks/compare/4c3a53e7238cbe9...u/jbosch/DM-191",8
"apply aperture corrections in measurement tasks
We need to interpolate the aperture correction to the position of every source, and apply this correction to all appropriate fluxes.  The HSC implementation of this work (as well as that of DM-435) was done on issue HSC-191: https://hsc-jira.astro.princeton.edu/jira/browse/HSC-191 There were changes to many packages, but the relevant ones for LSST are: https://github.com/HyperSuprime-Cam/afw/commit/057fb3c0581c512d5664f1883a72da950c9eae9d https://github.com/HyperSuprime-Cam/meas_algorithms/compare/HSC-3.0.0...u/jbosch/DM-191 https://github.com/HyperSuprime-Cam/pipe_tasks/compare/4c3a53e7238cbe9...u/jbosch/DM-191  Note that on the LSST side, we'll want to apply the aperture corrections either within a new plugin in meas_base or as a new part of BaseMeasurementTask, not as a change to the CorrectFluxes algorithm (which will be removed in the future along with the rest of the old measurement framework in meas_algorithms).",7
"Setup of four new measurement algorithms for processCcd testing
The goal of this story is to take the following algorithms and make them fully operational with processCcd.  PsfFlux,SdssShape,SdssCentroid,SincFlux.  This code was moved to meas_base in w14, but has yet to be used in full operation.  This is the first step in that process.  Each algorithm will at least have one test to confirm that it works, but the unit tests will be very simple.  The actual confirmation of full operability will the to run tests against real data, and compare against the existing meas_algorithms.  The algorithm code will still require cleanup even after this story is completed.  That is because it has intentionally been left the same as what existed in meas_algorithms.    The goal of this ticket is to confirm that the each algorithm can (1) complete its measurement with some reasonable result, (2) responds to its major configuration options, and (3) handles at least some of its defined exceptions (that is, flags) correctly.  Confirmation that the results are identical with meas_algorithms is a subsequent ticket  Cleanup and documentation of this code will be done in a subsequent ticket.",4
"Approve/Acquire HipChat licenses
-------- Original Message -------- Subject: Getting HipChat licenses Date: Wed, 09 Apr 2014 08:51:46 -0700 From: Mario Juric <mjuric@lsst.org> Organization: Large Synoptic Survey Telescope Inc. To: Jeffrey Kantor <JKantor@lsst.org>, Iain Goodenow <IGoodenow@lsst.org>,  Stefan Dimmick <SDimmick@lsst.org>  Jeff et al., 	I'd like to start the process to acquire HipChat licenses (we have 10 days left on our evaluation one). Right now we have 26 users, so our first total would be $52/month. I'm expecting it will eventually grow to ~50 users or so.  	In terms of an account to charge, this should really be considered a project-wide tool, but if that's going to cause unnecessary delays I'd propose we charge it to DM as a sign of our infinite kindness and good will :).  	HipChat wants to simply bill a credit card -- once we get the necessary approvals, Iain, I can add you as an admin, and you can use our corporate one if that's OK. ",1
"Write job ads for Tucson DM positions
nan",4
"Test four algorithms for compatibility with original meas_algorithms
Do a trial run of a small area of sky (using a single exposure from measMosaicData).  First create a source catalog using the old measurement.py, then run the same test with the measurement task in sfm.py.  Compare the results.  If the code has been ported correctly, they should match.",4
"Setup PeakLikelihoodFlux with new Algorithm Framework
Move PeakLikehoodFlux to meas_base framework ",1
"Setup Flux algorithms for testing with processCcd
Similar to DM-441 for flux algorithms GaussianFlux and NaiveFlux  Unit tests for major config options, just to be sure that they do something reasonable.  Test of at least one exception (flag)",2
"Test Centroid algorithms against meas_algorithms
nan",2
"Test Flux algorithms agains meas_algorithms
Test for compatibility of NaiveFlux, GaussianFlux, and PsfFlux against meas_algorithms  ",2
"reimplement shapelet PSF approximations
The CModel code we want to transfer from HSC in DM-240 currently relies on the old ""multishapelet.psf"" algorithm in meas_extensions_multiShapelet.  That means we either need to convert that PSF-to-shapelets code in meas_extensions_multiShapelet to use the meas_base interface, or we need add new PSF-to-shapelets code in meas_multifit.  I think the latter is the better choice, even if we delay DM-240 as a result; the heavy algorithmic code is already available as primitives in meas_multifit, so it should just be a matter of packing those into a simple driver, creating a config class for it, and testing it on a few real and simulated PSFs to learn reasonable defaults for the configs.",6
"Implement backup/restore for CSS
It'd be very useful to have a way to ""dump"" and ""restore"" entire contents of the key/value store in zookeeper. The dump part is pretty much there (can dump to an ascii file) ",4
"Add Classes of MeasurementError
Some measurement failures are global for a whole exposure, such as a missing Psf or Wcs.  The framework currently does not distinguish this from a failure in a single measurement.  Should add a new subclass of MeasurementError which can be thrown in these cases.  Should also add configuration option to the measurement framework to determine what should be done with this type of error.  We should also add an exception class and associated how-to-handle config for problems that indicate that something has gone wrong in pre-measurement processing, such as NaNs in the image.",4
"PixelsFlags, SkyCoordAlgorithm, and Classification
SkyCoord was moved to DM-441 - done in Python  Classification is also simple if done in Python  Pixel Keys will be done in C++  Some work left from SdssShape will be done with this ticket  Also, since these are out only two Python algorithms in the base set, I will add the exception handling and base fail() methods at this time. ",2
"add and use ""suspect"" flag in slots and slot-like measurements
We need to have both ""fatal"" and ""suspect"" flags to handle different levels of warnings in measurement.  (other tasks previously included on this ticket have been split into other tickets, including DM-461 and DM-984)",4
"lsst-build updates based on feedback from 1st month of use
change lsst-build interface to:   * rename 'prepare' to 'clone'   * use current directory to clone to   * remove the default REPOSITORY_PATTERN, as it isn't complete   * use the current username as default build tag prefix   * don't write the manifest by default; use --manifest=<filename> option instead   * refuse to change/overwrite repositories if they're dirty, use --force to override   * create a separate 'version' verb   * ProductFetcher.fetch doesn't need to return ref, sha1   * read config file within build directory, .btconfig   * rename the binary to bt   * have the versioner use git db by default, but fall back to generating versions from hash by default  See the linked web page for a mock of the command line interface.",6
"Understand galfast bugs
nan",2
"Alias measurement.plugins to measurement.algorithms
The config item in the old measurement task, measurement.algorithms was changed to measurement.plugins in meas_base.  The creates a backward compatibility issue for code which refers to this class member.  Jim's suggested fix is to alias plugins with algorithms in the new measurement task.",1
"Rework exceptions in css (python side)
Rework exceptions in css/KwInterface.py: split into key-value related exceptions, possibly moving the rest that deals with db/tables into client.  This came up in the CSS review, see DM-225: ""CssException feels a bit out of place....""",1
"Create LSSTsim processing example for SW User Guide
Create a worked example in the Software User Guide of processing raw data with obs_lsstSim. The example should include:   - Pointers to documents for creating multi-band, raw datasets with PhoSim - Creating a data repository for the data - Creating an astrometry_net_data repository - Processing raw files through processCdc (including ISR, measurement, etc.) - Creating a catalog of measurements - Visualizing the output using existing tools  It is possible to work initially with a limited set of data (a raft, or CCD), and to start with eImages rather than raw; the example can be elaborated in due course.",10
"Create an iPython Notebook visualization of the LSST Demo Data
Develop an iPython Notebook to illustrate the processing and results of the LSST Demo. This can be used as both a basic tutorial for the Stack, and a how-to for creating additional visualizations for DM. The visualization should include the source catalog, and possibly also one or more of the processed images. ",4
"Prioritize and define the backlog for Summer 2014
At the DMLT meeting in Seattle, finish the backlog prioritization for Summer 2014 cycle.  Definition of Done: * All epics and major stories defined for Summer 2014. * Rank-ordered in terms of priority. * Leadership teem agreement on their prioritization and backlog. * Teams identified to execute the stories, and stories labeled accordingly. ",2
"Document how to create an astrometry_net_data repository
Document how to create astrometry_net_data index files for an arbitrary astronomical dataset, as well as how to create a repository of such data to support processing with the LSST Stack. It must be possible to limit the area of sky coverage in the index files to that appropriate for the images that the user wishes to process. ",6
"Make JIRA notification e-mail more useful
From HipChat/Data Management:  [12:09] Mario Juric: @jbosch @KTL @KSK Could you double-check if any of you got an e-mail from Jira on Saturday (Apr 12th) re issue DM-78 (I made you reviewers, but it looks like you weren't notified)? [12:10] K-T Lim: I don't recall and can't determine now; it would have been deleted (irrevocably). [12:10] Simon Krughoff: I did get an email. [12:10] Jim Bosch: @mjuric, ah, it appears that I actually did.  The fact that I was a reviewer was just buried, and I didn't notice it. [12:10] Simon Krughoff: I must have missed that I was a reviewer. [12:10] Mario Juric: OK, thanks! 		That gives me not one, but two useful data points (#1 -- emails work, #2 -- they're useless :) ). [12:12] Simon Krughoff: I'm not sure why they are useless.  The emails from trac were a very important part of my workflow as far as being notified of review responsibility goes. 		Maybe it's just the volume from Jira. [12:14] Jim Bosch: Yeah, same here.  Though the volume from JIRA hasn't been so bad, so I don't think that's it.  Maybe my brain just has to get used to the new email format. [12:14] K-T Lim: (In my case, I'm mostly paying attention to the RSS feed although the mailbox serves as a backup.) [12:22] Robert Lupton: One of the things that made gnats a good bug tracker was that the emails contained the right amount of information (I did have source code...), and trac was pretty good too when we tuned it;  bugzilla always used to be awful.  I bet we can fiddle with Jira to make its mail more useful;  I don't just mean filtering what it sends, but making sure that each email is self contained, but not too long",1
"Build Base DMCS Archiver Command Receiver
Base DMCS Archiver  The Archiver receives commands from the OCS, and performs the following actions:  * init - Move from OFFLINE mode to IDLE state.  * configure - (arguments:  set of computers to use, the software to be executed, and parameters to control that software).  1) Verify format and accept command; 2) change to disable mode as if a disable command had been received; 3) install configuration;  4) return success to the OCS.  On failure (illegal configuration, cannot be installed properly), a command error with the failure reason is sent to the OCS.  prerequisite for configure command - sufficient replicator/distributor pairs are available.  Note that after configure, the Archiver remains disabled.  * enable - Subscribe to the startIntegration event.  * disable - Unsubscribe to the startIntegration event.  Note that this does NOT terminate any replicator jobs which are already executing.  * release - Unsubscribe to the startIntegration event, and go into OFFLINE state.  * abort - if received during a configure command, causes this component to go into the ERROR state with no configuration.  If received at any other time, the system transitions into an ERROR state, but nothing stops that was already in progress.  * reset - Unsubscribe to the startIntegration event, go into IDLE state, with no configuration",6
"Build Base DMCS Catch-Up Archiver Command Receiver
Base DMCS Catch-Up Archiver  The Catch-Up Archiver receives commands from the OCS, and performs the following actions:  * init - Move from OFFLINE mode to IDLE state.  * configure - (arguments:  set of computers to use, the software to be executed, and parameters to control that software).  1) Verify format and accept command; 2) change to disable mode as if a disable command had been received; 3) install configuration;  4) return success to the OCS.  On failure (illegal configuration, cannot be installed properly), a command error with the failure reason is sent to the OCS.  Prerequisite for configure command - sufficient catch-up-dedicated replicator/distributor pairs are available.  Note that after configure, the Archiver remains disabled.  * enable - 1) Allows Catch-Up Archiver to scan for unarchived images to be handled; 2) Enables the Orchestration Manager to schedule image archive jobs.  * disable - 1) Stop scanning for unarchived images; 2) Tell the Orchestration Manager to stop scheduling any new image archive jobs.  * release - Unsubscribe to the startIntegration event, and go into OFFLINE state.  * abort - if received during a configure command, causes this component to go into the ERROR state with no configuration.  If received at any other time, the system transitions into an ERROR state, but nothing stops that was already in progress.  * reset - Unsubscribe to the startIntegration event, go into IDLE state, with no configuration",6
"Build Base DMCS EFD Replicator Command Receiver
Base DMCS EFD Replicator  The EFD Replicator receives commands from the OCS, and performs the following actions:  * init - Move from OFFLINE mode to IDLE state.  * configure - (arguments:  set of computers to use, the software to be executed, and parameters to control that software).  1) Verify format and accept command; 2) change to disable mode as if a disable command had been received; 3) install configuration;  4) return success to the OCS.  On failure (illegal configuration, cannot be installed properly), a command error with the failure reason is sent to the OCS.  Prerequisite for configure command - communication with the US DAC EFD replica is possible.  Note that after configure, the EFD Replicator remains disabled.  * enable - Causes the Base DMCS to enable the US DAC EFD replica to be a slave to the Chilean DAC EFD replica.  * disable - Causes the Base DMCS to disable the slave operation of the US DAC EFD replica.  * release - Unsubscribe to the startIntegration event, and go into OFFLINE state.  * abort - if received during a configure command, causes this component to go into the ERROR state with no configuration.  If received at any other time, the system transitions into an ERROR state, but nothing stops that was already in progress.  * reset - Unsubscribe to the startIntegration event, go into IDLE state, with no configuration",6
"Build Base DMCS Alert Production Cluster Command Receiver
Base DMCS Alert Production Cluster  The Alert Production Cluster receives commands from the OCS, and performs the following actions:  * init - Move from OFFLINE mode to IDLE state.  * configure - (arguments:  set of computers to use, the software to be executed, and parameters to control that software).  1) Verify format and accept command; 2) change to disable mode as if a disable command had been received; 3) install configuration;  4) return success to the OCS.  On failure (illegal configuration, cannot be installed properly), a command error with the failure reason is sent to the OCS.  Prerequisite for configure command - sufficient workers are available.  Note that after configure, the Alter Production Cluster remains disabled.  * enable - Causes the Base DMCS to subscribe to the “nextVisit” topic in normal science mode; another event may be subscribed to in calibration or engineering mode.  * disable - Causes the Base DMCS to unsubscribe from the “nextVisit” topic.  It does NOT terminate any worker jobs already executing.  In particular, the processing for the current visit (not just exposure) will normally complete.  * release - Unsubscribe to the startIntegration event, and go into OFFLINE state.  * abort - if received during a configure command, causes this component to go into the ERROR state with no configuration.  If received at any other time, the system transitions into an ERROR state, but nothing stops that was already in progress.  * reset - Unsubscribe to the startIntegration event, go into IDLE state, with no configuration",6
"improve initialization of kvMap in testQueryAnalysis
Build the kvMap at build-time and embed it into the executable. (this was brought up in DM-225)",1
"improve generating kvMap in testFacade.cc
Generating the kvmap file, and pasting it into a string inside the test program. (this was brought up in DM-225)",1
"shorten internal names in zookeeper
rename DATABASE_PARTITIONING to PARTITIONING  rename DATABASES to DBS",2
"rename ""dbGroup"" to ""storageClass"" in CSS metadata
It is meant to be used to indicate L1, L2, L3... At Qserv design week we decided to rename it (original plan was to remove it all together) ",1
"Tweak metadata structure for driving table and secondary index
There seem to be confusion about driving table and secondary index. At the moment in zookeeper structure we have {code} /DATABASES/<dbName>/objIdIndex /DATABASES/<dbName>/TABLES/<tableName>/partitioning/secIndexColName /DATABASES/<dbName>/TABLES/<tableName>/partitioning/drivingTable /DATABASES/<dbName>/TABLES/<tableName>/partitioning/latColName /DATABASES/<dbName>/TABLES/<tableName>/partitioning/lonColName /DATABASES/<dbName>/TABLES/<tableName>/partitioning/keyColName {code}  Issues to think about:  * we can't call it objIdIndex, it is too lsst-specific.  * drivingTable and keyColName - perhaps these should be at database level, which means we would only allow one drivingTable and one secondary index per database?  * or, maybe instead of database level, it is a partitioning parameter? Note that two databases might use different name for secondary index or driving table, yet they might be joinable. That argues for introducing a new group, something like /DATABASE/partitioning in addition to /DATABASE_PARTITIONING.  * consider renaming drivingTable to keyTable  * do we really need secIndexColName and keyColName? Can't we get rid of one, and rename to keyColName? ",2
"rework ubuntu.patch
nan",1
"Generalizing data chunking (n-level chunking rather than stripes/subStripes)
It'd be cleaner to use numbering (e.g. 0 for no chunking, 1 for chunks, 2 for subchunks etc) instead of ""chunks"", ""subchunks"" throughout qserv code. This might also be true in partitioner, where flags like -S and -s etc are not entirely obvious.  This came up in the review of CSS, see DM-225.",3
"fix threading issues in CSS watcher
Fix problems with threads in watcher.py brought up in DM-225 by Serge:  * A thread per database doesn't scale  * There is a thread leak when a database is deleted  * There is another design problem, in that each database thread looks like it is holding on to the same lsst.db.Db instance under the hood. I don't remember any consideration for thread safety from the lsst.db code when I reviewed it. Note for one that it is not safe to use a MySQL connection simultaneously from multiple threads (and I seem to recall that you are caching a connection inside Db instances). In practice, even the Python GIL may not save you, since calls into C code (i.e. the mysql client library) may very well release it.",8
"Switch to the ""czar"" name consistently
1) Change lsst.qserv.master to  lsst.qserv.czar in the czar module.  2) Rename masterLib to czarLib  3) Rename startQserv to startCzar ",4
"Fix race condition when creating db (and elsewhere?) in client/qserv_admin_impl.py
This came up in the review of CSS, see DM-225:  QservAdminImpl: so, if there's a problem while inserting the various keys for a database, and another exception during cleanup, you can end up with a partially constructed database. How do you plan on handling this? This is perhaps another argument for trying to batch metadata more - if it's all in a single key, this worry goes away (at least in this particular case). The createDb implementations seem racy. Consider what happens if 2 admins try to create the same database D. Suppose admin A does the _dbExists(D) check first; it returns false. Then admin B comes along, and does the same check, which returns false again. Let's further suppose that admin B successfully issues all the key creates for D without being interrupted by A. Back to A: when A tries to create all the keys required for D, an exception will be raised immediately since D exists. At this point A will dutifully try to clean up (to avoid leaving around a partially constructed D), deleting D as created by B. End result: D was successfully created, no explicit drop was issued, but D does not exist. Database drops can also race with table creates: zookeeper doesn't have a native recursive delete. In other words, it is perfectly possible for an admin A to issue a database drop while someone else is issuing a table create. The recursive delete might descend into the table ""directory"" while the create is populating it with child znodes. The recursive delete will get an incomplete list of children, remove them, and finally fail to remove the table directory because the table create has since added more children, and it is illegal to remove a non-empty directory. At this stage, the drop will report success even though it has failed (that's the recursive delete behavior that kazoo gives you), and the table create will report success. But the database will still exist, and table keys may be partially present in zookeeper.",10
"qserv_admin needs to deal with uncommon names/characters
qserv/admin/bin/qserv_admin.py needs to deal with strange names (e.g. with embedded spaces or semi-colons), or at minimum, catch and forbid them.",3
"Rework exceptions in qserv client
There is a bunch of (I think) unnecessary translation from KvException to QservAdmException. Can't you just handle printing KvException in CommandParser.receiveCommands(), and get rid of the CSSERR error code? (This is in /admin/bin/qserv-admin.py)  (this came up in DM-225)",1
"rethink configuration for client
_fetchOptionsFromConfigFile in client/qserv_admin.py:: If you are going to use the ConfigParser library, please use SafeConfigParser (or RawConfigParser if you don't need string interpolation). But anyway, I'm not sure the INI file format is the right one to use here, as it forces the use of sections in parameter files, which doesn't make much sense (I notice your code just throws away the section names when reading parameter files). I found myself wishing that the admin client just had syntax for the various options, rather than relying on more or less undocumented parameter files.  (this came up in DM-225)",3
"Remove old partitioner/ loader and duplicator
Once Fabrice has migrated the integrated tests towards using the new partitioner and duplicator, we should delete the old partitioner/duplicator (in {{client/examples}}).",1
"Confusing error message (non-existing column referenced)
A query that references non existing column for non-partitioned table results in a confusing message: ""read failed for chunk(s): 1234567890"".  To repeat, run something like {code} SELECT whatever FROM <existingTable>; {code}  Similar error occurs when we try to reference non-existing table, try something like:  {code} SELECT sce.filterName  FROM StrangeTable AS s,       Science_Ccd_Exposure AS sce  WHERE  (s.scienceCcdExposureId = sce.scienceCcdExposureId); {code} ",5
"gracefully handle misconfigured scons
When I try to run scons using the latest master (89aaa6), it fails with  {code} scons: Reading SConscript files ... AttributeError: 'NoneType' object has no attribute 'rfind':   File ""/home/becla/cssProto/qserv_css6/SConstruct"", line 17:     state.init(src_dir)   File ""/home/becla/cssProto/qserv_css6/site_scons/state.py"", line 161:     _initEnvironment(src_dir)   File ""/home/becla/cssProto/qserv_css6/site_scons/state.py"", line 128:     _initVariables(src_dir)   File ""/home/becla/cssProto/qserv_css6/site_scons/state.py"", line 89:     (PathVariable('XROOTD_DIR', 'xrootd install dir', _findPrefix(""XROOTD"", ""xrootd""), PathVariable.PathIsDir)),   File ""/home/becla/cssProto/qserv_css6/site_scons/state.py"", line 53:     (binpath, binname) = os.path.split(binFullPath)   File ""/usr/lib/python2.7/posixpath.py"", line 83:     i = p.rfind('/') + 1 {code}  The scripts should check that requires variables are not set, and print appropriate error (and ideally, suggest how to fix it) ",1
"make Image construction robust against integer overflow
I just fixed a bug on the HSC side (DM-523) in which integer overflow in the multiplication of width and height in image construction caused problems.  We should backport this fix to LSST.",1
"Table column names in new parser
Running tests (qserv-testdata.sh) on pre-loaded data I have observed that many test fail for the only reason that the column names in the dumped query results are different between mysql and qserv. Here is an example of query reqult returned from mysql: {code} mysql> SELECT sce.filterName, sce.field, sce.camcol, sce.run FROM Science_Ccd_Exposure AS sce WHERE sce.filterName = 'g' AND sce.field = 670 AND sce.camcol = 2 AND sce.run = 7202; +------------+-------+--------+------+ | filterName | field | camcol | run  | +------------+-------+--------+------+ | g          |   670 |      2 | 7202 | +------------+-------+--------+------+ {code} and this is the same query processed by qserv: {code} mysql> SELECT sce.filterName, sce.field, sce.camcol, sce.run FROM Science_Ccd_Exposure AS sce WHERE sce.filterName = 'g' AND sce.field = 670 AND sce.camcol = 2 AND sce.run = 7202; +----------+----------+----------+----------+ | QS1_PASS | QS2_PASS | QS3_PASS | QS4_PASS | +----------+----------+----------+----------+ | g        |      670 |        2 |     7202 | +----------+----------+----------+----------+ {code}  We discussed this already with Daniel yesterday and at qserv meeting today, here I just want to collect what we know so far so that we can return to this again later.   As Daniel explained to me this is the result of the new parser assigning aliases to the columns which do not define aliases for themselves. This helps with tracking query proceeding through the processing pipeline. Daniel's observation is that different database engines may assign different names to result columns (or some may not even assign any names), there is no standard in that respect so there is no point in trying to follow what one particular implementation does. Additionally there are issues with conflicting column names and names which are complex expressions.  Difference in column names breaks our tests which dump complete results including table header. The tests could be fixed easily, we could just ignore table headers when dumping the data. More interesting issue is that there may be use cases for better compatibility between mysql and qserv including result column naming. In particular standard Python mysql interface allows one to use column names to retrieve values from queiry result. If qserv assigns arbitrary aliases to the columns it may confuse this kind of clients.  This issue depends very much on what kind of API qserv is going to provide to clients. If mysql (wire-level) protocol is going to be the main API (which would allow all kinds of mysql clients to talk to qserv directly) then we should probably think more about compatibility with mysql. OTOH if we decide to provide our own API then this may not be an issue at all (but we still need to fix current test setup which is based on mysql).  We probably should discuss API question at our dev meeting.",5
"Qserv returns error table instead of error code
Running the tests on pre-loaded data I noticed that for some queries qserv returns result which does not look like it is related in any way to the query - result column names are different from the columns in the query (different in a different way from DM-530). Here is an example of query and result produced: {code} mysql> SELECT sce.filterName, sce.field, sce.camcol, sce.run FROM   Science_Ccd_Exposure AS sce WHERE  sce.filterName like '%'    AND sce.field = 535    AND sce.camcol like '%'    AND sce.run = 94; +---------+------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+ | chunkId | code | message                                                                                                                                                                       | timeStamp   | +---------+------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+ |      -1 |    0 | NULL                                                                                                                                                                          | 1.39779e+09 | |      -1 |  100 | Dispatch Query.                                                                                                                                                               | 1.39779e+09 | |   32767 | 1200 | Query Added: url=xroot://qsmaster@127.0.0.1:1094//q/LSST/1234567890, savePath=/dev/shm/qserv-salnikov-80df10ee4ed55693702f55021486cd45647c3a58ce549e7c826d9626/7_1234567890_0 | 1.39779e+09 | |   32767 | 1300 | Query Written.                                                                                                                                                                | 1.39779e+09 | |   32767 | 1400 | Results Read.                                                                                                                                                                 | 1.39779e+09 | |   32767 | 1500 | Results Merged.                                                                                                                                                               | 1.39779e+09 | |   32767 | 1600 | Query Resources Erased.                                                                                                                                                       | 1.39779e+09 | |   32767 | 2000 | Query Finalized.                                                                                                                                                              | 1.39779e+09 | +---------+------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+ {code}  Daniel and Bill explained to me what is happening there - when query results in an error instead of returning mysql error condition (which is an error code plus some text) proxy produces a diagnostic result which is a table (above) containing some info which could be useful to diagnose the issue.   This feature looks potentially useful but it may also be confusing for clients like me who are not aware of this feature. For regular mysql client it may be actually harder to intercept errors because one would need to analyze returned table to understand that error condition happened. It may also be ambiguous in a sense that the legitimate query could produce result with the same column names.  Like in DM-530 this issue is tied to a question what kind of API we want to provide to qserv clients. If mysql wire-level protocol is going to be our main API then we should probably try to be more mysql-compatible. OTOH if we are going to hide everything behind our own API then we may have more freedom in re-defining what kind of result error condition produces.",4
"transfer multiband processing changes from HSC
We have a new multi-band processing scheme for coadds on the HSC side, encompassing changes in afw, meas_deblender, and pipe_tasks.  These should be transferred to the LSST side, with RFCs for any backwards incompatible changes.  Changes to the Footprint classes may only be temporary, as we plan to refactor those classes soon anyway, but they're still worth doing now to support the higher-level changes.",8
"Move HSC issues to hsc-jira.astro.princeton.edu
nan",10
"ensure pipe_tasks, obs*, and other packages are compatible with meas_base
The switch to meas_base will involve changing the names of most measurement algorithms, as we're using a new naming convention that provides more traceability.  This will break downstream code that:  - uses field names instead of slots to access measurements  - reads or modifies the list of configured-to-run algorithms.  Whenever possible, we should fix the former by converting them to use slots, as this will automatically provide backwards compatibility.  I'm not yet sure how to handle the latter; the easiest solution would be to give up on full backwards-compatibility with meas_algorithms, but we might be able to find some way to make the old names aliases to the new ones during the deprecation period. ",12
"scons rebuilds targets without changes
I'm seeing something strange when I run scons from current master - running 'scons install' after 'scons build' re-compiles several C++ files even though nothing has changed between these two runs: {code:bash} $ scons build scons: Reading SConscript files ... ... scons: Building targets ... scons: `build' is up to date. scons: done building targets.  $ scons install scons: Reading SConscript files ... ... scons: Building targets ... swig -o build/czar/masterLib_wrap.cc -Ibuild -I/usr/include/python2.6 -python -c++ -Iinclude build/czar/masterLib.i g++ -o build/czar/masterLib_wrap.os -c -g -fPIC -D_FILE_OFFSET_BITS=64 -fPIC -I/u2/salnikov/STACK/stack/Linux64/protobuf/2.4.1/include -I/u2/salnikov/STACK/stack/Linux64/xrootd/qs5/include/xrootd -I/u2/salnikov/STACK/stack/Linux64/mysql/5.1.65/include -Ibuild -I/usr/include/python2.6 build/czar/masterLib_wrap.cc g++ -o build/control/AsyncQueryManager.os -c -g -fPIC -D_FILE_OFFSET_BITS=64 -fPIC -I/u2/salnikov/STACK/stack/Linux64/protobuf/2.4.1/include -I/u2/salnikov/STACK/stack/Linux64/xrootd/qs5/include/xrootd -I/u2/salnikov/STACK/stack/Linux64/mysql/5.1.65/include -Ibuild -I/usr/include/python2.6 build/control/AsyncQueryManager.cc g++ -o build/control/dispatcher.os -c -g -fPIC -D_FILE_OFFSET_BITS=64 -fPIC -I/u2/salnikov/STACK/stack/Linux64/protobuf/2.4.1/include -I/u2/salnikov/STACK/stack/Linux64/xrootd/qs5/include/xrootd -I/u2/salnikov/STACK/stack/Linux64/mysql/5.1.65/include -Ibuild -I/usr/include/python2.6 build/control/dispatcher.cc g++ -o build/control/thread.os -c -g -fPIC -D_FILE_OFFSET_BITS=64 -fPIC -I/u2/salnikov/STACK/stack/Linux64/protobuf/2.4.1/include -I/u2/salnikov/STACK/stack/Linux64/xrootd/qs5/include/xrootd -I/u2/salnikov/STACK/stack/Linux64/mysql/5.1.65/include -Ibuild -I/usr/include/python2.6 build/control/thread.cc g++ -o build/merger/TableMerger.os -c -g -fPIC -D_FILE_OFFSET_BITS=64 -fPIC -I/u2/salnikov/STACK/stack/Linux64/protobuf/2.4.1/include -I/u2/salnikov/STACK/stack/Linux64/xrootd/qs5/include/xrootd -I/u2/salnikov/STACK/stack/Linux64/mysql/5.1.65/include -Ibuild -I/usr/include/python2.6 build/merger/TableMerger.cc scons: `install' is up to date. scons: done building targets. {code}  This is kind of unexpected, or at least I can't understand now why it happens. Trying to run with --debug=explain shows that some dependencies have disappeared and in some dependencies order is different. No clue yet what that means and how it could happen. Need to study our scons scripts to understand what is going on.",3
"rearchitect Qserv to fix dependencies between modules in qserv/core
I looked at includes in ""core/modules"", here is a summary.  * "":"" indicates a dependency * global, log, util, wbase, wlog and xrootd are low level, it is an easy, I didn't bother showing them below  * ""!!!"" indicates circular dependency  * lower-level modules first  {core} css:      <nothing> global:   <nothing> mysql:    <nothing> obsolete: <nothing> proto:    <nothing> sql:      mysql wconfig:  sql, mysql wpublish: sql, wconfig  !!! wsched <--> wcontrol !!! wcontrol <--> wdb wsched:   proto, wcontrol wcontrol: mysql, sql, wsched, wdb, wcontrol, obsolete, proto wdb:      sql, mysql, proto, wconfig, wcontrol  xrdfs:    wpublish, wdb, wconfig, sql, obsolete, wcontrol merger:   sql, xrdfs xrdoss:   obsolete, wpublish, xrdfs  !!! control <--> qdisp !!! qana <--> qproc !!! qana <--> query !!! parser --> query -->qana --> parser !!! query --> qana --> qproc --> query control:  css, merger, obsolete, qdisp, qproc, query qdisp:    control qana:     css, parser, qproc, query qproc:    css, merger, parser, proto, qana, qdisp, query query:    css, qana, qdisp parser:   query {core} ",10
"Base DMCS and Replicator Interaction for Simulator - v1
Create the Base DMCS and Replicator processes that demonstrate  Send startIntegration Send startReadout  Assume:    Archiver and Alert Production Cluster already enabled, replicators already registered in fully-operational pool.  Demonstrate:  Sending startIntegration event to the Base DMCS. Replicator jobs submission Replicator jobs subscribe to startReadout Sending the startReadout event to the replicator jobs. Replicator jobs receive startReadout Replicator jobs pull data ",10
"clean up include <> --> """" for third party includes
According to our coding standard 4.15: https://dev.lsstcorp.org/trac/wiki/C%2B%2BStandard/Files#a4-15.OnlysystemincludefilepathsSHALLbedelimitedwith  we should be using """" for boost, but in quite a few places we do not:  {code} grep 'include <boost' */* |wc      146     314    7916 {code} ",1
"cleanup includes - add module name
Change places like {code}#include ""cssException.h""{code} to {code}#include ""css/cssException.h""{code}   ",1
"Add distributor to simulator - v2
Add distributor to the simulator  Demonstrate:  Replicator node associated with to distributor node Replicator jobs send job info to distributor node Replicator jobs copy data to distributor node.",6
"Add Archive DMCS to simulator - v3
Add Archive DMCS  Demonstrate:  Receiving information from duplicator (visit id, exposure sequence number, raft id, and network address).",4
"Fix Doxygen Doc for new meas_base classes
The final stage of moving the old algorithms to meas_base will be to generate the doxygen documentation and be sure that it is useful.  We will do this after the  cleanup of the old meas_algorithms code.",3
"Investigate Approaches to Dcr
This story captures the work of investigating the implications of the different options to compensate for Dcr: ignore objects with extreme colors, compensate for Dcr in measurement, and compensate per-image at the per-object/pixel level.  We anticipate the latter option will be required going forward, but we need to do the background work to justify this.  We need to understand exactly which classes will need to make use of Dcr information, and what the limitations are of operating at the per-pixel level (e.g. blends).",20
"Validation of Dcr Approach at the Pixel Level
Before we start the process of extending the Dcr functionality into the stack (e.g. Psf, Wcs, etc), we need to validate that the implementation works at the pixel level.  In this task we will work with simulated images to debug and optimize the implementation, to the degree possible.",30
"Cleanup Source.h.m4
This include file was damaged somewhat by the addition of slot routines to work with the flattened field definitions.  It would be nice to put the Measurement abstraction back in place -- or get rid of it.  We need to decide whether the old slot and compound key mechanisms from SourceTable version 0 are going to be continued for doing this.   ",2
"The way the observation date is translated by obs_cfht breaks the defect registry.
The fields of the observation date and time as stored in the CFHT-LS headers are not padded with zeros.  This makes the sqlite DATETIME constructor return a NULL string when the observation time is < 10hrs.  The fix is to force the fields to be padded when the metadata from the input files are ingested.",1
"Get one HTCondor ClassAds scenario running
We have a worker script that takes three arguments: a CCD number, a cache limit, and a length of time.  The script starts out looking at a slot-specific filesystem for a calibration file /local/slot#/calib/CCD#.  If that file doesn't exist, it checks to see if there are more than the cache limit of files in /local/slot#/calib.  If there are, it removes one at random (later we could try other algorithms).  It then immediately transfers /gpfs/calib/CCD# (maybe a 1 GB file) to /local/slot#/calib/CCD#.  The script then waits for the given length of time and exits.  We make sure we can measure how many file copies from /gpfs are performed and when so that we can determine peak demand rates and average rates.  Meanwhile, the HTCondor hook checks for the same /local/slot#/calib/CCD# files and sets the ClassAds based on which CCD#s are present.  Scenario 1) 4 slots, 4 CCDs, cache limit 1, jobs take 1 min, total of 5 jobs per CCD, 1 job per CCD submitted every 2 min (1234 pause 1234 pause 1234 pause 1234 pause 1234)",14
"load non-LSST FITS tables as version 1
In DM-384 and DM-242, we disabled the periods-to-underscores translations for new tables (""version 1"") but left it in place for old tables (""version 0"").  In addition, when reading a table without a version number, we assumed it was version 0, to maintain backwards compatibility.  I think we should modify this slightly: we should assume a table without a version number is version 0 if and only if it also has the AFW_TYPE key.  Otherwise we should assume it is version 1.  This will allow us to load externally-produced tables without turning any underscores they contain into periods, while still maintaining backwards compatibility with older tables written by afw.",1
"Update all DM Software Copyright and License Agreement notices to reflect AURA/LSST
The lsstcorp.org/LegalNotices/{LsstLicenseStatement.txt  LsstSourceCopyrightNotice.txt} need to be updated to reference AURA/LSST. The referenced list of LSST partner institutions needs to be either resurrected or the reference deleted.  The git repository for devenv/templates needs the Copyright templates to be  updated.  LsstLicenseStatement.txt needs to be updated to include recent additions of 3rd party tools' Licenses (~10 tools)  to the DM stack  and all the QSERV 3rd party tools' Licenses (~25 tools).  The Copyright banner in all software needs to be updated to reflect the new reality of AURA/LSST in place of LSST Corporation.  Files with no Copyright banner, need to add it.  Update may occur 'the next time' the code file is updated. THis needs to be broadcast to the developers once the Copyright templates and the website versions are updated.",2
"running multiple Qserv installations on the same machine
It would be very useful to be able to run multiple installations of Qserv on the same machine (say, 2 developers playing with Qserv on the same machine). I guess we are almost there, we just need to know how to configure all ports so that we are not colliding. Can you test it, tweak whatever is necessary and document what is involved in changing defaults to a unique set of port numbers? ",6
"Setup multi-node Qserv 
We are currently focusing on single-node Qserv. It'd be nice to try setting up multi-node Qserv (say 4 workers and a czar on lsst-dbdev*), and improve installation scripts to simplify the process.",6
"Fix automated tests after css migration
After yesterday's merge of DM-58 into master automated tests do not work any more. The part which is broken now is loading of metadata into qserv. We need to replace old script which created metadata with something different that creates metadata using new CSS.   The code which loads metadata in tests is in QservDataLoader class, createQmsDatabase() method (in tests/python/lsst/qserv/test/ directory).",2
"reorganize client module
move everything in the client package (qserv_admin*, associated tests and examples) to admin/  move css/bin/watcher.py to admin/  ",1
"Investigate off-the-shelf data distribution tools
Research available off the shelf tools, try to find one that would best fit our needs. Produce a document (trac page).",12
"fix 12 issues in testCppParser (related to switching to CSS)
nan",3
"Add support for installing qserv on machines without internet
It is common we install qserv on machines that are on internal network without external network connectivity. How do we do that?",9
"Look into git-fat
Look into the use of git-fat with the LSST DM workflow.  Specifically, how does this work with anonymous access.",4
"Update parse/analysis tests to detect missing css-kvmap early
Due to the CSS code merge, the testCppParser test depends on an external kvmap file. If this file doesn't exist, nearly every test will fail. There isn't a way to check whether the cssFacade (constructed from kvmap file) is valid.  This ticket includes, at minimum: * changes to css/ and qproc/ to make the unit tests fail early if the facade could not be constructed.  Optionally, this ticket could include: * renaming testCppParser to testQueryAnalyzer * compile-time linking the kvmap into testCppParser to eliminate the need for an external kvmap file. ",4
"CSS throws exception if tableIsSubChunked is called for non partitioned table
nan",3
"referring to table without database context crashes czar
Running a query like ""select * from Object"" if we do not have database context results in  {code} terminate called after throwing an instance of 'lsst::qserv::css::CssException_InternalRunTimeError'   what():  Internal run-time error. (*** css::KvInterfaceImplZoo::exists(). Zookeeper error #-8. (/DATABASES/)) {code}  Need to gracefully catch the zookeeper -8. Need to detect that empty database name is passed in Facade. Need to avoid it higher up. ",3
"afw unit tests not built unless afwdata available
If afw_data is not available then the afw unit tests are not built. I think it would make more sense to build all of them and run those we can (not all depend on afw_data). One advantage is that a version of afw installed using ""eups distrib install"" would include built tests, so the tests could be run. This is presently not practical because the SConstruct file is not installed (I intend to open a ticket about that, as well).",1
"Switch kazoo version to 2.0b1 or later
While we aren't using any of the new features of Kazoo's 2.x series, the removal of zope.interface as a dependency is a worthwhile feature.  The 2.0b1 release seems at least as stable as our own code, so I don't think we'll see any negative effects.  This ticket covers: - Upgrade of the packaged kazoo from 1.3.1 to 2.0b1 (or later). - (optionally) patches for kazoo's setup.py so that it doesn't search for and try to download any dependencies. This can be done in a later ticket, though.  I note that kazoo can be run without installation: you can untar it, cd into the directory, and if you run python from there, you can immediately ""import kazoo"" and use it. Hence we could avoid setup.py completely and just copy the ""kazoo"" subdirectory into some directory in the PYTHONPATH.",2
"remove obsolete QMS-related code
try running  {code} find core css admin client tests site_scons | xargs grep -i qms {code}  There is a lot of old unused qms related code.",2
"Automated test differences after CSS migration
I'm running automated tests with recent master after merging DM-605 and observe some differences between mysql and qserv. Here I'm going to document all differences (and everything related).",6
"rename qserv_admin.py to qserv-admin.py
We have 6 scripts in qserv/admin/bin that start with ""qserv-"", and one that starts with ""qserv_"", we should rename qserv_admin to qserv-admin.",1
"commons.config should always be managed as a global variable
Qserv configuration object (i.e. commons.config) used in admin and tests should be managed the same way a logger object is (cf. http://hg.python.org/cpython/file/2.7/Lib/logging/__init__.py).  I.e. a function called config.getConfig(""config-type"") (returning an config occurence of a global dictionnary), should be used in order to ease retrieval of all configuration objects related to client, data, (and server?) anywhere in the admin/test python code.",2
"User friendly single node loading script
This entails the creation of an end-to-end loading script that, given database and table param files, a SQL table schema and one or more CSV data files, places appropriate metadata into zookeeper, runs the partitioner, and finally creates and loads chunk tables.",8
"Qserv configuration tool refactoring
* Instance configuration should not be in the same directory where qserv software is installed Qserv configuration, including things like mysql or zookeeper data directory should be separate from where Qserv software is installed. At the moment, when I want to work with branch ""a"" and ""b"", in order to switch from one to the other I have to run ""cd $QSERV_DIR/admin; scons"". This is ok the very first time after I created a branch, but it is way too heavy afterwards, because it wipes out all data from zookeeper and mysql.",10
"Package antlr 2.7 in eups
nan",5
"Make sure we have queryId in all places where needed in logging
nan",4
"Too many connections from czar to zookeeper
I have just managed to crash qserv czar by running repeated queries against it. What I see in the logs:  qserv-czar.log: {code} 2014-05-02 13:21:10,861:22525(0x7f76e37ab700):ZOO_ERROR@handle_socket_error_msg@1723: Socket [127.0.0.1:12181] zk retcode=-4, errno=104(Connection reset by peer): failed while receiving a server response 2014-05-02 13:21:10,861:22525(0x7f76e37ab700):ZOO_ERROR@handle_socket_error_msg@1723: Socket [127.0.0.1:12181] zk retcode=-4, errno=104(Connection reset by peer): failed while receiving a server response terminate called after throwing an instance of 'lsst::qserv::css::CssException_ConnFailure'   what():  Failed to connect to persistent store. {code}  and zookeeper.log: {code} 2014-05-02 13:21:10,861 [myid:] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:12181:NIOServerCnxnFactory@193] - Too many connections from /127.0.0.1 - max is 60 2014-05-02 13:21:10,861 [myid:] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:12181:NIOServerCnxnFactory@193] - Too many connections from /127.0.0.1 - max is 60 2014-05-02 13:21:14,585 [myid:] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:12181:NIOServerCnxn@357] - caught end of stream exception EndOfStreamException: Unable to read additional data from client sessionid 0x145bdd1b8440015, likely client has closed socket         at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:228)         at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:208)         at java.lang.Thread.run(Thread.java:744) 2014-05-02 13:21:14,585 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:12181:NIOServerCnxn@1007] - Closed socket connection for client /127.0.0.1:49913 which had 2014-05-02 13:21:14,585 [myid:] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:12181:NIOServerCnxn@357] - caught end of stream exception EndOfStreamException: Unable to read additional data from client sessionid 0x145bdd1b8440016, likely client has closed socket         at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:228)         at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:208)         at java.lang.Thread.run(Thread.java:744) 2014-05-02 13:21:14,586 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:12181:NIOServerCnxn@1007] - Closed socket connection for client /127.0.0.1:49939 which had {code}",1
"ORDER BY and DISTINCT do not work reliably in qserv
Queries with ORDER BY and DISTINCT are buggy. For example, results do not always come ordered and order changes from one run to another: {code} mysql> SELECT objectId FROM Object   WHERE qserv_areaspec_box(0, 0, 3, 10)  ORDER BY objectId; +-----------------+ | objectId        | +-----------------+ | 417857368235490 | | 417861663199589 | | 417865958163688 | | 420949744686724 | | 420954039650823 | | 424042121137958 | | 424046416102057 | | 427134497589192 | | 430226874040426 | +-----------------+ 9 rows in set (1.27 sec)  mysql> SELECT objectId FROM Object   WHERE qserv_areaspec_box(0, 0, 3, 10)  ORDER BY objectId; +-----------------+ | objectId        | +-----------------+ | 424042121137958 | | 424046416102057 | | 427134497589192 | | 430226874040426 | | 417857368235490 | | 417861663199589 | | 417865958163688 | | 420949744686724 | | 420954039650823 | +-----------------+ 9 rows in set (1.24 sec) {code}  This was done with testdata/case01 data, let me know if you need to load that data.  Also, using case03 data, e.g. {code} SELECT distinct run, field  FROM   Science_Ccd_Exposure WHERE  run = 94 AND field = 535; {code} returns 6 rows in qserv (vs 1 in mysql).  The full list of DISTINCT failures is (all with testdata/case03): - 0002_fetchRunAndFieldById.txt - 0021_selectScienceCCDExposure.txt - 0030_selectScienceCCDExposureByRunField.txt ",1
"Switch to using new partitioner, loader
Integrated tests procedure has to rely on new loader",4
"Non-partitioned table query returns duplicated rows
Running automated test I noticed that a query on non-partitioned table returns multiple copies of the same row, one copy per chunk. Here is example: {code} mysql> SELECT offset, mjdRef, drift FROM LeapSeconds where offset = 10; +--------+--------+-------+ | offset | mjdRef | drift | +--------+--------+-------+ |     10 |  41317 |     0 | |     10 |  41317 |     0 | |     10 |  41317 |     0 | |     10 |  41317 |     0 | |     10 |  41317 |     0 | |     10 |  41317 |     0 | |     10 |  41317 |     0 | |     10 |  41317 |     0 | |     10 |  41317 |     0 | |     10 |  41317 |     0 | |     10 |  41317 |     0 | |     10 |  41317 |     0 | |     10 |  41317 |     0 | +--------+--------+-------+ 13 rows in set (5.62 sec) {code}  Czar log file shows that it correctly finds that table is non-chunked but sends query to each chunk anyway: {code} 20140502 16:22:08.745081 0x3172430 INF *** KvInterfaceImplZoo::exist(), key: /DATABASES/LSST/TABLES/LeapSeconds/partitioning 20140502 16:22:08.745735 0x3172430 INF *** LSST.LeapSeconds is NOT chunked. 20140502 16:22:08.745762 0x3172430 INF *** KvInterfaceImplZoo::get2(), key: /DATABASES/LSST/TABLES/LeapSeconds/partitioning/subChunks 20140502 16:22:08.746393 0x3172430 INF *** LSST.LeapSeconds is NOT subchunked. 20140502 16:22:08.746409 0x3172430 INF getChunkLevel returns 0 ..... 20140502 16:22:08.757832 0x3172430 INF <py> Using 85 stripes and 12 substripes. 20140502 16:22:08.775586 0x3172430 INF <py> Using /usr/local/home/salnikov/dm-613/build/dist/etc/emptyChunks.txt as default empty chunks file. 20140502 16:22:08.791559 0x3172430 INF <py> empty_LSST.txt not found while loading empty chunks file. 20140502 16:22:08.791592 0x3172430 ERR <py> Couldn't find empty_LSST.txt, using /usr/local/home/salnikov/dm-613/build/dist/etc/emptyChunks.txt. 20140502 16:22:08.891239 0x7fbb28003660 INF QuerySession::_buildChunkQueries() : Non-subchunked 20140502 16:22:08.891498 0x7fbb28003660 INF Msg cid=6630 with size=153 20140502 16:22:08.891682 0x7fbb28003660 INF Added query id=6630 url=xroot://qsmaster@127.0.0.1:1094//q/LSST/6630 with save /dev/shm/qserv-salnikov-b93a8b7cca1128f50fa5531feb93f8f24a185f162d36c10ee76b8dca/1_6630_0 20140502 16:22:08.891694 0x7fbb28003660 INF Opening xroot://qsmaster@127.0.0.1:1094//q/LSST/6630 20140502 16:22:08.891705 0x7fbb28003660 INF QuerySession::_buildChunkQueries() : Non-subchunked 20140502 16:22:08.891882 0x7fbb28003660 INF Msg cid=6631 with size=153 20140502 16:22:08.892077 0x7fbb28003660 INF Added query id=6631 url=xroot://qsmaster@127.0.0.1:1094//q/LSST/6631 with save /dev/shm/qserv-salnikov-b93a8b7cca1128f50fa5531feb93f8f24a185f162d36c10ee76b8dca/1_6631_0 20140502 16:22:08.892087 0x7fbb28003660 INF Opening xroot://qsmaster@127.0.0.1:1094//q/LSST/6631 20140502 16:22:08.892097 0x7fbb28003660 INF QuerySession::_buildChunkQueries() : Non-subchunked 20140502 16:22:08.892275 0x7fbb28003660 INF Msg cid=6800 with size=153 20140502 16:22:08.892462 0x7fbb28003660 INF Added query id=6800 url=xroot://qsmaster@127.0.0.1:1094//q/LSST/6800 with save /dev/shm/qserv-salnikov-b93a8b7cca1128f50fa5531feb93f8f24a185f162d36c10ee76b8dca/1_6800_0 ... {code}  Looking at the code together with Daniel we found that at the Python level (czar/app.py) the code that dispatches query does not check for chunkLevel, this is likely why this happens. The code to look at is in {{InbandQueryAction._applyConstraints()}} method.",5
"Define C++ API for C++ Geometry
nan",10
"Query sessions are never destroyed
Please see DM-625, when I run say 10 ""select count(*) from LSST.Object"" queries, for each query a new AsyncQueryManager is created in dispatcher, but the sessions are never destroyed.",3
"admin/tests/test_qservAdminImpl.py has hardcoded connection info
nan",1
"complexity of eups dependencies relationships  for db package
Hello,  I'm currently trying to use the very last version of db package (the one which relies on sconsUtils), but, in order to make it works with Qserv, I had to introduce next update : {code:bash} fjammes@clrlsstwn02-vm:~/src/qserv-packager/dist/dependencies/db (master) $ git diff HEAD~1 diff --git a/ups/db.cfg b/ups/db.cfg index e1ae31b..a469061 100644 --- a/ups/db.cfg +++ b/ups/db.cfg @@ -3,7 +3,7 @@  import lsst.sconsUtils    dependencies = { -    ""required"": [""mysqlclient"", ], +    ""required"": [""mysql"", ],  }    config = lsst.sconsUtils.Configuration( diff --git a/ups/db.table b/ups/db.table index 8c8d831..9e770a3 100644 --- a/ups/db.table +++ b/ups/db.table @@ -1,5 +1,5 @@  setupRequired(python) -setupRequired(mysqlclient) +setupRequired(mysql)  setupRequired(mysqlpython)  setupRequired(sconsUtils) {code}  Is there a solution to describe  in eups that mysqlclient is included in mysql ?  Thanks,  Fabrice",1
"update overview docs to clarify roles of meas_multifit and shapelet packages
From the review of DM-17: {quote} It was not obvious how responsibility is split between {{meas_extensions_multishapelet}}, {{shapelet}}, and {{meas_multifit}}.  Shapelet could use an {{overview.dox}} file. {quote}  {{meas_extensions_multiShapelet}} is on its way out, so we'll wait until that's done and then document the relationship between meas_multifit and shapelet.",2
"meas_base plugin for sampling-based galaxy fitter
In addition to a CModel plugin for galaxy photometry (DM-240), we should create a plugin to do sampling-based galaxy fitting, using the optimizer as a starting point and the existing AdaptiveImportanceSampler class to do most of the work.  A major blocker for this is the fact that the {{SourceTable/SourceRecord}} don't currently provide any way to save multiple samples per object.  This issue may get worked on before it falls into an official sprint, as it's something I want to on my 20% time.",8
"Implement DISTINCT aggregate in qserv
It looks like DISTINCT aggregate is not supported yet in qserv. Daniel told me that this should be relatively straightforward to add. Adding this ticket so that we do not forget it.",2
"support samples and other many-to-one outputs in SourceRecord
For DM-645, I need to be able to save multiple samples (each a parameter vector and possibly a weight) with each {{SourceRecord}}, with the number of samples possibly differing from {{SourceRecord}} to {{SourceRecord}}.  I'll also want to save Mixture distributions, which can also be easily represented as a sequence-of-tuples of values to be associated with a record.  Since we don't want to hard-code the outputs of a particular plugin into {{SourceRecord}} (which is what I've done with the {{ModelFitRecord}} class in meas_multifit, which I'd like to remove), I think this means we want a more general way of allowing algorithms to define many-to-one tables to be associated with each source.  While few algorithms will likely want to save samples, this plugin may not be the only one, and it's likely a general many-to-one feature could be used by other algorithms to save diagnostics when configured to do so.  I'm worried this is another case where afw::table may be intruding on a feature best left to a true RDBMS, but I don't like the idea of tying an RDBMS directly to the source measurement framework either.  I'm open to other ideas, if anyone has one.  It's also worth pointing out that because this is blocking DM-645, which is important for my 20%-time science work, I'd like to get moving on this even before it becomes a real priority for LSST.  But I'd still like to get some design feedback on this.",20
"Add support for running unit tests in scons
Add code in scons that runs unit tests for Qserv.",5
"framework for documenting ""how to run qserv""
We need to have permanent location for how-to-run-qserv, currently we keep it in https://dev.lsstcorp.org/trac/wiki/db/Qserv/RedesignFY2014/Hackathon2/howToRunQserv. It should be in the code repo. Need to decide on how we format it, and need to expose it on our Qserv trac/confluence pages ",3
"Migrate database trac pages to Confluence
Need to migrate Qserv trac pages to Confluence.",10
"Run baseline HTCondor ClassAds Scenarios
Run initial HTCondor ClassAds Scenarios to verify that the implementation for utilizing Rank to place Jobs near data is operating as anticipated.  The main test of the baseline scenarios is to verify that, e.g., a job for a CCD that has calibrations advertised for a particular slot/node  will consistency be executed within that location for appropriate Rank expression. This is to occur even when other open slots are always available (e.g., 4 CCDs, 5 slots). ",4
"Run ""single slow worker"" HTCondor ClassAds Scenario
We run and study a ""single slow worker"" HTCondor ClassAds Scenario. The scenario is a perturbation of the baseline HTCondor ClassAds Scenario. In the baseline, Jobs for a given CCD  are consistently pinned to a slot/node that advertises the presence of associated data files/calibration files for that CCD.  A baseline run may proceed, for example,  with jobs for 4 CCDs repeating executing within same HTCondor slot on a node (even when spare processing slots are readily available.). The baseline is observed to be quite stable, as the pool is empty each time a wave of jobs is submitted. In the ""single slow worker"" scenario, we cause one of the jobs for a chosen CCD to stall (mocking up a slow file transfer, lengthy computation in an algorithm, etc), such that the pool is not empty at the the submission time for a wave of jobs.  We seek to observe how Rank places jobs in this scenario, and work to assign Rank (especially for spare slots) in an optimal way so as to minimize file transfers. ",4
"unknown column derails Qserv
Running a query that references invalid column hangs Qserv, it looks like the query is never squashed.  For example, I run a query: {code} SELECT distinct run, field  FROM   Science_Ccd_Exposure WHERE  run = 94 AND field = 535; {code} on the pt1.1 data set  Corresponding log from xrootd:  {code} Foreman:>>Unknown column 'run' in 'field list' Unable to execute query: CREATE TABLE r_7d8e5a45a6aba94de141cf1d8ab510c89_3598_0 SELECT run,field FROM LSST.Science_Ccd_Exposure \ AS QST_1_ WHERE run=94 AND field=535; <<---Error with piece 0 complete (size=1). Foreman:TIMING,q_f99cQueryExecFinish,1399667470 Foreman:Broken! ,q_f99cQueryExec---Unknown column 'run' in 'field list' Unable to execute query: CREATE TABLE r_7d8e5a45a6aba94de141cf1d8ab510c89_3598_0 SELECT run,field FROM L\ SST.Science_Ccd_Exposure AS QST_1_ WHERE run=94 AND field=535;  (during QueryExec) QueryFragment: CREATE TABLE r_7d8e5a45a6aba94de141cf1d8ab510c89_3598_0 SELECT run,field FROM LSST.Science_Ccd_Exposure AS QST_1_ WHERE run=94 AND field=535;  Foreman:Fail QueryExec phase for q_f99c: Unknown column 'run' in 'field list' Unable to execute query: CREATE TABLE r_7d8e5a45a6aba94de141cf1d8ab510c89_3598_0 SELECT run,field \ FROM LSST.Science_Ccd_Exposure AS QST_1_ WHERE run=94 AND field=535;  (during QueryExec) QueryFragment: CREATE TABLE r_7d8e5a45a6aba94de141cf1d8ab510c89_3598_0 SELECT run,field FROM LSST.Science_Ccd_Exposure AS QST_1_ WHERE run=94 AND field=535;  Foreman:(FinishFail:0x7f26700055c0) Db = q_f99cc9cd5519d465e673119a84b5570a, dump = /usr/local/home/becla/qserv/1/qserv/build/dist/xrootd-run/result/f99cc9cd5519d465e673119a84b\ 5570a hash=f99cc9cd5519d465e673119a84b5570a Foreman:Finished task Task: msg: session=7 chunk=3598 db=LSST entry time=Fri May  9 15:31:10 2014  frag: q=SELECT run,field FROM LSST.Science_Ccd_Exposure AS QST_1_ WHERE run=94 AND field=535, sc= rt=r_7d8e5a45a6aba94de141cf1d8ab510c89_3598_0 ScanSched:ChunkDisk remove for 3598 : SELECT run,field FROM LSST.Science_Ccd_Exposure AS QST_1_ WHERE run=94 AND field=535 BlendSched:Completed: (3598)SELECT run,field FROM LSST.Science_Ccd_Exposure AS QST_1_ WHERE run=94 AND field=535 ScanSched:Completed: (3598)SELECT run,field FROM LSST.Science_Ccd_Exposure AS QST_1_ WHERE run=94 AND field=535 ScanSched:_getNextTasks(32)>->-> ScanSched:ChunkDisk busyness: no ScanSched:_getNextTasks <<<<< BlendSched:Blend trying other sched. GroupSched:_getNextTasks(4)>->-> GroupSched:_getNextTasks <<<<< 140509 15:31:27 13960 cms_Finder: Waiting for cms path /usr/local/home/becla/qserv/1/qserv/build/dist/tmp/worker/.olb/olbd.admin {code}  and the log in czar has {code} 20140509 15:35:40.070654 0x7f4694003660 INF Still 1 in flight. {code}   ",4
"Database name used by  integration tests should use their own dedicated database, not ""LSST""
Currently automated tests use the database ""LSST"", and they will go ahead and delete that database without any warning. This is far from ideal, we should have a more unique database name. How about something like qservAutoTest_<uniqueId>.",9
"Parser has inverted order for ""limit"" and ""order by""
{code} SELECT run FROM LSST.Science_Ccd_Exposure order by field limit 2 {code}  Works in MySQL, fails in Qserv (ERROR 4120 (Proxy): Error executing query using qserv.)  {code} SELECT run FROM LSST.Science_Ccd_Exposure limit 2 order by field {code}  Works in Qserv, fails in MySQL (limit should be after order by) ",1
"""out of range value"" message when running qserv-testdata (loader.py)
Fabrice  I am getting ""out of range value"" when I run the qserv-testdata:  Are you seeing that too?   2014-05-09 18:11:55,975 {/usr/local/home/becla/qserv/1/qserv/build/dist/lib/python/lsst/qserv/admin/commons.py:134} INFO     stderr : /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'calib_detected' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'calib_psf_candidate' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'calib_psf_used' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_negative' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_badcentroid' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'centroid_sdss_flags' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_pixel_edge' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_pixel_interpolated_any' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_pixel_interpolated_center' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_pixel_saturated_any' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_pixel_saturated_center' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_pixel_cr_any' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_pixel_cr_center' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'centroid_gaussian_flags' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'centroid_naive_flags' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'shape_sdss_flags' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'shape_sdss_centroid_flags' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'shape_sdss_flags_unweightedbad' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'shape_sdss_flags_unweighted' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'shape_sdss_flags_shift' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'shape_sdss_flags_maxiter' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flux_psf_flags' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flux_psf_flags_psffactor' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flux_psf_flags_badcorr' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flux_naive_flags' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flux_gaussian_flags' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flux_gaussian_flags_psffactor' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flux_gaussian_flags_badcorr' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flux_sinc_flags' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_psf_flags' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_psf_flags_maxiter' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_psf_flags_tinystep' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_psf_flags_constraint_r' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_psf_flags_constraint_q' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_dev_flux_flags' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_dev_flags_psffactor' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_dev_flags_badcorr' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_dev_flags_maxiter' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_dev_flags_tinystep' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_dev_flags_constraint_r' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_dev_flags_constraint_q' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_dev_flags_largearea' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_exp_flux_flags' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_exp_flags_psffactor' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_exp_flags_badcorr' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_exp_flags_maxiter' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_exp_flags_tinystep' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_exp_flags_constraint_r' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_exp_flags_constraint_q' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_exp_flags_largearea' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_combo_flux_flags' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_combo_flags_psffactor' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_combo_flags_badcorr' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'calib_detected' at row 2   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'calib_psf_candidate' at row 2   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'calib_psf_used' at row 2   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_negative' at row 2   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_badcentroid' at row 2   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'centroid_sdss_flags' at row 2   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_pixel_edge' at row 2   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_pixel_interpolated_any' at row 2   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_pixel_interpolated_center' at row 2   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_pixel_saturated_any' at row 2   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_pixel_saturated_center' at row 2   self.cursor.execute(stmt) ",2
"partition package has to detect eups-related boost
partition package doesn't detect eups-related boost. This has to be fixed by using sconsUtils, or hand-made procedure.  {code:bash} [fjammes@lsst-dev lsstsw]$ export LD_LIBRARY_PATH=""$LSSTSW/anaconda/lib:$LD_LIBRARY_PATH"" [fjammes@lsst-dev lsstsw]$ setup boost 1.55.0.1+1 [fjammes@lsst-dev lsstsw]$ rebuild partition            partition:  ok (0.5 sec).                boost:  ok (0.3 sec).               python:  ok (0.3 sec).                scons:  ok (0.4 sec). # BUILD ID: b49               python: master-gcbf93ab65b (already installed).                scons: 2.1.0+8 (already installed).                boost: 1.55.0.1+1 (already installed).            partition: master-gf2ef2cf2dc ERROR (1 sec). *** error building product partition. *** exit code = 1 *** log is in /lsst/home/fjammes/src/lsstsw/build/partition/_build.log *** last few lines: :::::  scons: Reading SConscript files ... :::::  Checking for C++ library boost_system-mt... no :::::  Checking for C++ library boost_system... no :::::  Checking for C++ library boost_thread-mt... no :::::  Checking for C++ library boost_thread... no :::::  Checking for C++ library boost_filesystem-mt... no :::::  Checking for C++ library boost_filesystem... no :::::  Checking for C++ library boost_program_options-mt... no :::::  Checking for C++ library boost_program_options... no :::::  Missing required boost library! # BUILD b49 completed. {code}",3
"fix handling of nested control objects
Work on the HSC side has revealed some problems with nested control objects being wrapped into config objects.  This is a pull request for those changes (along with writing a unit test for some of them).  Some (but not all of these changes) are part of Trac ticket #3163 (https://dev.lsstcorp.org/trac/ticket/3163), which I'll now close as a duplicate.",2
"Citizen methods should be private and accessible only through a friend interface
The Citizen interface is useful, but it pollutes its derived classes with methods and attributes that can cause confusion later on (I've got a concrete example of that confusion that Perry and I just spent a few days tracking down - Citizen's {{getId()}} was being mistaken for {{SourceRecord.getId()}}).  I think everything Citizen provides should be hidden and only accessible through a friend interface, e.g.: {code} afw::image::Image<float> image(4, 5); daf::base::CitizenAccess::getId(image); {code}  We should also make an effort to ensure that other aspects of Citizen's design don't affect derived classes, perhaps by prefixing an name that could be seen by derived classes with a ""Citizen"" prefix; see https://dev.lsstcorp.org/trac/ticket/2461.",4
"Implement HTCondor dynamic classad solution for Slot based values
The HTCondor team will be updating their HOWTO for managing Slot based classads/dynamic classads set by a cron startd process.  We currently have a technique for  dynamic slot based values that is iinefficient from a negotiation perspective, and we will want to update to a more optimal approach that the HTCondor team plans to provide.",2
"Develop monitoring for identifying Data processed on a Node/in a Slot
To understand the effectiveness with which we are mapping Jobs to Data, it is vital to monitor/record what data has been processed on a given Node, or within a given Slot on a Node.   Under this issue we examine HTCondor monitoring standards like STARTD_HISTORY,  as well as more custom implementation of blackboard type records via  Job Update hooks to be  executed on the execute node (along the lines of OWL.) ",4
"Run HTCondor ClassAds Scenarios with heterogeneous data cache
The initial series of tests with HTCondor ClassAds work with jobs for individual ccds with a single file representing the data dependency  for the job. In this issue we consider the management of multiple types of data dependencies that may have to be cached for jobs (calibrations, templates, catalogs of sources/objects, etc). ",4
"Study ORDER BY support
We don't have a proper implementation of ORDER BY. Actually, to support ORDER BY properly, we really have to manage all the column names, so we would need to have an in-memory list of columns for the table in question. This is because the general case requires us to ORDER BY a column that may not exist in the select list. In this case, we must add it to the select list if it is not there (or apply *). The easiest solution is to only allow ORDER BY if the sort key column exists explicitly in the select list.",4
"Parser ignores syntax after LIMIT
Parser stops after the LIMIT condition, believing that it has a complete select statement. It ignores whatever is afterwards.To fix this, we would need to alter the parser to make sure that there isn't garbage afterwards. So we have to make sure that the only thing acceptable afterwards is a semicolon or a comment. The right thing to do is probably to add a grammar rule for this. The easiest thing is to check to see if the entire string was consumed (give-or-take a semicolon), but this would disallow comments.",4
"Estimate expected counts of unassociated sources 
We need to have an idea how many sources/forcedSource/diaSources that are not associated with any object we will have to deal with in the database. Can we have a rough estimate? (e.g., per DR).",2
"Fine-tune logging messages
Fine-tune log messages in Qserv (what messages are printed, what is the error level, etc)",5
"During scons configure : check if mysql isn't runing
Mysqld can't be configured is its running before configuration step.",1
"Minor possible enhancements in install procedure
Usefull enhancements :  - add swigged target to ""build"" alias (run scons install to see that swigged target are re-builded at install time)  Other possibles enhancements : - manage default (i.e. const.py) for server configuration file ? - state.py : where to save state ? print it to sdtout ? ",7
"Create tutorial on the use of eups
Create a beginner's guide to the use of EUPS for the DM Developer Guide. Describe the capabilities and basic usage for ""everyday"" user and developer activities. Provide pointers to the EUPS reference manual for advanced usage. ",10
"rename git repository qservdata to qserv_testdata
eups package have the same name as their related git repos. Renaming git repos would lead to a more understandable name.  Please note that the qserv-testdata may also be cloned from qservdata, and and qservdata be removed.  New repos will also have to be distributed with lsst-sw tool.",1
"Buildbot CI needs to save manifest file of failed build for later user debug
The manifest file created during a build instance is transient and removed as soon as the next build commences.  Due to that volatility, it's important to save the manifest to some well-known location so that the developer responsible for debug and repair can easily setup the failing environment. The location of the manifest file will be provided to the developer(s) in the failure notification.",4
"Use of HipChat for Buildbot CI failure notifications should be explored
K-T recommended the use of HipChat rather than email when notifying users of a buildbot build failure.  The purpose was twofold: get immediate attention from the developers and help change the culture towards using HipChat more.  This Issue is to explore the feasibility of using HipChat for the notifications.",1
"Better review notification e-mails
Russell writes:  {quote} I think our system for getting code reviewed using JIRA needs some improvements. It seems that people don't always know that they have been assigned to review a ticket. Also, even if I know I have been assigned to review a ticket, I find it hard to find on JIRA.  More concretely, I would like to see these improvements: - Much clearer notification that one has been assigned as a reviewer. Presently the email is quite generic and easy to miss. In fact I find that most JIRA notifications are rather hard to read -- it's not always easy to see what has changed and thus why I should care. The signal to noise ratio is poor.  - By default a user should see which issues they have been assigned as reviewer when they log into JIRA. (If there is a way to reconfigure the dashboard for this, I'd like to know about it, but it really should be the default). One way to fix this, of course, is to reassig the ticket when putting it into review, but we have good reasons to avoid that.  -- Russell {quote}  and I added:  {quote} In fact, you don't know that the ticket has passed into review unless you scroll all the way to the bottom of the comment.  If the comment associated with the change in status is long and you don't scroll all the way down, then you may not know that you were assigned to review.  With Trac, the important information was at the top of the e-mail. {quote}",2
"CSS keys are too fine-grain, consider merging them together (design)
I've seen a lot of sentiment (from Serge and Daniel) to try and combine keys in CSS. Current design has one key for each table-chunk (leading to tens of thousands of keys in production), and small-grain keys like ""nStripes"", ""nSubStripes"", and ""overlap"" in partitioning are each stored in separate key. This issue will capture discussion, decisions, and implementation of a more compact version. ",10
"cleanup extra file names in docstring
Reported by Serge in email:  When using doxygen to document C++ source, you can mark a comment block with just:  {code} /** @file   * Blah blah   */ {code}  in which case doxygen assumes you want the comment block tied to the file it appears in. We seem to have lots of ""@file <fileName>” statements all over the place, which is an extra thing we have to remember to change when renaming files. Is there some reason to do it that way that I’m missing?",1
"cleanup exception code in CSS
Reported by Serge:  In CssException.h you’ve got:  {code} class CssRunTimeException: public std::runtime_error { … }; class CssException_XXXX : public CssRunTimeException { … }; {code}  This is inconsistent (shouldn’t it be CssRunTimeException_XXX, or maybe even CssRunTimeError?), lengthy, violates the LSST C++ naming conventions, and doesn’t match the KvInterface docs, which all still talk about a CssException class that does not exist. Can we consider changing this to something more like:  {code} class CssError : public std::runtime_error class KeyError : public CssError class NoSuchTable : public KeyError class NoSuchDb : public KeyError class AuthError : public CssError class ConnError : public CssError {code}  ? Then we can succinctly throw and catch css::NoSuchTable, css::AuthError etc…",2
"IN2P3: Cloud-computing Platform / OpenStack
CC-IN2P3 offer computing resources via Openstack :  https://indico.in2p3.fr/getFile.py/access?sessionId=2&resId=0&materialId=0&confId=8236  This platform allow to boot virtual machines on multiples linux distribution. It could be a powerfull tool for continuous integration and testing.  Virtual machines are easilly available to all IN2P3 users, only a SSH account on ccage.in2p3.fr is required. ",15
"Prepare a fedora64 openstack image which allow to easily build and test Qserv
Qserv packaging procedure requires to often rebuild Qserv and relaunch integration tests.  IN2P3 Openstack platform offer next virtual machines :  {code:bash} [fjammes@ccage030 ~]$ nova flavor-list {code} | ID | Name              | Memory_MB | Disk | Ephemeral | Swap | VCPUs | RXTX_Factor | Is_Public | | 1  | m1.tiny           | 512       | 0    | 0         |      | 1     | 1.0         | True      | | 15 | cc.windows.small  | 4096      | 20   | 0         |      | 2     | 1.0         | True      | | 16 | cc.windows.xlarge | 8192      | 50   | 0         |      | 4     | 1.0         | True      | | 2  | m1.small          | 2048      | 10   | 20        |      | 1     | 1.0         | True      | | 3  | m1.medium         | 4096      | 10   | 40        |      | 2     | 1.0         | True      | | 4  | m1.large          | 8192      | 10   | 80        |      | 4     | 1.0         | True      | | 5  | m1.xlarge         | 16384     | 10   | 160       |      | 8     | 1.0         | True      | | 6  | cc.lsst.medium    | 4096      | 20   | 40        |      | 2     | 1.0         | False     | | 7  | cc.lsst.large     | 16384     | 20   | 160       |      | 8     | 1.0         | False     | | 9  | cc.lsst.xlarge    | 40000     | 20   | 160       |      | 20    | 1.0         | False     |  cc.lsst.xlarge would allow a quick build/test of new Qserv release.",5
"Reduce and comment client configuration file
Client configuration file '~/.lsst/qserv.conf) is used by integration test procedure.  Next improvments are required : 1. use templates in it 2. client config file should retrieve templated values from mother config   file""",1
"Upgrade various external packages
I note that the LSST and HSC versions of external packages are slightly out of sync.  I propose uprevving the LSST packages to match as HSC has tested these versions.  {quote} cfitsio               3.360             HSC cfitsio               3310+2            sims Winter2014 current b4 b5 b6 b3 doxygen               1.8.2+2           sims b4 Winter2014 current b5 b6 b3 doxygen               1.8.5             HSC eigen                 3.1.1+2           Winter2014 current b5 b6 b3 b4 eigen                 3.2               HSC fftw                  3.3.2+2           Winter2014 current b5 b6 b3 b4 fftw                  3.3.3             HSC gsl                   1.15+2            Winter2014 current b5 b6 b3 b4 gsl                   1.16              HSC minuit2               5.22.00+2         Winter2014 current b5 b6 b3 b4 minuit2               5.28.00           HSC mysqlclient           5.1.65+3          Winter2014 current b5 b6 b3 b4 mysqlclient           5.1.73            HSC pyfits                3.1.2+2           sims b4 Winter2014 current b5 b6 b3 pyfits                3.2               HSC scons                 2.1.0+7           sims b4 Winter2014 current b5 b6 b3 scons                 2.3.0             HSC sqlite                3.7.14+2          Winter2014 current b5 b6 b3 b4 sqlite                3.8.2             HSC wcslib                4.14        wcslib                4.14+3            b4 Winter2014 current b5 b6 b3 xpa                   2.1.14+2          Winter2014 current b5 b6 b3 b4 xpa                   2.1.15            HSC {quote} ",20
"Rewrite secondary index system and merge empty chunks functionality
We'd like to rewrite the indexing system so that it doesn't depend on a narrow innodb table. We'd also like to fold in the emptychunks functionality.  secondary index: dirColumn->(chunkId, subChunkId) emptychunks: hasChunk = chunkID -> bool  danielw has a dirt simple implementation of the ""create"" portion of the secondary index.",30
"Rendering an IR node tree should produce properly parenthesized output
It appears that rendering a tree of IR nodes doesn't always result in correct generation of parentheses. Consider the following tree: {panel} * OrTerm ** BoolFactor *** NullPredicate **** ValueExpr ***** ValueFactor: ColumnRef(""refObjectId"") ** BoolFactor *** CompPredicate **** ValueExpr ***** ValueFactor: ColumnRef(""flags"") **** Token(""<>"") **** ValueExpr ***** ValueFactor: Const(""2"") {panel}  which corresponds to the SQL for: ""refObjectId IS NULL OR flags<>2"". If one prepends this (via {{WhereClause.prependAndTerm()}}) to the {{WhereClause}} obtained by parsing ""... WHERE foo!=bar AND baz<3.14159;"" and renders the result using {{QueryTemplate}}, one obtains:      {{... WHERE refObjectId IS NULL OR flags<>2 AND foo!=bar AND baz<3.14159}}  This is equivalent to      {{... WHERE refObjectId IS NULL OR (flags<>2 AND foo!=bar AND baz<3.14159)}}  which doesn't match the parse tree - one should obtain:      {{... WHERE (refObjectId IS NULL OR flags<>2) AND foo!=bar AND baz<3.14159}}  This issue involves surveying all IR node classes and making sure that they render parentheses properly. {color:gray}(One way we might test for this is to parse queries containing parenthesized expressions where removal of the parentheses changes the meaning of the query. This would give us some IR that we can render to a string and reparse back into IR. If the rendering logic is correct, one should obtain identical IR trees).{color} Other possibilities that might explain the behavior above is that the input tree is somehow invalid or that {{WhereClause.prependAndTerm}} creates invalid IR.",8
"Implement abstract base class for approximated or interpolated fields
The user of an Approximate or Interpolate object doesn't care which of these they have, once the object has been constructed, and we should make these inherit from a common base class that only contains an interface for accessing the interpolated/approximated function while making no assumptions about its functional form.  The new class will represent a scalar field defined over an integer bounding box, and will have methods for evaluating the field at a point and creating an image of the field.  We could also consider giving it arithmetic interoperability with Image.  I don't have a strong candidate for the name; I've called it ""BoundedField"" on the HSC side but would like that to be revisited.  Adding derived classes to replace the functionality currently in the Approximate and Interpolate classes will be on a separate issue.  The main work on this issue is tweaking the design on the HSC side and getting signoff on the final design from LSST developers; the actual coding should be minimal, as it's just an interface and we already have a good starting point for it.  The HSC-side issue (which also includes work that is part of DM-1124) is here: https://hsc-jira.astro.princeton.edu/jira/browse/HSC-796 and the associated git commits are here: https://github.com/HyperSuprime-Cam/afw/compare/releases/S14A_0...tickets/DM-796",10
"Use geom eups package for installing geometry
Use geom eups package instead of downloading geometry.py during Qserv configuration step.",3
"Qserv release (12.04) - final build, testing and cutting release
nan",4
"Simplify (script) install procedure
Install procedure described in READMEs.txt is complex and error prone.  It could be encapsulated in two scripts :  1. the first for installing Qserv current version in the eups stack, following the LSST official install procedure, 2. the second, developer-oriented, for installing Qserv from a git repository to the eups stack installed during 1.  This two scripts logs could be colorized for better ergonomy.  ",3
"Integrate sciSQL in eups
In order to become compliant with eups, sciSQL install process may have to be refactored  :  - provide a scons build and install target which build and install sciSQL binary (.i.e. reduce waf tool features) - provide a configuration procedure (scisql-deploy.py) which install UDF in MySQL datadir, and deploy .so file in MySQL plugin directory.  Then it has to be packaged in eups format.",9
"Replacing boost system lib with eups libs breaks scons build
While detecting boost, Qserv build system checks for both system lib and then eups lib. This procedure use next code :  {code:python} class BoostChecker:     def __init__(self, env):         self.env = env         self.suffix = None         self.suffixes = [""-gcc41-mt"", ""-gcc34-mt"", ""-mt"", """"]         self.cache = {}         pass      def getLibName(self, libName):         if libName in self.cache:             return self.cache[libName]          r = self._getLibName(libName)         self.cache[libName] = r         return r      def _getLibName(self, libName):         state.log.debug(""BoostChecker._getLibName() LIBPATH : %s, CPPPATH : %s"" % (self.env[""LIBPATH""], self.env[""CPPPATH""]))         if self.suffix == None:             conf = self.env.Configure()              def checkSuffix(sfx):                 return conf.CheckLib(libName + sfx, language=""C++"", autoadd=0) {code}  and this last line run next gcc command :  {code:bash} g++ -o .sconf_temp/conftest_10.o -c -g -pedantic -Wall -Wno-long-long -D_FILE_OFFSET_BITS=64 -fPIC -I/data/fjammes/stack/Linux64/protobuf/master-g832d498170/include -I/data/fjammes/stack/Linux64/boost/1.55.0.1/include -I/data/fjammes/stack/Linux64/zookeeper/master-gc48457902f/c-binding/include -I/data/fjammes/stack/Linux64/mysql/master-g5d79af2a50/include -I/data/fjammes/stack/Linux64/antlr/master-gc05368a54f/include -I/data/fjammes/stack/Linux64/xrootd/master-gfc9bfb2059/include/xrootd -Ibuild -I/data/fjammes/stack/Linux64/anaconda/1.8.0/include/python2.7 .sconf_temp/conftest_10.cpp g++ -o .sconf_temp/conftest_10 .sconf_temp/conftest_10.o -L/data/fjammes/stack/Linux64/xrootd/master-gfc9bfb2059/lib -L/data/fjammes/stack/Linux64/protobuf/master-g832d498170/lib -L/data/fjammes/stack/Linux64/antlr/master-gc05368a54f/lib -L/data/fjammes/stack/Linux64/zookeeper/master-gc48457902f/c-binding/lib -L/data/fjammes/stack/Linux64/mysql/master-g5d79af2a50/lib -L/data/fjammes/stack/Linux64/boost/1.55.0.1/lib -lboost_regex-mt scons: Configure: yes {code}  As the ""-mt"" suffix is searched before the empty suffix, previous command succeed.In my example boost_regex-mt is a system lib. When launching ""scons build"", then CheckLib only looks for boost in /data/fjammes/stack/Linux64/boost/1.55.0.1/lib, not in /usr/lib/. This behaviour is eups-correct, but prevents to find boost_regex-mt.  In this example, a trivial solution is to reverse self.suffixes in python code, but a better solution would be to prevent g++ to use default search paths (e.g. : /usr/lib and /usr/include) in the second command. Is it possible to to it with scons ?  Mario, did you meet the same problem with sconsUtils ?    Thanks  Fabrice",3
"Update obs_decam for new CameraGeom
The obs_decam package worked on by Paul and Andy B. needs to be updated to reflect changes in the camera geometry.",6
"Stretch: Data and test script specification for daily/automated QA/integration tests
[I have renamed this Epic to better capture the current planned scope]          Per discussions I am pulling this Epic into 02C.01.02 [""my"" WBS]    The intent for it is to cover the science and scientific programming portion activities associated with setting up a daily automated QA/integration run.     The infrastructure activities to enable this  are covered by other Epics. It is a stretch because it will be primarily worked on by the Tucson Scientist, still to be identified ICW Tucson Scientific Programmer, still to be identified.     JK: In PMCS this would be New Hire LS6",70
"Identify Data Set for MiniProduction
nan",3
"Generate data for MiniProduction
Assuming that the data needed for mini-production are to be simulated, the input files need to be created and simulated.  The input data then need to be put in a repo with appropriate calibrations.",5
"Create scripts to run MiniProduction
Write command line script to run mini production.  The scripts should be able to be handed directly to the orca layer.",3
"Eliminate local pixel indexing; always use parent instead
As Jim Bosch notes in <https://dev.lsstcorp.org/trac/wiki/Winter2014/Bosch/Miscellaneous>:  Our image classes currently handle two different pixel coordinate systems which differ only by the offset commonly referred to as ""xy0"". The PARENT coordinate system puts the first pixel in the image at xy0; the LOCAL coordinate system always puts the first pixel at (0,0).  Our general rule has long been ""always pay attention to xy0"", which implies PARENT, but the Image class itself doesn't: in many operations, including subimage accessors and bbox getters, LOCAL is the default, whereas in others - particularly the pixel and iterator accessors - there isn't even an option to use PARENT. IMO this is the main source of image-offset bugs in the code.  We plan to address this issue as follows: * Modify the methods on image-like objects that return pixel iterators or locators based x and/or y index, so that they use parent coordinates. * Eliminate the ImageOrigin enum argument, which appears in image-like constructors, image-like getBBox methods. * Eliminate the image origin string argument in the butler's get and put methods for image-like objects.  At the end of this transition all indices will be PARENT indices; the concept of LOCAL will be gone. This will reduce ambiguity and eliminate an argument that is a source of confusion.  Furthermore, at Jim's suggestion and K-T's agreement, we plan to do this in two basic stages: * Rename methods or otherwise make changes that should break any code using LOCAL indices. Fix the code accordingly. * Rename methods back and eliminate the ImageOrigin argument. Fix the code accordingly.",53
"Exception naming convention
The naming convention for exceptions in pex_exceptions is quite redundant.  This issue will make the convention more compact and update all packages that make use of pex_exceptions.",5
"Evaluate moving to C++11 for .cc files
Check that {{C\+\+11}} works on .cc files.  Make {{C\+\+11}} the default in SconsUtils.",5
"Improve afw::CameraGeom::utils code
Some of the utility code in CameraGeom was not completely ported in W13 and documentation is in need of updating.",3
"Determine scope of XY0 convention update
It's unclear exactly how much effort will be involved in making a change to how the XY0 is used.  If the parent/child argument is removed completely this change could be quite invasive and wide reaching.",2
"Does SWIG 3.x work with DM stack?
This task is to evaluate whether we can use SWIG 3.x (and is it stable enough).  Assuming SWIG 3.x can be used with the stack, evaluate how much work will be involved in moving over to use it by default.",10
"Create scripts to assess and report MiniProduction quality.
Write a command line script to assess the quality of the mini production run.  This will involve comparing output data to data produced using a standard stack.  The script will provide a report.",7
"Create script to clean up after a MiniProduction run
The run of a mini production will produce an output repository.  It's likely that we will not want to save all output data.  A script to clean up and potentially save parts of the repo is needed.",3
"Package log4cxx
Fabrice, can you package log4cxx? I should have asked you earlier, sorry I waited so long, not it becoming urgent! Bill is almost done with his logging prototype and will be turning it into a real package, and we need to have log4cxx packages. Many thanks.  log4cxx version 0.10.0, which was released in 4/3/2008 but is still undergoing ""incubation"" at Apache. ",2
"XLDB in Rio
Prepare for and attend XLDB-South America in Rio.",15
"XLDB-2015 report
Writing the report, most work done by Daniel, with input from Jacek and K-T.",8
"Researching partnership opportunities
nan",8
"Restructure and package logging prototype
Restructure and package log4cxx-based prototype (currently in branch u/bchick/protolog). It should go into package called ""log""",8
"Access patterns for data store that supports data distribution 
Data distribution related data store includes things like. chunk --> node mapping, locations of chunk replicas, runtime information about nodes (and maybe also node configuration?). Need to understand access patterns - who needs to access, how frequently etc. ",5
"research mysql cluster ndb
Checkout mysql cluster ndb from the perspective of data distribution - could it be potentially useful to store data related to data distribution?",2
"Automated test should optionally ignore column headers
Some types of queries (like COUNT(*)) may return different column headers in qserv and mysql. This differences break our automated tests which dump and compare complete result including headers. It looks like we will not be able to guarantee that qserv can be made to return the same column headers as mysql except for providing aliases in the query itself. Running those queries without aliases is a legitimate use case so it would be nice to have an option in the test runner which ignores headers for some queries.",4
"Disable failing test cases in automated tests
There are currently 4 test cases failing in out automated tests. Until we have a fix we want to disable them.",1
"JOIN queries are broken
Running a simple query that does a join:  {code} SELECT s.ra, s.decl, o.raRange, o.declRange FROM   Object o JOIN   Source s USING (objectId) WHERE  o.objectId = 390034570102582 AND    o.latestObsTime = s.taiMidPoint; {code}  results in czar crashing with: {code} 2terminate called after throwing an instance of 'std::logic_error'   what():  Attempted subchunk spec list without subchunks. {code}  This query has been taken from integration tests (case01, 0003_selectMetadataForOneGalaxy.sql) ",3
"Understand DCR amplitudes using realistic distribution of stars
nan",4
"Determine refraction amplitudes as a function of SED
nan",2
"Model refraction amplitudes as a function of SED
nan",2
"Determine DCR ampltiudes as a function of SED
nan",2
"Model DCR amplitudes as a function of SED
nan",2
"Determine requirements on atmospheric measurements to predict refraction/DCR
nan",2
"Put together requirements to model refraction/DCR for a given source
nan",8
"SQL injection in czar/proxy.py
Running automated tests for some queries I observe python exceptions in czar log which look like this: {code} 20140529 19:47:19.364371 0x7faacc003550 INF <py> Query dispatch (7) toUnhandled exception in thread started by <function waitAndUnlock at 0x18cd8c0> Traceback (most recent call last):   File ""/usr/local/home/salnikov/qserv-master/build/dist/lib/python/lsst/qserv/czar/proxy.py"", line 78, in waitAndUnlock     lock.unlock()   File ""/usr/local/home/salnikov/qserv-master/build/dist/lib/python/lsst/qserv/czar/proxy.py"", line 65, in unlock     self._saveQueryMessages()   File ""/usr/local/home/salnikov/qserv-master/build/dist/lib/python/lsst/qserv/czar/proxy.py"", line 87, in _saveQueryMessages     self.db.applySql(Lock.writeTmpl % (self._tableName, chunkId, code, msg, timestamp))   File ""/usr/local/home/salnikov/qserv-master/build/dist/lib/python/lsst/qserv/czar/db.py"", line 95, in applySql     c.execute(sql)   File ""/u2/salnikov/STACK/Linux64/mysqlpython/1.2.3+8/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/cursors.py"", line 174, in execute     self.errorhandler(self, exc, value)   File ""/u2/salnikov/STACK/Linux64/mysqlpython/1.2.3+8/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/connections.py"", line 36, in defaulterrorhandler     raise errorclass, errorvalue _mysql_exceptions.ProgrammingError: (1064, ""You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'r' AND sce.tract=0 AND sce.patch='159,3';', 1401410839.000000)' at line 1"") ok 0.000532 seconds {code}  I believe this is due to how query string is being constructed in czar/proxy.py: {code:py} class Lock:      writeTmpl = ""INSERT INTO %s VALUES (%d, %d, '%s', %f);""  # ...................             self.db.applySql(Lock.writeTmpl % (self._tableName, chunkId, code, msg, timestamp)) {code}  If {{msg}} happens to contain quotes then resulting query is broken. One should not use Python formatting to construct query strings, instead the parameters should be passed directly to {{cursor.execute()}} method. ",2
"Zookeeper times out
I noticed running some queries, leaving system up and them returning few hours later and running more queries can result in:  {code} ZOO_ERROR@handle_socket_error_msg@1723:  Socket [127.0.0.1:12181] zk retcode=-4, errno=112(Host is down):  failed while receiving a server response {code}  It needs to be investigated (if we can reproduce) ",3
"Provide a ""stackfitCalib"" for coadds
Coadds have a Psf class that returns the proper sum of input PSFs at a point.  We need the same functionality for the Calib object associated with the Coadd Exposure.  This will require making a Calib a baseclass (it currently doesn't have a virtual dtor (although it does have virtual protected members)) ",4
"PhotoCalTask doesn't return information about which stars were used in calibration
The PhotoCalTask returns numpy arrays of the source and reference fluxes (and errors) of matched ""good"" photometric objects, typically stars.  However, while estimating the zero point, it clips outliers so the actual list of objects used is shorter.  Please add another output to the returned struct, a numpy Bool array ""good"", to indicate which objects are kept. ",1
"Cleanup in core/examples and core/doc
- core/examples and core/doc seems to be out of data.  Some cleanup here would be welcome.",1
"Define filter for Epics and mapping of Epics to WBS
nan",2
"qserv have to use boost from stack
To quote Jacek and KT: {code} Andy, re dm-751, KT says never use the system version.  J. {code}  So we need to switch qserv to eups-boost. This should be easy once DM-751 is done, just add boost to qserv.table. Then one can remove conditional part of {{BoostChecker}} which works with system-installed boost. ",1
"Simplify copying tables while adding columns
Currently, if I want to copy a table while adding a few columns (as specified by schema in the example) I need to do something like: {code}         cat = afwTable.SourceCatalog(schema)         cat.table.defineCentroid(srcCat.table.getCentroidDefinition())         cat.table.definePsfFlux(srcCat.table.getPsfFluxDefinition())         # etc.          scm = afwTable.SchemaMapper(srcCat.getSchema(), schema)         for schEl in srcCat.getSchema():             scm.addMapping(schEl.getKey(), True)          cat.extend(srcCat, True, scm) {code}  Please make this easier!  For example  - by adding a flag to the SchemaMapper constructor that automatically does the addMapping (should this be the default?)  - by making it possible to copy all the slots (maybe this'll be the case when the new alias scheme is implemented?).  Maybe we just need a new method: {code} cat = srcCat.extend(schema) {code} that does all the above steps.",4
"Reimplement C++/Python Exception Translation
I'd like to reimplement our Swig bindings for C++ exceptions to replace the ""LsstCppException"" class with a more user-friendly mechanism.  We'd have a Python exception hierarchy that mirrors the C++ hierarchy (generated automatically with the help of a few Swig macros).  These wrapped exceptions could be thrown in Python as if they were pure-Python exceptions, and could be caught in Python in the same language regardless of where they were thrown.  We're doing this as part of a ""Measurement"" sprint because we'd like to define custom exceptions for different kinds of common measurement errors, and we want to be able to raise those exceptions in either language.",8
"Design Prototypes for C++ Algorithm API
We've never really been happy with the new design for the C++ algorithm API, and Perry and Jim have a few ideas to fix this that need to be fleshed out.  Each of the subtasks of this issue will correspond to a different design idea.  Ideally, for each one, we'll try to do a nearly-complete conversion of the SdssShape algorithm (as a good example of a complicated algorithm) to see how these ideas work in practice.",6
"Algorithm API without (or with optional) Result objects
In this design prototype, I'll see how much simpler things could be made by making the main algorithm interface one that sets record values directly, instead of going through an intermediate Result object.  Ideally the Result objects would still be an option, but they may not be standardized or reusable.",3
"add persistable class for aperture corrections
We need to create a persistable, map-like container class to hold aperture corrections, with each element of the container being an instance of the class to be added in DM-740.  A prototype has been developed on DM-797 on the HSC side: https://hsc-jira.astro.princeton.edu/jira/browse/HSC-797 and the corresponding code can be found on these changesets: https://github.com/HyperSuprime-Cam/afw/compare/32d7a8e7b75da6f5327fee65515ee59a5b09f6c7...tickets/DM-797",2
"implement coaddition for aperture corrections
We need to be able to coadd aperture corrections in much the same way we coadd PSFs.  See the HSC-side HSC-798 and HSC-897 implementation for a prototype: https://hsc-jira.astro.princeton.edu/jira/browse/HSC-798 https://hsc-jira.astro.princeton.edu/jira/browse/HSC-897 with code here: https://github.com/HyperSuprime-Cam/meas_algorithms/compare/d2782da175c...u/jbosch/DM-798 https://github.com/HyperSuprime-Cam/meas_algorithms/compare/c4fcab3251...u/price/HSC-897a https://github.com/HyperSuprime-Cam/pipe_tasks/compare/6eb48e90be12d...u/price/HSC-897a",3
"reduce code- and object-duplication in aperture correction and PSF coaddition
The aperture correction code to be added on DM-833 will likely not be as closely integrated with CoaddPsf as it could be, because the original design of CoaddPsf didn't anticipate the addition of other, similar classes.  We should work on allowing these classes to share code.",8
"design Array fields for table version 1
While we're trying to eliminate the need for compound fields in afw::table, Arrays present a few problems.  We could use FunctorKeys, the way we plan to use other compound fields, but here we need to guarantee that the per-element keys are contiguous, and we also be able to support views.  We also need to determine the naming scheme.  Finally, we need to make sure these work with aliases and slots.",6
"Rewrite multiple-aperture photometry class
We've never figured out how to handle wrapping multiple-aperture photometry algorithms.  They can't use the existing Result objects - at least not out of the box.  We should try to write a new multiple-aperture photometry algorithm from the ground up, using the old ones on the HSC branch as a guide, but not trying to transfer the old code over.  The new one should:  - Have the option of using elliptical apertures (as defined by the shape slot) or circular apertures.  - Have a transition radius at which we switch from the sinc photometry algorithm to the naive algorithm (for performance reasons).",2
"Rename methods that return pixel iterators and locators in image-like classes, and change to use parent indexing
Image-like classes have a large number of methods that return pixel iterators and locators based on row and/or column. We wish to change these to use parent indexing (making row and column relative to XY0). As the first step in doing this, rename all these methods and modify them to parent indexing. Renaming will help identify and fix all code that uses these methods.",6
"Change code so ImageOrigin must be specified (temporary)
Image-like classes have a getBBox method and various constructors that use an ImageOrigin argument which in most or all cases defaults to LOCAL. As the first stage in cleaning this up, try to break code that uses the default as follows: * Remove the default from getBBox(ImageOrigin) so an origin must be specified. * Change the default origin of constructors to a temporary new value UNDEFINED  * Modify code that uses image origin to fail if origin is needed (it is ignored if bbox is empty) and is UNDEFINED.  Note: this is less safe than changing constructors to not have a default value for origin, because the error will be caught at runtime rather than compile time. However, that is messy because then the bounding box will also have to be always specified, and possibly an HDU, so it would be a much more intrusive change.",2
"Change data butler I/O of image-like objects to require imageOrigin if bbox specified (temporary)
As part of making PARENT the default for image origin, change the data butler to require that imageOrigin be specified if bbox is specified when reading or writing image-like objects.  Note: this ticket turns out to be unnecessary, as all the few necessary change are done as part of DM-840.",2
"Update code that uses pixel iterators and accessors
Fix all code that uses pixel iterators and locators as per the changes in DM-839. This affects roughly 60 files distributed over many packages.",16
"Restore names of methods that return pixel iterators and locators
Restore the names of methods that return pixel iterators and pixel locators on image-like classes. (This is part of the final stage of eliminating LOCAL pixel indexing).",2
"Eliminate ImageOrigin argument
Eliminate the ImageOrigin enum and argument from image-like classes.",2
"Eliminate image origin argument from butler for (un)persisting image-like objects
Eliminate the image origin argument for butler get and put when dealing with image-like objects.",2
"Change code to always specify image origin (temporary)
The next step after DM-840 is to change all code that uses image-like getBBox and constructors that take an image origin argument to always specify the origin (DM-840 will break any code that tries to use a default image origin).  This results in a working stack after DM-840.  Neither this nor DM-840 will be merged to master, though it can be used by HSC and other users as they transition to a default image origin of PARENT.  See DM-1176 for the final step.",10
"Update code to use restored names for methods that return pixel iterators and locators
For all code changed in DM-842, change it again to use the restored names for methods that return pixel iterators and locators.  This task is much simpler than DM-842 because it is merely renaming methods. Nonetheless, it touches many files in many packages.",4
"Specify ImageOrigin = PARENT in all cases
For all code that uses the ImageOrigin argument (either explicitly or using a bbox and the default value) change the code to use an explicit ImageOrigin of PARENT.  In other words, update all code to match the changes in DM-840 and DM-841.  This affects 40-ish modules in many packages. I am concerned about testing the modified code because we are missing unit tests for so many tasks. But we can do some data processing and check those results. I will want some help with that.",12
"Eliminate use of ImageOrigin argument
For all code that uses the ImageOrigin argument, eliminate the use of that argument. In other words, update all code to match the changes in DM-844 and DM-845,  This affects all the code affected by DM-848, plus any code that was already using ImageOrigin=PARENT, but is a simpler change.",5
"duplicate column name when running near neighbor query
Running a simplified version of near neighbor query on test data from case01:  {code} SELECT DISTINCT o1.objectId, o2.objectId FROM   Object o1,         Object o2 WHERE  scisql_angSep(o1.ra_PS, o1.decl_PS, o2.ra_PS, o2.decl_PS) < 1   AND  o1.objectId <> o2.objectId {code}  Result in an error on the worker:  {code} Foreman:Broken! ,q_38f9QueryExec---Duplicate column name 'objectId' Unable to execute query: CREATE TABLE r_13237cd4cfc9e0fa01497bcf\ 67a91add2_6630_0 SELECT o1.objectId,o2.objectId FROM Subchunks_LSST_6630.Object_6630_0 AS o1,Subchunks_LSST_6630.Object_6630_0 AS o2\  WHERE scisql_angSep(o1.ra_PS,o1.decl_PS,o2.ra_PS,o2.decl_PS)<1 AND o1.objectId<>o2.objectId; {code}  It is fairly obvious what is going on. ""SELECT t1.x, t2.x"" is perfectly valid, but if we add ""INSERT INTO SELECT t1.x, t2.x"", we need to add names, eg. something like ""INSERT INTO SELECT t1.x as x1, t2.x as x2""",8
"Passing error messages from czar to end user - design
nan",3
"Provide a detailed integration tests report
Test output is very low level. Indeed only a verbose logfile and SLQ queries output are currently available. Furthermore failing queries (i.e. .sql.FIXME) aren't launched.  This ouput could be leveraged to a detailed html web report (using sbadmin for example : http://startbootstrap.com/templates/sb-admin/) which could :  - launch all queries, even failing ones, - print Qserv services status after each query, - execution time for each query, - information about MySQL and Qserv results differences, - href to Qserv services log files - all other interesting information about queries execution    ",10
"apr and apt_util packages do not install shared library
When we installed apr and apr_utils packages (as a dependency of new log4cxx package, see DM-772) we discovered that both these packages only build static libraries but no shared libs are installed. This is problematic if mixed with shared libs and we use shared libs everywhere else. We certainly need to build shared libs for these packages, this ticket is to follow up on this problem.",2
"near neighbor does not return results
A query from qserv_testdata (case01/queries/1051_nn.sql) runs through Qserv, but it returns no results, while the same query run on myql does return results.  The exact query for qserv is:   {code} SELECT o1.objectId AS objId  FROM Object o1, Object o2  WHERE qserv_areaspec_box(0, 0, 0.2, 1)  AND scisql_angSep(o1.ra_PS, o1.decl_PS, o2.ra_PS, o2.decl_PS) < 1 AND o1.objectId <> o2.objectId; {code}",1
"disable extraneous warnings from boost (gcc 4.8)
Compiling qserv on ubuntu 14.04 (comes with gcc 4.8.2) results in huge number of warnings coming from boost. We should use the flag ""-Wno-unused-local-typedefs"".",1
"XLDB Rio - workshop report
nan",3
"XLDB - strategic positioning
Discussions with strategic partners. Improving website and adding new context (community, speakers). 1-pager document",3
"W'14 newinstall.sh picks up wrong python?
newinstall.sh fails with:  Installing the basic environment ...  Traceback (most recent call last):   File ""/tmp/test_lsst/eups/bin/eups_impl.py"", line 11, in ?     import eups.cmd   File ""/tmp/test_lsst/eups/python/eups/__init__.py"", line 5, in ?     from cmd        import commandCallbacks   File ""/tmp/test_lsst/eups/python/eups/cmd.py"", line 38, in ?     import distrib   File ""/tmp/test_lsst/eups/python/eups/distrib/__init__.py"", line 30, in ?     from Repositories import Repositories   File ""/tmp/test_lsst/eups/python/eups/distrib/Repositories.py"", line 8, in ?     import server   File ""/tmp/test_lsst/eups/python/eups/distrib/server.py"", line 1498     mapping = self._noReinstall if outVersion and outVersion.lower() == ""noreinstall"" else self._mapping                                  ^ SyntaxError: invalid syntax  Perhaps from running the wrong version of python.  Full script/log is attached. ",1
"lsst_dm_stack_demo
lsst-dm_stack_demo has obsolete benchmark files (circa Release 7.0)  which fail to serve the purpose of validating, for the user, the correct functioning of a freshly built Release v8.0 stack.   At the very least,  the benchmark files should be regenerated for each official Release. Tasks:   (1) Build the benchmark files for Release v8.0  (2) Debate (a) recommending the  use of 'numdiff'  to check if the output is within realistic bounds.   Or, (b) develop another procedure to better show how the current algorithms compare to the algorithms used at the benchmarked Release. (3) Depending on result of the debate on #2: for: (a) provide appropriate 'numdiff' command invocation in manual.; for (b) implement the new procedure.",40
"Segmentation fault from writing dotted FITS header keywords
Observed on HSC, but I'm not aware of any fix for this on the LSST side either.  {code:sh} pprice@tiger3:~ $ gdb python GNU gdb (GDB) Red Hat Enterprise Linux (7.2-60.el6_4.1) (gdb) r Starting program: /tigress/HSC/products-20130212/Linux64/python/2.7.6/bin/python  >>> from lsst.afw.image import ExposureF >>> exp = ExposureF(1,1) >>> exp.getMetadata().add(""A.B.C.D"", 12345) >>> exp.writeFits(""test.fits"")  Program received signal SIGSEGV, Segmentation fault. lsst::daf::base::PropertySet::combine (this=0x1eda370, source=...)     at src/PropertySet.cc:746 746	        if (dj == _map.end()) { (gdb) w #0  lsst::daf::base::PropertySet::combine (this=0x1eda370, source=...) at src/PropertySet.cc:746 #1  0x00002aaabac068dc in lsst::daf::base::PropertyList::deepCopy (this=0x1ddd4c0) at src/PropertyList.cc:74 #2  0x00002aaab7ba559e in lsst::afw::image::MaskedImage<float, unsigned short, float>::writeFits (this=0x1f1d8f0, fitsfile=..., metadata=<value optimized out>, imageMetadata=..., maskMetadata=..., varianceMetadata=...) at src/image/MaskedImage.cc:569 #3  0x00002aaab7b0a7f0 in lsst::afw::image::Exposure<float, unsigned short, float>::writeFits (this=0x1f1d8d0, fitsfile=...) at src/image/Exposure.cc:251 #4  0x00002aaab7b0a9fc in lsst::afw::image::Exposure<float, unsigned short, float>::writeFits (this=0x1f1d8d0, fileName=""test.fits"") at src/image/Exposure.cc:239 #5  0x00002aaab6b91614 in _wrap_ExposureF_writeFits__SWIG_0 (self=<value optimized out>, args=<value optimized out>) at python/lsst/afw/image/imageLib_wrap.cc:160786 #6  _wrap_ExposureF_writeFits (self=<value optimized out>, args=<value optimized out>) at python/lsst/afw/image/imageLib_wrap.cc:29886 	… {code}  This appears to only affect dotted keywords (which should be supported through use of {{HIERARCH}}, and even if they're not supported, this should never fail with a segfault).",4
"Optimize template engine used in configuration tool
string.Template and string interpolation are quite weak, some additional feature would be welcomed :  - interpolation interprets wrongly ""%d"" in template files as template - string.template doesn't manage correctly non referenced template parameters present in template files.",4
"Use an existing qserv_run_dir with a new Qserv instance/binary
Here's what should be added to qserv-configure.py :  - edit $QSERV_RUN_DIR/admin/qserv.conf and change Qserv instance dir to current one (which qserv-configure.sh), - check compliance of QSERV_RUN_DIR with new Qserv instance (version check ?) and/or update configuration files, - re-initialize services, if needed, without breaking already loaded data. ",4
"unset BASH_ENV in newinstall.sh or surrounding instructions
Nutbar users such as myself may have BASH_ENV set, which can mess with your carefully constructed shell environments.  Unsetting BASH_ENV should help.  This could perhaps go in newinstall.sh, or the documentation alongside where we suggest they unset other variables: https://confluence.lsstcorp.org/display/LSWUG/Building+the+v8.0+LSST+Stack+from+Source",1
"Update PhoSim tutorial to use CatSim for creating instance catalogs
Update the Process PhoSim Images tutorial (https://confluence.lsstcorp.org/display/LSWUG/Process+PhoSim+Images) to use CatSim to generate instance catalogs, once CatSim documentation is available. This will obviate the need for the helper script refCalCat.py. ",2
"SourceDetectionTask should only add flags.negative if config.thresholdParity == ""both""
The SourceDetectionTask always adds ""flags.negative"" to the schema (if provided) but it is only used if config.thresholdParity == ""both"".  As adding a field to a schema requires that the table passed to the run method have that field this is a significant nuisance when reusing the task.  Please change the code to only modify the schema if it's going to set it. ",1
"Provide Task documentation for DipoleMeasurementTask
See Summary. ",2
"Provide Task documentation for PsfMatchTask
See Description (it's currently called PsfMatch) ",4
"Provide Task documentation for ImagePsfMatchTask
See summary",2
"Provide Task documentation for SnapPsfMatchTask
See summary",2
"Provide Task documentation for AssembleCcdTask
See summary",4
"Provide Task documentation for IsrTask
See Summary",4
"Provide Task documentation for SourceDeblendTask
See Summary",2
"Provide Task documentation for CmdLineTask
See Summary",4
"Provide Task documentation for RepairTask
See Summary",4
"How to write your own command line task, including how-to-retarget sub-tasks
Please provide documentation on how to write a command line task, and how to retarget tasks (e.g. reusing bits of IsrTask for a new camera)  The documentation should include a complete annotated example.  ",4
"Improve install/configuration/tests documentation and migrate it to reST format
This ticket propose to migrate README and README-devel to reST format (see http://sphinx-doc.org/rest.html). The output is located here : http://lsst-web.ncsa.illinois.edu/~fjammes/qserv-doc/  Furthermore this ticket wil integrate Andy S. DM-622 value-added remarks about Qserv embedded documentation. {quote} README.txt needs a bit of formatting, whole ""NOTE FOR DEVELOPERS"" is one long line which may need scrolling depending on what do you use to read the file, same applies to README-devel.txt The install procedure in README.txt implies that the whole stack has to be installed including eups. If people have some part of it installed already the it would probably be better to reuse existing stack. Shall we spit install instructions into ""Install eups (if not installed already)"" and ""Install qserv""? README-devel.txt says ""Once Qserv is installed..."", I don't think that we need or want to install whole qserv before we start development (what if qserv is not available yet for the platform I'm trying to test). What probably needed is installed dependencies, and this should be covered by the comments before 'setup -r .' {quote} ",1
"Photometric calibration uses a column ""flux"" not the specified filter unless a colour term is active
The photometric calibration code uses a field ""flux"" in the reference catalog to impose a magnitude limit.  If a colour term is specified, it uses the primary and secondary filters to calculate the reference magnitude, but if there is no colour term it uses the column labelled ""flux"" and ignores the filtername.    Please change the code so that ""flux"" is ignored, and the flux associated with filterName is used.",1
"Replace getXXXKey for slots with returning functorKeys similar to existing compound Keys
Make any changes in the Functor Key capabilities necessary to support getXXXKey, getXXXErrKey, and getXXXFlagKey for the 3 different types of slots.   Change these routines in Source.h.m4 to return FunctorKeys for both version 0 and version 1 tables.  Then fixup any compilation breaks on the C++ size which this causes.  Remove the version 1 specific accessors.   I am assigning this to Jim to confirm that the first part is done, then he can assign the remainder to Perry either by reassigning, or as a subtask.",4
"Package xrootd-4.0.0rc3-qsClient2 with eups
nan",1
"Prevent conflict related to non-unique temporary files created during SciSQL install
SciSQL install sometime fails with next message :  {code:bash} [1/2] MySqlScript: scripts/install.mysql [2/2] MySqlScript: scripts/demo.mysql Running testHtm                          : OK Running testSelect                       : OK Running testAngSep.py                    : OK Running testMedian.py                    : OK Running testPercentile.py                : OK Running testS2CPoly.py                   : OK Running testS2PtInBox.py                 : OK Running testS2PtInCircle.py              : OK Running testS2PtInEllipse.py             : OK Running docs.py                          : FAIL [see /usr/local/home/salnikov/qserv-run/u.fjammes.DM-622-g86a30ec72a/tmp/scisql-0.3.2/build/tools/docs.log]   9 tests passed, 1 failed, and 0 failed to run    One or more sciSQL unit tests failed make: *** [install] Error 1 -- CRITICAL: Error code returned by command : / u s r / l o c a l / h o m e / s a l n i k o v / q s e r v - r u n / u . f j a m m e s . D M - 6 2 2 - g 8 6 a 3 0 e c 7 2 a / t m p / c o n f i g u r e / s c i s q l . s h   $ cat /usr/local/home/salnikov/qserv-run/u.fjammes.DM-622-g86a30ec72a/tmp/scisql-0.3.2/build/tools/docs.log rm: cannot remove `/tmp/scisql_demo_ccds.tsv': Operation not permitted Failed to run documentation example:     rm -f /tmp/scisql_demo_ccds.tsv     ERROR 1086 (HY000) at line 4: File '/tmp/scisql_demo_ccds.tsv' already exists {code}  It looks like test uses non-unique temporary file name and someone already tried to run installation on the same host. ",2
"Improve management of qserv-run-dir in start/stop scripts.
qserv-start.sh by default tries again to use /usr/local/home/salnikov/qserv-run/2014_05.0 directory. There may be a confusion if we have multiple run directories. Would it be better to install qserv start/stop scripts into run directory itself so that they don't need to guess anything?",2
"Add Doxygen documentation on rebuilds
Master-branch doxygen documentation should be rebuild on every full master build.",20
"Move qserv-testdata.py to qserv_testdata package
Andy S. suggests this during DM-622 review, but this needs to be studied deeply, as qserv_testdata goal is to store large datafile, difficult to version in git.",3
"Clearer and shorter output for qserv-configure.py
qserv-configure.py produces a lot of output which could be confusing if people try to look at it and understand it. It may be better to reduce it to something that just indicates that each step is completed successfully.",1
"Buildbot email should state if the build used master only or included other branches
Buildbot build status report currently doesn't state if the build was only for master or included other branches requested by the user.",4
"Use aliases to clean up table version transition
The addition of schema aliases on DM-417 should allow us to clean up some of the transitional code added on DM-545, as we can now alias new versions of fields to the old ones and vice versa.",2
"Move table versions to Schema
We've added a version number to afw::table::BaseTable to help with the transition to a new approach with different naming conventions and FunctorKeys instead of compound keys.  However, it looks like Schema objects need to know about this version number as well, in order to change Subschema objects to split/join using underscores instead of periods.  That means we'll have to move the version down into the schema object, which may affect a lot of downstream code.  Other than touching a lot of code, the changes should be trivial.",4
"measAlg.interpolateOverDefects doesn't accept a python list of Defects
The python binding of interpolateOverDefects should accept a python list of Defects as well as a swig-wrapped std::vector<Defect>; it doesn't (try using a python list in test818 in meas_algorithms/tests/Interp.py)  ",1
"Include aliases in Schema introspection
Schema stringification and iteration should include aliases somehow.  Likewise the extract() Python methods.",1
"fix int/long conversion on 32-bit systems and selected 64-bit systems
tests/wrap.py fails in pex_config on 32-bit systems and some 64-bit systems (including Ubuntu 14.04) with the following: {code:no-linenum} tests/wrap.py  ...EE.E. ====================================================================== ERROR: testDefaults (__main__.NestedWrapTest) Test that C++ Control object defaults are correctly used as defaults for Config objects. ---------------------------------------------------------------------- Traceback (most recent call last):   File ""tests/wrap.py"", line 89, in testDefaults     self.assert_(testLib.checkNestedControl(control, config.a.p, config.a.q, config.b))   File ""/home/boutigny/CFHT/stack_5/build/pex_config/tests/testLib.py"", line 987, in checkNestedControl     return _testLib.checkNestedControl(*args) TypeError: in method 'checkNestedControl', argument 2 of type 'double'  ====================================================================== ERROR: testInt64 (__main__.NestedWrapTest) Test that we can wrap C++ Control objects with int64 members. ---------------------------------------------------------------------- Traceback (most recent call last):   File ""tests/wrap.py"", line 95, in testInt64     self.assert_(testLib.checkNestedControl(control, config.a.p, config.a.q, config.b))   File ""/home/boutigny/CFHT/stack_5/build/pex_config/tests/testLib.py"", line 987, in checkNestedControl     return _testLib.checkNestedControl(*args) TypeError: in method 'checkNestedControl', argument 2 of type 'double'  ====================================================================== ERROR: testReadControl (__main__.NestedWrapTest) Test reading the values from a C++ Control object into a Config object. ---------------------------------------------------------------------- Traceback (most recent call last):   File ""tests/wrap.py"", line 82, in testReadControl     config.readControl(control)   File ""/home/boutigny/CFHT/stack_5/build/pex_config/python/lsst/pex/config/wrap.py"", line 212, in readControl     __at=__at, __label=__label, __reset=__reset)   File ""/home/boutigny/CFHT/stack_5/build/pex_config/python/lsst/pex/config/wrap.py"", line 217, in readControl     self.update(__at=__at, __label=__label, **values)   File ""/home/boutigny/CFHT/stack_5/build/pex_config/python/lsst/pex/config/config.py"", line 515, in update     field.__set__(self, value, at=at, label=label)   File ""/home/boutigny/CFHT/stack_5/build/pex_config/python/lsst/pex/config/config.py"", line 310, in __set__     raise FieldValidationError(self, instance, e.message) FieldValidationError: Field 'a.q' failed validation: Value 4 is of incorrect type long. Expected type int For more information read the Field definition at:   File ""/home/boutigny/CFHT/stack_5/build/pex_config/python/lsst/pex/config/wrap.py"", line 184, in makeConfigClass     fields[k] = FieldCls(doc=doc, dtype=dtype, optional=True) And the Config definition at:   File ""/home/boutigny/CFHT/stack_5/build/pex_config/python/lsst/pex/config/wrap.py"", line 131, in makeConfigClass     cls = type(name, (base,), {""__doc__"":doc})   ---------------------------------------------------------------------- Ran 8 tests in 0.017s  FAILED (errors=3) {code}  There is a partial fix on u/jbosch/intwrappers; this seems to work for Ubuntu 14.04, but not on 32-bit systems.",2
"qserv-configure.py is broken in master
It looks like there was a bug introduced either during the merge of DM-622 with master or right before that. Running {{qserv-configure.py}} from master fails now: {code} $ qserv-configure.py    File ""/usr/local/home/salnikov/qserv-master/build/dist/bin/qserv-configure.py"", line 229     (""Do you want to update user configuration file (currently pointing                                                                       ^ SyntaxError: EOL while scanning string literal {code} I assign this to myself, Fabrice is on vacation now and we need to fix this quickly.",1
"Create a ""stub"" package that checks for system dependencies for Qserv
Create a special ""stab"" package that checks and reports missing dependencies.",2
"setup standard aliases for frequently-used measurements
Frequently-used measurement fields such as classification and pixel flags should have shortened aliases that can be used instead of their full, package-qualified versions.  In some cases, these may serve as a sort of slot (e.g. we may have multiple classifications algorithms someday).  In this issue, we should audit all current measurement algorithm fields that don't already have a slot that works for them and consider whether there should be a standard alias.  We also need to work out a system for defining these aliases, probably in the config for SingleFrameMeasurementTask.",4
"Detailed documentation for meas_base tasks
We should follow RHL's example for detailed task documentation and document all meas_base tasks.",2
"Documentation audit and cleanup for meas_base plugins
Many meas_base Plugins and Algorithms have poor documentation, including several whose documentation is a copy/paste relic from some other algorithm.  These need to be fixed.",2
"add base class for measurement tasks
We should consider adding a base class for measurement tasks (SingleFrameMeasurementTask, ForcedMeasuremedTask) that includes the callMeasure methods.  I'm hoping this will help cleanup callMeasure and improve code reuse.",1
"Add FunctorKey to replace Coord compound keys
Unlike previous FunctorKey replacements, Coord compound Keys are used in the minimal schema for SimpleTable and SourceTable, which makes removing them problematic.",1
"convert measurement algorithms in ip_diffim
ip_diffim includes a few measurement algorithms which need to be converted to the new framework.",5
"convert measurement algorithms in meas_extensions_shapeHSM
This is a low-priority ticket to replace the old-style plugins in meas_extensions_shapeHSM with new ones compatible with meas_base.  As this isn't a part of the main-line stack, we should delay it until other the meas_base conversion is nearly (or perhaps fully) complete.",3
"convert meas_extensions_photometryKron to new measurement framework
This is a low-priority ticket to replace the old-style plugins in meas_extensions_photometryKron with new ones compatible with meas_base.  As this isn't a part of the main-line stack, we should delay it until other the meas_base conversion is nearly (or perhaps fully) complete.",3
"Finish the Log packaging
Finish the work started by Bill (DM-778)",6
"allow partial measurement results to be set when error flag is set
We need to be able to return values at the same time that an error flag is set.  The easiest way to do this is to have Algorithms take a Result object as an output argument rather than return it.  We'll revisit this design later. ",2
".my.cnf in user HOME directory breaks setup script
Presence of {{.my.cnf}} file in the user HOME directory crashes {{qserv-configure.py}} script if parameters in {{.my.cnf}} conflict with parameters in {{qserv.conf}}.  How to reproduce: * create .my.cnf file in the home directory: {code} [client] user     = anything # host/port and/or socket host     = 127.0.0.1 port     = 3306 socket   = /tmp/mysql.sock {code} * try to run {{qserv-configure}}, it fails with error: {code} /usr/local/home/salnikov/qserv-run/u.salnikov.DM-595/tmp/configure/mysql.sh: connect: Connection refused /usr/local/home/salnikov/qserv-run/u.salnikov.DM-595/tmp/configure/mysql.sh: line 13: /dev/tcp/127.0.0.1/23306: Connection refused ERROR 2003 (HY000): Can't connect to MySQL server on '127.0.0.1' (111) {code}  It looks like {{~/.my.cnf}} may be a left-over from some earlier qserv installation. If I remove it and re-run {{qserv-configure.py}} now it's not created anymore. Maybe worth adding some kind of protection to {{qserv-configure.py}} in case other users have this file in their home directory.",2
"add query involving a blob to the integration tests
We need to add a query (or more?) to the qserv_testdata that involve blobs. Blobs are interesting because they might break some parts of the qserv if we failed to escape things properly etc. ",2
"improve message from qserv_testdata
Currently, when I try to run qserv-benchmark but qserv_testdata was not setup, I am getting  {code} CRITICAL Unable to find tests datasets. -- FOR EUPS USERS : Please run :    eups distrib install qserv_testdata    setup qserv_testdata FOR NON-EUPS USERS : Please fill 'testdata_dir' value in ~/.lsst/qserv.conf with the path of the directory containing tests datasets or use --testdata-dir option. {code}  It is important to note in the section for eups users that this has to be called BEFORE qserv is setup, otherwise it has no effect. ",1
"make slot config validation more intelligent
Slot config validation currently assumes that field names match plugin names, which is not always a safe assumption.  This can prevent certain algorithms from being used in slots.  We probably can't do this validation in Config.validate(); we need to check in the measurement Task constructor after the schema has already been constructed.",1
"Make NoiseReplacer a context manager
I think the API for NoiseReplacer could be made more idiomatic (and possibly safer) by turning it into a ""context manager"" (i.e. so it can be used with the ""with"" statement).",1
"Adapt integration test to multi-node setup v1
Following DM-595 we can start qserv in multi-node configuration. Next step is to be able to run integration tests in that setup. This needs a bit of understanding how to distribute chunks between all workers in a cluster and how to load data in remote mysql server.",10
"rename config file(s) in Qserv
Rename local.qserv.cnf to qserv-czar.cnf. It is quite likely there are some other config files that would make sense to rename. If you see some candidates, let's discussion on qserv-l and do the renames.",1
"Modify assertAlmostEqual in ip_diffim subtractExposures.py unit test
In unit test, the comparison     self.assertAlmostEqual(skp1[nk][np], skp2[nk][np], 4)   fails.  However if changed to    self.assertTrue(abs(skp1[nk][np]-skp2[nk][np]) < 10**-4)  which is the desired test, this succeeds.   This ticket will remove all assertAlmostEquals from subtractExposure.py and replace with the fundamental comparison operator of the absolute value of the differences.",1
"Research Apache Mesos and Google Kubernetes
It'd be good to check out Apache Mesos and Google Kubernetes and see how relevant they are for Qserv / LSST Database ",4
"Provide Task documentation for ModelPsfMatchTask
See Description (it's currently called PsfMatch) ",2
"Migrate Qserv worker code to the new logging system
nan",8
"fix names of meas_base plugins to match new naming standards
Some meas_base plugins still have old-style algorithm names.",1
"Remove use of compound fields in minimal schema
If we want to ultimately remove compound fields, we need to remove the ""coord"" field from the SimpleTable/SourceTable minimal schema, and provide a different way to get ra,dec from a source.",4
"remove temporary workaround in new SkyCoord algorithm
SingleFrameSkyCoordPlugin is using the Footprint Peak, not the centroid slot.  According to comments in the code, this is a workaround for some problem with centroids.  This needs to be fixed.",1
"Classification should set flags upon failure
The classification algorithm claims it can never fail.  It can, and should report this.",2
"Detailed documentation for GaussianCentroid
We need more detailed documentation for the GaussianCentroid algorithm, in terms of how it actually computes the centroid.  We (Jim and Perry) have done what we can, but we need help from whoever actually wrote it (RHL, we think) to provide the rest.  In particular:  - Additional detail should be filled in in the class Doxygen for GaussianCentroidAlgorithm, in GaussianCentroid.h  - The ""noPeak"" flag field description and name should be compared to what the algorithm actually does with it.  It looks to me like it's a bit misnamed (and maybe shouldn't be considered an error condition at all, if we want to run this on difference images), but I'm not sure.",1
"convert GaussianFlux to use shape, centroid slots
We should cleanup and simplify the GaussianFlux algorithm to simply use the shape and centroid slot values instead of either computing its own or having configurable field names for where to look these up.",1
"Detailed documentation for SdssCentroid
We needed detailed documentation for how SdssCentroid actually works.  I think it involves fitting something like a symmetrized PSF model, but I'm sufficiently unsure of the details that RHL should write this, not me.  It should go in the Doxygen class docs for SdssCentroidAlgorithm in SdssCentroid.h in meas_base.",1
"fix testForced.py
testForced.py is currently passing even though it probably should be failing: it's trying to get centroid values from a source which has neither a valid centroid slot or a Footprint with Peaks (I suspect because transforming a footprint might remove the peaks).  Prior to DM-976, that would have caused a segfault; on DM-976, I've turned it into an exception, which is then turned into a warning by the measurement framework.",2
"Fix incorrect eupspkg config for astrometry_net
The clang patch from 8.0.0. version was (correctly) deleted. However, the patch identity was still left in the eupspkg config's protocol.  This will delete the last vestige of the formerly necessary clang patch.",2
"fix warnings related to libraries pulled through dependent package
This came up during migrating qserv to the new logging system, and it can be reproduced by taking log4cxx, see DM-983, essentially:  {code} eups distrib install -c log4cxx 0.10.0.lsst1 -r http://lsst-web.ncsa.illinois.edu/~becla/distrib -r http://sw.lsstcorp.org/eupspkg {code}  cloning log package (contrib/log.git), building it and installing in your stack, and finally taking the branch u/jbecla/DM-207 of qserv and building it.  The warnings looks like:  {code}/usr/bin/ld: warning: libutils.so, needed by /usr/local/home/becla/qservDev/Linux64/log/1.0.0/lib/liblog.so, not found (try using -rpath or -rpath-link) /usr/bin/ld: warning: libpex_exceptions.so, needed by /usr/local/home/becla/qservDev/Linux64/log/1.0.0/lib/liblog.so, not found (try using -rpath or -rpath-link) /usr/bin/ld: warning: libbase.so, needed by /usr/local/home/becla/qservDev/Linux64/log/1.0.0/lib/liblog.so, not found (try using -rpath or -rpath-link) {code}  and they show up when I build qserv package, and are triggered by the liblog. I suspect sconsUtils deal with that sort of issues, but since we have our own scons system for qserv it is not handled. Fabrice, can you try to find a reasonable solution for that? Thanks!",1
"make use of MeasurementDataFlags
We started adding a system to allow algorithms to declare what kind of data they can run on, but never really put it in place.  To do this, we should:  - Add more flags (at least NO_CALIB).  - Pass these flags in pipe_tasks and other places with tasks that use measurement tasks as subtasks (for instance, we should set NO_WCS and NO_CALIB during calibrate.initialMeasurement, and COADD in processCoadd).  - Add checks for these flags in appropriate algorithms.  For instance, PsfFlux should fail in construction if NO_PSF is set, and PeakLikelihoodFlux should fail if PRECONVOLVED is not set.",6
"qserv-version.sh produces incorrect version number
I have just installed qserv on a clean machine (this is in a new virtual machine running Ubuntu12.04) which got me version 2014_07.0 installed: {code} $ eups list qserv    2014_07.0    current b76 $ setup qserv $ eups list qserv    2014_07.0    current b76 setup $ echo $QSERV_DIR /opt/salnikov/STACK/Linux64/qserv/2014_07.0 {code}  but the {{qserv-version.sh}} script still thinks that I'm running older version: {code} $ qserv-version.sh 2014_05.0 {code}",2
"""source"" command is not in standard shell
{{qserv-start.sh}} script fails when installed on Ubuntu12.04: {code} $ ~/qserv-run/2014_05.0/bin/qserv-start.sh /home/salnikov/qserv-run/2014_05.0/bin/qserv-start.sh: 4: /home/salnikov/qserv-run/2014_05.0/bin/qserv-start.sh: source: not found /home/salnikov/qserv-run/2014_05.0/bin/qserv-start.sh: 6: /home/salnikov/qserv-run/2014_05.0/bin/qserv-start.sh: check_qserv_run_dir: not found {code}  It complains about {{source}} command. {{source}} is not standard POSIX shell command, it is an extension which exists in many shells. Apparently in older Ubuntu version {{/bin/sh}} is stricter about non-standard features.   To fix the script one either has to use standard . (dot) command or change shebang to {{#!/bin/bash}}. This of course applies to all our executable scripts.",2
"W15 Refactoring of Qserv
Refactoring Qserv code to make it cleaner, better, and most importantly more robust and resilient to failures.  JK: Refer to loading spreadsheet for PMCS assignments",84
"W15 Integration Testing of Qserv
Improvements to the integration tests suite for Qserv, including making it more generic (it currently uses a hardcoded database name), integrating new partitioner, and adding a data set from one of the last data releases.  JK: Refer to loading spreadsheet for PMCS assignments",19
"W15 Design Metadata Store for production tracking (v1)
Come up with a design of Metadata Store that will track all information in the Science Data Archive (across Data Releases), for both image and database repositories  JK: Refer to loading spreadsheet for PMCS assignments",45
"W15 Initial version of image and file archive
Build initial (alpha) version of system for tracking existing image data sets. (a) Build a web form where users enter information about existing data sets. (b) Design and build a MySQL backend (maybe through DataCat). (c) Implement crawler that automatically fetches metadata from FITS headers for data set registered by users.   Note that support for additional formats (eg config files) will be added in the future, not through this epic. Note that this initial version will not be production ready, we will make it more bullet-proof and feature rich during S15 cycle.  JK: Refer to loading spreadsheet for PMCS assignments",30
"W15 Butler (v2) for local data sets
Improvements and tweaks to the butler as needed based on the feedback from the Apps team  JK: Refer to loading spreadsheet for PMCS assignments",19
"W15 Management of distributed Qserv databases and tables
This includes tasks such as creating and deleting distributed databases managed by Qserv, creating and deleting partitioned tables based on the information that is stored in the CSS. Design and exploratory prototype.  JK: Refer to loading spreadsheet for PMCS assignments",39
"W15 Loading data into distributed Qserv-managed databases
Improvements to the existing data loader, to simplify data loading into Qserv. That includes integration with information in the CSS. This version will include non-parallel, single-node loader.  Relevant document (describing a sketch of much more advanced version): https://dev.lsstcorp.org/trac/wiki/db/Qserv/DataLoading  JK: Refer to loading spreadsheet for PMCS assignments",28
"S15 Implement Query Mgmt in Qserv
Initial version of system for managing queries run through qserv. This includes capturing information about queries running in Qserv. Note, we are not dealing with query cost estimate here, (it will be covered through DM-1490).",40
"eliminate confusing config side-effects in CalibrateTask
CalibrateTask does some unexpected things differently if you configure it certain ways, because it perceives certain processing as only being necessary to feed other steps.  In particular, if you disable astrometry and photometric calibration, it only runs measurement once, because it assumes the only purpose of the post-PSF measurement is to feed those algorithms.  This (as well as poor test coverage) made it easy to break CalibrateTask in the case where those options are disabled a few branches back.  After conferring with Simon and Andy, we think the best solution is to remove this sort of conditional processing from CalibrateTask, which should also make it much easier to read.  Instead, we'll always do both the initial and final phase of measurement, even if one of those phases is not explicitly being used within CalibrateTask itself.",1
"Create a permanent and accessible mapping of the BB# and the bNNN. 
Create a permanent and accessible mapping of the BB# and the bNNN.   The users are interested in the BB# since is is used to point to the STDIO file form the entire stack build. The bNNN is needed because the daily life of the developer revolves around the stack tagged alternately by the bNNN tags and/or the DM Release tags. ",2
"W15 Central State System, support for db/table/query metadata
Improvements to the CSS system to make it more thread safe, more compact representation of keys, and adding information that will be needed to handle distributed databases, tables and likely queries (if we decided to use CSS for Query management).  JK: Refer to loading spreadsheet for PMCS assignments",49
"W15 Image Cutout Service
First version of the images cutout service. It will rely on butler. The bulk of the work in this epic includes building a web-based front-end.  JK: Refer to loading spreadsheet for PMCS assignments",19
"Add / improve tests and examples for Log package
nan",1
"init.d/qserv-czar needs LD_LIBRARY path
With the addition of log we now need to find some shared libraries from stack. Current version of qserv-czar init.d script does not capture LD_LIBRARY_PATH, so we should add it there. ",1
"Remove unnecessary pieces from qserv czar config
The config file for the qserv czar has some items that are no longer relevant, and in this issue, we focus on the ones that are clearly the responsibility of our qserv css.  This ticket includes: -- removing these items from the installation/configuration templates -- removing these items from sample configuration files -- removing these items from the code that reads in the configuration file and sets defaults for these items -- fixing things that seem to break as a result of this cleanup.  danielw volunteers to assist on the last item, as needed.  ",2
"fix SubSchema handling of ""."" and ""_""
SubSchema didn't get included in the rest of the switch from ""."" to ""_"" as a field name separator.  As part of fixing this, we should also be able to simplify the code in the slot definers in SourceTable.",1
"track down difference in SdssShape implementation
The meas_base version of SdssShape produces slightly different outputs from the original version in meas_algorithms, but these should be identical.  We should understand this difference rather than assume its benign just because it's small.",2
"S15 Data Distribution & Replica Mgmt Design
This is a continuation of DM-779, continue research related to data distribution and replication. There are still many option questions: should it be centralized or peer-to-peer? Is bittorent useful? How much should be home grown vs off-the-shelf (and which off-the-shelf). There are no direct deliverables in FY2015, but it is a complex topic, so we should not wait until the last minute. This epic covers the R&D / design.",100
"W15 Qserv Stability / bug fixes
Tweaks, improvements, fixes to bugs discovered over the course of W15, to improve Qserv stability.  JK: Refer to loading spreadsheet for PMCS assignments ",83
"move algorithm implementations out of separate subdirectory
We should move the code in the algorithms subdirectory (and namespace) into the .cc files that correspond to individual algorithms.  They should generally go into anonymous namespaces there.  After doing so, we should do one more test to compare the meas_base and meas_algorithms implementations.",1
"audit and clean up algorithm flag and config usage
Check that meas_base plugins and algorithms have appropriate config options and flags (mainly, check that there are no unused config options or flags due to copy/paste relics).",1
"switch default table version to 1
Now that all tasks that use catalogs explicitly set the table version, it should be relatively straightforward to set the default version to 1 in afw.  Code that cannot handle version > 0 tables should continue to explicitly set version=0.",2
"Switch default measurement tasks to meas_base
We should set the default measurement task in ProcessImageTask to SingleFrameMeasurementTask, and note that SourceMeasurementTask and the old forced photometry drivers are deprecated.",2
"create forced wrappers for algorithms
We have multiple algorithms in meas_base which could be used in forced mode but have no forced plugin.  We should go through the algorithms we have implemented and create forced plugin wrappers for these.",1
"remove old forced photometry tasks
After meas_base has been fully integrated, remove the old forced photometry tasks from pipe_tasks",1
"Measurement - Calibration and Ingest
Create command-line tasks to transform raw measurement quantities (e.g. fluxes, positions in pixels) to global units (e.g magnitudes and positions in celestial coordinates).  Also requires interfacing with the measurement plugin system, as only plugins can be authoritative on how to transform their outputs.  JK: In PMCS this would be Bosch J 50% Gee P 50%",20
"convert afw::table unit tests to version 1
Most afw::table unit tests explicitly set version 0.  We should change these to test the new behaviors, not the deprecated ones.",2
"Audit TCT recommendations to ensure that all standards updates were installed into Standards documents.
Audit TCT recommendations to ensure that all standards updates were installed into Standards documents.  It was found that the meeting recorded in: [https://dev.lsstcorp.org/trac/wiki/Winter2012/CodingStandardsChanges] failed to include two recommendations:   * recommended: 3-30: I find the Error suffix to be usually more appropriate than Exception. ** current: 3-30. Exception classes SHOULD be suffixed with Exception.  * recommended but not specifically included: Namespaces in source files: we should use namespace blocks in source files, and prefer unqualified (or less-qualified) names within those blocks over global-namespace aliases. ** Rule 3-6 is an amalgam of namespace rules which doesn't quite have the particulars desired. FYI: The actual vote was to:  ""Allow namespace blocks in source code (cc) files.""  To simplify the future audit, all other recommendations in that specific meeting were verified as installed into the standards.",2
"add batch flag to newinstall.sh
In some cases (installing in some script environment) it is nice to be able to run without any interaction.  This will add a flag to tell the script to install without asking about anaconda and git.  It will assume the answer is 'yes' to any question it would have asked.",1
"Organize the content of the DM Developer Guide
The DM Developer Guide is presently a place-holder for information relevant to developers. It is time to re-organize the content, and to provide authoritative guidance to developers. This will involve contributions from various members of the DM Team, and likely some selected harvesting of the Trac/Wiki. ",10
"Fix overload problems in SourceCatalog.append and .extend
This example fails with an exception: {code:py} import lsst.afw.table as afwTable schema = afwTable.SourceTable.makeMinimalSchema() st = afwTable.SourceTable.make(schema) cat = afwTable.SourceCatalog(st) tmp = afwTable.SourceCatalog(cat.getTable()) cat.extend(tmp) {code}  Expected behavior is that the last line is equivalent to {{cat.extend(tmp, deep=False)}}.",1
"Update the DMS and Astro Glossaries
The DMS and Astro Glossaries in Confluence define a set of technical terms used in their respective domains. Some of the definitions are placeholders, and other terms used in the SWUG, DM Space, and DM Developer Guide have yet to be defined in one glossary or the other. It is time to update these docs.",2
"Determine problem with Mac OS X processCCD output when compared to 'identical' dataset generated on RHEL6
The benchmark file for the summer2012 demo is generated on  RHEL6. When using the dataset to compare stack output generated on Mac OSX 10.8.5 (and 10.9)  there are significant deviations.   Find out where the problem arises...in the stack during processing or in the comparison.",4
"Research how to kill query in mysql
In the port to the new Xrootd Ssi API, worker-side squashing was lost in the shuffle. The plumbing is different, and re-implementing squash functionality is not entirely straightforward, especially because the new API is still missing documentation and examples for implementing cancellation.  The consequences of not implementing this are minor--some extra work may be done by the worker, but not a whole lot, because user-level cancellation has not been implemented.  First step is to understand how mysql code is handling query killing.",4
"Investigate HTCondor config settings to control speed of ClassAd propagation
With default settings we do not have good visibility as to whether an updated ClassAd on a compute node (e.g., CacheDataList now has ccd ""S00"") will be in effect on the submit node in time for a Job to be matched to an optimal HTCondor node/slot.   There are several components (negotiator, schedd, startd) and their associated activities that could impact the time that it takes for a new ClassAd on a worker node to 'propagate' back to the submit side. We investigate these configuration settings to try to determine what thresholds for configuration settings are required to meet a given time cadence of job submissions.",2
"Enhance SWUG chapter on measurement
A basic description of measurement in the LSST Stack was created for the SWUG (see DM-692). However, this chapter lacks specifics about how the measurement algorithms work and does not yet mention some of the available algoritms. This task is intended to provide the next level of detail, with references to source-level descriptions that exist or are in progress. ",6
"afw::table - finish interface transition
JK: In PMCS Bosch J and Gee P",36
"Measurement - Convert Old Algorithms
JK: In PMCS Bosch J and Gee P",31
"Measurement - Finish Framework Overhaul
JK: In PMCS Bosch J and Gee P",51
"Design unit-test data package
Gather requirements for unit test datasets and determine how to organize and generate the data.",4
"Generate test data
Write scripts to generate the test data (as needed) and run them.",10
"Custom mapper for unit test data package
We need a custom mapper for the test data package, so we can run unit tests against it without depending on any particular obs_* package.",6
"Convert existing tests to use new package
After the new unit test data package is available, we should convert all tests that currently use afwdata or obs_test to use the new package instead, and then remove obs_test and afwdata.  Should have subtasks and a re-estimate of effort after auditing how much there is to be done.",30
"afw - Footprint Improvements
This epic contains several improvements to the Footprint class, including API refactoring, performance-related reimplementations, and some new features.  JK: In PMCS this would be Bosch J and Swinbank J",30
"Galaxy Fitting - Shear Precision Experiments
NB some of the below stories will actually be Lauren, not Perry.  Breakdown: pgee 58%; jbosch 21%; krughoff 6%; lauren 15%",100
"Minimal MultiFit system prototype
Experiment with a framework for running ""multifit""-style fitting algorithms (i.e. fit a model convolved with the PSF to multiple exposures simultaneously).    Earlier prototypes (meas_multifit) have demonstrated executable multifit code on a single system: repeating that work is not the priority here. Rather, we will produce a concept for the framework for running multifit at scale using large datasets on a cluster. It is not a requirement that it be possible to run an at-scale multifit job and produce useful results by the end of this epic; however, there should be proof-of-concept code which demonstrates the structures within which future multifit work will be carried out. This will be supplied to both Science Pipelines and Process Middleware developers.",100
"Galaxy Fitting - Sampling Algorithm Plugin
nan",60
"CModel robustness
The CModel code is currently a mixture of work ported from HSC and some new development on LSST. It needs to be validated on real data (HSC, SDSS, CFHT). All known failure modes should be eliminated. Where possible, improve its performance.",65
"Create utilities to allow camera testing team to use CameraGeom
The camera team can use the CameraGeom classes to reduce lab data for testing purposes.  Since the camera is relatively flexible, a good way of doing this is to create a camera at runtime from the header keys in the files to be reduced.    JK: In PMCS this would be Krughoff S",30
"Make the API for ISR explicit
The run method of the IsrTask currently takes a dataRef which has getters for calibration products.  This makes the task hard to re-use because one needs a butler and because the interface is opaque.  This task will make the IsrTask API more transparent.  JK: In PMCS this would be Krughoff S",20
"Re-write astrometry task to remove dependency on astrometry.net
The astrometry task currently depends as astrometry.net as the only possible algorithmic back end.  This also has created a dependency on the astrometry.net file format.    This task will remove the dependency on astrometry.net file format and will implement the current HSC astrometry solver in the astrometry task.  This will focus on solving single chips, not the full focal plane.    Note that DM-167 (which was closed with reference to this ticket) requests that we split photometric and astrometric catalogues, and this is still needed.    JK: In PMCS this would be Krughoff S and Owen R",80
"Command-line Tasks - Unit tests for ProcessCcd
We can get a long way toward improved test coverage in pipe_tasks by having Unit tests for ProcessCcd.  JK: In PMCS this would be Owen R",20
"Add numpy typemaps to afw interfaces so numpy types pass correctly to C++
Currently SWIGed interfaces do not handle numpy types correctly.  This can be fixed with the appropriate typemap.  JK: In PMCS this would be Owen R",11
"Extend Exposure classes to contain background models
Exposures carry many things: PSFs, FITS metadata, Detectors, etc.  Another object algorithms may want to access with an exposure is the set of background models subtracted from the exposure to that point.  This will involve porting afw.math.BackrgoundList to C++ and extending Exposure classes to contain the new BackgroundList object.  JK: In PMCS this would be Krughoff S",20
"Improve documentation of code from DM-70
DM-70 was merged before its code documentation was completely ready, in an effort to not leave such a large amount of code un-merged.  This ticket exists as the second half of DM-70 to clean up its documentation.",6
"Improve fault tolerance as demonstrated in nightly computing simulator
Adjust simulator parameters, mechanisms, and implementation to minimize time to recover and unprocessed/unarchived data resulting from a machine or network failure.  For W15: Pietrowicz, S - 100% Start Oct 2014, finish Nov 2014  For S15 rollover: Pietrowicz, S - 12% Start March 2015, finish April 2015",50
"Simplify and refactor the Event Services API
Remove old fixed metadata; refactor using more generic metadata interface.  For W15: Pietrowicz, S - 100% Start Dec 2014, finish Dec 2014  For S15 rollover: Pietrowicz, S - 11% Start March 2015, finish April 2015  ",18
"Write a log4cxx handler that emits events for log messages
Write a log4cxx handler that emits events for log messages.  For W15: Pietrowicz, S - 100% Start Jan 2015, end mid Jan 2015  For S15 rollover: Pietrowicz, S - 11% Start March 2015, finish April 2015",18
"requirements and specifications for initial version of inter-Task communication
gather requirements and specification for Implementation of  a gather/scatter mechanism for a set of Tasks executing in parallel using the Event Services, providing for eventual extension to support MPI or other communication mechanisms.  Start mid Jan 2015, finish Feb 2015.    JK: In PMCS this would be Pietrowicz S",50
"Chebyshev approximation object for aperture corrections
Using the interface defined in DM-740, implement a Chebyshev-based implementation to be used to interpolate aperture corrections across CCD-level interfaces.  A prototype is available on the HSC fork, which can be copied directly, modulo any interface changes on DM-740.  For more information, see the HSC Jira issue (which also includes the work associated with DM-740): https://hsc-jira.astro.princeton.edu/jira/browse/HSC-796 and the HSC git commits https://github.com/HyperSuprime-Cam/afw/compare/releases/S14A_0...tickets/DM-796",3
"avoid usage of measurement framework in star selectors
At least one of the star selectors uses the old measurement framework system to measure the moments of a cloud of points.  With the new versions of all the measurement plugins, it should be much easier (and cleaner) to just call the SdssShape algorithm directly, instead of dealing with the complexity of applying the measurement framework to something that isn't really an image.",3
"design new Footprint API
This issue is for *planning* (not implementing) some changes to Footprint's interface, including the following:  - make Footprint immutable  - create a separate SpanRegion class that holds Spans and provides geometric operators does not hold Peaks or a ""region"" bbox (Footprint would then hold one of these).  - many operations currently implemented as free functions should be moved to methods  - we should switch the container from vector<PTR(Span)> to simply vector<Span>, as Span is nonpolymorphic and at last as cheap to copy as a shared_ptr.  The output of this issue will be a set of header files that define the new interface, signed off by an SAT design review.  Other issues will be responsible for implementing the new interface and fixing code broken by the change.",8
"implement new Footprint API
This issue is the implementation for DM-1126, including fixing code broken by the API changes.  This issue should be given subtasks for discrete pieces of work as part DM-1126, as it has a lot of story points.  Because it's likely all of this needs to be merged at the same time, it should probably be done on a single branch, unless some of the earlier work can be done in a backwards-compatible way..",20
"Span-based grow operations for Footprint
The current grow operation for Footprints is very inefficient for isotropic grows.  A better algorithm can be found in the attached paper.",10
"Span-based topological set operations for Footprint
Implement span-based overlap tests and spatial union, intersection, and difference operators for Footprints.  Should be split into subtasks; first task would be to come up with the interface, and then each operation to be implemented should have one subtask each.  If complete after DM-1126 and DM-1127, these operations should be implemented in the SpanRegion class, and Footprint should delegate to that.",20
"refactor C++ Algorithm classes
Using one of the prototypes of DM-828, refactor the C++ Algorithm code in meas_base and any other packages.",10
"create PSF simulations for shapelet approximation test
To determine the number and order of shapelet expansions needed to approximate the LSST PSF, we need simulations of LSST PSFs, presumably generated using PhoSim.  I think we want something like 50-100 extremely high-SNR stars, drawn from realistic distributions of anything that would affect the PSF, including position on the focal plane and center position within a pixel.  I only care about getting these out as postage stamps, so I'll leave the question of how best to run PhoSim to get these up to someone else.  While this is part of a mostly-Princeton Epic, I'm assigning this issue to Simon as he'd be able to do it much faster than I could.  I'll also let him review and update the story points estimate.",6
"create galaxy simulations for shapelet approximation and truncation tests
Using the PSF simulations generated in DM-1131 and the GalSim package, generate ~20k galaxies for each of 3 shear values for each input PSF.  Will need to be divided into subtasks - figure out what the outputs should look like, write the code to generate the simulations, estimate the time need to run the simulations, run the simulations at scale, etc. ",30
"implement shear measurement driver for simulations
We'll need driver code to run the galaxy fitting algorithms on the simulations from DM-1132.  Efficiently saving all the samples could be challenging.",10
"implement SDSS PSF residual trick
SDSS galaxy fitting approximated the convolution of a galaxy model with the PSF as the convolution of the galaxy model with a simplified approximation to the PSF added to the difference between the PSF approximation and the true PSF.  We should do the same, as it'll be no worse than ignoring the difference between the PSF approximation and the true PSF, and it may greatly reduce the complexity needed in the approximation.",8
"test how large pixel region used in galaxy fitting needs to be
Using simulations built on DM-1132 and driver code from DM-1133, test different pixel region sizes and shapes, and determine at what point shear bias due to finite fit region drops below a TBD threshold.",20
"test number/order of shapelet expansions needed to approximate PSF
Run driver from DM-1133 on simulations from DM-1132 with different configuration for shapelet PSF approximation, increasing complexity until change in shear estimate drops below a TBD threshold.",14
"Evaluate python/c++ documentation generation and publication tools 
This epic related to documentation that is provided as part of normal development activities. The desire is to keep this documentation in and near the codebase as this is best practice for it being maintainable. At the other end, we wish to publish this documentation in a coherent and searchable way for users. A number of tools exist in this area and this item requires a preliminary evaluation to be made.   This is part of curating our documentation infrastructure.  [FE 75% DOC 100% starting August 20th] ",20
"Demonstrate & iterate with team on documentation toolchain   
Following from DM-1137, this epic relates to demonstrating various options for documentation tools workflows to the team, gathering input as to the preferred solution, adopting a workflow, and defining any specific implementation choices.   This is part of curating our documentation infrastructure. ",5
" Stack documentation infrastructure and migration
[Epic retitled and bumped in points to reflect wider scope and change in resource allocation]    This epic containes work on migrating the documentation infrastructure  into sphinx (from doxygen, Confluence) subject to RFC, continuous deployment of documentation, read-the-docs or  similar presentation, standardisation via teplates of common  information, a proposal for CI-ing examples and tutorials, release and install note documentation, and visual  design and javascript development for UI/UX    An additional request to migrate Word-based design documentation has been accepted. LaTeX support is being investigated.     [JS 100%]    ",63
"Investigate automatic MacOS X build/deploy
This item is to set up the same continuous integration process we have on Linux on a MacOSX test server.   JK: In PMCS this would be Economou F and New Hire LS3",4
"Evaluate merits of alternative CI and RFC
Evaluate whether we wish to continue buildbot development or use a different (or additional) continuous intergration system.  This is part of a continuous occasional process of evaluating whether our current toolchain choices are still meeting our needs.  [FE at 75%, JH at 75%]",30
"Regularise Nightly and Weekly builds 
The intent here is to create two seperate automated deployment environments, one based on a nightly (or ad-hoc) build, one one  aslower cadence (eg weekly). This will allow us to do intergration/QA runs on a bleeding or trailing edge as required.   JK: In PMCS this would be Economou F and New Hire LS3",6
"Investigate candidates for Verification and Integration Data Sets
The task here is to develop a data set that can be used both for continuous integration (build tests) and automatic QA (integration tests). We want to maximise the richness of the data set in terms of its usefulness, but minimise it in terms of its size. DN to co-ordinate contributions.     [DN 95%  FE 5%]",40
"Remove code made obsolete
nan",4
"Create a top-level qserv_distrib package
qserv_distrib will be a meta-package embedding qserv, qserv_testdata and partition.",2
"SUI: Research system framework for SUI development
Current IPAC development utilizes GWT, is it the right system for LSST SUI in 2022? I think this will be an on-going activity for the first two years.     10% of Goldina, Zhang, Ciardi, Surace 20% of Roby, Rector, Ly, Wu 40% Groom",100
"W15 Metadata Store for production tracking (prototype)
Build a first prototype of the Metadata Store.   JK: Refer to loading spreadsheet for PMCS assignments",45
"Fast image search
A lot of users will search for images using some (often advanced) spatial criteria. Need to implement something similar to what we have in UDFs/SciSQL to efficiently support this class of search. Check GIS support in MariaDB",10
"Fix example of IsrTask to be callable with data on disk
Currently the example of the IsrTask takes a fake dataref.  This is hard to use with real data.  In DM-1113 we will update IsrTask to not take a dataRef.  This will make it easy to update the example script to work with real data.  This ticket will also include removing from the unit tests any fake dataRefs that have become unnecessary as a result of DM-1299.  ",2
"Css C++ client needs to auto-reconnect
The zookeeper client in C++ that the czar uses doesn't auto-reconnect. This is a capability provided in the kazoo library that qserv's python layer provides, but isn't provided in the c++ client.  The zookeeper client disconnects pretty easily: if you step through your code in gdb, the zk client will probably disconnect because its threads expect to keep running. zk sessions may expire too. Our layer should reconnect unless there is really no way to recover without assistance from the calling code (e.g. configuration is wrong, etc.).  This ticket includes only basic reconnection attempting, throwing an exception only when some ""reconnection-is-impossible"" condition is met.",2
"Minor problems in lsstsw, related to Qserv offline install procedure
- on lsstsw master branch tip, ./stack/Linux64/lsst/9.2/bin/newinstall.sh doesn't seems to be the last version (it still install eups-1.3.0 instead od eups-1.5.0)   - on Fedora19, flock from util-linux 2.23.1 doesn't support next options : {code:bash} $ flock -w 0 200 flock: timeout cannot be zero {code} but {code:bash} flock -w 1 200 {code} works,  - newinstall.sh : would it be possible to enable automatic answers to git and anaconda install questions. For example, in order to easily enable automatic install on 300 nodes clusters ? (cancelled : covered by DM-1078)  - loadLSST.sh appends automatically http://sw.lsstcorp.org/eupspkg to EUPS_PKGROOT, and if first url in EUPS_PKGROOT isn't available eups fails without trying next ones => this isn't compliant with offline mode and introduce a work-around in Qserv offline mode install scripts. Would it be possible to define a lightly different behaviour for loadLSST.sh ?  - In newinstall.sh, l. 167, if Python version isn't correct, then exit *with error code*. ",3
"SUI web user interface prototype  
SUI team want to use this time to study the DM system and all the other design documents to come up with preliminary design of the SUI infrastructure with some prototyping along the way to help the proof of concept.  40% Wu 30%  Rector, Roby, Ly 10% Zhang, Goldina 80% Ciardi 90% Surace 60% Groom",100
"SUI Interface through Qserv with database and prototype 
Working closely with database group to define the interface needs between SUI and Qserv. Prototyping the functions will help us understand the interface better.  50% Rector 10% Ly 10% Wu",32
"SUI:  Query and display LSST image 
Define the APIs for image query with database group.   Depend on the implementation of APIs  Exercise the image cutout service DM-1977 been developed in SLAC.     ",6
"SUI catalog query interface prototype
Define the APIs for image query with database group  Depend on the implementation of APIs  30% Goldina 20% Ly 10% Wu",34
"SUI image visualization prototype (without searching LSST images)
Using the current software components developed in IPAC to put together a prototype of visualization capabilities. The purpose is to get feedback from DM people and potential users of the tool.  20% Roby 30% Zhang",34
"SUI 2D plot for catalog prototype
Using the current software components developed in IPAC to put together a prototype of visualization capabilities. The purpose is to get feedback from DM people and potential users of the tool.    30% Goldina 10% Ly",38
"SUI catalog and image interactive visualization with LSST data
Using the current software components developed in IPAC to put together a prototype of visualization capabilities. The purpose is to exercise the data access APIs developed by SLAC and get feedback from DM people and potential users of the tool.  20% Goldina, Zhang 10% Roby, Ly, Wu, Ciardi ",20
"Cleanup SdssShape
We should do a comprehensive cleanup of the SdssShapeAlgorithm class.  This includes removing the SdssShapeImpl interface (never supposed to have been public, but it became public) from other code that uses it, and integrating this code directly into the algorithm class.  We should also ensure that the source from which the algorithm is derived is clearly cited -- that's Bernstein and Jarvis (2002, http://adsabs.harvard.edu/abs/2002AJ....123..583B); see also DM-2304.",8
"Port meas_algorithms unit tests for plugins
While test coverage isn't complete, there are many unit tests for measurement plugin algorithms in meas_algorithms that have not yet been ported to meas_base.  We should make sure any of these tests that aren't already covered in meas_base are moved over before we remove the meas_algorithms versions of things.",6
"remove dependency between sconsUtils and eups
sconsUtils currently depends on eups, (and as far as I understand it, it should not...)",4
"experiment with building MyISAM as a shared library
Per Monty (phone call Aug 28, 2014), this should be easy, code to look at is in storage/myisam/mi_test*",2
"zookeeper port numbers should be configurable
The test programs in core/modules/css/ has hardcoded zookeeper port numbers. That needs to be fixed, it needs to be configurable.",1
"User-friendly install/configure/test scripts
- stdout, stderr output should be colorized, whereas redirecting it into a file shouldn't.",3
"FY19 Add Support for ForcedSource Table in Qserv
ForcedSources will needs special handling  * ra/decl columns are not part of ForcedSource table, we will need them for partitioning  * we might want to store them in subchunks? ",53
"Make default image origin PARENT in all cases
After DM-840 is finished, and code updated for it (DM-846), make the new default image origin PARENT, remove the UNDEFINED image origin enum, and update at least some of the code that explicitly specifies PARENT (there is little harm in leaving this explicit, other than it does not set an optimal example).",4
"Implement sharable just-in-time subchunk management on the worker
The current subchunk management on the worker is very simple. When a query fragment is selected for execution, it builds exactly the subchunks it needs, performs the queries, and destroys the subchunks. This presents a problem when we have concurrent queries that need the same subchunks. Thus an earlier query will destroy its subchunks (some/all of which are needed by another concurrent query) and cause the later queries to fail.  The suggested approach is as follows: * Each query, instead of building subchunks on its own, delegates responsibility for subchunk creation/deletion to another class (e.g. SubChunkManager, [but try to find a better name]). * Each query requests a subchunks and releases subchunks using a lock()/unlock() mechanism (in the SubChunkManager API), or wrapped in an interface like boost::lock_guard wraps boost::mutex where acquisition and deletion can be handled by scoping. (It might even be possible to use boost::lock_guard mechanism.) * Now SubChunkManager blocks calls to lock() until it has created the necessary subchunks and modified its records to note that these subchunks are needed by one additional query). A subsequent unlock() call from the client indicates that SCM can decrement its counter and destroy the appropriate subchunks.  I (danielw) think the data structure to manage this is straightforward and the code for creation/deletion of subchunks already exists, but it might not be obvious how to route the plumbing so that queries can request/release appropriately, to an instance (probably only one per worker instance) owned by an object somewhere else.  Please feel free to use a better or more maintainable approach. ",15
"Channel was inactive for too long error
There as an issue with the receiveEvent call that can cause an exception ""Channel was inactive for too long"".  The fix for this (according to the ActiveMQ users list and archives) is to either completely disable the inactivity monitor or to increase the inactivity limit to something extremely high.  The fix is adding:  ""wireFormat.maxInactivityDuration=0"" to the URL in establishing the connection.  There might also be a way of doing this directly in the ActiveMQ broker, but so far I haven't seen anything that would let me specify that.",2
"Learn the OCS middleware
OCS will deliver the OCS Middleware software in November 2014.  There will be a workshop held at SLAC the week of November 10th.    I spoke with K-T and we should be able to attend this remotely, if necessary.  We need to spend some time getting familiar with this software and how it will integrate with the Base DMCS for AP.",6
"rewrite low-level shapelet evaluation code
While trying to track down some bugs on DM-641, I've grown frustrated with the difficulty of testing the deeply-buried (i.e. interfaces I want to test are private) shapelet evaluation code there.  That sort of code really belongs in the shapelet package (not meas_multifit) anyway, where I have a lot of similar code, so on this issue I'm going to move it there and refactor the existing code so it all fits together better.",2
"Write a transition plan to move gitolite and Stash repositories to GitHub
As recommended by the SAT meeting on 2014-09-16, we need this document to promote the use of GitHub by other subsystems within the project and to understand the impacts on DM.  The plan should include, but is not limited to: * Whether and how the repositories should be reorganized. * How existing commit attributions will be translated. * Moving comments in Stash to GitHub",20
"There is a bug in the prescan bbox for megacam.
The bounding box of the prescan region in the megacam camera should have zero y extent (I think).  Instead it goes from y=-1 to y=2.  This is either a bug in the generation of the ampInfoTables or in the way the bounding boxes are interpreted.",1
"exampleUtils in ip_isr is wrong about read corner
https://dev.lsstcorp.org/cgit/LSST/DMS/ip_isr.git/tree/examples/exampleUtils.py#n95 Says that the read corner is in assembled coordinates.  This is not true, it is in the coordinates of the raw amp.  That is, if the raw amp is in electronic coordinates (like the lsstSim images) it is always LL, but if it is pre-assembled, it may be some other corner.  This should probably use the methods in cameraGeom.utils to do the image generation.",1
"Support some mixed-type operations for Point and Extent
The current lack of automatic conversions in python is pretty irritating, and I think it's a big enough issue for people writing scripts that we should fix it.  In particular, allow {code} Point2D + Extent2I Point2D - Extent2I Point2D - Point2I  Extend2D + Extent2I Extend2D - Extent2I {code} (and the respective operations in the opposite order where well defined) It would also be good to allow the all functions expecting PointD to accept PointI, but I'm not sure if swig makes this possible.  It's probably not worth providing C++ overloads for all of these functions (and to be consistent we should probably do all or none).  I realize that you invented these types to avoid bare 2-tuples, but I'm not convinced that we shouldn't also provide overloads to transparently convert tuples to afwGeom objects.",2
"Revisit log integration with Xrootd
The integration of logging with xrootd needs to be revisited, for both the czar and the worker, after a discussion with Xrootd devs about API changes. We want to accomplish 2 things:  * Logging from xrootd client should go through the qserv-czar's logger, so it can be saved in the same file and enabled/disabled as a component. * Logging from the xrootd server may also find a benefit in using our logger as a backend",20
"add ColumnView support to FunctorKeys
FunctorKeys currently only work with individual records, but at least some could work with columns as well.  Need to add an interface for this and decide how to handle cases where it the FunctorKey doesn't support it.",2
"Invert buffering for czar in row-based result handling
The XrdSsi API performs some impedance-matching in buffering transfers. The current code (introduced in DM-199) doesn't leverage this because its flow model is based on the older mysqldump-based results passing. With dump files, we don't know the size of the fragments expected, so we are just passing buffers of bytes and building up a text blob to ingest. With the row-based protocol, we have sizes specified, hence the receiver can specify exactly what sizes of bytes are needed. Implementation of this ticket should simplify and reduce the code overall. ",10
"anaconda is too outdated to work with pip
The version of anaconda distributed with the stack is too outdated to be used with pip (and probably other things). The issue is an unsafe version of ssh.  A workaround is to issue this command while anaconda is setup: {code} conda update conda {code} Warning: it is unwise to try to update anaconda itself (with ""conda update anaconda"") because that will revert some of the changes and may result in an unusable anaconda.  I think what is required is an obvious change to ups/eupspkg.cfg.sh  The current version of anaconda is 2.0.1 based on http://repo.continuum.io/archive/  Note: there is no component for anaconda. I will submit another ticket.",2
"cleanup order/grouping of header files
We want: * header for the class * then system * then third party * then lsst * then qserv  We currently don't have the ""lsst"" group (with a few exceptions), and we call the last one ""local"" in most places.",1
"makeMaskedImage leaks memory
Calling afw.image.makeMaskedImage leaks memory when called from Python, because it returns a raw pointer without telling Swig %newobject.  To fix it, it'd be better to just have it return by value instead of by pointer, though this might involve fixing some downstream C\+\+ code (Python code should not be affected).",1
"compute linear parameter derivatives more intelligently in optimizer
The numerical derivatives computed by the optimizer currently don't distinguish between the linear parameters (for which derivatives are trivial) and nonlinear parameters (for which they're hard), because we don't pass the information that distinguishes them to the object that computes the derivatives.  If we move the computation of derivatives from the Optimizer class to the Objective class, we should be able to compute the derivatives much more efficiently.  While this doesn't matter much when fitting single component galaxy models (because there's only one linear parameter in that case), it should matter quite a bit when fitting high-order shapelets to PSF models.",4
"Refactor meas_base Python wrappers and plugin registration
meas_base currently has a single Swig library (like most packages), defined within a single .i file (like some packages).  It also registers all of its plugins in a single python module, plugins.py.  Instead, it should:  - Have two Swig libraries: one for the interfaces and helper classes, and one for plugin algorithms.  Most downstream packages will only want to %import (and hence #include) the interface, and having them build against everything slows the build down unnecessarily.  The package __init__.py should import all symbols from both libraries, so the change would be transparent to the user.  - Have separate .i files for each algorithm or small group of algorithms.  Each of these could %import the interface library file and the pure-Python registry code, and then register the plugins wrapped there within a %pythoncode block.  That'd make the implementation of the algorithms a bit less scattered throughout the package, making them easier to maintain and better examples for new plugins.",3
"Support multiple-aperture fluxes in slots
We should be able to use multiple-aperture flux results in slots.  While this is technically possible already by setting specific aliases, it doesn't work through the usual mechanisms for setting up slots (the define methods in SourceTable and the SourceSlotConfig in meas_base).  After addressing this, we should remove the old SincFlux and NaiveFlux algorithms, as the new CircularApertureFlux algorithm will be able to do everything they can do.",2
"Refine Butler prototype
A prototype of the new Butler is part of the S14 delivery.  This needs to be refined into a production package and the rest of the code needs to be ported to use it.",15
"S15 Multi-node Multi-query Integration Testing Harness
Build end-to-end integration test harness for Qserv that will run queries on multi-node system.",56
"LSE-68: Bring pull interface to CCB approval
Bring a long-pending set of changes, primarily the adoption of the ""pull interface"" for all DM-Camera image requests, to CCB approval",6
"LSE-68: Major phase-3 details (W15)
Deepen ISD to cover: supply of crosstalk constants, deletion policy details, content of the new-data notification, availability of a pass-through tag in data, and other Phase 3 matters.  Edit ISD to ensure that it covers WFS and guider requirements as well.  Deliverables: * Marked-up LSE-68 with combination of DM proposals for Phase 3 details and, where that is not a realistic approach, specific questions for Camera DAQ team. * Bring a proposal to the CCB by the end of the cycle.",16
"LSE-69: Bring Summer 2014 work to CCB approval
Bring Summer 2014 work on LSE-69 to CCB approval, ideally by 20 October in time to be part of the CD-2 document package.",6
"LSE-72: Bring Summer 2014 work to CCB approval
Remaining work is to proofread the SysML-ization by Brian Selvy of the LSE-72 draft, do any required cleanup in conjunction with the OCS team, and advocate for LCR-202 (already exists) at the CCB.",3
"Refine requirements and use cases for Level 3 facilities
Refine the requirements and use cases for the three branches of Level 3 capabilities exposed to users:  * Level 3 programming toolkit (user reconfiguration / extension of DM pipelines and stack)  * Level 3 compute cycle delivery (user access to 10% of compute base)  * Level 3 data product storage    Deliverables:  * Refinement, if necessary, to Level 3 requirements in DMSR  * Flowed-down requirements as a separate document.  Sufficient detail to allow a breakdown of the deliverables in the three areas of Level 3 by annual release cycle through construction period.",20
"LSE-72: Remaining Phase 2 details
Sort out remaining Phase 2 details of LSE-72, especially:  * details of the configuration mechanism (yet to appear even in LSE-70), the publication of available configuration keys, the efficient publication of configuration contents, OCS-driven configuration ""knobbing"" * specific list of events to be published by DM * more specific EFD-query language * EFD deployment model (understanding of distributed design envisioned by OCS and its implications for the ICD)",10
"LSE-72: Phase 3 details
Advance Phase 3 details as needed to eliminate obstacles to OCS and DM development during Summer 15.",10
"LSE-75: Bring Summer 2014 work to CCB
Bring a small set of technical changes (e.g., pointing notice moved to OCS ICD) and the addition of PSF reporting to T&S into the SysML version of LSE-75 and submit a change request to get this approved.",6
"LSE-75: Refine WCS and PSF requirements
Clarify the data format and precision requirements of the TCS (or other Telescope and Site components) on the reporting of WCS and PSF information by DM on a per-image basis.  Depends on the ability of the T&S group to engage with this subject during the Winter 2015 period.  Can be deferred to Summer 2015 without major impacts.  Current PMCS deadline for Phase 3 readiness of LSE-75 is 29-Sep-2015.",8
"LSE-130: Bring Summer 2014 work to CCB approval
Bring the current list-oriented version of LSE-130 into the camera-test-plan-oriented format requested by the Camera group, moving the old list(s) into LDM-272 for reference.  Negotiate what further work is required to make the document acceptable to the Camera group for CCB approval in time for CD-2.  Nominally this requires CCB approval by 20-Oct-2014 in order to have an approved document properly released to the CD-2 committee.",16
"LSE-130: Make requests more quantitative
Proceed, as enabled by input from the DM Apps group and the Camera, to make the individual data requests in LSE-130 more quantitative.",12
"LSE-140: Bring Summer 2014 work to CCB approval
Convert the existing LSE-140 draft to SysML, produce a docgen, and review with Jacques Sebag.  Bring to CCB meeting on 8 October under existing LCR-201.",6
"Complete data entry of LSE-140 revised draft into EA
nan",2
"Risk Register refresh 1/2015
Periodic review of DM risk register contents.  Covers preparation for a review expected at the end of January 2015, the only one during Winter 2015.",3
"TOWG - Contributions to the Operations Concept Document
Covers contributions to the writing and editing of the Operations Concept Document during the Winter 2015 cycle.  Deliverables: * Timescales diagram and explanatory text * Framework for Nominal Operations chapter * Contributions to KTL's writing of the Data Processing Operations chapter * General proofreading",10
"Install scisql plugin (shared library) outside of eups stack.
sciSQL plugin is currently deployed in eups stack (i.e. $MYSQL_DIR/lib/plugin) during configuration step. Nevertheless eups stack should be immutable during configuration step.  MySQL plugin-dir option may allow to deploy sciSQL plugin outside of eups stack (for example in QSERV_RUN_DIR).",3
"add per-exposure callbacks for measurement plugins
We should give measurement plugins an opportunity to do some work whenever we start processing a new Exposure.  This give them a good time to throw exceptions when there's something obviously wrong with the exposure (e.g. it's missing a Psf).  It also gives them an opportunity to do work that could be used to speed up per-source processing.  One specific case I have in mind is in shapelet PSF approximation for galaxy fitting, where it could be very valuable to use an initial fit to an average PSF to initialize (and speed up) the first to per-source PSFs.  One question here is how to to allow information to be passed from the per-Exposure method to the per-Source methods - we should not do that via plugin instance attributes, which means we probably want to have the per-Exposure method return an arbitrary object that would be passed to the per-Source methods.  Unfortunately, I don't see a way to avoid having that change the signature for those methods for all existing plugins, so this should be done relatively early, before we have too many user-contributed plugins.",3
"Reimplement CSS using JSON-packing, czar kazoo + c++ snapshotting phase 1
Transient cache for CSS, populated from python, no C++ interface to zookeeper. Pack multiple keys using json. Tenative packing spec is in DM-705  The v1 implementation transitions the zk access from the c++ layer into python, with code that understands how to unpack json-encoded data. This can coexist with the existing qserv_admin zk schema. v2 applies packing/unpacking logic in the creation/manipulation code in qserv_admin. ",10
"CSS design for query metadata v2
nan",3
"CSS design for query metadata v1
The goal of this ticket (and DM-1250) is to try to understand what kind of per-query metadata is necessary to provide client-transparent query processing in case when czar/proxy could die or be restarted.  Some relevant info is in the Trac: https://dev.lsstcorp.org/trac/wiki/db/Qserv/CSS/RunTimeState",3
"Implement structure for DB/table metadata in CSS
nan",4
"Requirements gathering for Metadata Store
nan",10
"Metadata Store - design v1
Research potential off-the-shelf candidates. Propose initial version of metadata design. ",5
"Metadata Store - experimental prototype v1
This first prototype will involve capturing metadata from a small set of image files. * Create a new schema file in the cat package (lsstSchema4mysqlW15.sql) with definition of tables that will be used to capture image metadata (see Exposure tables in other schema files), and some basic structure for capturing information about the entire repository. Decide which keywords are standard, and which should go to flat key-value area * Pick a directory (or several) with images, candidates: /lsst7/dr7/runs, /lsst7/releaseW13EP, /lsst3/DC3/data/obs/ImSim/pt1_2, /lsst/DC3/data/pt1.2.3k/datarel/ImSim * create test database, load schema * extract metadata for all images in a given directory and load metadata from fits headers.",10
"Metadata Store - design v2
nan",5
"Metadata Store - experimental prototype v2 (DataCat)
Integrate prototype v1 with Fermi's DataCat (e.g., reuse logic for reading headers using afw, store in DataCat). Experiment with foreign tables.",6
"Update documentation and automatic install script w.r.t. Qserv 2014_09.0 release
Creation of qserv_distrib and distribution of Qserv via official LSST repositories have to be taken into account in Qserv documentation and automatic install script.",4
"evaluation of Cinder as OpenStack storage cache in the LSST Middleware
Deliverable: evaluation of Cinder as OpenStack storage cache in the LSST Middleware Daues G 100%"" ",100
"implementation of data movement in the production system in the existing hierarchy of NFS disk, condo, tape
Deliverable: implementation of data movement in the production system in the existing hierarchy of NFS disk, condo, tape Freemon M 50% ",54
"incorporation of sizing model into data center requirements
Deliverable: incorporation of sizing model into data center requirements Freemon M 100% ",7
"data center requirements document
Deliverable: data center requirements document Petravick D 10% ",11
"completed governance of security plan for review
Deliverable: completed governance of security plan for review Petravick D 10% ",1
"security plan october.
Deliverable: security plan Ephibian 10% ",1
"addition of procurement of physical goods to contract
Deliverable: addition of procurement of physical goods to contract Petravick D 5%, Gelman M 10% ",6
"insertion of wide area simulator into test stand
Deliverable: insertion of wide area simulator into test stand Freemon M 100% ",18
"report of traffic shaping
Deliverable: report of traffic shaping Freemon M 100% ",7
"upgraded KVMs
Deliverable: upgraded KVMs Mather B 50% ",9
"sizing model critique report
Deliverable: sizing model critique report Perez A 50% ",29
"more efficient VMware infrastructure
Deliverable: more efficient VMware infrastructure  W15: Glick B 50%, Elliott M 25%, Mather B 10% 38 SP estimated  S15: Mather B 40%, Glick B 25% 6 SP estimated",6
"technical roadmap critique report
Deliverable: technical roadmap critique report Perez A 50%",29
"Upgrade NFS Storage Servers with new hardware
Deliverable: new hardware, ZFS filesystem, 3x 100+ TB + 1 spare 100+ TB Elliot M 75% ",55
"deployment plan for version 1 of OpenStack
Deliverable: deployment plan for version 1 of OpenStack Glick B 75%, Elliot M 15%, Mather B 10%, Wefel P 10%",56
"Procure replacement development infrastructure
Procure, install, test, and deploy hardware to replace existing LSST development cluster infrastructure.    Assignees: Bill Glick, Matt Elliot, Bruce Mather, Paul Wefel, Jason Alt  Duration: November - December 2015",45
"level of effort
Deliverable: level of effort Wefel P 100%, Freemon M 5%",100
"level of effort
Deliverable: level of effort Voiciu L 100%",28
"level of effort
Deliverable: level of effort Petravick D 50%, Gelman M 50%, Glick B 30%, Mather B 50%",100
"written plan for next period epics
Deliverable: written plan for next period epics Petravick D 50%, Gelman M 50%, Glick B 50%",10
"Fix Scisql deployment test error (doc.py)
Deployment test in tools/docs.py fails due to a wrong ""scisql_index"" path in scisql documentation.  Fortunately, qserv-configure.py doesn't stop on this error.",2
"meas_base ResultMappers should be FunctorKeys
The ResultMapper classes in meas_base should inherit from FunctorKey, and support bidirectional transfers involving the Result structs and records.",3
"add Schema method to join strings using the appropriate delimiter
Delimiters in Schema field names are version-dependent.  One can currently use {{schema[""a""][""b""].getPrefix()}} to join fields using the appropriate delimiter, but this is confusing to read.",1
"multi-level replacement in Schema aliases
Schema aliases should support more than one level (i.e. an alias may resolve to another alias).",2
"remove meas_extensions_multiShapelet from release packages/buildbot
nan",1
"rename meas_multifit to meas_modelfit, and add to lsst_apps
nan",1
"Improve Startup of HTCondor Jobs
Adjust configuration parameters of HTCondor config and/or submission files to improve speed at which HTCondor jobs start in both the replicator pool and worker pool.",2
"Improve worker fault tolerance of missing distributor data
Instances of worker jobs might ask the Archive DMCS which distributor to connect to, only to connect to that distributor which has recently been rebooted, so the distributor might not have that information.   At this point, the Distributor could send an “expired” notice of some kind to the Archive DMCS to clear it’s cache, and also tell the worker that it has no file of the type it’s looking for.  The worker would then go back to the Archive DMCS. ",6
"Propose and document a recipe to build Qserv in eups
In-place build is available and documented.",2
"Identify test data and camera
To test processCcd we need to identify a set of data to process (possibly mocked).  This implies that there will also need to be a minimal camera to go with the minimal data.  This task is to identify the minimal data and find a location for it.",4
"Implement designed tests for processCcd
Implement the designed tests with the installed data.",8
"Generate use case
We should first sit down with a Camera team rep (Jim C. maybe) to define the tool they need.  How dynamic are the data?  What is the layout of the data?  We should also get some example test data to work with.",6
"Sync test data headers with standards
The headers will need to be vetted against known FITS standards.  The headers should be valid and should not contain any non-standard keywords.  There should also be a census of FITS standards that may be used for defining sensor layout.",8
"Implement a tool to generate a Camera from FITS images
Implement the tool to generate the camera object.  It should be a command line tool that will take a path to a file.  It should have an option to persist the generated camera.  It should also have the ability to plot the camera.",10
"Verify use of the tool with the camera team
We will need to run the tool in the camera team's system to make sure the interfaces are as they expected.  It will also help to make sure that the tool addresses all of the cases.",6
"Design the API
The API will have to be able to accommodate data we know about, so will need to deal with reasonable missing data.  It should also not preclude extension.  This will need to be RFC'd since it is an API change.",8
"Implement and test the new API
Once the API is designed and signed off on, the API will need to be implemented and tested.  This will require updating all obs_* packages that use the current interface.",10
"C++ code changes required for --std=c++11
Some C++ code requires changes for modern C++ compilers if it is to be compatible with C++11 and the older standard. Here is my list, so far (ignoring the known issue of MacOS having two different standard C++ libraries)  Implicit conversion of shared_ptr to bool no longer works: <http://stackoverflow.com/questions/7580009/gcc-error-cannot-convert-const-shared-ptr-to-bool-in-return> In C++11, shared_ptr has an explicit operator bool which means that a shared_ptr can't be implicitly converted to a bool. This is likely to show up in a lot of packages. So far found in: - pex_policy - meas_algorithms  warning: adding 'int' to a string does not append to the string seen in daf_persistence  warning: 'va_start' has undefined behavior with reference types seen in pex_logging Trace.h and one other place. Fixed by not using references (and thus copying the arguments), rather than using pointers, to avoid changing the APIs. this is an old issue; but not all compilers warned about it: See <http://stackoverflow.com/questions/222195/are-there-gotchas-using-varargs-with-reference-parameters>  warning: 'register' storage class specifier is deprecated seen in: - boost 1.55.0.1 - Eigen 3.2.0 - a pex_logging .i file I silenced this warning in sconsUtils because it's too much trouble to upgrade boost and Eigen, and the warnings are very obtrusive  in daf_persistence: {code} src/DbStorageImpl.cc:87:35: warning: first declaration of static data member specialization of 'mysqlType' outside namespace 'persistence' is a C++11 extension [-Wc++11-extensions]     dafPer::IntegerTypeTraits<1>::mysqlType = MYSQL_TYPE_TINY; {code} I have not figured out how to fix this one.  clang 6 is pickier about the order of instantiation: in afw (fixed in DM-1302) Many errors such as the following: {code} include/lsst/afw/table/Flag.h:27:8: error: explicit specialization of 'lsst::afw::table::FieldBase<lsst::afw::table::Flag>' after instantiation struct FieldBase<Flag> {        ^~~~~~~~~~~~~~~ include/lsst/afw/table/Key.h:49:39: note: implicit instantiation first required here {code} Fixed by including Flag.h in Key.h  clang 6 warns about ambiguous unsequenced modifications. Seen in shapelet, for example: {code} src/HermiteTransformMatrix.cc:107:59: warning: multiple unsequenced modifications to 'kn' [-Wunsequenced]         for (int kn=jn, koff=joff; kn <= order; (koff += (++kn)) += (++kn)) {                                                           ^          ~~ {code} Fixed by using a += ++b, a += ++b, since it was the cleanest solution I could find.  clang 6 is pickier about instantiation classes in the wrong namespace: in shapelet (fixed by not instantiating the anonymous classes): {code} src/MatrixBuilder.cc:945:1: error: explicit instantiation of 'lsst::shapelet::<anonymous namespace>::SimpleImpl' must occur in namespace '' INSTANTIATE(float); {code}  meas_algorithms produced this warning because the class member _wcsPtr was a reference (which was unecessary): {code} src/ShapeletKernel.cc:188:34: warning: binding reference member '_wcsPtr' to a temporary value [-Wdangling-field]         _interp(interp), _wcsPtr(wcsPtr->clone())                                  ^~~~~~~~~~~~~~~ {code}  clang 6 warns about expressions such as if (!a == 0) because the ! is applied to ""a"", not the result of the ""a == 0"". Fixed in the obvious way in several places.  meas_algorithms seg-faults on loading unless boost is built with C++11 support. See ticket DM-1361 ",4
"Tests fail in shapelet when building on OS X 10.9
When building the master on pugsley.ncsa.illinois.edu, shapelet builds successfully, but two tests fail:  {code} pugsley:lsstsw mjuric$ cat build/shapelet/tests/.tests/*.failed tests/testMatrixBuilder.py  .F..... ====================================================================== FAIL: testConvolvedCompoundMatrixBuilder (__main__.MatrixBuilderTestCase) ---------------------------------------------------------------------- Traceback (most recent call last):   File ""tests/testMatrixBuilder.py"", line 310, in testConvolvedCompoundMatrixBuilder     self.assertClose(numpy.dot(matrix1D, coefficients), checkVector, rtol=1E-14)   File ""/Users/mjuric/test/lsstsw/stack/DarwinX86/utils/9.2+8/python/lsst/utils/tests.py"", line 328, in assertClose     testCase.assertFalse(failed, msg=""\n"".join(msg)) AssertionError: 1/50 elements differ with rtol=1e-14, atol=2.22044604925e-16 0.175869366369 != 0.175869366369 (diff=1.99840144433e-15/0.175869366369=1.13629876856e-14)  ---------------------------------------------------------------------- Ran 7 tests in 0.323s  FAILED (failures=1) tests/testMultiShapelet.py  ...F... ====================================================================== FAIL: testConvolveGaussians (__main__.MultiShapeletTestCase) ---------------------------------------------------------------------- Traceback (most recent call last):   File ""tests/testMultiShapelet.py"", line 88, in testConvolveGaussians     self.compareMultiShapeletFunctions(msf3a, msf3b)   File ""/Users/mjuric/test/lsstsw/build/shapelet/python/lsst/shapelet/tests.py"", line 107, in compareMultiShapeletFunctions     self.compareShapeletFunctions(sa, sb, rtolEllipse=rtolEllipse, rtolCoeff=rtolCoeff)   File ""/Users/mjuric/test/lsstsw/build/shapelet/python/lsst/shapelet/tests.py"", line 86, in compareShapeletFunctions     rtol=rtolEllipse)   File ""/Users/mjuric/test/lsstsw/stack/DarwinX86/utils/9.2+8/python/lsst/utils/tests.py"", line 328, in assertClose     testCase.assertFalse(failed, msg=""\n"".join(msg)) AssertionError: 1/5 elements differ with rtol=1e-14, atol=2.22044604925e-16 2.44929359829e-16 != 1.13310777953e-15 (diff=8.881784197e-16/1.13310777953e-15=0.783842839795)  ---------------------------------------------------------------------- Ran 7 tests in 0.131s  FAILED (failures=1) {code}  ===============  More info on pugsley.ncsa.illinois.edu:  	pugsley:lsstsw mjuric$ sw_vers 	ProductName:	Mac OS X 	ProductVersion:	10.9.5 	BuildVersion:	13F34  	pugsley:lsstsw mjuric$ clang -v 	Apple LLVM version 5.1 (clang-503.0.40) (based on LLVM 3.4svn) 	Target: x86_64-apple-darwin13.4.0 	Thread model: posix  ============  The files are in {{/Users/mjuric/test/lsstsw/build/shapelet/}}.",1
"Pre-CCB review of LSE-140 docgen
nan",2
"Edit agreed-upon changes into Word version of LSE-69
A meeting around 9/26/2014 agreed on a set of revisions to LSE-69, with some language still needed from [~gpdf].  This action is to edit the tracked-changes Word version of LSE-69 containing the notes from that meeting into a final copy that can be reviewed by the Camera team and used as input to editing the SysML version of the ICD.",3
"Create change request for LSE-69
Create a change request to bring LSE-69 up to date and capture the Summer 2014 work.",1
"Enter LSE-69 update into EA as SysML
Covers entering the contents of the LSE-69 update into EA as SysML, with associated updating of diagrams, and the creation of a docgen'ed version for CCB action.",1
"Proofread docgen'ed version of LSE-72
Brian Selvy is producing a SysML version of the LSE-72 updated edited by [~gpdf].  The action here is to proofread the docgen of that version once it is ready.",2
"Identify Conditions information in LSE-130 that is required for Alert Production
LSE-69 declares that there are two categories of Conditions data (telemetry) required by DM from the Camera: those items that are needed for Alert Production (for which the AP components at the Base will need a whitelist, and for which the Camera has a tighter latency requirement), and those that are not (but are then presumably needed in DRP or other deferred productions).  It states that the subset needed for AP should be enumerated in LSE-130.  The action here is to create an initial version of that list.",2
"Publish Qserv S14 version on lsst distribution server
In order to publish this version please tag Qserv master tip with ""2014_09.0"" and then run: {code:bash} ssh lsstsw@lsst-dev # command below can't be runned in buildbot, as it doesn't support qserv_distrib build rebuild -t 2014_09.0 qserv_distrib # bXXX is provided by previous command publish -t qserv -b bXXX qserv_distrib publish -t 2014_09 -b bXXX qserv_distrib {code} ",1
"advance to assigning tier-2 and tier-3 reliability levels 
Accommodated Ron Lambert's input on networking equipment. Assigned tier-2 and tier-3 levels to processing systems. ",4
"Deploy LSST stack within OpenStack instances on ISL testbed
Deploy the LSST Stack within OpenStack instances within the ISL testbed -- this could be for multiple flavors CentOS, Ubuntu, etc, and this could be done by pulling Docker Images to the instances.   There will also likely be some initial debugging of starting instances within the ISL platform as a new installation has been stood up Sept 2014. ",1
"Create Docker Image / Dockerfile for LSST Stack for ubuntu
Create an installation of the LSST Stack v9_2  within a Docker Image for ubuntu for easing the import of LSST software into an OpenStack instance,  We create  the image utilizing a Dockerfile to make systematic  the creation of such images. ",2
"update expected results file in SDSS demo test
Update the expected outputs in the SDSS DM stack demo repo to match what we expect from the new meas_base framework.",1
"Prepare quotes to order new VMware hardware & licenses
nan",2
"Setup temporary NFS datastore for VMs
We expect the primary datastores will be local to the VM compute node, but this NFS datastore will allow us to live migrate a handful of VMs between compute nodes.  We may also use this or another datastore as place to create backup snapshots of particular VMs.",2
"Setup new VM compute nodes
Once the new VM compute nodes arrive, the hardware needs to be installed, ESXi installed, and networked for use by vSphere.",6
"Expire Workers that receive no files
In instances where the files the worker expected to get never arrive, there should be a way of the worker to recognize this (say, a timeout after waiting for the Archive DMCS for some time), and exit.",4
"Configure and test new VM infrastructure
This would be configuring to work vSphere, setting up permissions and groups, adding datastores, and testing the use of the new setup.",10
"Automatic expiration of replicator jobs
Replicator jobs that receive no data specifically for the visit, raft, and exposure sequence ID within a certain amount of time should self expire to prevent future jobs from running.  If this is not done, jobs will back up in the HTCondor queue.",4
"Order new NFS servers
We are waiting on the UIUC Business Offices to provide us a new account number for ordering these servers.  As soon as we get that account number we need to order the hardware from KOI Computing.",6
"Plan new NFS mounts & data organization
This is a task of deciding the new client mounts for the 3 new NFS servers.  These should match up with the new storage hierarchy and classification <https://wiki.ncsa.illinois.edu/display/LSST/Storage+Structure>.",1
"Install new NFS servers
nan",10
"squash edge errors in SdssCentroid
SdssCentroid doesn't trap exceptions that are thrown due to being too close to the edge, resulting in noisy warnings in the logs.  Instead, it should catch the low-level exception and re-throw as MeasurementError, after defining a flag field for this specific failure mode.",1
"address no-shape warnings in GaussianFlux
GaussianFlux relies on the shape slot, and puts noisy warnings in the logs when the shape slot fails.  However, we probably don't want to add a new flag for GaussianFlux to indicate this failure mode, because it'd be entirely redundant with the shape slot flag.  We should figure out some other way to squash this warning - how we do that may depend on whether this is addressed before or after the C++ redesign.  We should also consider having GaussianFlux add an alias to the schema to point back at the shape slot flag, creating what looks like a specific flag for this failure while actually just being a link back to the shape slot flag.  That's probably not worth doing within the current C++ interface, however, as it'd require some unpleasant mucking around with ResultMappers.",2
"resolve factor of two difference in GaussianFlux
After changing the implementation of GaussianFlux to use the shape slot rather than estimate the shape itself by re-running the SdssShape code, Perry saw a 5-15% difference in the fluxes (I'm not sure of the sign).  The new behavior (using the shape) is consistent with what we'd have gotten with the old code when the little-used ""fixed"" config option was enabled (not surprising, as that just looked up the SdssShape measurement by name, instead of via slots).  I suspect the difference is coming in because of the factor of two between SdssShape's ""raw"" measurements - the actual Gaussian-weighted moments - and the factor of 2 it applies to make its measurements equivalent to ideal unweighted moments.  The correct weight function to use for GaussianFlux includes this factor of 2 (i.e. it's larger than the ""raw"" moments), and it's likely either the old code wasn't including this or the new code isn't.  We need to determine which one, and if necessary, fix the new code.",2
"Test the creation of basic OpenStack instances on the new ISL testbed [IceHouse]
A new version & implementation of the ISL OpenStack testbed is up and running. The new cloud is using IceHouse, the ninth OpenStack release. We get started on this platform by verifying that basic instance creation is working.  We target the creation of an instance through the (Horizon) GUI interface,  and via the nova CLI. ",1
"Create instance with a Floating IP Associated through the nova CLI
We see that in working with the Horizon GUI, it is fairly straightforward to give an instance a public IP address by associating a Floating IP with the current local IP.    However, we will want to be able to accomplish this task both remotely and programmatically within workflow.  As a step towards this, we target the solution of this via the nova CLI.",2
"Review advance doc to review with the data center working group
Consult with M. Freemon on the sizing model given the generic description of the computing needed tier-3 and tier-2 support. Make a pass through the underlying data center standards doc for things  ""any competent designer should know""  for example wall plugs on separate circuits form computer circuits.  add that to the document, and call meeting of working group for review in late Oct.",4
"Security Plan advancement for October
get a draft Project Office detail plan consistent with the level of devleopment of the master plan. (ephiphian). Draft a data classification plan, refer to plan and classes of data in drat materials. Obtain some central place in LSST documentation framework to hold materials being drafted (if supported by the LSST system) Recieve feedback based on portion of plan submitted to LSST project office. Incorporate feedback. (if minor) re-plan (if major)",18
"Make QSERV_RUN_DIR scripts able to detect qserv install paths using eups
Ticket related to Mario email (subject: [QSERV-L] Some points/actions from the discussion today) :  Making your ""qserv data"" directory independent of where qserv is installed   I think this is a big one, and largely independent of EUPS. You have a problem where you want to use one set of test data potentially with different qserv binaries (not at the same time, of course). I'd argue you should refactor the scripts generated by qserv-configure to either:   * get _all_ their information about various paths from a _single_ file, for example, from etc/paths.cfg.sh. Then you can easily regenerate just that file when you need to switch to a different qserv (or zookeper, or what not), or... * refactor the generated scripts to learn from the environment which binaries to run. I.e., if $QSERV_DIR is defined, use that qserv, etc. This will let you switch binaries by simply setup-ing the new one with EUPS.   The two are not mutually exclusive -- e.g., all of this logic could be in etc/paths.cfg.sh, and depending on whether this is a development build or a ""non-EUPS"" build, it can either pick up the paths from the environment or hardcode them.   Assuming you did that, your development loop may look something like this:   {code}     # assuming that qserv-configure.py has already been run     # in ../qserv-run       # do something with qserv-a clone     cd qserv-a     setup -r .     ... do some edits ...     scons       # now do the tests     cd ../qserv-run     ./bin/qserv-start.sh     ... do tests ...     ./bin/qserv-stop.sh       # now switch to qserv-b clone     cd ../qserv-b     setup -r .       # and do the tests again     cd ../qserv-run     ./bin/qserv-start.sh     ... do tests ...     ./bin/qserv-stop.sh  {code}  that is, as qserv-start picks up the relevant products from the environment, there's no need to rebuild/reconfigure the qserv-rundirectory each time. ",7
"Read through log4cxx documentation and log.git code
Read through the log4cxx documentation and become familiar with how the log.git package is set up.",2
"Write/configure tests with existing configurations and appenders
In order to be come more familiar with how to use the log.git package, write some tests to see how existing configurations and appenders are used by the log.git package.",4
"Write DM message appender class
Write DM message appender class to be used with log.git package.   This might entail writing a configurator class as well;  that depends on the investigation of how configurations/appenders are used.",16
"Write Unit test for new DM message appender class
Write unit tests for DM message appender class.  This might also require some tests for a configurator class, if that class is created.",2
"Define gather/scatter mechanism for tasks with generic API
Define gather/scatter mechanism for tasks with generic API.  This will use event services (DM Messages) initially, but we would like to support MPI or other communication mechanisms as well.  This will involve consulting with Paul about how the existing code is structured. ",8
"OCS Middleware workshop
Attend OCS Middleware workshop.  This will probably have be done remotely because I have a personal conflict with that time that will prevent me from attending in person.",4
"Write example programs for OCS Middleware
Write some example programs to get familiar with the OCS middleware software. The OCS middleware will later be integrated with the AP, in the base dmcs and replicator jobs.",4
"Refine Event base class to allow ActiveMQ filterable settings
The current Event.cc base class needs to be refined to remove and old-style data release terms that aren't used anymore.  Plus, it needs to be easily extensible to allow other types of dictionaries of terms that will be used in the message headers to make them filterable on the server side.",2
"Update tests to use unit test framework
The tests for this package predate the unit test framework that other package use.  Update the tests to uses the unit test framework and get rid of any duplicate  or obsolete tests.",5
"Change marshaling code to use json
The marshaling code for non-standard (i.e., non-filterable) components of messages is custom and not standard.  Change this to use JSON.",10
"General cleanup of Events package
There are some obsolete classes and code in the ctrl_events package that needs to be removed and/or updated.  PipelineLogEvent, for example.   That not only is no longer used, but it is applications specific, and should have been part of another package in the first place, subclassed from this package.",35
"ORIGINATORID value can churn too quickly.
The ORIGINATORID is a 64-bit word consisting of an IPv4 host address, 16-bit process id, and 16-bit local value.   In addition to the 16-bit process id not being standard across platforms (Mac OS >Leopard goes to 99999), the churn rate for the local value should be much higher than just 16 bits.  This could be fixed to changing ORIGINATORID to a 32-bit process id and a separate value for the local value, which would be specified together in the DM event selector.   I have to look into this more to see if this is a viable solution.  This might need to go to three separate values to future proof it (i.e., ipv6).",8
"Install docker 1.1.2 in an ISL OpenStack CentOS  instance, perform basic checks
Install docker 1.1.2 in an ISL OpenStack CentOS 6.5 instance, and perform  basic checks such stopping and starting the docker daemon, changing default settings such as size limit of containers/images,  pulling  standard images from docker hub, starting containers from these images, etc. ",2
"Re-arrange how Qserv directories are installed
we already touched question about installation directory structure at a meeting, maybe we can improve things by re-arranging how things are installed      we are currently installing stuff into four directories: cfg, bin, lib, proxy     to make it look more standard and to avoid clash with qserv source directories we could move cfg and proxy to a different location (something like share/qserv to make it more root-install-friendly in case we ever want to install under /usr)     this change (if you want to do it) deserves separate ticket, do not do it in this ticket ",3
"End-to-end demo  fails to exit with the correct status  when Warning would be correct.
The output  of demo2012 results in an output file which is compared against a benchmarked file. Currently the comparison allows a deviation from the benchmark based ""on the the number of digits in the significands used in multiple precision arithmetic"";  that  number is currently set to 11.  An example using that setting is: @ Absolute error = 5.9973406400e-1, Relative error = 9.1865836000e-4 ##2566    #:25  <== 29.7751550737 ##2566    #:25  ==> 29.7478269835  Additionally, the current use returns: a 'Fatal' error if the code itself fails to execute correctly; a ""Warning' error if the any of the benchmarked quantities do not meet the comparison criteria; 'Success' if the comparison meets all criteria.  It is noted that the buildbot 'Warning' color indicator is not currently being displayed when the comparison fails. That is a coding error.  This Issue will: * ensure that BUILDBOT_WARNING(S) causes the correct display color when appropriate.   This Ticket has been split into two parts. This is part 1.  Part 2 is DM-1379 ",2
"improvements to PsfFlux
While using it as an example for the redesign of DM-829, I came up with some ideas for how to improve PsfFlux's handling of edge/bad pixels:  - Add a flag indicating that at least one pixel was rejected  - Add thresholds (number? fraction of PSF model size?) for how many non-rejected, non-edge pixels we need to even attempt a fit.",1
"Fix minor loose ends from new result plumbing
Some of the more minor issues raised in comments from DM-199 were left undone prior to merging. This ticket addresses them.  For more information, please see:  https://github.com/LSST/qserv/pull/2 . ",1
"Edit pull interface and other Summer 2014 work into LSE-68 in Word
Deliverable: circulate a Word-based draft of LSE-68 in which the ""push"" interface is removed, and the ""pull"" interface is refined to include the guider and other Summer 2014 work.  Note that the use of ""pull"" for the guider applies whether or not the proposed guider redesign is accepted.",2
"Avoid use of ~/.my.cnf (used by css watcher)
See https://jira.lsstcorp.org/browse/DM-1258?focusedCommentId=29230&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-29230.  {{~/.my.cnf}} is used by css watcher, an optional tool used for monitoring css. It is a symlink to $QSERV_RUN_DIR/etc/my-client.cnf.  The css watcher could use MySQL credentials located in ~/.lsst/qserv.conf (used by integration tests wich are a Qserv/MySQL client)",2
"replace ""bad data"" flag in SdssCentroid
SdssCentroid has a ""bad data"" flag that doesn't actually convey any information about what went wrong.  This should be replaced with one or more flags that provide more information.",3
"Errors in testHealpixSkyMap.py
There is a failing unit test when healpy is supplied.  The problem is that the method boundary is not defined in the version of healpy we supply for the sims, however boundaries does exist.  If I replace boundary with boundaries, the test passes.",1
"Create Docker Image / Dockerfile for LSST Stack for CentOS6.5
Make a Dockerfile for systematic generation of docker images using a Centos6.5 base image  containing the LSST Stack (v9_2 at the moment) and library dependencies.",2
"Ensure that the partition package is C++11 clean and compiles on OSX 10.9
The LSST buildbot infrastructure recently changed to building everything with --std=c++0x, which broke the partition package, and hence automated Qserv builds. While debugging this, I discovered that the partition package does not build on OSX 10.9, and considering how minimal its dependencies are, it really should. The OSX issue can be fixed by avoiding {{using boost::make_shared}}.  The partition package should be cleaned up to avoid all use of {{using}}. If we decide to use C++11 in Qserv, then the codebase should also be modernized (in particular, there are use-cases for static_assert, nullptr, etc... ). ",2
"Develop expanded and updated Information Security Program from NSF guidance/templates, using existing documents (e.g. LSE-99) as starting point 
Develop expanded and updated Information Security Program from NSF guidance/templates, using existing documents (e.g. LSE-99) as starting point.  Create master document and subordinate documents, allowing for existing AURA, site, and institutional polices to be incorporated as appropriate.  JK: In PMCS this is Petravick D and Ephibian (LeClair L)",56
"Investigate deblending in one band followed by using the resulting templates in all bands
(See also [HSC-1025| https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1025])  One poor-man's approach to deblending multiple bands and visits is - Deblend in one band (or a combination, e.g. chi^2 band) using an SDSS-like algorithm that produces templates for each child - Take the templates (or possible model fits to those templates) and use them to deblend the objects in all other bands  It will probably be necessary to include undetected objects in this fit (cf. [HSC-1023|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1023]), using point-source models.  Note that this does not allow for the different seeing in each band.  This is not obviously catastrophic for reasonably wide blends as the total flux in each pixel is conserved, and if only one template is important near an object's centre the exact form of the template is unimportant.",40
"Investigate deblending in one band followed by linear fits of models
(Cf. [HSC-1024|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1024])  One poor-man's approach to deblending multiple bands and visits is  - Deblend in one band (or a combination, e.g. chi^2 band)  - Fit models in that band  - In all bands separately fit those models simultaneously to all object in the blend, allowing only a minimum of coefficients to float (ideally only amplitudes, but given the realities of real galaxies the bulge and disk will need to both be fit, or the Sloan Swindle components, or the Lensfit Swindle, or ...).  The correct PSF for each image would be used.  It will probably be necessary to include undetected objects in this fit (cf. [HSC-1023|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1023]), using point-source models.  It is not clear how much of a poor-man's solution this actually is, or whether it will work quite well.",40
"Merge Footprints from different bands/epochs
The current concept of the deblender assumes that the inputs are  - A merged set of Footprints that define which pixels are part of the blend  - A merged set of Peaks within that merged Footprint  Please generate these merged Footprints (which will be defined in (x, y) coordinates in the tract/patch coordinate system). ",5
"Generate a master list of Objects given detections in multiple bands/epochs
Once we've detected Sources in multiple bands we need to merge the positions to generate Objects.  This is a little complicated (or at least messy):  - The positions have errors  - If the seeing is different in different visits, objects may be blended in some but not all exposures  - If we use more than one detection pass (e.g. smoothing when looking for faint objects, not smoothing for bright) this has similar-but-different consequences (but we should probably deal with in the per-band processing)  - Objects move, so even if the positions are within the errors the motion may still be detectable ",5
"Submit LCR for LSE-68
Create an LCR, including a summary of changes, for LSE-68.",2
"Edit LSE-68 changes into EA
nan",2
"Improve output from integration tests
Currently the integration tests print pages and pages of output, it is hard to see what failed and how. It'd be nice to clean that out, and instead, print some short summary, perhaps in a form of a summary table what queries has run, what succeeded, what failed etc. HTML format might be a reasonable way to display it.",6
"Eups 1.5.4 requires each new shell to source the eups setups.sh
eups v 1.5.4 requires each new shell to source ...eups/../bin/setups.sh.  This requires the buildbot scripts: runManifestDemo.sh, create_xlinkdocs.sh, be updated to individually  do that task.  Add  demo2012: bin/demo.sh . ",2
"Reimplement CSS using JSON-packing, czar kazoo + c++ snapshotting phase 2
The v1 implementation (DM-1249) transitions the zk access from the c++ layer into python, with code that understands how to unpack json-encoded data. This can coexist with the existing qserv_admin zk schema. v2 applies packing/unpacking logic in the creation/manipulation code in qserv_admin. v2 scope will be defined further, once DM-1249 enters review.  Update: Phase 2 includes removal of c++ zk code and unification of Qserv css modules into a single place.",10
"Design CSS schema to support database deletion
Need to implement deleting databases. Deliverable: a design of the system that will be capable of deleting a distributed database including all copies of that database on all workers, all replicas of all chunks are deleted. It should be possible to ""create database x"" at any time later.",2
"Improve documentation of pixel systems in obs_lsstSim
There is not much documentation of the coordinate systems in use by CameraGeom.  This is by nature documentation that is instrument specific, so should go in the obs_ package for each instrument.",1
"Create suite of Dockerfiles / docker images for LSST Stack for ubuntu, CentOS
Building on issues DM-1317 and DM-1375 where initial images and Dockerfile's were constructed, we can now use these Dockerfile's as prototypes to extend the set of Dockerfiles & images.  We observe that by making  simple (scriptable) edits to the initial Dockerfile, we can run 'docker build' to make docker images for several combinations of OS and base compiler gcc version.",2
"Prepare a SL6x openstack image for with Qserv development environment
Qserv packaging procedure requires to often rebuild Qserv and relaunch integration tests.  IN2P3 Openstack platform offer next virtual machines :  {code:bash} [fjammes@ccage030 ~]$ nova flavor-list {code} | ID | Name              | Memory_MB | Disk | Ephemeral | Swap | VCPUs | RXTX_Factor | Is_Public | | 1  | m1.tiny           | 512       | 0    | 0         |      | 1     | 1.0         | True      | | 15 | cc.windows.small  | 4096      | 20   | 0         |      | 2     | 1.0         | True      | | 16 | cc.windows.xlarge | 8192      | 50   | 0         |      | 4     | 1.0         | True      | | 2  | m1.small          | 2048      | 10   | 20        |      | 1     | 1.0         | True      | | 3  | m1.medium         | 4096      | 10   | 40        |      | 2     | 1.0         | True      | | 4  | m1.large          | 8192      | 10   | 80        |      | 4     | 1.0         | True      | | 5  | m1.xlarge         | 16384     | 10   | 160       |      | 8     | 1.0         | True      | | 6  | cc.lsst.medium    | 4096      | 20   | 40        |      | 2     | 1.0         | False     | | 7  | cc.lsst.large     | 16384     | 20   | 160       |      | 8     | 1.0         | False     | | 9  | cc.lsst.xlarge    | 40000     | 20   | 160       |      | 20    | 1.0         | False     |  cc.lsst.xlarge would allow a quick build/test of new Qserv release.",5
"Add return code for integration tests
Integration test have to returns non-zero when failing. This will ease use of CI or debugging tools ({{git bisect}}, buildbot).",1
"Stand up netem server with Dell PE 1950
nan",4
"Create persistent volume of Cinder block storage and attach to instance
We create a persistent volume of Cinder block storage and attach to working instance.  When it was created, the instance does have a specified amount of ephemeral disk, but this disk will be destroyed with the instance.  We want to test that we can create a persistent volume of block storage, attach it to an instance, format the storage with a file system, and mount the volume for use with processing, where data/output can be retained after the instance is destroyed. ",1
"Document how to switch Qserv or dependency in eups
DM-1338 may allow to switch Qserv or dependency version using eups, without having to re-configure Qserv (i.e. without any change in QSERV_RUN_DIR).  There may have conditions for this to work. That's why it's reasonable to get some user feedback on DM-1338 before documenting this feature.",2
"Add return codes for qserv-check-integration.py and qserv-testunit.py
nan",1
"Improve Qserv Configuration Procedure
Based on input from review of DM-1338, and discussions at hangout https://confluence.lsstcorp.org/display/DM/LSST+Database+Hangout+2014-10-29: implement -keepdata option.  * --keepdata will preserve qserv_meta.conf and my.cnf. Regarding the latter, this is necessary because user might have customized data_dir. * The structure of qserv_meta.conf and my.cnf must remain the same between existing version of Qserv and the to-be-configured version of Qserv. It is users's responsibility to ensure the structure did not change. Note in particular, the template files (with the exception of my.cnf) should not be customized",7
"Process sample sdss data with LSST Stack in a Docker container in OpenStack
Process sample sdss data, starting with the lsst_dm_stack_demo, with LSST Stack in a Docker container in an OpenStack instance. ",2
"SUI define local hardware needs to host the test DB and application servers
We need have Qserv and the test DB in IPAC to do development and test locally, to access the  data through Qserv APIs.",10
"SUI install Qserv and test DB, enable access to catalogs for SUI team members
Install Qserv and test DB in local hosts.  Enable SUI team members to access the catalogs. ",6
"State diagram for jobs in progress
Build a state diagram showing job progress throughout a run.",13
"afw tests use the same name for a dummy file: test.fits
The afw package  uses a file named: test.fits in multiple testers.  If the user sets up the build to use multiple CPU (-j #), then there is the risk that the shared filename will be affected by more than one tester at a time.   In the case which provoked this Issue, the tester: testSimpleTable.py, reported that the file was missing.  A simple rerun managed to get past the error.  I recommend that the different testers use uniquely named demo files.",1
"Github Transition: Naming conventions for repositories
The Simulations team has requested that repos in general and the numerous DM repos in particular are prefixed in a way that would make fitering them out of searches and display easy (for example, their repos are prefixed sims_*)  This would be an evident useability aid to DM developers and outside contributors too.   Obtain a decision on how to allow users to quickly isolate repositories they are interested in. ",3
"Github Transition: Storage of large data files
Github currently has fixed 100MB per blob or 1GB per repo limits. Sims has at least one file in its repo whose history exceeds this (sims_meas) and this has been raised as an issue before.  Obtain a decision on a suitable way forward for the time being that would allow the upload of the repositories on Github. Experienced to be reviewed in a few months time to consider whether has proved satsifactory.",4
"Github Transition: Stash-stored pull requests, extraction
Extract comments from Atlassian Stash made during pull requests /code reviews and their associated SHA1s if it all possible.   ",4
"Github Transition: pull request discussion, retention - proof of concept
Store review comments / PR discussion into relevant git repositories  Code & process to do this on a continuous basis post-transition will be a different issue, ",1
"Test absence of individual components
Test absence of individual components of the AP simulator.  Bring each down, run the system, and restart just those components to see if the system still operates as expected",2
"check if Qserv compiles with C++11
nan",1
"Fine tune czar and worker database initialization
For now, Qserv databases are the same on the master and on the worker. It means that czar tables are created on the worker and vice-versa.  This ticket aims at creating the right tables at the right places, and also at sharpening permissions on these tables.  This is done by splitting configuration script qserv-czar.sh, which configure a mono-node instance, in two scripts: - qserv-czar.sh, for configuring the czar - qserv-worker.sh, for configuring workers.  see https://confluence.lsstcorp.org/display/DM/LSST+Database+Hangout+2014-10-29  Please note that this code can't be validated with mono-node integration tests (whereas it doesn't break them): indeed it requires a mono-node installation with 2 MySQL instance, one for the worker and one for the master. Updating the mono-node configuration and integration tests with such a feature would make them far more complex.  A quick and dirty, hard-coded, testbed is available in u/fjammes/DM-1446-test. It has been used successfully to test this ticket.  This ticket will also be validated during next Qserv install on in2p3 clusters.",4
"Improve spatial-selection flexibility by parsing ptInSphBox-like syntax instead of qserv_areaspec_box
Note: it is not clear that we should do this.  This is an idea to change the syntax for spatial area selection. Currently, we have SELECT...FROM...WHERE qserv_areaspec_box(...) ...  This forces the area selection to apply the box (or appropriate) cut on the tables referenced in the FROM list that are partitioned, with those columns. It would be nice to allow spatial selection based on tables (and columns?) explicitly specified by the user, and then compute the appropriate chunk coverage and WHERE clause conditions and predicates. This is a pretty big deal, and would affect all spatial-select user queries, though the compatibility can be mitigated by supporting both syntaxes.",100
"Move code for mock images into afw so it reusable.
There is some code in the exampleUtils in $IP_ISR_DIR/examples that could be of wider use.  Specifically there is code to generate mock darks, flats, and raw data from a mock camera.  There is also code to generate a mock dataRef.  It could be used more widely if moved someplace else.  Russell suggested afw.cameraGeom.utils.",1
"newinstall.sh should check that python2 is available
The standards now suggest using: {code} #!/usr/bin/env python2 {code} instead of {code} #!/usr/bin/env python {code} This doesn't work with (at least) Mac OSX 10.9 system python.  newinstall.sh should check if this works and suggest a fix (creating a symlink).",1
"Detect lua version in admin/templates/configuration/etc/init.d/mysql-proxy
nan",1
"Add data versions to Zookeeper
Track versions of data inside zookeeper, and detect from Qserv code if Qserv code is compatible with given format of data.",2
"LOE - Week ending 10/31/14
nan",8
"LOE - Week ending 10/24/14
nan",6
"finish porting meas_algorithms unit tests
nan",4
"C++ Redesign -- Result definition for custom algorithms
Additions to Jim's redesign to make it easier to define custom results.",3
"Add NaN check to PixelFlags
The test of PixelFlags in measureSources.py (from measAlg) requires a check to be sure that the center inputs are not NaN.",1
"SdssShape shiftMax config item is being ignored
The code we ported from meas_algorithms sets the maxShift to 2 without regard to the config item which is supposed to set that value.",1
"Design Review prep for C++ redesign
Write up the design developed on DM-829 and push it through review.",1
"Github Transition Plan: Reverse mirror for beta-tester repositories
Test the reverse mirror for beta-testers.  Straw man:  1. Break the mirror for anybody beta-testing github workflow 2. Mirror back to new gitolite area: ""mirror"")  Method:  https://help.github.com/articles/duplicating-a-repository/  ",1
"Rewrite obs_test
I want to use obs_test to test processCcd, but it contains no imaging data. I will replace the current data with new imaging data (a portion of one LsstSim CCD), add a camera and rewrite the mapper (subclassing CameraMapper).  This also requires rewriting the unit tests in obs_test and in one other package that uses obs_test.",10
"Execute multi-platform lsst_dm_stack_demo test with Fig orchestration
In DM-1431 we demonstrated processing of sample data within Docker containers within an OpenStack instance.  Data was processed for containers based on CentOS6.5, Ubuntu 13.10, Ubuntu 14.10.  We consider a multi-platform ""testing"" scenario, where we process the sample data in numerous containers based on various platforms/OSs all simultaneously on an OpenStack instance.   This scenario entails starting up and managing multiple Docker containers.  Fig (http://www.fig.sh) is a new tool for starting up and managing services via Docker containers.   It is a common use case with Docker to have individual components of an application each run separately in a container (resulting in numerous  containers running on a node or cloud env, i.e., a 'Docker stack'), with the containers having linkages/dependencies to be managed. Fig may be used to encode the relationship between the containers in the Docker stack, and to start up such a set of containers.  We install and examine Fig, and apply Fig to our multi-platform ""testing"" scenario.   ",4
"Fix 2014_09 documentation
Replace {{NEWINSTALL_URL=http://sw.lsstcorp.org/pkgs/}} with: {{NEWINSTALL_URL=http://sw.lsstcorp.org/eupspkg/}}  and {{eups distrib install qserv_distrib 2014_10.0}} with:  {{eups distrib install qserv_distrib -t 2014_10}}",1
"Secure MySQL root password in configuration templates
MySQL password in written in multiple file during configuration procedure. One single file (QSERV_RUN_DIR/tmp/my.cnf) should be used, and removed at the end of configuration procedure. qserv-meta.conf also contains MySQL password and should be also secured (move password to qserv-configure.py cmd line?).",4
"Study fig to manage Qserv cluster
http://www.fig.sh/  coordinate with @GregDaues who is also investigating fig.",5
"Buildbot master takes exception when exiting from mail notifier after dynamic email sent.
Buildbot master exits without posting the required statically-addressed email notification if a dynamically-addressed was sent.   This fix needs to ensure that the required (by buildbot specification) static email is sent even if it has to be directed to a dead-letter box (which it is).",3
"Discover/learn what others are doing in astronomy software
Attend the annual  ADASS conference to keep up with the software development in the astronomy community.  Trey Roby, Tatiana Goldina, Xiuqin Wu plan to attend the ADASS 24.",20
"SUI: study other plot packages
Firefly uses clientsidegchart at https://code.google.com/p/clientsidegchart/ for XY plot. This package has not been updated since 2010. We need to find out more about other plotting packages and consider if we need to switch. The candidates are:  1. flot plotting library for JQuery. http://www.flotcharts.org/ 2. Highcharts JS at http://www.highcharts.com/",10
"Finalize the Design of Query Metadata
Fine-tune the design outlined in DM-1251 (metadata for capturing information about running queries in Qserv.).",6
"Modify czar to interact with query metadata
Modify czar code: it should interact with the query metadata (store information for long running queries, retrieving).",6
"FY18 Design and Implement Query Cost Estimate
Design and implement system that will estimate query cost. In particular, we will need to know if the query is interactive, or should be scheduled on shared scan, which shared scan etc. Estimating cost will likely involve looking at number of chunks involved, number of joins, and complexity of math operations. Consider extending API and allow users to specify a hint. ",79
"Add queries on partitioned table for Qserv integration test dataset #3
Qserv test dataset #3 runs only queries on non-partitioned tables. Adding queries on partitioned table would increase drastically the coverage of these tests.  Please note that a long-term solution could be to launch all the *.FIXME queries and to have a more detailed report. For example define queries which must pass for the integration test to succeed, and test queries which may pass and log their results in a report.",4
"Allow newinstall.sh to run in batch mode without installing Anaconda
Installing Qserv on a cluster without internet access requires newinstall.sh to run in batch mode without installing Anaconda (whose install requires internet access). So Anaconda should have a {{-anaconda=yes|no}}. Usefulness of git {{-git=yes|no}} option, against full batch option answering ""yes"" everywhere could also be studied.",2
"Package Qserv mono-node instance in Docker
Learn Docker basics and then package a Qserv mono-node instance.",5
"Package Qserv master and worker instance in Docker
Learn Docker basics and then package a Qserv mono-node instance.",5
"confusing error message when enabling unregistered items in RegistryField
pex_config seems to split out this confusing error message when trying to enable (i.e. append to .names) a registry item that doesn't exist: {code:hide-linenum}   File ""/home/lam3/tigress/LSST/obs_subaru/config/processCcd.py"", line 51, in <module>     root.measurement.algorithms.names |= [""jacobian"", ""focalplane""]   File ""/tigress/HSC/LSST/lsstsw/anaconda/lib/python2.7/_abcoll.py"", line 330, in __ior__     self.add(value)   File ""/tigress/HSC/LSST/lsstsw/stack/Linux64/pex_config/9.0+26/python/lsst/pex/config/configChoiceField.py"", line 72, in add     r = self.__getitem__(value, at=at) AttributeError: 'SelectionSet' object has no attribute '__getitem__' {code}",1
"Support new version of newinstall.sh
newinstall.sh now creates loadLSST.bash instead of loadLSST.sh. This has to be taken in account in Qserv automated install script: qserv-install.sh and in Qserv documentation.",1
"Reverse image slicing doesn't work as expected
The lsst.afw.image image-like classes support slicing, but reverse slicing does not work as expected. Here are some examples: {code} from lsst.afw.image import ImageF im = ImageF(10, 10) im[:,:].getDimensions() # is (10,10) as expected im[0:10, 0:10].getDimensions() # is (10,10) as expected im[::-1, ::-1] # should be the data with x and y reversed, but fails with: Box2I(Point2I(-2,-2),Extent2I(12,12)) doesn't fit in image 10x10 im[9:1:-1, :].getBBox() # starts at 0,0, as expected, but is 10x10 instead of 9x10 {code}",1
"Remove unnecessary config/install steps
Some existing installation configuration steps are not needed any more. For instance, we don't need $RUN/q or $RUN/result any more-- these paths are handled directly by qserv code, so the filesystem lookup shouldn't ever take place.  Completion of this ticket should remove unnecessary steps and concepts from installation, simplifying future maintenance. ",2
"C++ Standards Rule 3-9 needs rewrite
Jim Bosch reviewed Section 3 of the C++ Standard.  He noted this change which I felt warranted SAT clarification:  3-9: This is a confusing conflation of two entirely different concepts: it seems to discourage all global variables, regardless of whether they're in a namespace, but only to discourage free functions when they aren't in namespace. If that reading is correct, I think both are sensible recommendations, but they need to be clarified, and probably split up. If the reading should be that all free functions are discouraged, that'd be a terrible rule we violate all over the place. If the reading should be that global variables are only discouraged when not in a namespace, that needs to be clarified (and IMO it could be bumped up to a complete prohibition).  Refer to: https://confluence.lsstcorp.org/pages/viewpage.action?pageId=16908685 Rule 3.9.   The current Rule states: 3-9. Global variables and functions SHOULD be avoided and if used MUST always be referred to using the '::' operator. _________________________________________ ::mainWindow.open(), ::applicationContext.getName(), ::erf(1.0) __________________________________________ In general, the use of global variables should be avoided. Consider using singleton objects instead. Only use where required (i.e. reusing a framework that requires it.) See Rule 5-7 ( .https://confluence.lsstcorp.org/pages/viewpage.action?pageId=16908706#C++Statements-5-7 ).  Global functions in the root namespace that are defined by standard libraries can often be avoided by using the C++ versions of the include files (e.g. ""#include <cmath>"" instead of ""#include <math.h>""). Since the C++ include files place functions in the std namespace, ""using namespace std;"", which is permitted by Rule 5-41, will allow these functions to be called without using the '::' operator. In cases where functions are only available in the C include files, the '::' operator must be used to call them. This requirement is intended to highlight that these functions are in the root namespace and are different from class methods or other namespaced free functions. ",1
"calling extend with a SchemaMapper should support positional arguments
Calling {{catalog.extend(other, mapper)}} isn't equivalent to {{catalog.extend(other, mapper=mapper)}} because the second argument is the boolean {{deep}}.  When a SchemaMapper is passed as the second argument, we should recognize it for what it is.",1
"sconsUtils fails to identify Ubuntu's gcc
On Ubuntu 12.04, gcc --version says: {code:hide-linenum} gcc (Ubuntu/Linaro 4.6.3-1ubuntu5) 4.6.3 Copyright (C) 2011 Free Software Foundation, Inc. This is free software; see the source for copying conditions.  There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. {code}  This apparently isn't quite what sconsUtils expected, because it says: {code:hide-linenum} scons: Reading SConscript files ... Checking who built the CC compiler...(cached) error: no result CC is unknown version unknown {code}  Happily, everything seems to work anyway, as the fall-back options for the unknown compiler work fine with this one.",1
"Use eups swig instead of system swig during Qserv build
Qserv build system use system swig, it should be fixed to use eups swig. ",3
"Add support for ""SET @@session.autocommit""
The SUI team is using JDBC connector and queries like  ""SET @@session.autocommit = {0}"".format(switch)  are derailing everything. We should add support for these queries. ",2
"check out FIrefly package, build and run an applicaiton
Get familiar with FIrefly package, build and run an application.  Understand the coordinate grid overlay on an image. ",10
"Fix confusing error message
Selecting from a table that does not exist, e.g. something like:  select count(*) from whdfd  produces a strange error:  ERROR 4110 (Proxy): Qserv error: 'Unknown error setting QuerySession' ",1
"Fix ""stripes and substripes must be natural numbers"" bug
on user side:  {code} ERROR 4110 (Proxy) at line 1: Qserv error: Unexpected error: (<class 'lsst.qserv.czar.config.ConfigError'>, ConfigError(), <traceback object at 0x26b8b90>) {code}   in czar log:  {code} 1114 22:30:10.155 [0x7fafcefab700] ERROR root (app.py:370) - Unexpected error: (<class 'lsst.qserv.czar.config.ConfigError'>, ConfigError(), <traceback object at 0x26b8b90>) Traceback (most recent call last):   File ""/usr/local/home/becla/qservDev/Linux64/qserv/2014_09.0/lib/python/lsst/qserv/czar/app.py"", line 360, in __init__     self._prepareForExec()   File ""/usr/local/home/becla/qservDev/Linux64/qserv/2014_09.0/lib/python/lsst/qserv/czar/app.py"", line 437, in _prepareForExec     self._addChunks()   File ""/usr/local/home/becla/qservDev/Linux64/qserv/2014_09.0/lib/python/lsst/qserv/czar/app.py"", line 535, in _addChunks     self._computeConstraintsAsHints()   File ""/usr/local/home/becla/qservDev/Linux64/qserv/2014_09.0/lib/python/lsst/qserv/czar/app.py"", line 529, in _computeConstraintsAsHints     self.pmap = self._makePmap(self.dominantDb, self.dbStriping)   File ""/usr/local/home/becla/qservDev/Linux64/qserv/2014_09.0/lib/python/lsst/qserv/czar/app.py"", line 478, in _makePmap     raise lsst.qserv.czar.config.ConfigError(msg) ConfigError: ""Partitioner's stripes and substripes must be natural numbers.""  {code}  To reproduce:  {code} qserv-check-integration.py --case-no 01 --load {code}  then {code} mysql --port 4040 LSST -e ""select count(*) from Object"" {code}   ",2
"Switch readMetadata' return value from PropertySet to PropertyList
It'd be useful if readMetadata would return PropertyList instead of PropertySet, because in many situations order matters. For details, see discussion in DM-1517",2
"Investigate halos around stars
Andrew Becker reports on dm-users: {quote} We have been using the LSST stack to reduce CFHT data at UW, and have come across a potential bug in the DM software.  I don't see how this could be something intrinsic to the data, instead it seems like it could be a bug in the software triggered by its application to new data.  I have a how to reproduce at NCSA at /lsst/home/becker/CFHT.  Follow commands in the SETUP file to get an up-to-date version of the stack (b449) and a modified version of ip_isr (a hack to avoid overscan correction) and a private branch of obs_cfht (u/krughoff/ossos_support).  If you run the commands in SOURCE_ME it will run the processCcd on a single CCD, in 2 modes, yielding 2 output directories.  The second call merely sets the bin sizes to be the same amongst all the various background subtractions, but is sufficient to trigger the bug.  A basic script included in the directory differences the 2 output calexps (a straight -=) and ds9 is used to view the diff.  If you pan to e.g. 1845,546 you'll see a feature like attached in the difference (out1 on the left, out2 in the middle, diff on the right).  This can be seen in various other parts of the image, as irregular annuli around bright blended objects.  It suggests to me that during measurement some substamp manipulations are leaking into the original image.  But you can kind of see in the center image that the second run seems to trigger the issue. {quote}  And then: {quote} So to summarize:     export LSSTSW=~lsstsw    source ~lsstsw/bin/setup.sh    setup lsst_apps -t b449    setup -k -r /nfs/lsst/home/becker/LSST/DMS/obs_cfht    setup -k -r /nfs/lsst/home/becker/LSST/DMS/ip_isr    setup -k -r /nfs/lsst/home/becker/LSST/DMS/processFile    cd /nfs/lsst/home/becker/CFHT   will set up your environment.  The following is sufficient to recreate the bug, and create postIsrCCD images for processFile:     processCcd.py input/ --id visit=1612606 ccd=8 --output out1/ --config isr.doAssembleDetrends=False isr.fringeAfterFlat=False isr.fringe.pedestal=False isr.doBias=False isr.doFlat=False isr.doFringe=False isr.doWrite=True calibrate.doPhotoCal=False calibrate.doAstrometry=False doDeblend=True     processCcd.py input/ --id visit=1612606 ccd=8 --output out2/ --config isr.doAssembleDetrends=False isr.fringeAfterFlat=False isr.fringe.pedestal=False isr.doBias=False isr.doFlat=False isr.doFringe=False isr.doWrite=True calibrate.doPhotoCal=False calibrate.doAstrometry=False doDeblend=True detection.background.binSize=512 calibrate.detection.background.binSize=512 calibrate.background.binSize=512   Following are the calls I made to processFile.py (using my modified version of the processFile.git package), and which return images whose difference is exactly 0.  So I don't really get whats happening with processFile, but it doesn't seem to be accepting my command-line config changes.     processFile.py out1/postISRCCD/13AP06/E+0+0/2013-03-11/r/postISRCCD-1612606-08.fits --outputCalexp calexp1.fits -c calibrate.doPhotoCal=False calibrate.doAstrometry=False doDeblend=True  calibrate.repair.doCosmicRay=False     processFile.py out2/postISRCCD/13AP06/E+0+0/2013-03-11/r/postISRCCD-1612606-08.fits --outputCalexp calexp2.fits -c calibrate.doPhotoCal=False calibrate.doAstrometry=False doDeblend=True  calibrate.repair.doCosmicRay=False  detection.background.binSize=512 calibrate.detection.background.binSize=512 calibrate.background.binSize=512     processFile.py out2/postISRCCD/13AP06/E+0+0/2013-03-11/r/postISRCCD-1612606-08.fits --outputCalexp calexp3.fits -c calibrate.doPhotoCal=False calibrate.doAstrometry=False doDeblend=True  calibrate.repair.doCosmicRay=False  detection.background.binSize=1024 calibrate.detection.background.binSize=1024 calibrate.background.binSize=1024 {quote}",2
"Update processFile to use new measurement framework
processFile.py uses the old measurement framework in meas_algorithms, but all the other components expect it to be using the new measurement framework in meas_base.",4
"Draft security risks into the Center's template
The Cyber security center has provided a risk template consistent with their templates,  The Center attempted to populate their templates with material from LSE 99  to the templates,   The impedance mis match was too large, The way forward is seem as  attempting a high level decomposition of risk into the templates.",4
"Identify potential KVM hardware
Identify potential KVM hardware that would meet our needs.  e.g. a current version of the Avocent or Dell KVMs used at NCSA.",1
"Reorganize docker image repositories and align with github
A heterogeneous collection of docker images have been accumulating within the public docker repository  daues/lsstdistrib . Such a heterogeneous collection prevents the assignment of a ""latest"" tag to allow users to easily obtain the most recent image for a particular item (detailed version numbers, labels currently required.)     Thus we should break out the single repository into multiple repositories where are ""latest"" tag will be effective.  We also make  github repositories of matching names to hold the Dockerfiles which produced images (a common pattern for github/dockerhub usage, especially with automated builds; so we start this practice.) ",2
"Test if IPMI can replace KVM
Can we use IPMI in place of a KVM?  Can we reliably do the following across our various server vendors?  - power cycle hung systems  - view and use text console  - view and use remote gui console  - boot from a remote ISO",3
"Create a LSST base CentOS image
nan",2
"Document use of LSST image for CentOS
nan",1
"Gather Open Stack needs/requirements from DM team
nan",9
"Assist DM team in accessing test open stacks
nan",1
"Fix qserv_testdata documentation
qserv_testdata relies on sconsUtils, and its build procedure has to be clearly documented.",1
"Add support for mysql JDBC driver
SUI which rely on JDBC fail because they internally issue some queries that are not yet supported by Qserv. Need to patch it (in the short term), and add proper support (in the long term). This story covers the patching only. The queries that upset Qserv are listed below.   {code} SHOW VARIABLES WHERE Variable_name ='language' OR Variable_name = 'net_write_timeout' OR Variable_name = 'interactive_timeout' OR Variable_name = 'wait_timeout' OR Variable_name = 'character_set_client' OR Variable_name = 'character_set_connection' OR Variable_name = 'character_set' OR Variable_name = 'character_set_server' OR Variable_name = 'tx_isolation' OR Variable_name = 'transaction_isolation' OR Variable_name = 'character_set_results' OR Variable_name = 'timezone' OR Variable_name = 'time_zone' OR Variable_name = 'system_time_zone' OR Variable_name = 'lower_case_table_names' OR Variable_name = 'max_allowed_packet' OR Variable_name = 'net_buffer_length' OR Variable_name = 'sql_mode' OR Variable_name = 'query_cache_type' OR Variable_name = 'query_cache_size' OR Variable_name = 'license' OR Variable_name = 'init_connect'  SELECT @@session.auto_increment_increment  SET NAMES latin1  SET character_set_results = NULL  SET autocommit=1  SET sql_mode='STRICT_TRANS_TABLES' {code}",2
"Add support for BIT columns
The mysql/SchemFactory code has incomplete logic for dealing with columns that have BIT type - in particular, it doesn't properly handle the length of a bit field, and it sets the SQL type to ""BIT?"", which causes table creation failures later on. Fixing this requires modifying SchemaFactory, and making sure that bit values are  transmitted properly.",4
"Add support for transmitting [VAR]BINARY column data
The code that pulls data out of a {{MYSQL_ROW}} and puts it into a protobuf {{RowBundle}} does not handle binary data correctly. See [_fillRows|https://github.com/LSST/qserv/blob/master/core/modules/wdb/QueryAction.cc#L217] for the relevant code.  The issue is that the generated {{add_column(const char*)}} member function of {{RowBundle}} treats the input as C-style NULL-terminated strings. But in the case of {{BINARY}} column data (and {{VARBINARY}}/{{BLOB}} variants, maybe also {{BIT\(n\)}}), the contents can contain embedded NULLs. We are currently using such columns for user defined types in MySQL (e.g. image bounding polygons), so it's important to get this right. On the protobuf side the fix is as simple as calling {{add_column(const char* value, int size)}} instead. I'm not a MySQL C API expert, but the size will presumably have to be obtained/derived from the corresponding {{MYSQL_FIELD}}.  ",8
"Drawing speed will detect and optimize for large datasets
nan",10
"Finished Background conversion
nan",2
"Set up GIT hub for ipac firefly
nan",18
"Span-based shrink operations for Footprint
Analogous to DM-1128, but shrinking rather than growing footprints.",5
"Install docker in an ISL OpenStack Ubuntu instance, perform basic checks
nan",1
"Prototype HTM-based spatial binning to visualize large number of catalog sources
Currently we are using a generic 2-d binning algorithm, that is finding minimum and maximum values of the two columns to be visualized and bins the values into 2-d grid with the specified number of grid cells.  This algorithm distorts data in the pole regions and whenever data are on both sides of ra=0. More smart binning based on a spatial index is necessary when reducing the number of (ra,dec) entries intended for visualization.  ",16
"Resolving QServ database configuration/connectivity issues
To start the development I should be able to connect to the QSERV development database via VPN and run simple queries.   Ideally, I'd like to be able to connect to QSERV with JDBC, view the data, run spatial queries. I also need an access to data dictionary to interpret data.",10
"Evaluate SDSS catalog access
SDSS allows two ways to access their catalog data: via HTTP Post service, accessible to anonymous users, and via CasJobs services, which require having an account. Evaluate how SDSS catalog data can be accessed from an application by prototyping single and multiple target searches accessing SDSS services. ",10
"FY17 Add Support for User Upload Tables for Qserv
Users will need to be able to upload a list of ""things"" to workspace, then request ""repeat a given query for each ""thing"" from the uploaded list"". Example of ""things"": ra/dec points, ra/dec points + distances, object ids, object names, bounding boxes, etc. In IPAC terminology, this is called ""table upload queries"". Note, we will need to assign some sort of unique id to such list. Results must contain information which result correspond to which ""thing"". E.g, if user asks for neighbors near (1,1), (4,4), it is not enough to just return list of neighbors, we need to tell which neighbor is for which point.  In Qserv, that means that user will upload a special table with these ""things"", and then issue a query that involves join against that table.   We need to evaluate and agree on how generic these upload tables can be.  We might need to do some special optimizations (e.g., related to spatial searches). ",53
"FY17 User Upload Tables for ImgServ
Users will need to be able to upload a list of ""things"" to workspace, then request ""repeat a given query for each ""thing"" from the uploaded list"". Example of ""things"": ra/dec points, ra/dec points + distances, object ids, object names, bounding boxes, etc. In IPAC terminology, this is called ""table upload queries"". Note, we will need to assign some sort of unique id to such list. Results must contain information which result correspond to which ""thing"". E.g, if user asks for neighbors near (1,1), (4,4), it is not enough to just return list of neighbors, we need to tell which neighbor is for which point. In Qserv, that means that user will upload a special table with these ""things"", and then issue a query that involves join against that table.  We need to evaluate and agree on how generic these upload tables can be.  We need to decide on format / location: csv? Table in database?   We might need to do some special optimizations (e.g., related to spatial searches).",53
"A better coordinate grid overlay
Some of the grid lines are not drawn right and the labels are not in the locations.",10
"Research Javascript Frameworks: General Overview
Begin looking into JS frameworks. Start to look into AngularJS, try to write some sample code.  Attempt to understand the main concepts. Gat an overview of the others out there.  ",12
"Work with the database group to define initial concepts for backend interface API 
We are starting to gel around some ideas. ",10
"Improve management of integration test datasets description
Currently data set description (i.e. data file extension, compressed extension, schema file extension) is hard-coded in python/lsst/qserv/tests/datareader.py  This could be improve (standard format for test dataset, meta service, meta-configuration file, ...)",5
"Implement drawing only active tab with new data model
nan",6
"LOE - Week ending 11/07/14
nan",7
"LOE - Week ending 11/14/14
nan",6
"LOE - Week ending 11/21/14
nan",8
"LOE - Week ending 11/28/14
nan",12
"Create integration test case using data duplicator
Integration tests should provide a new test case which use sph-duplicate in partition package.",6
"Setup Qserv for SUI tests
Setup Qserv on lsst-db2 with and load some reasonable data set (perhaps PT 1.2). One potential caveat: we need to setup access for some accounts that are ideally other than our internal qsmaster.",2
"Test deblended CModel colors
Using HSC data, examine color-color and color-magnitude diagrams with deblended CModel magnitudes.  Investigate outliers by looking at images and deblended HeavyFootprints.",8
"Basic validation of LSST pipeline on HSC data
Get the pipeline running on HSC data to the point where nothing is obviously wrong.",8
"add support for ""freeze-drying"" measurement failures
We should make it easy for users to inspect and capture problems in measurement algorithms into an on-disk format that allows them to be reproduced later with minimal setup (ideally, the package would require no access to the original data or configuration).  While this issue is mostly concerned with capturing problems in measurement algorithms, an extension to capture deblender failures should be considered as a future extension.",6
"Support Mac OS in scisql-deploy.sh
cf. Andy Connolly message: {quote} Just as an FYI on my mac scisql-deploy.py was looking for 0.3.4/lib/libscisql-scisql_0.3.so but there is only 0.3.4/lib/libscisql-scisql_0.3.dylib {quote}",1
"Sanitize AstrometryTask interface
Currently the AstrometryTask and Astrometry class share work.  E.g. distortion is done in AstrometryTask but matching is done in Astrometry.  AstrometryTask also makes assumptions about what fields are available in the solver config.    The AstrometryTask interface should be sanitized so that it can be used as a thin wrapper for calling any astrometry solver.  Top level config params should go in the AstrometryTaskConfig and solver level work should be done in the solver class and configured at the solver level.",10
"Rework Astrometry class
The Astrometry class shares information upstream with AstrometryTask.  This should be factored out so that the Astrometry class can be configured and called via a single well known method (solve?).  One thing the Astrometry class that is needed by down stream processing (PhotoCal) is to match sources.  This is currently a private method, but should be made public so that  it can be used without running AstrometryTask.",4
"Move photocal out of meas_astrom
It is confusing that photocal is in meas_astrom.  I assume that is historical.  I think it could probably live in pipe_tasks.",2
"Implement replacement for A.net index files
Astrometry.net index files are hard to generate and hard to read.  We need another, more flexible, more standard system for storing reference files.  We should also be able to read FITS files and other formats, but having a standard format with the utilities to create and query them is a must.  Coming up with a format that satisfies astrometry.net's solver may be too hard, because a.net requires quads, which a non-blind solver may not need. However, a format that we can convert to something suitable for a.net would probably suffice (conversion would presumably be a separate task that is run once).  It will be easier to identify a suitable format once we have identified at least one solver other than than a.net that we wish to implement or adapt. hscAstrom appears to use a catalog of star positions, which is nice and simple.",5
"Implement a replacement solver to the current A.net solver
This should be further refined.  The solver will be required to work with several input formats.  It will only be required to solve in the in the presence of a reasonable starting point with reasonable pointing errors.  Failure should be graceful.  If multiple, equivalent solutions are found, this should be reported (for situations including perfect grids of sources).",15
"Qserv spatial restrictor names are case sensitive
The SQL grammar treats Qserv spatial restrictor names case insensitively, but {{qana/QservRestrictorPlugin.cc}} does not, which means that one must use e.g. {{qserv_areaspec_box}} rather than {{qserv_areaSpec_box}}. We are loose with case in a lot of our wiki pages, so we really should fix this to avoid confusing users. Also, case insensitivity is consistent with MySQL behavior for UDF names.",1
"Research how to integrate different image metadata stores with DataCat
Data Release Production will generate highly structured image metadata (exposure* tables). If we decide to use DataCat (e.g., for keeping more ad hoc metadata), the question arises if/how to integrate all this together: * should we integrate all exposure* tables from all releases into DataCat? (eg via foreign tables) * should we keep them distinct, and integrate at higher level (e.g., Metadata Service)",4
"Design system for tracking existing images/files
We have a lot of files/images already brought in or generated through Data Challenges done to date. We need a system for cataloging them. This story will define such system, eg, sketch of UI, underlying technology used (DataCat, plain mysql, schema etc).",6
"Define structure of web form for collecting metadata about existing data sets
Web alpha version of the form (using django or Fermi Java webservices code) that collects input from users about data repositories. Authentication not covered in this version.",2
"Implement FITS header crawler and integrate it with the form
Implement crawler that walks through registered repos (through the form) and loads metadata from fits headers into the mysql backend",7
"Research and experiment with building form for capturing user input
Need to build a form that will be used to capture user input about existing image repositories (users will be registering their files/repositories). Options to consider: python-based django, java-based system that is part of Fermi DataCat. ",4
"Break down & discuss DM-1074
I will take the lead on DM-1074. First step will be to sit with [~jbosch], get a feeling for what needs to be done, and sketch out a set of stories.",1
"Convert test_qservAdmin.py into a real unit test
Need to turn ./admin/tests/test_qservAdmin.py into a real unit test. In the past it was broken and it went unnoticed, see DM-1395",1
"Remove check for stack dir write access in qserv-configure.py
qserv-configure.py checks for write access to stack dir, this should be replace by check for read access, or removed.  fjammes@qserv-build-server-xlarge:~/src/qserv$ qserv-configure.py --all INFO: Qserv configuration tool =======================================================================  WARNING : Do you want to erase all configuration data in /home/fjammes/qserv-run/2014_10.0 ? [y/n] y INFO: Copying template configuration from /home/fjammes/stack/Linux64/qserv/2014_10.0/cfg/templates to /home/fjammes/qserv-run/2014_10.0 INFO: Creating meta-configuration file: /home/fjammes/qserv-run/2014_10.0/qserv-meta.conf INFO: Reading meta-configuration file /home/fjammes/qserv-run/2014_10.0/qserv-meta.conf INFO: Defining main directory structure INFO: No write access to dir /home/fjammes/stack/Linux64/qserv/2014_10.0 : [Errno 13] Permission denied: '/home/fjammes/stack/Linux64/qserv/2014_10.0/write_tester' ",2
"Research Javascript Frameworks: Understand Angular & React
Write some prototype code with Angular and then try to do the same thing with React",10
"Clean up multi-component Footprints
Following the landing of DM-1545, it's possible for an erosion operation on a footprint to cause it to split into multiple components and for peaks which were previously inside the footprint to not fall inside any of those components.  Here, we should provide a ""clean up"" operation that takes a multiple-component footprint and splits it into a set of contiguous footprints with appropriate peak lists.",4
"init script fails to start xrootd after crash
I think we saw this issue in the past, not sure it was actually fixed back then or just was to reintroduced one more time.  After crash of xrootd the regular {{etc/init.d/xrootd start}} fails to start it: {code} [salnikov@lsst-dbdev2 dm-621]$ ~/qserv-run/dm-621/etc/init.d/xrootd status xrootd is dead but PID file exists                         [FAILED]   see /usr/local/home/salnikov/qserv-run/dm-621/var/run/worker/xrootd.pid [salnikov@lsst-dbdev2 dm-621]$ ~/qserv-run/dm-621/etc/init.d/xrootd start Starting xrootd: (already up)                              [  OK  ] [salnikov@lsst-dbdev2 dm-621]$ ~/qserv-run/dm-621/etc/init.d/xrootd status xrootd is dead but PID file exists                         [FAILED]   see /usr/local/home/salnikov/qserv-run/dm-621/var/run/worker/xrootd.pid {code} I can start it with {{restart}} but I think that {{start}} should detect that xrootd is dead ({{status}} does that) and start service correctly.  This issue may exist for other services but I did not check. ",1
"Design of calibration and ingest system
Produce a confluence page describing the approach to be taken to solve DM-1074. Ensure that all the relevant folks have reviewed that page and are happy. Break down DM-1074 into stories appropriate to that design.",10
"Determine if Astrometry class is desired
The question is whether the Astrometry class is the thing that is overridden or if the AstrometryTask has component configurables that are overridden.  Also, determine location default implementation.",2
"Add support for c-style comments in front of queries sent to qserv
Connecting to qserv from java fails because the jdbc driver inserts comments. ""/* ... */"" in front of queries (example pasted below). The fix involves removing the comments at the proxy level   {code} SQLException: Qserv error: 'ParseException:ANTLR parse error:unexpected token: /:'  Query being executed when exception was thrown: /* mysql-connector-java-5.1.34 ( Revision: jess.balint@oracle.com-20141014163213-wqbwpf1ok2kvo1om ) */SHOW VARIABLES WHERE Variable_name ='language' OR Variable_name = 'net_write_timeout' OR Variable_name = 'interactive_timeout' OR Variable_name = 'wait_timeout' OR Variable_name = 'character_set_client' OR Variable_name = 'character_set_connection' OR Variable_name = 'character_set' OR Variable_name = 'character_set_server' OR Variable_name = 'tx_isolation' OR Variable_name = 'transaction_isolation' OR Variable_name = 'character_set_results' OR Variable_name = 'timezone' OR Variable_name = 'time_zone' OR Variable_name = 'system_time_zone' OR Variable_name = 'lower_case_table_names' OR Variable_name = 'max_allowed_packet' OR Variable_name = 'net_buffer_length' OR Variable_name = 'sql_mode' OR Variable_name = 'query_cache_type' OR Variable_name = 'query_cache_size' OR Variable_name = 'license' OR Variable_name = 'init_connect' {code}",1
"Prevent collisions in /tmp related to scisql deployment
Deploying scisql involves creating a file in  /tmp. The file never gets removed. This can cause the following error when Qserv is installed later on the same machine: {code} ERROR: failed to open output file /tmp/scisql_demo_htmid10.tsv for writing {code}  We should switch to using a temporary file instead of file with fixed name.",2
"Qserv scons scripts do not pick up the version of swig provided by eups
See the summary. The consequence is that the Qserv integration tests fail on systems that provide swig 2.x+ - there seems to be some implicit dependency on SWIG 1.x. The reason may be that swig is getting confused about shared_ptr to objects that are defined in one module, but used in another (recent swig reorganization split czar and css into two separate swig modules).",2
"Sanitize configs
Some solver specific information is stored in the AstrometryTask config.  Further, the solver config is accessed inside the AstrometryTask run method.  This mixing of information make it hard to make the framework pluggable.  Solver configuration should be confined completely within the solver class (whether it be part of the Astrometry class or a configurable of its own).",2
"Add unit tests to test c-style comments in/around queries
I should have thought about it when doing DM-1601 but I didn't... it'd be good to add test queries that test comments before/after/inside query.",1
"Move meas_algorithms unit tests to meas_base framework
The following tests in meas_algorithms need to be ported to the meas_base framework:  measure.py psfSelectTest.py testPsfDetermination.py ticket2019.py testCorrectFluxes.py (though this cannot be done until the algorithm exists)",2
"Research off-the-shelf solutions for harvesting metadata
Relevant links:  * http://www.ivoa.net/documents/SIA/20141024/index.html - there’s a list of metadata in the document  * CAOM (Common Archive Object Model): http://www.cadc-ccda.hia-iha.nrc-cnrc.gc.ca/caom2/",4
"Integrate Metadata Store prototype v1 with cat and db  
Integrate prototype developed through DM-1255 with cat and db as appropriate (don't hardcode schema, don't hardcode credentials, etc).",4
"Experiment with DataCat foreign tables
This is a placeholder story, we should break it down into more smaller stories...",15
"Add support for mysql JDBC driver (v2)
MySQL client 4.1 and higher is stripping out comments before sending them to server, so the fixes done in DM-1539 are not sufficient.",1
"Design and implement CSS structure for distributed Qserv setup
For management of the distributed databases/tables we need info in CSS about all workers and tables. The info will be created by data loader and updated by replicator which do not exist yet. For this issue we need to provide python API which can fill the same information in CSS so that we can build and test other pieces needed for this epic.",5
"Implement remote host access for management framework
To manage remote workers we need a way to access services on remote machines that run workers. Services may be something like mysql (which we would prefer to run without TCP port open) and optionally xrootd. This ticket will implement Python module which will hide a complexity of doing things like ssh/port forwarding/authentication from the client.",5
"Client library for accessing distributed database/table information from CSS
Provide Python interface for accessing information in CSS which is relevant to distributed management, such as database/table/node data. This interface can be used also by data loader and replicator.",8
"Implement distributed database creation
Implement Python library which creates databases on all active workers based on info from CSS.",10
"Implement distributed table creation
Implement Python library for creating mysql tables on all active workers.",11
"Move CSS documentation close to code 
CSS documentation about the structure is currently in trac (which is readonly), at https://dev.lsstcorp.org/trac/wiki/db/Qserv/CSS. We need to move it close to the source code, e.g., to a doc file.",2
"Add unit test to to verify zookeeper versioning works
This is a follow u pto DM-1453, we need to add a unit test that will prove that mismatched versions in zookeeper and software are properly handled.",1
"Add unit test for queries from DM-1539
Need to add unit test for queries listed in DM-1539",1
"Qserv should report it's version
It should be possible to determine which version of qserv we are running by looking at information from log files. So, in practice, we probably should generate in scons a unique id (from sha from git?) and compile it into the code.",1
"SciSQL should report its version
It should be possible to determine which version of scisql we are running.",1
"Build 2014_12 Qserv release
The title says it all. Please open ticket for next release when closing this one.",1
"Integrate metaserv, imgserv and dbserv with Data Access Services
* Create dbserv for handling database related web-based queries, support running queries through web (initial version).  * Integrate dbserv, metaserv and imgserv RESTful API into the webserv - want to run all services (meta, image, db) under one server.  * Also want to be able to run them separately, so have some handy servers for each service * Proof of concept for supporting multiple formats (html, json)",22
"Adopted/Retired RFCs are not counted as resolved
Also, marking an RFC as Adopted brings up a box with a message applicable to the Closed status.",1
"New RFCs should result in dm-devel E-mails and HipChat postings
Email notices of new RFCs filed with a DM component go to dm-devel Email notices of new RFCs filed with a DM component go to Data Management chat room Email notices of all new RFCs go to Bot: RFC room ",1
"Build 2014_11 Qserv release
The title says it all. Please open ticket for next release when closing this one.  Tasks to do:  - publish last buildbot build under a temporary eups-tag (""qserv-dev"")and test it, if it works fine: - create git-tags for Qserv and dependencies - publish the release with eups-tags ""qserv"" and ""YYYY_MM"" - generate and publish the doc for release ""YYYY_MM"" - update release number in Qserv code and set ""YYYY_MM+1"" as release in dev and ""YYYY_MM"" as stable release (update {{admin/bin/qserv-version.sh}}, {{doc/source/conf.py}}, {{doc/source/toplevel.rst}}) - generate and publish the doc for release ""YYYY_MM+1""  - look at the doc - commit  {{admin/bin/qserv-version.sh}}, {{doc/source/conf.py}}, {{doc/source/toplevel.rst}} with current ticket number   This procedure should be validated and documented.",1
"Update build process for Firefly opensource
Update build and deploy related scripts to reflects the changes affected by open sourcing of Firefly.",7
"Remove redundant CORS headers from Firefly's http response
Make sure CORS related headers are not sent when the Origin header is missing.  Firefox does not like it.",1
"Research popular web development technologies
Research popular web development technologies to prepare Firefly for the future with the focus on front-end framework. ",6
"Document ""getting started"" procedure for new stack developers
Document a procedure for building the stack on a new system in a way that is appropriate for both project members and external contributors.  This can be linked as a ""getting started"" guide from http://dm.lsst.org/.",5
"LOE - Week ending 12/5/14
nan",8
"preparation work for FIrefly open source
1. discuss with IPAC director and managers to open source Firefly 2. study the major open source license,Apache, GPL, BSD 3-clause, MIT.  3. final decision: BSD-3 clause",6
"Use an OpenStack instance to run an HTCondor Central manager
Use an OpenStack instance to run an HTCondor Central manager",4
"W16 Research technologies for Data Access
Research technologies potentially useful for Data Access / Database. This epic covers Apache Mesos, Google Kubernetes, MaxScale, Serf, Consul, and MemSQL.",38
"The existing FITS reader class needs to be refactored to improve the performance(1)
- checkout the classes - understand the FITS standard and current implementation",16
"Extend data loading script to support multi-node setup
Implement simplest use case for data loading with one or more worker database separate from czar database. Simplest means minimal functionality in what concerns access to workers, just assume for now that we can connect to every worker directly using regular TCP connection. It should be possible to just add a list of worker nodes as an option to loader script and send the chunks in some random order to the workers in that list. Of course chunks for different tables should end up on the same host, so some form of chunk management is needed (use for now whatever is defined in CSS doc on trac).",8
"Working with SLAC on definition of metadata store
Follow up metadata store schema development to ensure SUI will be able to use it. Define the fields that should go into Data Definition table. Define the fields that must be present in the image metadata table, which SUI will be searching.",2
"hipchat support for maigically urlifying docushare documents
It would be generally use full if references to official documents in hipchat (or it's successor) automagically generated urls for official document handles.  Eg: Document-1234, LSE-123",1
"LOE - Week ending 12/12/14
nan",8
"Add git bisect tool for Qserv repos
{code:bash} fjammes@clrlsst-dbmaster-vm:~/src/qserv (u/fjammes/DM-627 $%) $ qserv-test-head.sh -h  Usage: qserv-test-head.sh [options]    Available options:     -h          this message     -q          quick: only rebuild/install new Qserv code,                 and perform test case #01    Rebuild from scratch, configure and run integration tests against   a Qserv git repository.   Pre-requisite:     source loadLSST.bash     setup qserv_distrib -t qserv     setup -k -r ${QSERV_SRC_DIR}    Can be used with 'git bisect' :     cd ${QSERV_SRC_DIR}     git bisect start     git bisect bad     git bisect good git-commit-id     git bisect run /home/fjammes/src/qserv_testdata/bin/qserv-test-head.sh {code}  Code is in DM-627 ticket branch.",2
"Aperture Flux back into an abstract class
The last change to ApertureFlux to make it work with the new C++ design changed ApertureFluxAlgorithm into an instantiatable class.  However, I have now figured out how to make this work with SWIG while still allowing measure and fail to be defined by default at the ApertureFlux level..  So this issue is to put things back in order.",1
"Statistics tests use a constant image
I just noticed that the tests for Statistics (clipped mean etc.) use a constant image.  We should be testing a Gaussian field (although that makes the tests a little trickier) ",2
"czar log4cxx link/load bug
Under ubuntu 14.04 (at least), the czar falls over at load time with an unresolved sym for typeinfo for log4cxx::helpers::ObjectPtrBase while loading the css python wrapper shared lib.",2
"Make qserv dependencies build on OS X with clang
Fix anything necessary for qserv dependencies to build on OS X with clang.  Note -- making qserv itself build is more complicated and may require a separate ticket.",4
"fix dependency problems in obs_subaru scons scripts
When building obs_subaru with -j4, it often tries to build files related to the defects before the main Python module is built, resulting in import errors (because the scripts it invokes depend on the main Python module).    We need to rewrite the SCons scripts to ensure this dependency is captured.    A preliminary look indicated that this is not entirely trivial, and I'll have to remind myself a bit of how some things in SCons work to get it done, so I'm putting this off for a future sprint.    In the meantime, the workaround is to build obs_subaru with no parallelization.",2
"Understand historical written docushare materials dealing with the operational security environment.
researched the docushare traversing the plans for materials to be embedded in the OCS and similar systems, as well as extant operational plans. The goal was to understand how to separate the the security responsibilities of development, what security constraints ought to be but on items that are delivered, and what to tell the camera and telecscope teams  to mender ea smooth integration with IT security systems upon delivey and integration of their sub systems in Chile.",3
"Install PgMySQL and use to connect to local Qserv.
Used ""pip"" to install it. ""Conda"" should work as well. Therefore, it should be easy to make it part of the delivered system: VM, container, tar file, after the fact download, etc. It has documentation, uses the MIT license, under active development and available from PyPI. DB connection is straight forward and requires little experience to get meaningful work done.",2
"Create SQL code to read Qserv into Python Pandas data frame
This works well, at least for a simple case. You can move directly from a query statement to a Pandas data frame for analysis in just a few lines of code. Here is the start of an iPython Qserv session showing how easy it is.  In [6]: import pandas as pd In [7]: import pymysql as db  In [8]: conn = db.connect(host='lsst-db1.ipac.caltech.edu',port=4040, user='qsmaster', passwd='', db='LSST')  In [11]: df = pd.read_sql(""select deepCoaddId, tract, patch, ra, decl from DeepCoadd"", conn)  In [12]: df Out[12]:     deepCoaddId  tract   patch        ra      decl 0      26607706      0  406,11  0.669945  1.152218 1      26673242      0  407,11  0.449945  1.152218 2      26804242      0   409,2  0.011595 -0.734160 3      26673154      0   407,0  0.449945 -1.152108 … ",2
"Explore queries for Qserv database.
Use a local database here at IPAC with 5 Qserv tables in it. Looked at several Python query interfaces. Used the pymysql interface for testing because it's pure Python and because I found reports suggesting it was almost as fast as the mysqldb interface that requires C language libraries.  Ad hoc queries can be constructed in three lines of code, so useable in a science environment.  Found a couple of bugs in Qserv that were reported.",6
"Begin looking at how Python Pandas can be used for LSST data analysis.
Pandas is well integrated with the other parts of SciPy: numpy, matlibpy, etc.  It’s a good candidate for data analysis, especially where time series are involved. However, there are no multidimensional columns, poor metadata support for FITS files and a need to use masks instead of NaN values. These may, or may not, be problems.  There is a 400 page book about Pandas, so it will take some further time to learn its value, especially with astronomical data in different situations.",5
"Allow SWIG override for broken SWIG installations
Dependency on SWIG 2.0+ was introduced into Qserv, and this broke Qserv building on systems relying on SWIG 1.3.x.  This ticket introduces basic code to override SWIG_LIB on those systems to allow use of the broken installation (some SWIG search paths are fixed during its build process otherwise).",1
"Data ingest scripts cleanup
nan",10
"Identify dead code
nan",1
"Write documentation for SSI interface
nan",5
"multi-error utility class
QueryAction::Impl currently has a handful of private members/methods related to maintaining a collection of errors that occurs while processing a query (_addErrorMsg, IntString, IntStringVectory, _errors, etc.)  It would be useful to split this stuff out into a separate, re-usable utility class, and extend, then extend with some additional functionality (output stream operator, subclass from std::exception, maybe capture __LINE__ and __FILE__, integrate with logging, etc.)  [Fabrice: discuss requirements and basic design w/ Fritz and/or Daniel]",10
"Minor bug in a test
tests/centroid.py has a bug in testMeasureCentroid: ""c"" is undefined in the following bit of code: {code} if display:     ds9.dot(""x"", c.getX(), c.getY(), ctype=ds9.GREEN) {code}",1
"Implement ""unlimited"" result size handling
DM-854 exposed an issue in handling large results. Result rows are returned from worker to czar in protobufs messages. However, protobufs messages should not be larger than some number of megabytes, according to protobufs documentation. IOW, protobufs is not designed to handle messages on the order of hundreds of megabytes. There may be some code in the protobufs implementation that does not scale beyond messages of a few megabytes. Hence, in order to send larger amounts of result rows, we need to use multiple messages. The current protobufs definition for result messages includes a placeholder for chaining result messages, but there is no code on the czar or worker that implements result chaining.   The scope of this ticket is to implement message chaining for results on the worker and on the czar, in such a way that it places no limits on the overall number of result rows (if there is a limit, it should be no smaller than quadrillions and be well-documented).  ",15
"Define interfaces for Data Access Services
nan",8
"Implement interfaces for Data Access Services
Implement proof of concept, skeleton of the prototype. The work will continue in follow up stories in February and in S15.",8
"Integrate image cutout service interfaces with butler
nan",5
"Finish image cutout service implementation
Define appropriate interfaces and connect them with the RESTful API (see DM-1695).",9
"Finish metadata store prototype
* A quick proof-of-concept prototype of loading tool for loading database-information into metadata store.   * Handling mysql credentials through auth file in home dir instead of hardcoded values.",4
"Create read only OpenStack volume and execute processing scenario
Create read only OpenStack volume and execute processing scenario",4
"Clone OpenStack volume for use against multiple instances
Clone OpenStack volume for use against multiple instances",2
"Examine fqdn/hostname assignment for OpenStack instance
Examine fqdn/hostname assignment for OpenStack instance",4
"S15 Implement Database & Table Mgmt
Continuation of DM-1036, making the code for managing distributed databases and tables more feature reach, including features such as deletion.",53
"S15 Run Large Scale Qserv Tests
Run large scale tests to uncover unexpected issues and bottlenecks.",48
"S15 Tune Qserv
Fix scalability and performance issues uncovered through large scale tests DM-1704",100
"S15 Analyze Qserv Performance
Final analysis of Qserv performance, measure KPIs. Based on LDM-240, we are aiming to demonstrate:  * 50 simultaneous low volume queries, 18 sec/query  * 5 simultaneous high-volume queries, 24 h/query  * data size: 10% of DR1 level.  * Continuous running for 24 h with no software failures.  ",5
"S15 Refactor Qserv
Ongoing refactoring of Qserv - code cleanup, tightening interfaces etc.",100
"W16 Improve Query Coverage in Qserv
Currently Qserv supports only a limited subset of queries. We need to make sure it supports all queries that users need to run.",46
"Implement result sorting for integration tests
We need to be able to sort results, because we can't always rely on ORDER BY. So we need a formatting per query in the integration tests (sort result for some, don't sort for others etc.)    The following queries have been disabled because we don't have result sorting, so once it is implemented, we will need to re-enabled them prior to closing this ticket:  {code}  case02/queries/0003_selectMetadataForOneGalaxy_withUSING.sql  case02/queries/3001_query_035.sql  case02/queries/3008_selectObjectWithColorMagnitudeGreaterThan.sql  case02/queries/3011_selectObjectWithMagnitudes.sql  case02/queries/3011_selectObjectWithMagnitudes_noalias.sql  {code}",8
"ValueError in lsst.afw.table.Catalog.extend()
{code} from lsst.afw.table import BaseCatalog, Schema  s = Schema() c1 = BaseCatalog(s) c2 = BaseCatalog(s)  c1.extend(c2) {code}  The above fails, saying:  {code} Traceback (most recent call last):   File ""test.py"", line 7, in <module>     c1.extend(c2)   File ""/Users/jds/Projects/Astronomy/LSST/stack/DarwinX86/afw/10.0+3/python/lsst/afw/table/tableLib.py"", line 6909, in extend     _tableLib.BaseCatalog_extend(self, iterable, deep) ValueError: invalid null reference in method 'BaseCatalog_extend', argument 3 of type 'lsst::afw::table::SchemaMapper const &' {code}",1
"S15 Improve MetaServ: RESTful, Basic Image Search, DDL, Config Files
Implement beta version of the Metadata Service. This version will support basic image search, DDL, config files, and RESTful interfaces for MetaServ and Database (Qserv).",57
"S15 Add Support for Image Stitching and Rotation, RESTful APIs
Implement image stitching and rotating, including RESTful APIs.",35
"S15 Image & File Archive v2
System for tracking existing image data sets integrated with metadata services.",5
"Integrate MetaServ with Schema Browser
Schema browser displays detailed info about schema, including custom fields like UCD, units etc. This information is stored as comments embedded in the master version of the schema (in ""cat"" repo). Currently we are generating ascii from the master schema for schema browser, and we load it into mysql, then schema browser reads it from mysql. This story involves changing schema browser such that it will read the schema information directly from MetaServ.",6
"Disable query killing
Apparently killing a query through Ctrl-C is confusing xrootd. Disable query killing (which seems to be only partly implemented).",1
"Implement query killing through Ctrl-C
Need to properly implement query killing through Ctrl-C",16
"Make secondary index for director table only
Following discussion on qserv-l, we only need to generate ""secondary"" index for director table, no other table is supposed to have it. Need to modify data loader to recognize which table is director table and generate index only for that table. ",2
"S15 Improve Query Coverage in Qserv
Query coverage in the qserv integration testing is very limited, we have been turning off more and more queries and we were making the qserv code and the data loader more strict. This epic covers work (fixes and improvements) related to * re-enabling test queries marked as ""fixme"" (when it make sense, some queries are for features that are not implemented yet) * adding more queries to test interfaces and features that are implemented but are not currently tested.",40
"LOE - Week ending 12/19/14
The System Administration team at NCSA worked on the following LOE tasks this week: - RMA'ed RAID card for lsst-dev <https://jira.ncsa.illinois.edu/browse/LSST-599> - Updated user's updated public ssh keys <https://jira.ncsa.illinois.edu/browse/LSST-614> - Rebuilt failed drive in lsst-dbdev2 <https://jira.ncsa.illinois.edu/browse/LSST-610> - Increasing drive size of lsst-eval <https://jira.ncsa.illinois.edu/browse/LSST-611> - Researched & repaired lsst20 NFS issues <https://jira.ncsa.illinois.edu/browse/LSST-591> - Upgraded lsst-xfer to 10G networking",14
"stack build fails on gcc 4.8 with opt=3
The stack fails to build with gcc 4.8, with a test failure in meas_base (though a similar problem on the HSC fork suggests the problem is actually in afw).  [~darko] reports that on OpenSuse 13.1, the failure goes away when compiling with opt=1 instead of the default opt=3, indicating that the problem is overly aggressive optimization.  This, and the fact that a traceback of the HSC-side failure implicates MaskedImage::getXY0, leads me to guess that something is going wrong with the alignment of the Eigen data members in afw::geom::Point.  Until that theory is disproven, this probably belongs in my court, though I'd be happy to let someone steal it from me if they're interested in working on it.",4
"Stabilize Firefly Repository
- Make GitHub firefly ready but not public - Add firefly to LSST git - Clean up any lingering issues from separation - break up IRSA Viewer from Firefly Viewer",14
"Research Javascript Frameworks: Work toward future proposal
Take prior research and come up with rough firefly migration proposal.  Include how to use a hybrid system for foreseeable future. Write some prototype code.",16
"fix table file handling of MANPATH in dependencies
As discussed on DM-1220, the table files for:  - mysqlproxy  - protobuf  - lua  - expat should have the MANPATH entry removed entirely, while:  - xrootd should have "":"" added to the end of its MANPATH value, to allow the default paths to be searched as well.",1
"Fix error on duplicate result_id_m table while launching qserv integration tests
Next command:  {code} qserv-check-integration.py --case=01 --load;  qserv-check-integration.py --case=02 --load {code} Fails, most of the time, with next error in logs: {code} qserv@clrinfoport09:~/src/qserv (u/fjammes/DM-627 *+)⟫ cat ~/qserv-run/2014_12/var/log/qserv-czar.log | grep result_1211906_m 0103 16:38:18.576 [0x7f23c4ff9700] DEBUG root (build/rproc/InfileMerger.cc:432) - InfileMerger create table:CREATE TABLE qservResult.result_1211906_m (`ra` DOUBLE) 0103 16:38:18.661 [0x7f23c4ff9700] DEBUG root (build/rproc/InfileMerger.cc:354) - InfileMerger sql success: CREATE TABLE qservResult.result_1211906_m (`ra` DOUBLE) 0103 16:38:18.661 [0x7f23c4ff9700] DEBUG root (build/rproc/InfileMerger.cc:445) - InfileMerger table qservResult.result_1211906_m is ready 0103 16:38:18.686 [0x7f23c77fe700] INFO  root (build/rproc/InfileMerger.cc:299) - Merging w/CREATE TABLE qservResult.result_1211906 SELECT ra AS ra FROM qservResult.result_1211906_m ORDER BY ra 0103 16:38:18.720 [0x7f23c77fe700] DEBUG root (build/rproc/InfileMerger.cc:354) - InfileMerger sql success: CREATE TABLE qservResult.result_1211906 SELECT ra AS ra FROM qservResult.result_1211906_m ORDER BY ra 0103 16:38:18.720 [0x7f23c77fe700] INFO  root (build/rproc/InfileMerger.cc:305) - Cleaning up qservResult.result_1211906_m 0103 16:38:18.721 [0x7f23c77fe700] INFO  root (build/rproc/InfileMerger.cc:317) - Merged qservResult.result_1211906_m into qservResult.result_1211906 0103 16:38:24.716 [0x7fe3cf7fe700] DEBUG root (build/rproc/InfileMerger.cc:432) - InfileMerger create table:CREATE TABLE qservResult.result_1211906_m (`QS1_COUNT` BIGINT(21)) 0103 16:38:24.717 [0x7fe3cf7fe700] ERROR root (build/rproc/InfileMerger.cc:351) - InfileMerger sql error: Error applying sql. Error 1050: Table 'result_1211906_m' already exists Unable to execute query: CREATE TABLE qservResult.result_1211906_m (`QS1_COUNT` BIGINT(21)) 0103 16:38:24.717 [0x7fe3cf7fe700] ERROR root (build/rproc/InfileMerger.cc:438) - InfileMerger error: Error creating table (qservResult.result_1211906_m) 0103 16:38:34.672 [0x7fe3deffd700] INFO  root (build/rproc/InfileMerger.cc:299) - Merging w/CREATE TABLE qservResult.result_1211906 SELECT SUM(QS1_COUNT) AS OBJ_COUNT FROM qservResult.result_1211906_m 0103 16:38:34.673 [0x7fe3deffd700] ERROR root (build/rproc/InfileMerger.cc:351) - InfileMerger sql error: Error applying sql. Error 1054: Unknown column 'QS1_COUNT' in 'field list' Unable to execute query: CREATE TABLE qservResult.result_1211906 SELECT SUM(QS1_COUNT) AS OBJ_COUNT FROM qservResult.result_1211906_m 0103 16:38:34.673 [0x7fe3deffd700] INFO  root (build/rproc/InfileMerger.cc:305) - Cleaning up qservResult.result_1211906_m 0103 16:38:34.674 [0x7fe3deffd700] INFO  root (build/rproc/InfileMerger.cc:317) - Merged qservResult.result_1211906_m into qservResult.result_1211906 {code}  So it seems test case #01 create table .result_1211906_m, and then test case #02 try to re-use this name for an other query.  Qserv result tables aren't cleaned (here Qserv has be stopped): {code} ls  ~/qserv-run/2014_12/var/lib/mysql/qservResult/ db.opt                result_1211336_m.MYI  result_1211340_m.MYI  result_1211343_m.MYI  result_1211346_m.MYI  result_1211382_m.MYI  result_1211385_m.MYI  result_1211902_m.MYI  result_1211905_m.MYI result_1211334_m.frm  result_1211337_m.frm  result_1211341_m.frm  result_1211344_m.frm  result_1211347_m.frm  result_1211383_m.frm  result_1211386_m.frm  result_1211903_m.frm  result_1211906_m.frm result_1211334_m.MYD  result_1211337_m.MYD  result_1211341_m.MYD  result_1211344_m.MYD  result_1211347_m.MYD  result_1211383_m.MYD  result_1211386_m.MYD  result_1211903_m.MYD  result_1211906_m.MYD result_1211334_m.MYI  result_1211337_m.MYI  result_1211341_m.MYI  result_1211344_m.MYI  result_1211347_m.MYI  result_1211383_m.MYI  result_1211386_m.MYI  result_1211903_m.MYI  result_1211906_m.MYI result_1211335_m.frm  result_1211339_m.frm  result_1211342_m.frm  result_1211345_m.frm  result_1211381_m.frm  result_1211384_m.frm  result_1211901_m.frm  result_1211904_m.frm result_1211335_m.MYD  result_1211339_m.MYD  result_1211342_m.MYD  result_1211345_m.MYD  result_1211381_m.MYD  result_1211384_m.MYD  result_1211901_m.MYD  result_1211904_m.MYD result_1211335_m.MYI  result_1211339_m.MYI  result_1211342_m.MYI  result_1211345_m.MYI  result_1211381_m.MYI  result_1211384_m.MYI  result_1211901_m.MYI  result_1211904_m.MYI result_1211336_m.frm  result_1211340_m.frm  result_1211343_m.frm  result_1211346_m.frm  result_1211382_m.frm  result_1211385_m.frm  result_1211902_m.frm  result_1211905_m.frm result_1211336_m.MYD  result_1211340_m.MYD  result_1211343_m.MYD  result_1211346_m.MYD  result_1211382_m.MYD  result_1211385_m.MYD  result_1211902_m.MYD  result_1211905_m.MYD {code}  Changing _idCounter to next value in appInterface.py: {code:python} self._idCounter = int(time.time() % (60*60*24*365) * 10) {code} solves the problem, but a deeper explanation will allow to bring a more robust fix.",8
"Build 2015_01 Qserv release
See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.",1
"refactor fftools viewer to have derived viewers / start LSST SUI git repo
Do the following:  - refactor fftools to allow for specialized viewers. - make a generic fftools viewer. - make an irsaviewer package in ife-new that will build. - generalize the catalog search and add factories to access the lsst search process. ",10
"Have newinstall.sh check itself against distrib version
We want to alert people who are just using a newinstall.sh they have lying around (old or hacked up or...) that they are not using the official server version.  ",1
"Migrate qserv modules (python code) to new logging system
nan",8
"deblender artifacts in noise-replaced images
We still see noise artifacts in some deblended images on the LSST side when running the M31 HSC data.  They look like the result of running NoiseReplacer on HeavyFootprints in which the children can extend beyond the parents.  This was fixed on the HSC side on DM-340 (before the HSC JIRA split off), and I *think* we just need to transfer the fix to LSST.",1
"Catch-all epic for essential fixes during DM-W15-4
nan",10
"CSV reader for Qserv partitioner doesn't handle no-escape and no-quote options properly
Both the no-quote and no-escape CSV formatting command line options should not have a default value, as specifying any value turns off field escaping and quoting. Furthermore, when quoting is turned off, the reader incorrectly treats embedded NUL characters as a quote character.",1
"Fix SWIG_SWIG_LIB empty list default value
See Serge message to Qserv-l ""xrootd premature death"": {quote} However, there are bigger problems. First of all, master doesn’t build for me. I get this error:    File ""/home/lsstadm/qserv/SConstruct"", line 104:     env.Alias(""dist-core"", get_install_targets())   File ""/home/lsstadm/qserv/SConstruct"", line 90:     exports=['env', 'ARGUMENTS'])   File ""/home/lsstadm/stack/Linux64/scons/2.3.0+1/lib/scons/SCons/Script/SConscript.py"", line 609:     return method(*args, **kw)   File ""/home/lsstadm/stack/Linux64/scons/2.3.0+1/lib/scons/SCons/Script/SConscript.py"", line 546:     return _SConscript(self.fs, *files, **subst_kw)   File ""/home/lsstadm/stack/Linux64/scons/2.3.0+1/lib/scons/SCons/Script/SConscript.py"", line 260:     exec _file_ in call_stack[-1].globals   File ""/home/lsstadm/qserv/build/SConscript"", line 39:     canBuild = detect.checkMySql(env) and detect.setXrootd(env) and detect.checkXrootdLink(env)   File ""/home/lsstadm/qserv/site_scons/detect.py"", line 225:     xrdLibPath = findXrootdLibPath(""XrdCl"", env[""LIBPATH""])   File ""/home/lsstadm/qserv/site_scons/detect.py"", line 213:     if os.access(os.path.join(path, fName), os.R_OK):   File ""/home/lsstadm/stack/Linux64/anaconda/2.1.0/lib/python2.7/posixpath.py"", line 77:     elif path == '' or path.endswith('/'):  which is caused by the fact that env[“LIBPATH”] looks like:  [[], '/home/lsstadm/stack/Linux64/antlr/2.7.7/lib', '/home/lsstadm/stack/Linux64/boost/1.55.0.1.lsst2/lib', '/home/lsstadm/stack/Linux64/log4cxx/0.10.0.lsst1+2/lib', '/home/lsstadm/stack/Linux64/xrootd/4.0.0rc4-qsClient2/lib', '/home/lsstadm/stack/Linux64/zookeeper/3.4.6/c-binding/lib', '/home/lsstadm/stack/Linux64/mysql/5.1.65.lsst1/lib', '/home/lsstadm/stack/Linux64/protobuf/2.4.1/lib', '/home/lsstadm/stack/Linux64/log/10.0+3/lib']  The first element is [], which comes from https://github.com/LSST/qserv/blob/master/site_scons/state.py#L173 where a PathVariable called SWIG_SWIG_LIB is given a default value of []. I can fix the build by changing the default to an empty string… but I don’t know enough scons to say whether that’s the right thing to do. Can one of the scons gurus confirm that’s the right fix? {quote}",1
"make lsst-sui & firefly git repo / make an lsst viewer
nan",4
"Update auto build tool to work with new split repositories 
After the repository split, changes are required to get the auto build tool to work properly. Firefly and Firefly based applications are built using Gradle system.  ",8
"Create an integration test case with GB-sized data
It's difficult to load manually data in Qserv, so a way to do that is to use integration test framework to automatically do this.  Big data file won't be stored in git, but the user wil lhave to retrieve them manually, and the test case won't be executed by integration tests.",4
"Provide input data for exampleCmdLineTask.py
{{pipe_tasks/examples/exampleCmdLineTask.py}} reads data from a repository. The comments in {{pipe_tasks/python/lsst/pipe/tasks/exampleCmdLineTask.py}} suggest that  {code} # The following will work on an NCSA lsst* computer: examples/exampleCmdLineTask.py /lsst8/krughoff/diffim_data/sparse_diffim_output_v7_2 --id visit=6866601 {code}  There are a few problems with that:  * External contributors don't have access to {{lsst*}}; * Even though that data exists now, it's unclear how long it will remain there, or what steps are being taken to preserve it; * The mention of this data is fairly well buried -- it does appear in the documentation, but it's certainly not the first thing a new user will stumble upon.  At least the first two points could be addressed by referring to a publicly available data repository. For example, the following works once {{afwdata}} has been set up:  {code} examples/exampleCmdLineTask.py ${AFWDATA_DIR}/ImSim --id visit=85408556 {code}  Although this has the downside of only providing a single image.",1
"Export SUI data (DC_W13_Stripe82_subset)
- import sui.sql.bzip2.out (produced by Serge) into MySQL for DeepSource and DeepForcedSource tables: - remove columns chunkId and subChunkId for each chunk table - merge all chunk table into the main table - join DeepSource and DeepForcedSource to add coordinates of DeepSource (director) object in DeepForcedSource table. then dump  DeepSource and DeepForcedSource  to files DeepSource.csv and DeepForcedSource.csv {code:sql} SELECT f.*, COALESCE(s.ra, f.ra), COALESCE(s.decl, f.decl) FROM DeepForcedSource f LEFT JOIN DeepSource s ON (f.deepSourceId = s.deepSourceId) INTO OUTFILE '/db1/dump/DeepForcedSource.csv' FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '""' LINES TERMINATED BY '\n'; {code} - Load this file using Qserv loader.  A sample should be made and tested first to validate this procedure. This sample could be added in qserv_testdata",3
"overhaul slot and alias system
While working on DM-1218 and DM-464, I've grown quite dissatisfied with the current state of the slot and alias mechanisms, and we now have a concrete proposal for larger-scale changes on RFC-11.  Unfortunately, I don't think we'll be in a good position to do much about this until we've completed the transition to meas_base and removed the old measurement framework in meas_algorithms.",6
"move SourceRecord/Table/Catalog to meas_base
We can make address a lot of dependency issues if we move the Source classes to meas_base, because we'll no longer have low-level code (e.g. afw::table persistence) in the same module as very high-level code (e.g. slots).   It will also put all the slot code in the same place, instead of spreading it across two pacakges.  This should be straightforward, except that we'll have a lot of downstream code to (trivially) change, and there's a good chance Swig will get confused somewhere along the way.",2
"Remove in-memory support of old-version afw::table objects
After removing the old measurement framework in meas_algorithms, we should also end support for version=0 Schemas in memory, and instead convert version=0 Schemas to version 1 when we unpersist them.",11
"afw::table - post-transition improvements
Breakdown: jbosch 40%; swinbank 60%",23
"Measurement - Framework Improvements
Add new features in meas_base that are desirable, but not required to replace functionality in meas_algorithms.  Breakdown: pgee 70%; jbosch 30%",60
"Support DDL in MetaServ - design
DDL information is embedded as comments in the master version of the schema (in ""cat"" repo). Currently we are only using it for schema browser. This story involves designing the procedure involving loading DDL information into MetaServ. We need to be ready to support a variety of scenarios: * we are getting already preloaded database, need to just load metadata about it to metaserv (we might have the original ascii file with extra information, or not) * we are starting from scratch, need to initialize database (including loading schema), and need to load the information to the metaserv * we already have the database and metadata in metaserv, but we want to change something (eg. alter table, or delete table, or delete database).",2
"move executionOrder from plugin config class to plugin class
We originally put the executionOrder parameter (which determines when a plugin is run, relative to others), in the config object, simply because that's where it was in the old framework.  But it's really not something that should be configurable, as it depends only on the inputs the algorithm needs, which don't change.",1
"Create a search processor to do cone/box search on a QSERV catalog
Create a search processor which accepts cone and box spatial constraints, queries a catalog, stored on QSERV (DeepSource), and returns the relevant rows as an IPAC table or RawDataSet.  The implementation is querying DeepSource catalog using qserv_areaspec_circle and qserv_areaspec_box spatial restrictors. ",11
"Read SUI requirements, send a list of questions to the group scientist
Read the requirements document on Archive Browser and Query Tools, make a record of unclear items, send questions to our scientist (D. Ciardi) ",5
"Add support for running unit tests in qserv/admin
This came up during review of DM-370: ""We do not run tests scripts in admin/ during regular build, SConscript in admin/ does not support that unfortunately."" This story involves tweaking SConscript to enable running unit tests automatically.",1
"Setup work environment / test builds for refactored repositories
After the repository was split into firefly and new-ife, it was necessary to understand the changes, check for inconsistencies, test builds, set up new IDEA project, etc. ",3
"Obtain and use catalog dd (data definition)
Get catalog metadata, which should include column description, units, and type. Use it in tool tips and possibly to create flexible constraints.  A temporary solution is to get metadata from an internal lsst_schema_browser_S12_lsstsim database on lsst-db.ncsa.illinois.edu  A permanent solution would be querying catalog metadata from metadata store.",10
"XYPlotter should be caching and restoring plot metadata
For efficiency, XYPlotter is designed to create up to 4 cards. When the card limit is exceeded, a previously created card is reused to plot catalog data for the current catalog. When card is reused the previous plot metadata (like column selections, grid option, etc.) are lost.",4
"Remove optimisation flag management in partition SConstruct
- eups build with -g and -O3 by default.  - Developpers can build their sources with next options:  scons debug=false opt=3  - Nevetheless partition SConstruct add -O2 option if debug is disabled, which is orthogonal. ",2
"The existing FITS reader class needs to be refactored to improve the performance(2)
- FITS reader class refactoring, improve readability - validation of the new code against the old one - unit test - performance improvement recording if possible",16
"fix faint source and minimum-radius problems in Kron photometry
This transfers some improvements to the Kron photometry from the HSC side:  - HSC-983: address failures on faint sources  - HSC-989: fix the minimum radius  - HSC-865: switch to determinant radius instead of semimajor axis  - HSC-962: bad radius flag was not being used  - HSC-121: fix scaling in forced photometry  The story points estimate here is 50% of the actual effort, as the work (already done) also benefited HSC.",5
"Fix errors in parsing or rendering nested expressions
Qserv has errors rendering nested expressions in predicates of the WHERE clause. It is unclear whether the problem is in constructing the predicate representation or in rendering the representation (or both).  Example: {code} SELECT  o1.objectId FROM Object o1  WHERE ABS( (scisql_fluxToAbMag(o1.gFlux_PS)-scisql_fluxToAbMag(o1.rFlux_PS)) -              (scisql_fluxToAbMag(o1.gFlux_PS)-scisql_fluxToAbMag(o1.rFlux_PS)) ) < 1 {code} Yields: {code} SELECT o1.objectId FROM LSST.Object_100 AS o1 WHERE ABS((VALUE_EXP FACTOR FUNCTION_SPEC scisql_fluxToAbMag(VALUE_EXP FACTOR COLUMN_REF o1.gFlux_PS)-FACTOR FUNCTION_SPEC scisql_fluxToAbMag(VALUE_EXP FACTOR COLUMN_REF o1.rFlux_PS)))<1 {code}  I probably left out a more general implementation one/both of those parts of query parsing/analysis.",4
"Add rotAngle to baseline schema
Add ""rotAngle DOUBLE"" to every table that has image ra/decl.  ",1
"Implement using multiple disk spindles
Qserv should be able to take advantage of multiple disk spindles (JBOD-type architecture). In practice that means either relying on something like native mysql partitioning, or tweaking loader so that it can distribute chunks across multiple disks (and patch symlinks in mysql data_dir).  ",20
"OpenStack automation via Python scripts : Launch an Instance
In our introductory work with OpenStack we have been utilizing the Horizon GUI interface for first steps, followed by the use of command line tools (the 'CLI') (e.g., nova, cinder, etc) as shown in DM-1334  DM-1700 , DM-1701. While it is possible to write automation scripts that utilize the CLI, an approach based on 'pure' Python scripting would fit more seamlessly into the LSST software development process. Enabling OpenStack automation via Python offers the opportunity to integrate provisioning of resources into the overall flow of LSST workflow & processing (e.g., DRP.)  The OpenStack services expose native Python APIs that expose the same feature set as the command-line tools.  The required python packages (python-keystoneclient, python-novaclient, ..) are installed on the head node 'vlad-mgmt' of the NCSA ISL OpenStack, and so initial Python scripting can be executed/tested there.  A first Python script will perform required authentication and launch an instance. ",4
"OpenStack automation via Python scripts : Software installation/test on an LSST node
The set of packages that will enable us to write against the native Python APIs of the OpenStack services is  {code} python-keystoneclient python-glanceclient python-novaclient python-quantumclient python-cinderclient python-swiftclient {code}  We begin testing these in DM-1787  on the NCSA OpenStack head node, but eventual use within LSST orchestrating workflow would entail these being installed on LSST nodes in the LSST stack.  In this issue we perform a basic installation of these packages into the system space on an LSST node/VM for testing.  These are managed in github, and we install these via 'pip install' onto an LSST VM for initial tests.",4
"Experiment with afwtable, meas_base, pipe_{tasks, base}
nan",4
"Produce detailed prototype & accompanying documentation
nan",4
"Produce straw-man prototype
nan",3
"Update documentation and automatic install script w.r.t. new newinstall.sh script
newinstall.sh script has evolved and breaks Qserv install procedure.",1
"Pull distEst package into obs_subaru
Reducing HSC data requires an estimate of the distortion, which is provided by the HSC package distEst.  This can be pulled into obs_subaru to consolidate code and reduce dependencies.  I propose to treat distEst as legacy code, which means I will pull it into obs_subaru without major changes to the code style.",6
"Use anonymous NCSA rsync server to distribute large test datafiles.
DM-1755 has been done before this feature was available. It uses rsync over ssh which require use to have an ssh-key on lsst-dev.  NSCA rsync server can now be accessed with next syntax: {code:bash} rsync -av lsst-rsync.ncsa.illinois.edu::qserv/qserv_testdata/datasets/case04/data/DeepSource.csv.gz . # LIST FILES AVAILABLE IN THE MODULE NAMED ""qserv"" rsync lsst-rsync.ncsa.illinois.edu::qserv  # To add content to this module/group, you can copy files into the following path:   /lsst/rsync/qserv/ {code}  rsync over ssh feature should be kept, in order to distribute private data are distributed.",4
"Improve test coverage for case04
Most of queries used in GB-sized case04 return empty results.  These queries should be added: {code:bash} fjammes@lsst-db2:~/src/qserv_testdata (u/fjammes/DM-1755) $ mysql --host 127.0.0.1 --port 4040 --user qsmaster qservTest_caseSUI_qserv -e ""SELECT count(*) FROM DeepForcedSource"" +----------------+ | SUM(QS1_COUNT) | +----------------+ |       33349940 | +----------------+ fjammes@lsst-db2:~/src/qserv_testdata (u/fjammes/DM-1755) $ mysql --host 127.0.0.1 --port 4040 --user qsmaster qservTest_caseSUI_qserv -e ""SELECT * FROM DeepForcedSource LIMIT 1"" {code}  But, even better, SUI team could provide some more interesting query to Qserv team in order to improve case04 quality.",3
"Package flask
The Data Access Webservice APIs are relying on flask, so we need to package flask according to the LSST standards. For my initial testing, I just run ""sudo aptitude install python-flask"".  ",1
"Regression testing of AP Simulator
Run the AP simulator to make sure that none of the changes to the ctrl_events package break anything.",1
"Regression testing of Orca
Do some test runs using Orca to make sure Orca still works after the the changes to ctrl_events.",1
"remove unused local typedefs
gcc 4.8 now warns about locally-defined typedefs that aren't used.  We have a few of these in ndarray and afw::gpu that should be removed.",1
"S15 Explore Qserv Authorization
Explore authorization centrally: use information generated by parser. Either generate dummy query and run on mysql that runs near czar, or use info produced by parser to determine if user is authorized.  Note, we want to limit this to ~1 week, just to reveal potential problems, or do a quick proof of concept.",8
"Study the current SUI requirement 
Study the current requirement carefully to make sure they all make sense and we can do it.",4
"Study the current SUI requirement 
nan",4
"Study the current SUI requirement
nan",2
"Study the LSST data products document and give a summary to the team
nan",6
"Study the current SUI requirement
nan",2
"Study SUI requirement and summarize all the input from other team memebers
nan",10
"segfaults in ip_diffim on gcc 4.8
I'm seeing test segfaults in ip_diffim on gcc 4.8, similar to those resolved on DM-1725, but with no similar smoking gun yet.  Preliminary indication is that the problem is actually in meas_algorithms.",2
"Prepare initial content
Prepare LSE-130 content as far as possible without input from the new collimated-projector calibration plan.",4
"Determine LSE-130 impact of collimated projector calibration plan
During a working meeting with Robert Lupton and Chris Stubbs, determine the impact on LSE-130 of the introduction of the collimated projector for calibration.",8
"Prepare draft of LSE-130 for Camera and CCB review
Produce a reviewable draft of LSE-130 based on decisions on calibration operations",4
"Support Camera CD-2 (mainly re: LSE-130)
Provide slides and other information needed for CD-2, mainly relative to the open questions around LSE-130",2
"Support LSE-130 review by CCB (mainly Camera)
Respond to comments, perform revisions to LSE-130 as necessary based on feedback from CCB review of the document",4
"Convert LSE-130 to SysML
Following CCB recommendation of approval of LSE-130 draft, convert Word draft to SysML and provide a docgen to Robert McKercher for final posting. ",2
"Create and post docgen of LSE-68
To support discussions with the Camera, post a provisional docgen of LSE-68 to the appropriate Confluence page.  Use knowledge from EA training to improve template.",4
"Support completion of final document
Based on CCB approval of LSE-72 on 10 October, support the completion of the final copy of the document for posting on Docushare.",1
"Complete LSE-140 work as needed to produce final document
Complete any review-driven revisions of LSE-140 and support the CCB meeting and following final document preparation.",2
"LSE-140: Collect desired changes for future release
Prepare for a future revision (Phase 3) of LSE-140.  Collect issues to be addressed in the revision.  Determine if any affect Phase 2 scope (which would require a prompt revision).  It is not anticipated that there will be an actual revision of LSE-140 during the Winter 2015 cycle, because additional detail on calibration requirements will not be available in time.",1
"Fix czar assertion failure
Reported by Tatiana: I am encountering this once in a while.   qserv-czar.log  python: build/rproc/ProtoRowBuffer.cc:69: int lsst::qserv::rproc::escapeString(Iter, CIter, CIter) [with Iter = __gnu_cxx::__normal_iterator<char*, std::vector<char, std::allocator<char> > >, CIter = __gnu_cxx::__normal_iterator<const char*, std::basic_string<char, std::char_traits<char>, std::allocator<char> > >]: Assertion `srcBegin != srcEnd' failed.  Czar is dead and qserv stops responding after that.  ----  For more details, search in qserv-l archives mails with ""czar assertion failure"" subject: https://listserv.slac.stanford.edu/cgi-bin/wa for complete description.",4
"Define issues to be addressed
Work with TCS contacts (Jacques Sebag, Paul Lotz, etc.) to define the principal issues",1
"Produce draft of LSE-75 with agreed revisions
Produce a draft of LSE-75 with the following agreed revisions: * remove reference to advance notice of pointing, now in LSE-72 * add reference to PSF reporting",1
"Catch-all epic for essential fixes during DM-W15-1,2,3 
nan",10
"Create primary calibration plugins
nan",9
"Develop General Acceptable Use Policy
Don Petravick and Lee LeClair",6
"Develop Information Classification Policy
Don Petravick, Lee LeClair",6
"Develop Incident Response Policy
Don Petravick, Lee LeClair",6
"Develop Data Management Sub-Project Plan and Risk Table
Don Petravick, Frossie Economou",6
"Develop PMO Sub-Project Plan and Risk Table
Lee LeClair, Iain Goodenow",6
"Develop EPO Sub-Project Plan and Risk Table
Don Petravick, Frossie Economou",6
"Develop Camera Sub-Project Plan and Risk Table
Don Petravick, Richard Dubois",6
"Develop Telescope and Site Sub-Project Plan and Risk Table
Lee LeClair, German Schumacher",6
"Package ISP Documents into LSST Standard Format for Control and Delivery
Robert McKercher",4
"Deblend post-merge objects
nan",4
"Task-level processing for merged objects
nan",5
"Fix query error on case03: ""SELECT scienceCcdExposureId FROM Science_Ccd_Exposure_Metadata"" 
Xrootd prevents the worker to return more than 2MB data.  On GB-sized data: {code} mysql --host=127.0.0.1 --port=4040 --user=qsmaster --batch  -e ""SELECT scienceCcdExposureId FROM Science_Ccd_Exposure_Metadata""                                                                                                                                                                     ERROR 4120 (Proxy) at line 1: Error during execution: -1 Ref=1 Resource(/chk/qservTest_case03_qserv/1234567890): 20150123-16:27:45, Error merging result, 1420, Result message MD5 mismatch (-1) {code}  On integration test case 04: {code} qserv@clrinfoport09:~/src/qserv (u/fjammes/DM-1841 *)⟫ mysql --host=127.0.0.1 --port=4040 --user=qsmaster qservTest_case04_qserv  -e ""SELECT * FROM DeepForcedSource""   ERROR 4120 (Proxy) at line 1: Error during execution: -1 Ref=1 Resource(/chk/qservTest_case04_qserv/6970): 20150204-16:23:43, Error merging result, 1420, Result message MD5 mismatch Ref=2 Resource(/chk/qservTest_case04_qserv/7138): 20150204-16:23:43, Error merging result, 1420, Result message MD5 mismatch Ref=3 (-1) {code}",5
"Permit PropertySets to be represented in event payloads
In the old marshalling code, property sets were representable within the payload of the event.   This was removed in the new marshalling scheme.   There are things (ctrl_orca) that still used this, so this needs to be added to the new marshaling code.  At the same time, new new filtering code can not allow this to be added, because the JMS headers only take simple data types.",2
"Test Qserv on SL7
Needed to run Qserv on CC-IN2P3 cluster.",2
"Coordinate implementation of web form for collecting data about existing data sets
The form is being implemented by the DataCat team. Need to coordinate (including with the NCSA team which parts are covered by which team), test, fine tune etc.",4
"Add support for large results in XrdSsiRequest::GetResponseData
GetResponseData needs to handle data sets beyond 2 MB. The problem is discussed in more details in story DM-1841",12
"SUI work with DB team to define the image query API
IPAC SUI team will work with SLAC database team to define the image query APIs.  IPAC needs to make sure the APIs are sufficient to satisfy the UI needs. SLAC will implement them. .",10
"Study RESTful API and work with SLAC team  to define image query APIs
nan",2
"Discuss, review, and define image query APIs with SLAC team
nan",2
"Image query API discussion and review
nan",2
"Image query API discussion 
nan",1
"Image query API discussion
nan",1
"SUI propose a structure definition for user workspace
Workspace is an integral part of SUI. We want to start the discussion and definition of workspace concept and structure.     SUI team had several discussions and Xiuqin presented the results at the DM AHM at SLAC. The slides and the discussion notes are here: https://confluence.lsstcorp.org/display/DM/Workspace+discussion",20
"Specify mechanism for periodic (nightly/weekly) build distribution
nan",3
"Implement nightly/weekly release automatic distribution - Part I
This ticket covers code in sqre-codekit to do migrate as much of the process of special machines, and git tag repos on the basis of eupspkg manifests.",4
"Github Transition Plan: Write document for CCB
nan",7
"Release engineering W15 bucket
Bucket epic for activities related to stack releases during W15",24
"Publish v10_0 release
nan",12
"Update documentation for v10_0 release
All done bar obtaining some release notes. ",2
"Workflow tool improvements w15 bucket
Bucket epic for w15 for improvements with JIRA, Hipchat, etc ",12
"JIRA project for RFCs
nan",3
"Implement Github transition plan
nan",10
"Review existing Level 3 documentation
Review existing requirements in this area.  Find all relevant existing project-controlled and other key documents.",4
"Document as-is Level 3 requirements and conceptual design
Produce a single jumping-off point for documentation on all aspects of Level 3, on Confluence.  Ensure that flowdown for existing Level 3 requirements in SysML is modeled.  Describe the high-level conceptual design.",4
"tighten control over heterogeneous DictFields
DM-1218 added support for DictFields with heterogeneous item types, which probably allows a bit too much freedom (the rest of pex_config is much more strongly-typed).  Instead of passing None to allow any type to be used, we should pass a tuple of supported types.",1
"Define JSON Results for Data Access Services
As discussed at [Data Access Hangout 2015-02-23|https://confluence.lsstcorp.org/display/DM/Data+Access+Hangout+2015-02-23], we should support json format. This story covers defining structure of JSON results for Data Access Services (dbserv, imgserv, metaserv) ",3
"SUI requirement refinement to define many unclear areas
In the current SUI requirement document, many areas are not clearly defined. We want to put more description and definition for those areas, and  identify and define the missing functions.  The goal is to generate a requirement document for DM review and put it under version control. ",50
"SUI User workspace specification
At the 2015 Deb DM AHM at SLAC, SUI led a discussion of workspace. https://confluence.lsstcorp.org/display/DM/Workspace+discussion  We want to continue the discussion, understand the user needs, identify the DM groups involved.  The goal is to generate a document to capture the functions requirement of workspace.     The first version of document is here https://confluence.lsstcorp.org/pages/viewpage.action?pageId=41783931",30
"SUI 2D data visualization (XY plot)
Better algorithm in spatial binning to visualize large number of catalog sources Plot histogram for tabular data Plot basic light curve ",40
"SUI the alert subscription system specification
Identify parties involved in the alert system generation, broadcast, subscription.  Understand the flow of the alert from generation to notifying users. Understand the requirement for SUI subsystem ""Alert subscription and notification"". ",30
"SUI infrastructure implementation
Identify the hardware resources needed at NCSA for short term development and  Set up the basic git repository and build system Explore multi resolution images display for background iamge",40
"SUI functional design 
understand current use cases, collect and define  more use cases. Design the major functions of major components in SUI, mainly Firefly package.",60
"SUI web interface and Python interaction
To facilitate  users to interact with Firefly visualization components in IPython notebook, to allow users to control the display with Python script. ",60
"Collect, understand, and define more use cases
This is an on-going effort. The collected use cases will be posted at confluence page https://confluence.lsstcorp.org/pages/viewpage.action?pageId=41784036. ",20
"Implement RESTful interfaces for Database (GET)
Implement RESTful interfaces for Database (see all D* in https://confluence.lsstcorp.org/display/DM/API), based on the first prototype developed through DM-1695. The work includes adding support for returning appropriately formatted results (support the most common formats). This covers ""GET"" type requests only, ""POST"" will be handled separately.",5
"Improvements to web form 
nan",5
"organize the workspace discussion and present a good proposal
SUI team had several discussions and Xiuqin presented the results at the DM AHM at SLAC. The slides and the discussion notes are here: https://confluence.lsstcorp.org/display/DM/Workspace+discussion",10
"summarize WEBDAV capabilities and past experience using it
WEBDAV could be a candidate for managing the user workspace.  summarize its capabilities and past experience, collect some use cases will help us to make a better decision.",4
"Contribute to the workspace capability discussion 
This include past experience, collection of use cases. ",2
"HDF5 file format study
Xiquin, Loi, Trey, and myself discussed HDF5 as a default format to return result set and metadata from lower-level database services vs. traditional IPAC table. Here is the summary:  Advantages of IPAC Table format  - Simple and human-readable, contains a single table - Fixed length rows (easy to page through) - Supported by many astronomical tools  - Provides a way to pass data type, units, and null values in the header - More metadata can be added through keywords (attributes)  Disadvantages of IPAC table format   - Steaming can not be started before all data are received – need to know column width before the table can be written (csv is better alternative) - Only alpha-numeric and '_' characters are allowed in column names (small subset of available characters) - Only predefined datatypes and one attribute type (string) - ASCII representation requires about twice as much storage to represent floating-point number data than the binary equivalent.  Advantages of HDF5  - Can represent complex data and metadata (according to LOFAR, good to represent time series) - Structured data, arbitrary attribute types, datatypes can be combined to create structured datatypes - Flexible datatypes: can be enumerations, bit strings, pointers, composite datatypes, custom atomic datatypes - Access time and storage space optimizations - Partial I/O: “Chunked” data for faster access - Supports parallel I/O (reading and writing) - Built-in compression (GNU zlib, but can be replaced with others) - Existing inspection and visualization tools (HDFView, MATLAB, etc.)  Disadvantages of HDF5  - Complex - Tuned to do efficient I/O and storage for ""big"" data (hundreds of megabytes and more), not efficient for small reads/writes. - Requires native libraries (available in prepackaged jars, see below) - Not human readable - (?) Not yet widely supported by astronomical tools (counter-examples: AstroPy, IDL, more at hdfgroup site)  Tools and Java wrappers:  * JHI5 - the low level JNI wrappers: very flexible, but also quite tedious to use. * Java HDF object package - a high-level interface based on JHI5. * HDFView - a Java-based viewer application based on the Java HDF object package.  * JHDF5 - a high-level interface building on the JHI5 layer which provides most of the functionality of HDF5 to Java. The API has a shallow learning curve and hides most of the house-keeping work from the developer. You can run the Java HDF object package (and HDFView) on the JHI5 interface that is part of JHDF5, so the two APIs can co-exist within one Java program. (from StackOverflow answer, 2012)  * NetCDF-Java is a Pure Java Library, that reads HDF5. However, it's hard to keep pure java version up-to-date with the standard, does not support all the features.  A way to set up native libraries (3rd option from JHDF5 FAQ):      ""Use a library packaged in a jar file and provided as a resource (by putting the jar file on the class path). Internally this uses the same directory structure as method 2., but packaged in a jar file so you don't have to care about it. Jar files with the appropriate structure are cisd-jhdf5-batteries_included.jar and lib/nativejar/.jar (one file for each platform). This is the simplest way to use the library.""       ",1
"S15 Butler (v3)
Improvements and tweaks to the butler as needed.",19
"Research support for integrated view of all databases
Metadata Store needs to contain information about all databases (data release databases, Level 1, Level 3 user databases).",11
"Add support for IPAC table format
Implement support for result formatting in IPAC table format.",6
"Research supporting cutout from images with overlaps
Research producing cutout from images with overlaps.",10
"SUI refactor Firefly package
The team has studied and researched the web UI framework (DM-1148) in W15. A new framework will be decided in February 2015. Many classes in Firefly package need to be refactored to use the the new framework. This epic in S15 will be our first attempt for it. Developers will be involved in this effort: Trey Roby, Loi Ly, Tatiana Goldina, Lijun Zhang, Xiuqin Wu",100
"Design CSS schema to support table deletion
Table/chunk deletion can be an extended process as some worker nodes may be temporarily down. We need to define a process and its supporting structures in CSS to allow gradual deletion of individual chunks and full tables.  Deliverable: a design of a system capable of deleting a distributed table (all chunks, all replicas). It should be possible to create a table with the same name after deletion.",4
"Modify CSS structure to support table deletion
Modify CSS structures to support DROP TABLE, as defined in DM-1896.",2
"Consistency checking for table data CSS 
CSS data on tables/chunks/nodes is supposed to be consistent at all times. Would be nice to have a tool that verifies consistency, probably including checking actual worker state.",4
"Tool to dump CSS information
CSS information tree may become large and it would be nice to have a tool to examine that tree or parts of it. Something that dumps the tree in user-friendly way and allows filtering or summarizing.",2
"Worker management service - design
We need to replace direct worker-mysql communication and other administrative channels with a special service which will control all worker communication. Some light-weight service running alongside other worker  servers, probably HTTP-based. Data loading, start/stop should be handled by this service.",5
"Re-implement data loading scripts based on new worker control service
Once we have new service that controls worker communication we'll need to reimplement WorkerAdmin class based on that.",8
"Implementation of calibration transformation framework
Following DM-1598 there will be a detailed design and prototype implementation for the calibration & ingest system. This issue covers cleaning up that code, documenting it, having it reviewed, and merging to master.",2
"Continued footprint improvements
A redesigned API and support for topological operations within the Footprint class.  This continues the work started in DM-1107 in W15.  Breakdown: jbosch 15%; swinbank 85%",8
"QSERV issues when working with scisql_s2PtInCircle
This problem has been adressed un u/fjammes/DM-1841.  Here's Tatiana report:  {quote} This error happens on all DeepForcedSource queries. (It happens on DeepSource too, but not always.)  [2015-01-15 11:02:29] [Proxy][4120] Error during execution: -1 Ref=1 Resource(/chk/LSST/6970): 20150115-11:01:05, Complete (success), 0, Ref=2 Resource(/chk/LSST/7138): 20150115-11:02:19, Complete (success), 0, Ref=3 Resource(/chk/LSST/7140): 20150115-11:02:18, Error merging result, 0, Ref=4 Resource(/chk/LSST/730 (-1)  Query examples:  select * from DeepForcedSource where ra>0.4 and ra<0.6 and decl>0.9 and decl<1.1;  select * from DeepForcedSource where scisql_s2PtInCircle(ra, decl, 0.5, 1.1, 0.138) = 1; {quote}",2
"Outline an expandable Python framework for advanced users 
Outline an expandable Python framework for use by advanced users in interactive or batch mode. Many users should be able to contribute software to the framework following simple API guidelines. Also look at configuration and delivery systems for the software. Start by looking for existing Python software that could be used in the areas of: - Database & file access - Data analysis frameworks - Display of data,, especially in an Astronomical context. - Reading and writing data in different formats - Graphing data locally or over the web - Science and astronomy data analysis - Event interfaces - VO functionality and connecting with existing software like DS9,, Aladin, Topcat, etc. - Ways to integrate modules with an LSST focus like 	AGN 	Large Scale Structures 	Galaxies 	Local Volume 	Solar System 	Astrostatistics 	Stellar Pops 	Strong Lensing 	Supernova 	Transients 	Weak Lensing 	Camera - Configuration and delivery system  ",12
"Backport HSC multi-band deblend processing
Breakdown: lauren 60%; price 40%  We need to transfer the recent HSC-side multi-band deblender changes to the LSST side, including all HSC issues in this query: https://hsc-jira.astro.princeton.edu/jira/issues/?filter=11603",35
"Interface design for full focal plane PSF estimation
Breakdown: swinbank 40%; jbosch 30%; rhl 30%  This is mostly design work, but I'd like the goal to be a new Python command-line task that repeats as much of the current ProcessCcd as necessary to run full focal plane PSF estimation, and would serve as a starting point for a future-proof visit processing script.  Some issues include:  - Gather requirements from e.g. DESC people as to needed inputs and data flow.  - Get details of what camera/telescope systems will provide, and figure out how those related to what DESC needs.  - Design classes for camera/telescope engineering data and wavefront information.  Figure out how they'll be managed on disk (stored with Exposure, part of CameraGeom, with flats, biases, etc.).  - Discuss parallelization needs with middleware team, and determine a way forward that lets us design interfaces using future parallelization schemes that don't exist yet.  - Design Python task interface for full focal plane PSF estimation.  - Implement command-line task that calls the full focal plane PSF estimation task.  - Implement placeholder PSF estimation subtask that just uses existing PSF determiners on single CCDs.  This may be too ambitious for the 40 story points we've allocated, and should discuss either adding more effort or reducing the scope.",45
"DRP DM-S15-1,2,3 Bugs and Papercuts
Breakdown: jbosch 16%; lauren 16%; rhl 20%; pgee 16%; price 16%; swinbank 16%",20
"DRP DM-S15-4,5,6 Bugs and Papercuts
Breakdown: jbosch 16%; lauren 16%; rhl 20%; pgee 16%; price 16%; swinbank 16%",20
"Prototype command interaction with Firefly
nan",10
"Research Javascript Frameworks: Finish new framework proposal
nan",18
"Create a deployable installation package for Firefly
With Firefly being open-source, we should provide a simple all-in-one installation package so a user can quickly setup and deploy an instance the Firefly Tools web application.",10
"Fine-tune data access interfaces
Brought up by Gregory via comments on the API page.  Data release selection in queries: I see that the /db/... queries take a ""?"" query parameter ""db"" with an example value of ""DR1"", i.e., a data release selector.  A couple of remarks: * Will this query parameter be provided for all the Level 2 image data products, e.g., for retrievals of coadded images? **If so, then it needs an equivalent to the M4 ""GET /meta/v0/db/<type>"" query. * I assume the ""db"" query parameter defaults to the most recent data release. * Will the M4 query return an indication of which ""?db="" value is the current default? * I assume that the numeric-identifier components of the various paths are unique only within a single data release.  That means that eventually, in user documentation, we should make sure that they understand that they can't scan through different releases' versions of the same image (for example) just by varying the ""?db="" parameter.   * Are the identifiers also unique within a particular type (i.e., ""raw"", ""template"", ""coadd"", ""calexp"", etc.)?  -----  Distinguishing L1 and L2 versions of reprocessed data products  Since most or all of the L1 data products will be regenerated in each data release, the catalog and image APIs should presumably allow the user to distinguish between the two.  I see how this could be done for catalogs - the ""?db="" parameter presumably allows selecting something like ""L1"" (for the actively updated Level 1 database) in addition to the above-documented ""DR1"", ""DR2"", etc.  Will the L2 table names for the reprocessed L1 data products be generally expected to be the same as for L1?  (Barring the discovery of a serious issue that requires revision of the schema for the reprocessing.) How will the L1 and reprocessed-L1 image data products be distinguished?",9
"Fix missing virtual destructors
The compiler is warning about some derived class hierarchies that are lacking virtual destructors.  We should add at least empty implementations to the base classes of these hierarchies.",1
"write Unit test and validation classes to validate the FITSreader refactoring 
nan",8
"Address misc. compiler warnings
Fix places where compiler is warning about some things we are doing on purpose and which we don't intend to change.  This helps keep compiler noise down so its easier to notice ""real"" warnings.",1
"update shapeHSM wrappers to latest external version
The HSM shape code has undergone many improvements and bug fixes as part of being included in the GalSim package, and we've recently included those in the HSC fork of meas_extensions_shapeHSM (HSC-129, HSC-1093).  We should transfer those changes to the LSST side before tackling DM-981 (or at least before finishing it).  The story point estimate here is for the work already done on the HSC side (with the usual 50% factor for shared work).  The transfer to the LSST side should be essentially no effort.  To the extent that EVM cares about this, the credit should go to [~price], even though I ([~jbosch]) am doing the transfer.",6
"Make unit tests use shared libraries
Many (all?) unit tests are currently built as static executables which include all needed object files. This has several issues associated with it: - many files are compiled twice, once as *.os files for shared libraries, second time as *.o file for unit tests - unit tests do not test actual code in the shared libraries but instead separately-built copy of the same code  We should change our procedure and make unit test to link against shared libraries to avoid these problems.",4
"Setup network for IPMI
nan",2
"Setup IPMI bastion hosts
nan",5
"Document how to use IPMI with LSST infrastructure
nan",1
"Base configuration of NFS servers
install and configure OS",3
"Test new NFS servers
Test to confirm servers are configured optimally",15
"Deploy first of the NFS servers
nan",20
"LOE - Week ending 12/26/14
nan",4
"LOE - Week ending 1/9/15
nan",8
"LOE - Week ending 1/16/15
nan",4
"LOE - Week ending 1/23/15
nan",8
"LOE - Week ending 2/6/15
nan",8
"LOE - Week ending 2/13/15
nan",8
"LOE - Week ending 2/20/15
nan",8
"LOE - Week ending 2/27/15
nan",8
"Define instrumental inputs to PSF estimation
Define inputs needed to build physically-motivated PSF models beyond what's contained in the image data and the current CameraGeom.  This includes:  - static engineering data from lab tests  - slowly-varying engineering data (measured daily, weekly, etc.)  - per-visit auxiliary data from telescope and camera (including, but not limited to wavefront sensor data).  The focus here should be on APIs, not the technical details of the data; we want to define class hierarchies (perhaps some polymorphic ones) that can be used to pass this data around in the future.  We also want to identify and characterize any preprocessing that needs to be done before they can be used for PSF estimation.  Experts who should be consulted include [~gpdf] (who is in charge of making sure the camera and telescope interfaces to DM are well-defined), camera/telescope people he recommends, and people from the LSST DESC who have worked on physical PSF estimation and know something about the things they'll want (e.g. Michael Schneider, Aaron Roodman).  This is essentially a requirements-gathering task, so the output should be a confluence page, not code.",10
"API for instrumental inputs to PSF estimation
Following the requirements-gathering in DM-1939, come up with classes that can be used to pass the needed inputs to the PSF estimation code, and at least sketch out roughly how they will need to be managed by the butler and loaded by the framework code.  The output of this ticket should be an API design in confluence (possibly on the same page used for DM-1939), and an associated RFC.",10
"Organize SUI design discussions
Organize the SUI team for web UI discussions to capture as much functions and components as possible.  A draft design document should be produced out of those discussions. ",10
"Test data development and HSC stack integration
Breakdown: price 20%; lauren 80%  This epic is focused on general stack testing and integration on HSC data, with a goal of getting LSST-side reductions of HSC data to the same level of quality  and robustness currently present on the HSC fork of the stack, which will involve a combination of backporting minor fixes from the HSC codebase and tuning parameters for LSST-side algorithms that supercede their HSC-side counterparts.  This will allow science-level algorithms to be tested using HSC data, and will provide functionality important for science-grade tests on other real data (such as the ongoing CFHTLS reprocsessing at IN2P3).  This effort will be focused on single-epoch processing (i.e. ProcessCcdTask), with coadd-level processing a stretch goal dependent somewhat on the completion of astrometric calibration work at UW.",45
"HSC backport: convert Peak to PeakRecord
This issue covers transferring all changesets from [HSC-1074|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1074] and its subtasks, as well as:  - An RFC to propose the API change, and any requested modifications generated by the RFC.  - Additional fixes to downstream code that's broken by this change (HSC-side changesets should be present for most of downstream fixes, but perhaps not all).",8
"HSC backport: guarantee consistent handling of peaks in deblender
This issue covers transferring changesets from:  - [HSC-134|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-134]  - [HSC-1109|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1109]  - [HSC-1083|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1083]  ",4
"HSC backport: multiband processing for coadds
This issue includes transferring changesets from many HSC issues:  - HSC-1060  - HSC-1064  - HSC-1065  - HSC-1061  Most of this is in multiBand.py in pipe_tasks, but there are scattered changes elsewhere (including updates to camera mappers to include the new datasets, for which we'll need to modify more than just obs_subaru).  However, before we make these changes, we'll need to open an RFC to gather comments on the design of this task.  We should qualify there that this is not a long-term plan for consistent multiband processing (which we'll be starting to design on DM-1908), but a step towards better processing in the interim.  Note: while I've assigned this to [~lauren], as I think it will be very helpful for her to get familiar with this code by doing the transfers, the RFC will have to involve a collaboration with [~jbosch], [~price], and Bob Armstrong, as we can't expect someone who wasn't involved in the design to be able to write a document justifying it.",8
"HSC backport: low-level Footprint merge code
This is a transfer of changesets from the follow epics:  - [HSC-1020|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1020]  - [HSC-1075|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1075]: only the afw changes  Because this is purely an addition to the interface, and we're planning to redesign that interface in DM-1904, I don't think we need an RFC here.",4
"Add support for async request cancellation to xrdssi
nan",12
"S15 Implement Query Cancellation
Add support for query cancellation.",32
"Add abstraction in czar for unit tests
Add hooks in czar that will let us build unit tests.",8
"Unit test for query cancellation
nan",8
"Change log priority for message ""Unknown column 'whatever' in 'field list'""  
Next message should be logged with ERROR priority:  {code} 0204 15:08:03.748 [0x7f1f4b4f4700] INFO  Foreman (build/wdb/QueryAction.cc:250) - [1054] Unknown column 'whatever' in 'field list'   {code}",1
"Post meas_base move changes to Kron
These are to note leftovers from DM-982.  They could be done in a single issue. 1.  I commented code out referring to correctfluxes, but it will need to be restored once it is available in the new framework.  2.  Jim asked me to replace the computeSincFlux which is currently in PsfImage.cc in meas_algorithms with a similar call in meas_base/ApertureFlux.cc.  I did not do this because it became rather complicated, and can just as easily be done when the meas_algorithms routine is moved or removed.  Basically, the templating in ApertureFlux is on Pixel type, whereas in meas_algorithms it is on ImageT (where ImageT is not necessarily a single class hierarchy -- e.g., Image and MaskedImage).  So I left this for now.",1
"HSC backport: deblended HeavyFootprints in forced photometry
This is a transfer for changesets for [HSC-1062|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1062].    Unlike most of the HSC backport issues for multiband deblending, these changes will require significant modification the LSST side, because we need to apply them to the new forced measurement framework in meas_base rather than the old, HSC-only one in meas_algorithms and pipe_tasks.    Also include [HSC-1256|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1256], [HSC-1218|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1218], [HSC-1235|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1235], [HSC-1216|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1216].",20
"International Network Design and Implementation
FIU/Amlight is expected to provide a full capacity 3 x100 Gbps link by FY17.  ",12
"REUNA will provide a “pre-operations” link between La Serena and Santiago
REUNA will provide La Serena - Santiago links at full capacity 100 Gbps link by FY17 and a 40 Gbps secondary link by FY20.  This link will support testing and development prior to that time.",14
"Chilean National Network Design and Implementation
Refer to REUNA MREFC sub award contract for deliverable details.  This covers non-contract work by the AURA/LSST.",20
"S15 Data Distribution & Replica Mgmt Prototype
This epic covers building an initial prototype of the Data Distribution and Replication system. The design is covered through DM-1060",30
"Prepare data set for large scale tests
Load data set for large scale tests (duplicate, partition, load)",10
"Run large scale tests
Coordinate running large scale tests",6
"Parallelization requirements for PSF estimation
We need to gather algorithmic ideas for how full focal plane PSF estimation will work from a parallelization and data flow standpoint, and discuss with the middleware team how these should be handled from an interface standpoint.  Questions include:  - Will we need to do significant cross-CCD image processing or require significant memory for these tasks?  If so, should we structure this via message passing between CCD-level processes, or scatter-gather?  - Assuming a scatter-gather approach, will we need multiple scatter/gather iterations when processing a single visit?  - How much data will be passed between threads/processes?  Would this include complex serializable objects, or just POD arrays?  - Will different PSF estimation plugins will have different parallelization requirements?  Or, can we define the plugin interfaces at a low-enough level that parallelization can be handled by the framework?  If plugins do need to control their own parallelization, how do we make parallelization interfaces accessible to the plugins?",8
"Python interface for full-visit PSF estimation
Create a Python interface for a pluggable PSF estimation system that supports algorithms that will operate over full images.  This should include a sketch of how a calling command-line task, and placeholders for parallelization interfaces that may not yet be finalized.  The output of this issue is a completed RFC on the design.",6
"Command-line driver and placeholder implementation for PSF estimation
Create a command-line task that makes use of the new PSF estimation interface, duplicating as much of ProcessCcdTask's functionality as necessary to provide the inputs to PSF estimation (I expect this new task to ultimately replace ProcessCcdTask).  This may have to include workarounds or temporary implementations for parallelization features that are not yet available.  Create a simple PSF estimation placeholder that simply uses existing single-CCD PSF-determiners.",6
"CModel flux validation and testing
Investigate the performance of the new version of the CModel code on various test datasets, including HSC data (following DM-1942), SDSS, and possibly CFHT data.  Breakdown: lauren 100%",35
"Create a kind of Wcs that encapsulates a TAN WCS and a distortion model
We can simplify the astrometry solver if we have a Wcs that encapsulates a pure tangent-plane WCS and a distortion model that takes converts between PIXELS and TAN_PIXELS. This is useful because at the early stages of processing raw data we have a TAN WCS from the telescope control system and a pretty good estimate of distortion from the optical model (represented in the camera geometry).  The result will be a Wcs whose sky<->pixel transformation is a reasonable approximation of reality (and a much better approximation than the tangent-plane WCS that is currently available. This will potentially eliminate a significant amount of confusing code that attempts to correct for distortion by manually applying the optical distortion model.",4
"Build 2015_02 Qserv release
See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.",1
"Fix enclose, escape, and line termination characters in qserv-data-loader
Add this string to mysql loader 'LOAD DATA INFILE' command:   {code}  q += ""ENCLOSED BY '%s' ESCAPED BY '%s' LINES TERMINATED BY '%s'"" % (enclose, escape, newline) {code} and add params in cfg file.",2
"S15 Implement Fully-RESTful Data Access Web Service
Improvements to the skeleton of Data Access Web Service built in W15. Make all responses fully RESTful (including results, and errors, setting response headers). Fine-tune API, add support for versioning, unit testing.",39
"Research and Document API Versioning
Research and document versioning of the RESTful API (through flask blueprints). In particular, need to understand how to avoid code duplication between different versions of API.",4
"Add SQLite-based v0.1 unit testing for metaserv
Add unit tests for the RESTful flask based API. I think it'd be most useful if we could  a) load some test data into underlying metaserv  b) run programmatically things like ""curl -H accept:text/html http://127.0.0.1:5000/meta/v0/db/L2/DC_W13_Stripe82/tables"" etc (more examples in dax_*serv/README.txt) and verify that we got what we expected. Do it for both json and html.",4
"Add error handling for webserv
Add basic error handling for the RESTful flask-based API",6
"Improve security for mysql in python
Revisit all code that talks to mysql from python to use parameter bindings instead of direct string substitutions. In practice: {code} conn.execute(""SELECT * FROM t WHERE name=%s"" % theName) {code} should be replaced with {code} conn.execute(""SELECT * FROM t WHERE name=%s"", (theName,)) {code}  For details, see http://mysql-python.sourceforge.net/MySQLdb.html#some-examples ",5
"Fix JDBC timestamp error
JDBC driver returns an error on next query:  {code:sql} sql> select * from Science_Ccd_Exposure [2015-02-06 13:39:37] 1 row(s) retrieved starting from 0 in 927/970 ms [2015-02-06 13:39:37] [S1009] Cannot convert value '0000-00-00 00:00:00' from column 32 to TIMESTAMP. [2015-02-06 13:39:37] [S1009] Value '[B@548997d1' can not be represented as java.sql.Timestamp {code}",1
"Redesign/Refactor WCS and Coord
%50 KSK, %50 RO Currently WCS is mutable and Coord objects are heavyweight.  Refactor WCS to be immutable and make Coord less heavyweight.  Include lists of Coord objects.  It's possible astropy could inform in that area.  Also, remove TanWcs in favor of TanSipWcs since TanWcs can have SIP terms.",40
"Update analysis tasks: diffim and snap combination
50% KSK 50% RO  A small amount of this work is in 02C.03.01 The diffim task needs to be looked at.  It needs to be updated to use current mechanisms.  It should also be refactored to split out some of the procedural code into methods.  In a similar task, the lsstSimIsrTask needs to include a real snap combine step.  Currently, one of the snaps is dropped on the floor.  For this round just implement naive snap addition and morphological CR rejection.",43
"Define API for Stack Astrometric Calibration
70% RO 30% KSK This should also include a minimal implementation.  This should be done with an eye toward photometric calibration.  Prerequisite: Get multi epoch (multi-band?) catalogs of centroids from some trusted source (CFHT, HSC?).  1. Load all stars that overlap a patch for all epochs. 2. Associate all stars on each chip.   2a. Implement K-D tree 3. Fit model for rigid chip system + optics + atmosphere.  Eigen for sparse model fitter.   3a. Allow for external catalog.   3b. This could include a class to fit XYTransforms 4. Turn result into WCS.  So maybe a down scope for a single cycle epic is to get matches and interfaces for models and solvers. ",75
"Research DCR in the context of DiffIm including possible algorithms for mitigation.
It is not clear how template coadds will be built.  This includes understanding the data necessary to generate a template for the entire sky.  This epic is to identify possible techniques as well as the risks associated with each technique.    This does not need to pin down the exact algorithm or the specific selections, but should inform what further development is necessary to avoid putting alert generation at risk.",19
"Refactor Approximate and Interpolate classes
100% RO A base class for this will be created in DM-740 as a part of epic DM-85.    This epic is to implement the classes to replace the original functionality.",34
"SQuaRE Support
50% KSK 50% RO SQuaRE has asked that we leave 20 SP free per Cycle to help out.",20
"Story point display and roll-up in epic display
I understand that there is a pending request to display the story points for individual story issues in the mini-table in which they are displayed for an epic.  It would also be useful to see a rolled-up total of the story points for the defined set of stories - so that, among other things, this could be compared to the story point value for the epic.  Ideally the story points for the roll-up might be displayed as ""nn (mm)"" where nn is the total points and mm is the number of points remaining to do (or done already - I don't care which as long as the definition is clear).",1
"Define faulty/consistent states and recovery process
Whiteboard session(s) to gather a list of invariants / principles that define a consistent state, a list of fault conditions, and the steps in failure recovery.",12
"Data transport mechanism for data distribution
Decide on a transport mechanism for data (bit torrent, scp, or ?). We must take into consideration whether the data source matters in this choice (e.g. tape vs known good node), as well as how to identify that a data source is the correct one (e.g. via checksums and sequence numbers).",12
"Architecture for failure detection and resolution
Come up with an architecture for detecting failure or non-nominal conditions (e.g. under replication). The core question to resolve is whether we go with a distributed approach, or with centralized control.",16
"Research existing theory and prior art
Peruse the distributed systems literature for prior approaches to this problem, and examine existing system implementations for components/ideas we could reuse.",10
"Document data distribution/replication plan
Produce an overview document that explains our definitions, architecture and strategies for dealing with data distribution and replication. ",10
"Define strategy for adding and removing data 
Define a strategy (push/pull distributed/decentralized) for recognizing incoming data and cleaning up/removing stale/deleted data. Is data ingest just another form of failure recovery?",10
"Define data distribution/replication testing strategy
Once we decide on a design for data distribution / replication, we should come up with a test plan.",4
"Package Reorganization (Science Pipelines)
Breakdown: jbosch 50%, swinbank 50%",38
"switch ndarray to external package
There is already an external ndarray project on GitHub (we've been using a fork of that).  We should merge the forks and switch to using the external package. ",2
"merge ""basics"" packages
Create detailed RFC and implement merge of base, utils, and daf_base.  See also https://confluence.lsstcorp.org/display/DM/Summer2015+Package+Reorganization+Planning",2
"separate pex_exceptions from base and rename
Remove dependency on base from pex_exceptions and rename to just ""exceptions"" (after RFC).  See also https://confluence.lsstcorp.org/display/DM/Summer2015+Package+Reorganization+Planning",1
"Move Wcs from afw::image to afw::coord
Create RFC and implement move of Wcs from afw::image to afw::coord.  See https://confluence.lsstcorp.org/display/DM/Summer2015+Package+Reorganization+Planning",1
"move Jarvis/shapelet code to legacy package
Create RFC and remove Jarvis/shapelet package from meas_algorithms, into new legacy sci package.  See also https://confluence.lsstcorp.org/display/DM/Summer2015+Package+Reorganization+Planning",1
"move Psf, Kernel code to new afw::convolution subpackage
Create detailed RFC and implement move for these packages.  See also https://confluence.lsstcorp.org/display/DM/Summer2015+Package+Reorganization+Planning",2
"split pipe_base into command-line and non-command-line components
Create detailed RFC and implement package split, to separate basic Tasks (to be used as e.g. subtasks) from CmdLineTask and ArgumentParser.  See also https://confluence.lsstcorp.org/display/DM/Summer2015+Package+Reorganization+Planning",1
"Implement image stitching
This story involves implementing code that stitches images, simple case that does not involve tract boundaries. More advanced case in covered in separate ticket. We will need to determine WCS information for the target images.",6
"Create interface and utility package for single-frame/forced processing
Create detailed RFC and implement move of interface and utility code from multiple existing packages to new package.  See also https://confluence.lsstcorp.org/display/DM/Summer2015+Package+Reorganization+Planning",2
"Design and implement RESTful API for image stitching and rotation
nan",4
"Split PSF estimation and PSF model code into separate package
Create detailed RFC and implement move of concrete PSF estimation code and Psf subclasses from meas_algorithms to separate package.  See also https://confluence.lsstcorp.org/display/DM/Summer2015+Package+Reorganization+Planning",1
"rename packages with minimal reorganization
meas_deblender, ip_isr, meas_astrom, meas_modelfit, and ip_diffim do not require major refactoring to fit into the new package reorganization, but they should be renamed (with sci_prefixes).  See also https://confluence.lsstcorp.org/display/DM/Summer2015+Package+Reorganization+Planning",1
"Split measurement plugins into separate packages
Create detailed RFCs and implement splitting measurement plugins into separate package.  May want one package for extremely basic plugins, always-used plugins (PixelFlags, TransformedCentroid, etc).  See also https://confluence.lsstcorp.org/display/DM/Summer2015+Package+Reorganization+Planning",2
"Split coaddition code and single-frame/forced command-line drivers
This should split all content in pipe_tasks into two packages (aside from what may have been removed in previous issues).",2
"Research how to support L3
Research implications of having to deal with updatable Level 3 data.",12
"Architecture for supporting small non-partitioned tables
Some tables, like Exposure, provenance, will not be partitioned, and the current plan is to either replicate them on each node, or store on shared file system. Need to decide how it will be dealt with.",6
"Research software deployment on the qserv cluster
Discuss and decide how to deploy and upgrade Qserv software",10
"Investigate procedures for package reorganization
e.g. develop script to handle bulk namespace changes.",4
"DRP S15 support for SQuaRE
Breakdown: jbosch 16%; lauren 16%; rhl 20%; pgee 16%; price 16%; swinbank 16%",20
"FY15 Key Performance Metrics
Collect data, compile scripts, perform measurements as necessary to report figures in respect of the FY15 key performance metrics.  Breakdown: lauren 50%; rhl 50%",16
"Support Exposure Use Cases
Development in support of Exposure Use Cases",19
"Implementing stitching multiple patches across tract boundaries in a coadd
nan",4
"refactor afw Swig to improve build times
I have an idea for how to improve Swig build times that we should get vetted (and possibly improved upon) by a true Swig expert (even if that costs a bit of $$).  This issue includes vetting that idea (splitting up classes into multiple per-package module builds), getting it through the RFC process, and implementing it in afw.",10
"Add image-query related KPIs to the plan
Existing plan in LDM-240 does not mention image related KPIs. Need to come up with a road map, and propose KPIs. This should be synchronized what realistically NCSA cluster can deliver in any given FY.",5
"FY19 Setup Database for Deep Drilling
nan",53
"FY18 Setup Calibration Database
Need to think through issues related to supporting calibration. Schema, requirements that will require special optimizations.",39
"S17 Build Prototype of AlertProd and L1 User Database
Build a working, non-optimized prototype of the [Alert Production and L1 User Database|http://ldm-135.readthedocs.org/en/master/#alert-production-and-up-to-date-catalog].    Deliverable: working, non-optimized prototype of AlertProd Database.",79
"X16 Revisit Design of AlertProd & L1 Db
Revisit the [design of Alert Production Database and Level 1 User database|http://ldm-135.readthedocs.org/en/master/#alert-production-and-up-to-date-catalog], including schema, indexing, partitioning, synchronization, replicating, fail over. Verify that the latest requirements match the design. This epic will likely involve experimenting and light-weight standalone prototyping related to parts of the system that might be non-trivial to scale or to implement.    Deliverable: a refreshed design document for Alert Production and L1 User Database.",19
"FY17 Design Internal DRP Database
Internal DRP DB will be used to store  * all bookkeeping (provenance, what run what did not, etc) * intermediate data products (might be larger than final data products),  * a subset of data (what we need by DRP), eg foorprints of objects  Internal DRP DB might need its own spatial engine.  It is expected that SDQA will run on that database.   Need to think through issues related to supporting internal Data Release Production. Schema, requirements that will require special optimizations.  Need to define reliability requirements.  In limited cases pipelines might want to use internal db (instead of files). Example: select all objects from a given region. Need to understand query load and complexity coming from DRP.",79
"FY18 Revisit Design of L3 Support in Qserv
Need to think through issues related to supporting Level 3 databases",100
"FY19 Design Next-to-database Data Analysis System
Need to design the system that will allow users run their own custom data analysis next to database.",100
"FY19 Implement Next-to-database Data Analysis
Need to implement the system that will allow users run their own custom data analysis next to database.",100
"W16 Improve Data Provenance Design
We have a detailed design of the Provenance, described at https://dev.lsstcorp.org/trac/wiki/db/Provenance. Work covered by this epic involves:  1. Revisiting the design and tweaking it as necessary. In particular:  * Describing in more details interactions with key data producers (DRP, AlertProd, Calibration, L3 data brought in by users).  * Estimating the size of provenance data  * Considering querying the provenance data    2. Evaluating existing off-the-shelf provenance systems/tools.    Deliverable: a document describing data provenance architecture / schema supported by a standalone proof-of-concept prototype.",69
"Catch all epic for essential fixes in Science Pipelines DM-W15-5
nan",10
"server side preparation for  histogram plot (1)
Convert necessary code to make it possible for a JavaScript component to place a JSON request to the server and to parse the resulting RawDataSet.  ",6
"Client side plot display for histogram
- Create a React JavaScript component, which takes the data and renders histogram.  - Make it possible to call this component from GWT code, using experimental JsInterop technology in GWT 2.7",10
"SUI Investigate L3 data/tools requirements, evaluate potential tools 
There are many overlap areas in L3 data analysis tools with the general science user tools. We want to identify those requirements and needs to help making SUI components adaptable for L3 data production and analysis.",30
"Start requirements gathering for pipeline QA visualization needs  
Gather use cases for pipeline QA visualization tools. We want to build the SUI components in such a way that they could be used to support QA needs. ",36
"SUI Build the visualization components that could be used independently
Currently we identified three basic components: Image visualizer,  tabular data display,  2D XY Plot. All three could share the data model and provide inter activities between the components.  ",80
" Integration and test monitoring architecture Part I
[retitled to better capture cycle scope]    Develop and deploy a layer to capture the outputs, initially numeric,  of integration testing afterburners such as sdss_demo, hsc_demo, and  others developed this cycle. Also capture meta-information such as  execution time and memory footprint. Propose log format to standardise  production of such informations. Investigate notification system based  on trending away from expected values. Investigate data provisioning  of integration tests such as storage of test data in GithubLFS.    [75% JMP 25% JH]        ",100
" Firefly-based data display for SQuaSH - Part I
[Epic retitled to better reflect cycle scope]    This epic covers work relating to working on the visualisation side of  the Science QA Analysis Harness. It is a timeboxed effort to come up  to speed with Firefly in particular, evaluate it against our needs,  and provide any feedback to the Firefly team. Some prototyping of  visualising integration dataset products will also be involved.     [AF 100%]         ",45
"Maintain list of OSes that pass build and integration testing 
Provide an automatiically generated and updated pages showing operating systems that are successfully building  and integrating the stack from source.   [FE at 75%, JH at 75%]",20
"Specify system for performing CI on Docker stack containers
Investigate how we can CI first-party Docker containers with runnable stacks    [JH 100%]",30
"Release engineering  Part One
Bucket for public stack releases  [FE at 75%, JH at 75%]",40
"Miscellaneous service support improvements
JIRA, comm toos,  etc for DM and  non-DM teams (indicate)    In order to avoid fractional story points, some 0    [FE at 75%, JH at 75%]",16
"Attend Scale 13x conference
Attend database talks, in particular the MaxScale proxy talk (http://www.socallinuxexpo.org/scale/13x/presentations/advanced-query-routing-and-proxying-maxscale?utm_campaign=north-american-trade-shows&utm_source=hs_email&utm_medium=email&utm_content=16099082&_hsenc=p2ANqtz-_MFjfxvpCdmV_Ax2RKDdOGypHPQ85UL-UMuy0eRs_MrlJ2qJVp-MXx-g7_-dAQsq0trpA61hkZrzO-3gp6bKVkpK52fQ&_hsmi=16099082).  If anyone has questions they would like me to ask, please post them here as well.  I will post notes to this issue. ",2
"Data loader should always create overlap tables
 We have discovered that some overlap tables that are supposed to exist were not actually created. It looks like partitioner is not creating overlap files when there is no overlap data and loader is not creating overlap table if there is no input file. Situation is actually symmetric, there could be non-empty overlap table but empty/missing chunk table. When we create one table we should always make another as well. ",2
"Clean up QuerySession-related code in czar
(created in response to DM-211) This ticket should address the following inelegancies in the qserv-czar.   * QuerySession->QueryPipeline. The ""session"" abstraction has moved to a better place. The iterator portion should be shifted into its own separate class, though perhaps still associated with QueryPipeline. The iterator portion's new home should be amenable to eventually moving the actual query materialization to the worker, though we shouldn't introduce new abstractions until we are actually ready to move the substitution/materialization to the worker.  * QueryContext needs to be split into incoming external QueryContext and a sort of QueryClipboard for passing information between analysis/manipulation plugins. Eventually, I imagine a chain/tree of them attached to the select statements themselves in order to represent subquery scope nesting (which is complicated to represent and to reason about--nesting and the resulting namespace resolution is tricky), but I don't think we should try doing the chaining in the first phase. For this ticket, create QueryClipboard to hold the portion for interchange between analysis plugins. Query analysis plugins would then pass this object (which points at an immutable? QueryContext) between themselves. QueryClipboard probably should live in qana, QueryContext in query (unless there is a good reason to move it).   ",8
"Rename TaskMsgFactory2
Rename TaskMsgFactory2 to TaskMsgFactory.    Please see DM-211 for more information.",1
"Port fault-recovery testing code to XrdSsi
Please see DM-211 for the origin of this ticket.  BillC put in code to introduce random errors in query dispatch as part of working on code to recover from faults. In the port to the XrdSsi API, we did not port this code. This story is to introduce the ability (compile-time configurable, if not command-line or dynamically configurable) to simulate these sorts of faults to exercise the fault-recovery (confined to retrying on transient-ish failures) code.",12
"Creates overlap tables even if empty while loading data
Query execution expects all chunk and overlap tables to exist, even if they are empty. In short term, that means loader should: * look at all chunks and add corresponding overlap chunks, * look at overlap chunks and add missing empty chunk table ",2
"FY17 Implement Data Verification Tool
Need a tool for verifying whether data is in consistent stage (e.g., right after loading, after some upgrades, in general at any given time).  The list of things to check include: * empty chunk file, * xrootd exported DB, * data tables * overlap tables, * data_0123456789 tables * chunkId, subChunkId columns existence  Some of the above can be automatically fixed on the spot when problem is discovered.",90
"Add test case to catch missing empty chunks or overlaps
Discussed at db hangout 2015-02-18.   We need a use case to test for missing empty overlap chunk tables and/or empty chunk tables.",2
"FY17 Design L2 Catalog Swap/Release Automation
Need to think through the issues related to releasing L2 catalog / swapping a new one in place of an old one",54
"FY17 Build AP-ready Data Provenance System
Improvements to the first version of the standalone prototype built through DM-2042. Discussions with the Application Team on capturing provenance and integrating DRP with the provenance system. Add scaffolding / unit tests that will simulate data producers, in particular DRP.  Deliverable: DRP-ready System for capturing provenance.",79
"FY18 Integrate AlertProd with Data Provenance
Integrate Alert Production with the Provenance system.",56
"FY17 Implement Async Queries in Data Access Web Services
Work includes: * asynchronous requests, request status, retrieving results for dbserv and imgserv",26
"FY19 Implement Partial Query Results
nan",100
"S17 Improve ImageServ
nan",9
"W16 Add Support for Multi-table Shared Scans
Implement multi-table shared scans. Ensure that shared-scans are not delaying interactive queries. The baseline architecture of the shares scans are described in [LDM-135 Shared Scanning|http://ldm-135.readthedocs.org/en/master/#shared-scanning].",100
"F16 Qserv KPMs
nan",24
"F17 Run Large Scale Qserv Tests
nan",26
"F18 Run Large Scale Qserv Tests
nan",26
"FY19 Implement Missing Features in Qserv for L3
Need to think through issues related to supporting Level 3 databases",100
"FY20 Improve Design of Next-to-database Data Analysis
Need to implement the system that will allow users run their own custom data analysis next to database.",100
"FY18 Demonstrate Fault Tolerance
nan",100
"FY18 Implement Basic Resource Mgmt for DB
Includes things like query throttling per user for all databases (L1, L2, L3)",54
"FY19 Optimize Resource Mgmt for DB
nan",79
"FY18 Implement Basic Resource Mgmt for Images
nan",54
"FY19 Optimize Resource Mgmt for Images
nan",79
"W16 Distributed Loader - Research
In production, we will need a distributed loader that will be capable of loading entire data set produced by DRP within 24-48 hours. This epic involves researching all the needs, requirements and constraints, and exploring what the best architecture for a distributed loader would be. Related doc: [LDM-135 §8.15.2|http://ldm-135.readthedocs.org/en/master/#data-loading]",28
"W16 Distribution and Replica Mgmt Prototype v2
This epic involves building a complete, working prototype of the Qserv data distribution and replica management.",100
"FY18 Implement L2 Catalog Swap/Release Automation
Need to think through the issues related to releasing L2 catalog / swapping a new one in place of an old one",100
"FY17 Add Support for Managing Per-user Access for DB
nan",53
"FY18 Add Support for Managing Per-user Access for Image and File Archive
nan",79
"FY18 Integrate Qserv with EFD
nan",79
"Port metaREST.py to db
metaREST_v0.py in metaserv is currently using MySQLdb instead of going through the db API, because we need to use parameter binding for security reasons. We should switch to using db, once the db interfaces will support it. ",1
"Port dbREST.py to db
dbREST_v0.py in dbserv is currently using MySQLdb instead of going through the db API, because we need to use parameter binding for security reasons. We should switch to using db, once the db interfaces will support it. ",1
"Long term database work planning
Long term planning (updating LDM-240).",8
"Package andyH xssi fixed version (>2MB answer pb) in eups
See DM-1847 - Andy made a patch, it'd be good to the xrootd we use for our stack.",1
"FY18 Revisit L2 Catalog Schema
Revisit the baseline schema",53
"FY18 Implement Internal DRP Database
Implement Internal DRP Database as designed in DM-2038",60
"FY17 Improve ImageServ
nan",80
"FY19 Demonstrate Qserv Fault Tolerance
Including multi-master failover",100
"FY19 Optimize Partitioning Granularity
We have been always talking about having ~20K chunks per table, and it was driven primarily by spreadsheet-based analysis. We need to look in more details into that, and perhaps even change the model if needed, e.g., introduce  different partitioning for larger tables, like ForcedSource.",53
"FY17 Improve Query Coverage in Qserv
Currently Qserv supports only a limited subset of queries. We need to make sure it supports all queries that users need to run.",53
"FY18 Improve Query Coverage in Qserv
Currently Qserv supports only a limited subset of queries. We need to make sure it supports all queries that users need to run.",79
"FY17 Support Explain, Show, List Commands
Implement [explain|http://dev.mysql.com/doc/refman/5.0/en/explain.html] and [show|http://dev.mysql.com/doc/refman/5.0/en/show.html] commands for Qserv. Also, commands such as ""list tables"" will need to be intercepted and overloaded. ",53
"FY19 Implement Multi-master for Qserv
nan",100
"FY19 Make Database Secure
Revisit security issues, such as sql injections, detecting and shielding from DoS attacks, etc.",79
"FY19 Build/Setup Basic Qserv Monitoring
frontend/worker health monitoring (and management?)",79
"FY18 Implement Failover for L1 Database
Need to implement and test failover - a failure of the master copy of L1 database, and automatic fail over to a replica. The design of the Alert Production L1 database is covered [here|http://ldm-135.readthedocs.org/en/master/#alert-production-and-up-to-date-catalog].",80
"W16 Optimize Secondary Index - Research
Work on the secondary index (objectId --> chunkId / subChunkId mapping). This needs to be scalable to 40B entries. Since we are planning to ingest all data from DRP in <2 days, building should take <2 days. This epic involves researching applicable technologies (including experimenting with most promising ones). Deliverable: proposed technology / architecture along with measures performance at production scale (40 B entries). ",48
"FY18 Add Support for Non-partitioned Tables
Non partitioned tables will need special attention. Options include: a. replicating them on each worker node b. keeping them on a shared file system c. federating  Need to thing through these issues, pick the best architecture and implement it.",79
"FY17 Revisit Qserv Deployment on Cluster
nan",60
"FY18 Design Qserv Software Upgrading
Need to understand how to do software update for Qserv ",26
"FY19 Improve Query Coverage in Qserv
nan",90
"FY20 Improve Qserv Monitoring
frontend/worker health monitoring (and management?)",79
"Resolve compiler warnings in new measurement framework
When building {{meas_base}}, or any other measurement plugins which follow the same interface, with clang, I see a bunch of warnings along the lines of:  {code} In file included from src/ApertureFlux.cc:34: include/lsst/meas/base/ApertureFlux.h:197:18: warning: 'lsst::meas::base::ApertureFluxAlgorithm::measure' hides overloaded virtual function       [-Woverloaded-virtual]     virtual void measure(                  ^ include/lsst/meas/base/Algorithm.h:183:18: note: hidden overloaded virtual function 'lsst::meas::base::SimpleAlgorithm::measure' declared here:       different number of parameters (4 vs 2)     virtual void measure( {code}  This is an artefact of a [workaround for SWIG issues|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=20284390]; the warnings aren't indicative of a fundamental problem, but if we can avoid them we should.  While we're at it, we should also fix:  {code} include/lsst/meas/base/ApertureFlux.h:233:1: warning: 'ApertureFluxResult' defined as a struct here but previously declared as a class       [-Wmismatched-tags] struct ApertureFluxResult : public FluxResult { ^ include/lsst/meas/base/ApertureFlux.h:65:1: note: did you mean struct here? class ApertureFluxResult; ^~~~~ struct {code}",1
"W16 Understand Async Queries in Qserv
Understand how disruptive the changes related to implementing asynchronous queries will be for Qserv.    Delivarable: brief description outlining changes needed, with story point estimate.",10
"Add parameter binding to db interface
nan",1
"Validate user query in dbREST
Need to validate query (from security standpoint that user enters through rest api.",1
"Support DDL in MetaServ - implementation
DDL information is embedded as comments in the master version of the schema (in ""cat"" repo). Currently we are only using it for schema browser. This story involves building tools that will load the DDL schema into MetaServ. Design aspects are covered in DM-1770.",8
"Add meas_extensions_shapeHSM to lsstsw, lsst_distrib
meas_extensions_shapeHSM has just been resurrected from bitrot, and should be included in our distribution.    Contrary to DM-2140, it should probably not be included in lsst_apps, as it's not clear we want to add a dependency on tmv and GalSim there.",1
"General OpenStack Learning
nan",6
"Setup spare test hardware for OpenStack testing
nan",20
"Test Ubuntu OpenStack
nan",6
"Test Mirantis OpenStack & Fuel
nan",6
"Figure out OpenStack networking (vLAN, routing, etc)
nan",6
"Figure out OpenStack integration with LDAP
nan",17
"Log fails on uniccode string
Log is currently failing if we pass unicode string, it is easy to reproduce by doing: log.info(u""hello""). It fails with:  {code}   File ""/home/becla/dataArchDev/Linux64/log/master-gfab0203bd3+3/python/lsst/log/log.py"", line 103, in info     log("""", INFO, fmt, *args, depth=2)   File ""/home/becla/dataArchDev/Linux64/log/master-gfab0203bd3+3/python/lsst/log/log.py"", line 94, in log     _getFuncName(depth), frame.f_lineno, fmt % args)   File ""/home/becla/dataArchDev/Linux64/log/master-gfab0203bd3+3/python/lsst/log/logLib.py"", line 648, in forcedLog_iface     return _logLib.forcedLog_iface(*args) TypeError: in method 'forcedLog_iface', argument 6 of type 'std::string const &' {code}",1
"Data loader crashes on uncompressed data.
Vaikunth just mentioned to me that the is a crash in data loader when it tries to load uncompressed data: {noformat} root - CRITICAL - Exception occured: local variable 'outfile' referenced before assignment Traceback (most recent call last): File ""/home/vaikunth/src/qserv/bin/qserv-data-loader.py"", line 312, in <module> sys.exit(loader.run()) File ""/home/vaikunth/src/qserv/bin/qserv-data-loader.py"", line 248, in run self.loader.load(self.args.database, self.args.table, self.args.schema, self.args.data) File ""/home/vaikunth/src/qserv/lib/python/lsst/qserv/admin/dataLoader.py"", line 168, in load return self._run(d atabase, table, schema, data)   File ""/home/vaikunth/src/qserv/lib/python/lsst/qserv/admin/dataLoader.py"", line 192, in _run     files = self._gunzip(data)   File ""/home/vaikunth/src/qserv/lib/python/lsst/qserv/admin/dataLoader.py"", line 388, in _gunzip     result.append(outfile) UnboundLocalError: local variable 'outfile' referenced before assignment {noformat}  It looks like we never tested loader on uncompressed data and there is a bug in handling uncompressed data. ",1
"Add support for JSON - define structure
As discussed at [Data Access Hangout 2015-02-23|https://confluence.lsstcorp.org/display/DM/Data+Access+Hangout+2015-02-23], we should support json format. This includes defining the exact format, and implementing it. This story covers defining the format.",2
"Implement Image Response for ImgServ
This story covers implementing proper response, and the header metadata for the fits image response.",3
"Setup webserv for SUI tests
We need to setup a service (eg on lsst-dev) that can be used by the IPAC team to play with our webserv/metaserv/dbserv/imgserv.  The server runs on lsst-dev machine, port 5000. To ssh-tunnel, try: {code} ssh -L 5000:localhost:5000 lsst-dev.ncsa.illinois.edu {code}  An example usage:  {code}   curl 'http://localhost:5000/db/v0/query?sql=SHOW+DATABASES+LIKE+""%Stripe%""'   curl 'http://localhost:5000/db/v0/query?sql=SHOW+TABLES+IN+DC_W13_Stripe82'   curl 'http://localhost:5000/db/v0/query?sql=DESCRIBE+DC_W13_Stripe82.DeepForcedSource'   curl 'http://localhost:5000/db/v0/query?sql=DESCRIBE+DC_W13_Stripe82.Science_Ccd_Exposure'   curl 'http://localhost:5000/db/v0/query?sql=SELECT+deepForcedSourceId,scienceCcdExposureId+FROM+DC_W13_Stripe82.DeepForcedSource+LIMIT+10'   curl 'http://localhost:5000/db/v0/query?sql=SELECT+ra,decl,filterName+FROM+DC_W13_Stripe82.Science_Ccd_Exposure+WHERE+scienceCcdExposureId=125230127'   curl 'http://localhost:5000/image/v0/raw/cutout?ra=7.90481567257&dec=-0.299951669961&filter=r&width=30.0&height=45.0' {code} ",2
"Refactor Geom class in Firefly
The Geom class was ported from C code 20 years ago.  It needs to refactor to comply with Java OO design.  ",8
"Review at DM leadership team meeting
review document  with Kantor, KT, Hobblit, and Lambert,  including prep time ",3
"Refactor document for that specifications are clearer
1) Have one basic definition of racks and other components in the specifications.  2) Fully write up first full draft  specification for the supporting material  handing area.",3
"receive / process comments  from Jeff Barr a
receive edits from Jeff Barr,  accept the formatting and mechanical l edits. Compose separate email list issues related to non LSST tenants in the  room.  ",1
"Investigate  Commerical  vendor to deal with comments on requirements. 
process email discussion about the need to liaison with the putative Chilean design contractor.    Kantor suggests a contractor to support requirements may be apropos. ",1
"Work inside NCSA to connect procurement contract Modification to OSPRA contract officet
work Jeff's proposal until it reached university contract officer. -- January meeting  -- clarify  purchasing rules -  Internal discussion of property management, -  General work within  contract modification process. ",6
"Implement JSON Results for MetaServ and DbServ
Implement JSON results for Metadata Service (see all M* in https://confluence.lsstcorp.org/display/DM/API),  and Database Service (see all D*) as defined in DM-1868",3
"Disable testDbLocal.py in db if auth file not found
tests/testDbLocal.py can easily fail if required mysql authorization file is not found in user home dir. Skip the test instead of failing in such case.",1
"Adapt integration test to multi-node setup v2
Following DM-595 we can start qserv in multi-node configuration. Next step is to be able to run integration tests in that setup. This needs a bit of understanding how to distribute chunks between all workers in a cluster and how to load data in remote mysql server.",10
"Worker management service - impl
We need to replace direct worker-mysql communication and other administrative channels with a special service which will control all worker communication. Some light-weight service running alongside other worker  servers, probably HTTP-based. Data loading, start/stop should be handled by this service.",10
"Implement worker-side squashing
In the port to the new Xrootd Ssi API, worker-side squashing was lost in the shuffle. The plumbing is different, and re-implementing squash functionality is not entirely straightforward, especially because the new API is still missing documentation and examples for implementing cancellation.  The consequences of not implementing this are minor--some extra work may be done by the worker, but not a whole lot, because user-level cancellation has not been implemented.",12
"Migrate Qserv to external sphgeom
Migrating qserv to the new c++ geometry API required porting a fair amount of code from the python layer and updating the plumbing in the czar. During implementation, the sphgeom was in the process of finding a home, so the sg code was temporarily placed under core/modules.  This ticket covers: * removing core/modules/sg * updating code to point at the external sphgeom * updating build-logic to properly depend on and link with external sphgeom.",4
"S17 Design Prototype EFD Schema for DRP
The epic involves understanding the structure of the EFD database produced by the Engineering and Facility team, and designing schema that will be best suited for Data Release Production. Note that the original EFD database may not even be in MySQL, there were discussions to store it in Postgresql.  Deliverable: Alpha version of the EFD database schema for DRP with ""real"" data loaded (if available).",52
"FY18 Design DRP-ready EFD Schema
nan",79
"Move astrometry_net wrapper code from meas_astrom to meas_astrometry_net
We would like to remove all astrometry.net wrapper code from meas_astrom and put it in a new package with a name such as meas_astrometry_net.  This will also require moving any abstract task base classes into a lower-level package such as meas_astrom.",6
"FY17 Data Loader for Large Tables with No Position Information
We need to load some tables (e.g., ForcedSource) that lack director positioning, we will only have the director's primary key. The general case is very expensive (lookup position and chunk for each position), however the fact such tables will be spatially-ordered when loading helps.",60
"Update the astrometry.net astrometry solver to use the new standard schema
DM-1576 provides a new astrometry solver and a new schema for reference objects. However, the old astrometry.net astrometry solver still uses the old schema. It would be wise to convert the old solver to the new schema so that the match list returned by it is in standard format.",4
"Large scale test planning
Need to come up with a plan which data set we will use for large scale tests, and how we will produce it.",10
"Documentation for data loader
Vaikunth had some ""expected"" troubles playing with data loader options for his DM-1570 ticket. Main issue I believe is the absence of the documented use cases and their corresponding data loader options. I'll try to add a bunch of common use cases to RST documentation and also verify that all options behave as expected.",2
"Define command line tasks for pre-ingest transformation
DM-1903 provided a command line task which would transform a {{src}} catalogue into calibrated form. Here, we build on that to provide command line tasks for all source catalogues which will need to be ingested; will include at least {{deepCoadd_src}}, {{goodSeeingCoadd_src}}, {{chiSquaredCoadd_src}}.",6
"Provide transformations for ""big three"" measurements
Provide standard calibration transformations for each of shape, flux and centroid and make sure they are returned as the default transformation for all algorithms measuring those quantities.",10
"Add assertXNearlyEqual to afw
We often want to compare two WCS for approximate equality. afw/image/testUtils has similar functions to compare images and masks and I would like to add one for WCS    This ended up being expanded to adding functions for many afw classes (not yet including image-like classes, though existing functions in image/testUtils for that purpose should probably be wrapped or rewritten on a different ticket)",5
"Ensure proper functioning of HSC distortion correction within obs_subaru
There may be some discrepancy between the pixel units being passed to distest.cc compared to what it is expecting (units of pixels).  This needs to be investigated further and remedied in such a way that all other representations (e.g. in camera.py) are consistent with the other obs_XXX representations.",6
"Create form framework in React
We want to create a new frame work for entering data for forms and dialogs. This is in javascript based on React.js.  This is the first step in our javascript conversion.",10
"Prototype HTM-based spatial binning to visualize large number of catalog sources
See story DM-1551.",8
"Build 2015_03 Qserv release
See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe. ",1
"Add typemaps for numpy scalars
Add typemaps so that we can use numpy scalars to call C++ functions that take plain old scalar types (e.g. float, double or int). At present attempting to pass numpy scalars will fail unless the type is one of a restricted subset, e.g. float and numpy.float64 succeed but numpy.float32 is rejected as being an incompatible type, and similarly for integer types.",4
"Acquire development data
We'll need a reference set of data to work against.  This could be SDSS, CFHT, or simulated.  Should be 10? epochs with realistic atmospheric conditions taken at similar airmass and hour angle.  Single band is fine for now.",4
"Produce task API
This will require a new task, so will require a new interface and associated RFCs.  The interface should take an arbitrarily large stack of catalogs with or without a reference catalog.  It should return a stack ow WCSs that map from the individual coordinate systems to the reference.",6
"Break down monster DM-1108 stories
[~pgee] -- After finishing the measurement work, your next priority is to get started on DM-1108. However, the stories you have been assigned there are currently too big for useful scheduling (20-30 SPs is a mini-epic; we're looking for less than 10 SPs per story). The first task therefore is to work with [~jbosch], and others if required, to break them down and come up with a set of stories which usefully reflect the work which needs to be done.",4
"Deploy and test network emulation for nightly processing testbed
Deploy and test network emulation for nightly processing testbed.    Assignees: Paul Wefel, Steve Pietrowicz, James Parsons  Duration: January - February 2016",19
"Alert Production Simulator
Start March 2015, finish July 2015 Pietrowicz S - 100%",96
"Complex Event Processing
Start May 2015, finish June 2015",31
"OCS Software
Start July 2015, finish August 2015 Pietrowicz S - 100%",10
"Configuration Management (Puppet)
Start March 2015, finish May 2015 Mather B - 40%",9
"Setup qserv prototype for qserv & SUI teams
Start July 2015, finish August 2015 Glick B - 25%   Qserv requirements: - SUI will be testing against lsst10 (or IPAC qserv) for now ?   SUI requirements:  Xiuqin's 'short term' version: - 1 VM - SUI build server     - 4GB memory and 200GB hard disk should be good enough. - 1 VM - Apache server as a proxy and web front end     - 4GB memory and 100GB hard disk should be enough     - port 80 accessible from outside - 2 VMs - Tomcat servers      - each has 16GB memory, access to 1TB of shared hard disk     - Port 8080 should be open for Apache server to access     - Port 8009 should be open to each other so they can replicate cache. (First 2 VMs are not absolutely needed. We can always use one of the hosts in number 3 to do build and host Apache server.)  Trey Roby's 'long term' (in 2+ years) SUI requirements: - 2 vm/machines for Tomcat servers, they are fairly large       - each 100 GB mem       - each 16 processors       - 1 TB disk space shared and accessible between. - 1 vm/machine for Web Server, can be small 	- 16 GB mem       - 30 GB disk       - 4 processors - 2 small vm/machines for playing around with workspaces/L3 concepts       - each has 8 GB mem       - each 10 GB disk       - 4 processors       - Can share the Tomcat servers disk  space",10
"Storage Policies and Alignment
Start March 2015, finish August 2015 Freemon M - 100%",50
"File System Research and Prototyping
Start March 2015, finish September 2015 Freemon M - 100%, Glick B - 25%, Daues G - 40%, Elliot M - 25%",100
"File Management Technology
Start March 2015, finish September 2015 Daues G - 40%, Freemon M - 100%",50
"Understand GPFS and commercial filesystems between data centers
Start May 2015, finish June 2015 Petravick D - 50%, TBD from SET group",6
"Update Sizing Model
Start March 2015, finish September 2015  Alt J - 50%, Petravick D - 10%",22
"Base Data Center Requirements
Start March 2015, finish May 2015 Petravick D - 50%",18
"Start understanding inheritability and reusability of dataset types
In order to allow for on-the-fly Task creation of dataset types, the essentials of each type need to be encapsulated in code.  That code should be reused across all similar dataset types, and there are opportunities for inheritance and specialization, particularly in cases like simple file-oriented mappers.  Investigate this by prototyping a number of possibilities.",4
"Wide-Area Network Work
Start March 2015, finish September 2015 Wefel P - 25%",20
"LOE - S15 (sys admin)
Glick B - 25%, Mather B - 40%, Elliot M - 25%, Freemon M - 100% Wefel P - 25%",80
"LOE - S15 (management)
Petravick D - 50%, Gelman M - 50%",20
"LOE - S15 (misc)
All NCSA team",10
"LOE - Week ending 3/6/15
- backup issues with lsst-stor141 (https://jira.ncsa.illinois.edu/browse/LSST-632) - setup jumbo frames on lsst-xfer (https://jira.ncsa.illinois.edu/browse/LSST-628) - crashplan reconfig on lsst-xfer (https://jira.ncsa.illinois.edu/browse/LSST-629)",8
"LOE - Week ending 3/13/15
- crashplan issue with lsst-netem (https://jira.ncsa.illinois.edu/browse/LSST-633) - yum/glibc issue with lsst-dbdev1 (https://jira.ncsa.illinois.edu/browse/LSST-631) - account for Jacques Sebag (https://jira.ncsa.illinois.edu/browse/LSST-624)",8
"LOE - Week ending 3/20/15
- lsst-dbdev2 drive failure (https://jira.ncsa.illinois.edu/browse/LSST-636) - account for Colin Slater (https://jira.ncsa.illinois.edu/browse/LSST-634) - disable Robyn Allsman's accounts (https://jira.ncsa.illinois.edu/browse/LSST-623)",26
"LOE - Week ending 3/27/15
nan",21
"LOE - Week ending 4/3/15
- researched buildbot slowness on lsst-dev <https://jira.lsstcorp.org/browse/DM-2388> - researched buildbot slowness on lsst-dev <https://jira.ncsa.illinois.edu/browse/LSST-638>",16
"LOE - Week ending 4/10/15
nan",16
"LOE - Week ending 4/17/15
nan",17
"LOE - Week ending 4/24/15
- Researched how to monitor network drops, errors, etc <https://jira.ncsa.illinois.edu/browse/LSST-641> - Opened up SUI ports on lsst-dev <https://jira.ncsa.illinois.edu/browse/LSST-651> - Moved /nfs/admin/ to /condo/admin/ - Review of EL 6.x kernel security patch - Fixed jumbo frame issues, primarily with old VMs that needed new version of NIC <https://jira.ncsa.illinois.edu/browse/LSST-652>",15
"Prototyping with Puppet
nan",5
"Test Puppet with base configuration manifests
nan",38
"Develop use cases for TOWG
Start March 2015, finish May 2015 Petravick D - 50%, Glick B - 25%, Gruendl R - 5%",20
"ISO Work
Start March 2015, finish September 2015 Withers A - 25%",40
"Extend API: expose cursor
Extend API to expose cursor. This was brought up by Andy in DM-2137. ",1
"Define ntermediate plan for MacOSX builds
 We have  1. Obtain a dedicated colo OSX server  2. Have done some testing using the SQuaRE vagrant-sandbox harness  It is therefore a plausible avenue forward to do at least a nightly build/deploy/intgeration-test on OSX pending more extensive arrangements requiring purchase of hardware.  ",4
"Github transition for DM
DM's transition for code repositories to Github is complete.  Outstanding are data repositories; a cleanup of contrib/externals; and supporting the Stash move. ",10
"Workflow improvements for Sims / PST projects
New wokflow for Sims Merge of Opsim and CATsim New workflow for PST ",5
"Prototype automated system for release preparation builds
 Prototype an environment that allows automatic   - Provisioning of a VM for a certain OS - Install the Stack prerequisites for that OS - Build the stack via newinstall.sh from the production server - Run integration tests (in the curent case the sdss test  https://github.com/lsst-sqre/sandbox-stackbuild",22
"Galaxy Fitting via ""ngmix""
Provide wrappers that let us run Erin Sheldon's [ngmix|https://github.com/esheldon/ngmix] as part of the DM pipeline.  Issues so far only cover getting the a single-frame (visit or coadd) version of the code running.  ngmix can also to simultaneous fitting to multiple exposures, but it's not yet clear how we'll want to handle the I/O and that interacts with a future multifit plugin framework.",45
"Implement SExtractor's SPREAD_MODEL
The new SExtractor star/galaxy classifier, SPREAD_MODEL, is popular with everyone who has tried it, and should be simple to implement by building on code in meas_modelfit.  See definition and discussion here: http://arxiv.org/abs/1306.4446",2
"Define common interface for star/galaxy classifiers
We need some common fields for star/galaxy classifiers so they can participate in a slots-like mechanism once we have several of them.  Most of these can produce a floating point number between 0 and 1 (but sometimes it's not limited to that range), and it's rarely a true probability.  We may want to make a boolean that results from a threshold on these be the common interface, but we don't necessarily want to hard-code such a threshold into the processing either - especially when we could also use a FunctorKey to get a boolean from the floating point value.",2
"add third-party package builds for ngmix dependencies
In addition to numpy and scipy, ngmix depends on the emcee and statsmodel packages.  While it can build without them, we probably want the full functionality.  I also see some undeclared dependencies on the ""esutil"" and ""fitsio"" packages (all from esheldon's GitHub), and there may be a few more dependencies on some of his own packages.    This issue includes creating a third-party build for ngmix itself.",6
"Add SFM plugin for ngmix MCMC sampling
Add an SFM plugin for ngmix MCMC fitting, as in the example in the ngmix README.    This should depend on DM-5429 (or a suitably configured modelft_ShapeletPsfApprox) for PSF approximation.    For now, we should just take the mean of all parameters in the MCMC samples and write those to the record, as we currently don't have any way to save all of the samples.    Testing and tuning this algorithm to get it working well should be deferred to another issue.  The only requirement here is that it be able to run without crashing (even if that means setting the number of samples small).",10
"make a simple build for Firefly package
We want to have a out of box build for users of Firefly package. It will include a simple Firefly viewer. ",6
"Allow eups xrootd install script to be relocatable
xrootd lib/ directory should be s relative symlink to lib64, no a full path link.",1
"Setup in2p3 cluster for Qserv team
- create accounts - update umask on stack  to each account - provide easy ssh config if possible - setup up build procedure (each developer can build Qserv using tag git and 'git' version is set up by default on all the Qserv if ti exists)",2
"remove PSFAttributes
PSFAttributes has long been deprecated, and we just need a little more work to remove it:  - Add an effective area accessor to the Psf interface, and implement it in ImagePsf.  - Replace usage of PSFAttributes with usage of Psf accessors.  This may require a little work if code depends on the details of how the shape was calculated, as PSFAttributes provided support for more algorithms than we will going forward.",2
"Improve build system for sphgeom
nan",2
"Create pilot condor jobs
Create long-running jobs to reduce the startup time for new HTCondor jobs.   This can be implemented as a parent/child, or as a on_exit_remove=false directive in HTCondor.  I suspect it will be a combination of the two.",21
"Implement task switching between work job machines
AP requires that jobs are handed off to different worker job clusters as the previous set of images is being worked on.",4
"Refactoring
The initial prototype of the AP simulator needs to be refactored to improve how tasks are handled by the components for further development.",25
"Implement API for reading simulated camera data
Currently this is generated by the replicator and sent to the distributor.  The idea where is to put the API in place so that the data will be transferred from outside of the replicator to it, and then passed on.",10
"Implement file transfer API 
Create file transfer API so we can easily test different types of file transfer mechanisms to/from the AP.",4
"Move VMs to Docker containers
We anticipate being able to move from the VMs that we currently use to using docker.  This will require some coordination with Greg Daues to see how HTCondor is configured.  ",2
"Unify logging strategy for python scripts
- add -vvv option  - remove default value for configuration file in logger, provide it at each script level (i.e. integration test, data loader).   - if it exists, provide configuration file option explicitly to all called submodules which uses it.    See admin/python/lsst/qserv/admin/logger.py  {code:python}   14 def get_default_log_conf():                                                                                                                                                                  15     default_log_conf = ""{0}/.lsst/logging.ini"".format(os.path.expanduser('~'))                                                                                                               16     return default_log_conf                                                                                                                                                                  17                                                                                                                                                                                              18 def add_logfile_opt(parser):                                                                                                                                                                 19     """"""                                                                                                                                                                                      20     Add option to command line interface in order to set path to standar                                                                                                                     21     configuration file for python logger                                                                                                                                                     22     """"""                                                                                                                                                                                      23                                                                                                                                                                                              24     parser.add_argument(""-V"", ""--log-cfg"", dest=""log_conf"",                                                                                                                                  25                         default=get_default_log_conf(),                                                                                                                                      26                         help=""Absolute path to yaml file containing python"" +                                                                                                                27                         ""logger standard configuration file"")                                                                                                                                28     return parser                                                                                                                                                                            29                                                                                                                                                                                              30                                                                                                                                                                                              31 def setup_logging(path='logging.ini',                                                                                                                                                        32                   default_level=logging.INFO):                                                                                                                                               33     """"""                                                                                                                                                                                      34     Setup logging configuration from yaml file                                                                                                                                               35     if the yaml file doesn't exists:                                                                                                                                                         36     - return false                                                                                                                                                                           37     - configure logging to default_level                                                                                                                                                     38     """"""                                                                                                                                                                                      39     if os.path.exists(path):                                                                                                                                                                 40         with open(path, 'r') as f:                                                                                                                                                           41             logging.config.fileConfig(f)                                                                                                                                                     42         return True                                                                                                                                                                          43     else:                                                                                                                                                                                    44         logging.basicConfig(level=default_level)                                                                                                                                  45         return False    {code}",6
"Document HOW-TO setup-up krb5 for easy cluster access
{code:bash} su aptitude install krb5-user # edit /etc/krb5.conf w.r.t ccage one # then as desktop user kinit ssh ccqservxxx {code}  /etc/krb5.conf {code:bash} [libdefaults] 	default_realm = IN2P3.FR  ... 	allow_weak_crypto = true   ... [realms] 	IN2P3.FR = { 		kdc = kerberos-1.in2p3.fr:88 		kdc = kerberos-2.in2p3.fr:88 		kdc = kerberos-3.in2p3.fr:88     		master_kdc = kerberos-admin.in2p3.fr:88     		admin_server = kerberos-admin.in2p3.fr     		kpasswd_server = kerberos-admin.in2p3.fr     		default_domain = in2p3.fr {code}  sshconfig: {code:bash} Host ccqservbuild GSSAPIAuthentication yes GSSAPIDelegateCredentials yes ForwardX11 yes HostName ccqservbuild.in2p3.fr #ProxyCommand ssh -W %h:%p cc   Host ccqserv1* GSSAPIAuthentication yes GSSAPIDelegateCredentials yes ForwardX11 yes HostName %h.in2p3.fr ProxyCommand ssh -W %h:%p ccqservbuild {code}",2
"Fix problems with mysql timeout
We added some code for supporting reconnecting (see https://dev.lsstcorp.org/trac/ticket/3042) but clearly not enough to recover from connection timeouts. This needs to be addressed.",1
"The TAN_PIXELS cameraGeom coordinate system should be with respect to the center of the focal plane
The TAN_PIXELS cameraGeom coordinate system (the position on a detector if there is no optical distortion) is presently defined with respect to the center of the detector -- i.e. a star at the center of the detector will have the same position in PIXELS and TAN_PIXELS coordinates. That is a mistake. TAN_PIXELS should be defined with respect to the center of the focal plane, since it then reflects the effects of having optical distortion or not.  Fixing this will help meas_astrom match stars. The effects of not fixing it are making the matcher search farther for a fit. As long as we allow sufficient offset in the matcher config the current system will work, but it is not ideal.",2
"Implement connection pool
Implement a class that manages a connection pool, and optionally, if configured, restarts connection as needed in case of timeout.",1
"Switch to using db connection pool
Switch to using the db connection pool. Note, in addition to getting auto-reconnect, in metaserv that would handy if we need to talk to multiple database servers simultaneously.",1
"Participate in design process
Participate and guide the SUI design process, generate charts and documents as appropriate",9
"Move javascript code into firefly repo and begin creating a real input form
nan",10
"Work with Camera & Pipeline team to spec out  proof of concept tools
nan",14
"Personnell requisitions 
Work though recruiting for software effort.  Investigated and filled the ""kenton"" recruiting pattern at NCSA -- (few explicit requirements, many desirable)  Began discussion to break down hires for ""2nd"" floor  work -- to be in the LSST group v.s support groups -- ADS and Doug's group",3
"Arrange for commercial object store presentation
arrage for presentations next week w.r.t commercial object store.  The vendor in question is know to NCSA and has claims to have produced a commercial object store having both NFS,  GPFS  and swift interfaces. ",1
"Begin WBS review 
Begin  comprehensive review of the WBS.   Forced on overall framework and begin  workflow systems  ",1
"Security officer orientation
begin orientation of LSST ISO ALEX Withers. ",1
"Internet2 TIER investigation
nan",1
"Unable to start cmsd on Qserv worker node
Some build issues have qlready been fixed in commit: 9dd378829e8751a6852356967411c20580e2a1c3  Here's the log:  {code:bash} [fjammes@ccqserv101 ~]$ cat /qserv/qserv-run/var/log/worker/cmsd.log 150309 21:19:46 9794 Starting on Linux 3.10.0-123.8.1.el7.x86_64 Copr.  2004-2012 Stanford University, xrd version v20140617-203cf45 ++++++ cmsd worker@ccqserv101.in2p3.fr initialization started. Config using configuration file /qserv/qserv-run/etc/lsp.cf =====> all.adminpath /qserv/qserv-run/tmp =====> xrd.port 1094 =====> xrd.network nodnr Config maximum number of connections restricted to 4096 Config maximum number of threads restricted to 2048 Copr.  2007 Stanford University/SLAC cmsd. ++++++ worker@ccqserv101.in2p3.fr phase 1 initialization started. =====> all.role server =====> ofs.osslib libxrdoss.so  =====> oss.localroot /qserv/qserv-run/xrootd-run =====> cms.space linger 0 recalc 15 min 10m 11m =====> all.pidpath /qserv/qserv-run/var/run =====> all.adminpath /qserv/qserv-run/tmp =====> all.manager ccqserv100.in2p3.fr:2131 =====> all.export / nolock The following paths are available to the redirector: w  /   ------ worker@ccqserv101.in2p3.fr phase 1 server initialization completed. ++++++ worker@ccqserv101.in2p3.fr phase 2 server initialization started. Plugin Unable to find  required version information for XrdOssGetStorageSystem in osslib libxrdoss.so ------ worker@ccqserv101.in2p3.fr phase 2 server initialization failed. 150309 21:19:46 9794 XrdProtocol: Protocol cmsd could not be loaded ------ cmsd worker@ccqserv101.in2p3.fr:1094 initialization failed {code}",2
"Read through Don's SCADA notes and comment
nan",1
"Revisit db and dbPool, separate connection from utilities
Revisit whether we need something better than a very basic db connection pool.    It may be worth looking at http://docs.sqlalchemy.org/en/rel_0_9/core/pooling.html (or even sqlalchemy in general). Note the Pooling Plain DB-API Connections section - one can use sqlalchemy pooling independently of the other library features.    Separate utilities like createDb(), dbExists() and such from core part that deals with connections / sqalchemy.",12
"Improve Webserv/Metaserv
Work includes implementing features requested by SUI (schema metadata, units etc)",35
"Support metadata for databases without description
The metaserv should be able to support databases for which we don't have the ascii schema with descriptions and special tokens (ucd, units etc). This story involves implementing it. In practice, the metaserv/bin/metaBackend will need to be extended to implement ""ADD DB""",4
"Measurement transforms for Flux
Provide calibration transforms for flux measurements to magnitudes.",3
"Measurement transforms for centroids
Provide calibration transforms for all algorithms measuring centroids.",5
"Measurement transforms for shapes
Provide calibration transforms for algorithms measuring shapes.",2
"Update dev quick-start guide to new git repositories
The quick-start documentation for developers still points to the old git repositories. The RST document needs to be updated to the GitHub repos.",1
"obs_test's table file is out of date
obs_test's table file is somewhat out of date. Problems include:  - afw is required but missing  - meas_algorithms and skypix are used by bin/genInputRegistry.py, which is only used to create the input repo so these can be optional  - daf_persistence is not used  - daf_base is only used by bin/genInputRegistry.py, so it can be optional (though it is presumably setup by daf_butlerUtils in any case)",1
"Improve xssi API to send a few bytes with the message informing the client that a response is available on  the server
This would allow Qserv no to send the first protobuf header as a xrootd in-band message, and save some resources (network and CPU due to xrootd/TCP/IP encapsulation)",6
"Clarify expectations for unauthenticated user data access
h4. Short version:  Clarify what existing community practices, notably including VO interfaces, appear to rely on the availability of unauthenticated access to information in astronomical archives.  h4. Details:  At the February DM All Hands, [~frossie] raised an objection when it was mentioned that there is a presumption that all user access to LSST data through the DM interfaces (as opposed to through EPO) will be authenticated.  We don't appear to have ever documented an explicit requirement that all access be authenticated.  The basic controlling requirement is OSS-REQ-0176, ""The LSST Data Management System shall provide open access to all LSST Level 1 and Level 2 Data Products, as defined in the LSST System Requirements and herein, in accordance with LSSTC Board approved policies. ..."", which was a carefully crafted indirection at a time when the policy for non-US/Chile access was still being developed.  However, this presumption has been around for a long time.  It is inherent to the project policy that access to the non-Alert data will be limited to individuals who are entitled to it.  No matter what we think the final policy might be, we do have to design a system that can be consistent with this policy.  [~frossie] stated that the astronomical community relies on certain types of data and metadata - she mentioned coverage maps, among others - being available through unauthenticated interfaces.  This ticket is to ask her (and others) to collect documentation of those existing practices, so that we can figure out what the expectations may be and how to respond to them in our design.",2
"Remove deprecated merging code: rproc::TableMerger
rproc::TableMerger seems to be replaced with rproc::InfileMerger, so this class could certainly be removed easily. ",2
"Revisit exceptions in db module
Revisit db/python/lsst/db/exception.py. Perhaps get rid of the numbers.",5
"KT reading list for operational requirements
nan",2
"Observatory site requirements reading
nan",2
"Setup hosts for SUI (2x Tomcat, Apache, and build)
Xiuqin's 'short term' version: 1 VM - SUI build server 4GB memory and 200GB hard disk should be good enough. 1 VM - Apache server as a proxy and web front end 4GB memory and 100GB hard disk should be enough port 80 accessible from outside 2 VMs - Tomcat servers each has 16GB memory, access to 1TB of shared hard disk Port 8080 should be open for Apache server to access Port 8009 should be open to each other so they can replicate cache. (First 2 VMs are not absolutely needed. We can always use one of the hosts in number 3 to do build and host Apache server.)  The 2 Tomcat servers are larger than we can currently support as VMs.   We've discussed repurposing 2 of the older LSST ""cluster/condor"" nodes (e.g. lsst14 & lsst15) for this purpose.  But, ideally these could be implemented with the new vSphere hardware if the timeframe works.",4
"review """"data center in a box"" mali.  Recover consultant's contact into 
reviewed the data center in a  box, recovered consultant's name prior to drafting a SOW.",1
" attend DDN WOS briefing, write summary note. 
as described above.  Summary note is attached. also looked for use of this product in DOE labs, who would be consumers  of LSST data.  Discovered that it had been investigated for use in HEP a few years earlier, but that is was not adopted because, at that time the hardware and software were coupled.",1
"misc for week of march 9
finalize job descriptions. Meet with kantor additional hour of  orientation for the ISO. group meeting  Misc.",1
"Simplify interactions with XrdOss
The qserv code is still using the old ssi scheme for the cmsd, this needs to be rewritten. For  details, see  https://listserv.slac.stanford.edu/cgi-bin/wa?A1=ind1503&L=QSERV-L#3",5
"Setup IRODS zone on ISL OpenStack
We begin an examination of iRODS for managing data collections. We perform initial testing using resources available on NCSA's ISL OpenStack.  To mock up a zone or 'data grid' managed by iRODS, we set up an ICAT server, a resource server (this is a data storage resource that does not run the central database), and a client host. ",4
"Save iRODS installations/servers as docker images
We install and configure iRODS servers (an ICAT server, a resource server, a client host) in docker and make images, pushing the results to a docker hub repository. ",4
"develop/propose storage policies
nan",6
"develop/propose storage procedures
nan",10
"develop/propose storage implementation
nan",3
"Reprise SDRP processing metrics
In support of an SDRP-based science talk of Yusra AlSayyad, we spent some cycles gathering/summarizing processing middleware results and metrics from the US side of processing of the Split DRP.  This information from notes, logs, databases, etc provided contextual information on the processing campaign that produced the SDRP science results. ",2
"Use parallel ssh to manage Qserv on IN2P3 cluster
IN2P3 sysadmin won't manage Qserv through puppet. So Qserv team has to provide ssh scripts to do this.  ",5
"Integrate changes from Events code review
nan",6
"Move afw_extensions_rgb functionality into afw proper
See RFC-32 ",1
"(In)equality semantics of Coords are confusing
Viz:  {code} In [1]: from lsst.afw.coord import Coord In [2]: c1 = Coord(""11:11:11"", ""22:22:22"") In [3]: c1 == c1, c1 != c1 Out[3]: (True, False) In [4]: c2 = Coord(""33:33:33"", ""44:44:44"") In [5]: c1 == c2, c1 != c2 Out[5]: (False, True) In [6]: c3 = Coord(""11:11:11"", ""22:22:22"") In [7]: c1 == c3, c1 != c3 Out[7]: (True, True) {code}  {{c1}} is simultaneously equal to *and* not equal to {{c3}}!",1
"useValueEquality and usePointerEquality fail to fail
These SWIG macros return a class instead of raising an exception instance when the equality operation fails.",1
"Add unit tests to SchemaToMeta
Add unit tests, also improve variable names as suggested by K-T in comments in DM-2139",1
"Install and learn to use iPython notebook
nan",4
"Participate in design discussion
Participate  in the design discussions three times weekly for two to three months. ",9
"Participate in design discussion
Participate in the design discussions three times weekly for two to three months. ",9
"Participate in design discussion
Participate in the design discussions three times weekly for two to three months. ",9
"Identify the hardware resources needed at NCSA for short term development 
Supply the hardware resources needed at NCSA for short term development. It is captured in DM-2327  ",1
"make PixelFlagsAlgorithm fully configurable
PixelFlagsAlgorithm currently hard-codes the mask planes it considers.  This should be fully configurable instead.  It also overloads the ""edge"" flag to mean both ""EDGE mask plane was set"" and ""centroid was off the edge of the image"".  These should be different flags.  We may also want to have this algorithm use SafeCentroidExtractor.'  Finally, the algorithm is woefully undertested.",2
"standardize handling of missing peaks in centroiders
GaussianCentroid has a NO_PEAK flag that it sets when there is no Peak to use as an input.  SdssCentroid does not.  This behavior should be standardized.  Maybe we should use SafeCentroidExtractor here?",1
"RGB code introduces dependency on matplotlib
While the new RGB code looks like it's just calling NumPy, NumPy is actually delegating to matplotlib under the hood when it writes RGB(A) arrays.  It also turns out that code is broken in matplotlib prior to 1.3.1 (though that shouldn't be a problem for anyone but those who - like me - are trying to use slightly older system Python packages).  I think think this means we should add an optional dependency on matplotlib to the afw table file, and condition the running of the test code on matplotlib's presence (and, ideally, having the right version).  I'm happy to do this myself (since I'm probably the only one affected by it right now).",1
"Revisit the choice of using flask
We should quickly revisit if flask is the right choice for us.  Related: reportedly, our simple flask-based webserver is using more CPU in an idle state than expected. It might be useful to profile things, and look into that. ",1
"run lsstswBuild.sh in a clean sandbox
The ""driver"" script, lsstswBuild.sh, used by the buildbot slave on lsst-dev to initiate a ""CI run"" has a number of environment assumptions (binaries in the $PATH, paths to various components, hostnames, etc.).  This prevents it from [easily] being invoked on any other host.  As lsstswBuild.sh builds a number of packages that are not in the lsst_distrib product, the os level dependencies for these other products need to be determined.  In addition, the current version of lsstswBuild.sh and related scripts on lsst-dev are not version controlled.",8
"Move QuerySession::_stmtParallel from query::SelectStmtPtrVector to query::SelectStmtPtr
QuerySession::_stmtParallel is a vector but it seems only it's first element is used, so storing it in a vector doesn't seem necessary.  Code can be easily simplified here. This should lead to mode understandable code.  QuerySession public members and method comments could also be improved here.",4
"run lsstswBuild.sh under Jenkins on EL6
* Demonstrate lsstswBuild.sh being invoked by jenkins on EL6 (same OS as lsst-dev). * Experiment with a single build slave attached to a jenkins master * Investigate configuration management of jenkins builds.",6
"Improve logger use in qserv
Qserv logger must be easily configurable. Next technique, based on log4cxx documentation allows to do it easily.  Example:  In QuerySession.cc, initialize a static logger: {code:c++} namespace lsst { namespace qserv { namespace qproc {  LOG_LOGGER QuerySession::_logger = LOG_GET(""lsst.qserv.qproc.QuerySession""); {code}  then use it in Query session member functions:  {code:c++}         if (LOG_CHECK_LVL(_logger, LOG_LVL_DEBUG)) {             std::ostringstream stream;             _showFinal(stream);             LOGF(_logger, LOG_LVL_DEBUG, ""Query Plugins applied:\n %1%"" % stream.str());         } {code}  And use log4cxx.property to easily configure, AT RUNTIME, logging for each Qserv module class:  {code} # logger for all module will inherit this one log4j.logger.lsst.qserv=ERROR # this could be generalized to all Qserv modules: log4j.logger.lsst.qserv.qproc=INFO # can also be done at the class level for advanced debugging #log4j.logger.lsst.qserv.qproc.QuerySession=DEBUG {code}  And then in the log: {code} /home/qserv/qserv-run/2015_03/var/log/qserv-czar.log:0319 17:08:48.786 [0x7f208da93740] DEBUG lsst.qserv.qproc.QuerySession (build/qproc/QuerySession.cc:118) - Query Plugins applied: {code}  This proposal is a draft and should be improved before implementing it.",8
"evaluate NCSA proposal to investigate CEPH in the context of NCSA Integrated systems lab
The integrated systems lab (ISL) is the orgianizational vehicle used to investigate pre-production technologies at NCSA.   Since  We still lack the ability to procure goods,  I evaluated and commented on an ISL proposal to investigate the CEPH file system for its properties as an alternative to the LSST baseline file system GPFS. ",1
"revise and circulate data center requirements note
Reconvert the ~10 TBD's  in the priori version of the note  — I’ve kept the stipulation that end of service-lifee stuff will leave these spaces  but added an appendix that “this is what the central space should provide”  My understanding is  there are now discussions on whether that central space will exist of not.   The requirements  can be promoted to center requirements no central space is evient. — The maximal average weight for a rack was computed from LDM-144 and is given.  — There are more cu ft estimates for  the need to dispose of dunnage and packing material. — TBD’s w.r.t overhead cabling are specificed. — The non- LSST requrements are in there, and have been as far as I am able to ascertain them from   champaign urbana.   Ron has been stating requirements as “rows”   I have never fixed row length  thinking  that is for the designer to do.   We’ve stated that rows are shareable, but racks are not. so what’s in the docs is definitive unless/until non LSST requirements can be stated in the same terms.     ""Shall support 16 racks for the NOAO tenant"". is what I had.  power requirements as per  the common space, becasue we want a maximally flexible space. ",2
"Management
Meetings -- Monday CAM meeting, Friday standup and infrastructure.  Internal NCSA group meeting,  Internal NCSA ""comp pol"" technical coordination meeting.  Screen existing candidate pool for likely people to fill opening,  Interviewed one person checked references + Misc.",3
"Retrieve HSC engineering data
HSC data becomes public 18 months after it was taken, so data taken during commissioning are now available.  We would like to use this data for testing the LSST pipeline.  It needs to be downloaded from Japan.",2
"Make sure the command-line parser warns loudly enough if no data found
A user recently got confused when calling parseAndRun didn't call the task's run method. It turns out there was no data matching the specified data ID. Make sure this generates a loud and clear warning.",1
"migrate package deps from sandbox-stackbuild to a proper puppet module
There is a growing list of known package dependencies in the sandbox-stackbuild repo and a need to use this information for independent environments (such as CI).  This list of packages should be lifted out into an independent puppet module that can be reused.",2
"Implement data loading in worker manager service
This is a separate ticket for implementation of the data loading part of the worker management service (started in DM-2176). Some ideas and thoughts are outlined in that ticket.",6
"Build testQDisp.cc on ubuntu
testQDisp.cc needs flags -lpthread -lboost_regex to build on ubuntu.",1
"Errors need to be checked in UserQueryFactory from QuerySession objects
UserQueryFactory doesn't check its QuerySession object for errors after setQuery. Thus it continues setting things up after the QuerySession knows the state is invalid.",1
"Migrating to GWT 2.7
To use JavaScript Interop functionality, we need to migrate to GWT 2.7",10
"React components for Form
To familiarize myself with React and to prepare the ground for moving to React-based user interfaces, we need to create React components for the form. This story includes CheckboxGroup, RadioGroup, and Listbox input fields.",10
"FY17 Enable DC Analyses through Qserv
Load data challenge data into Qserv and enable analytics of the DC data through Qserv.",53
"FY18 Enable DC Analysis through Qserv
Load data challenge data into Qserv and enable analytics of the DC data through Qserv. ",53
"FY17 Fix Qserv Bugs
Bucket epic for unexpected bug fixes.",53
"FY18 Fix Qserv Bugs
Bucket epic for unexpected bug fixes.",53
"FY19 Fix Qserv Bugs
Bucket epic for unexpected bug fixes.",53
"FY20 Fix Qserv Bugs
Bucket epic for unexpected bug fixes.",100
"W16 Butler (v4)
nan",100
"Allow qserv-admin.py to delete a node
Registered workers in CSS with qserv-admin.py are currently not able to be removed (no DELETE NODE type command). Also, changing node status from ACTIVE to INACTIVE needs to be fixed.",1
"Change integration test user from root to qsmaster
Currently integration tests use root account as default user - this should be changed to qsmaster for the future.",2
"investigate configuration management for jenkins
The most popular Puppet module for managing Jenkin's {code}jenkinsci/puppet-jenkins{code} is able to create a master and build slaves but is missing the functionality to manage several master configuration options that otherwise require manual setup.  We need to investigate the difficulty of managing a Jenkin's master configuration values in an idempotent manner.",4
"convert Statistics to use ndarray natively
The Statistics class predates ndarray, and hence uses some hackish Image-class emulators/wrappers to deal with 1-d arrays.  It'd clean things up considerably to have it use ndarray under the hood, and have the Image-based interfaces interact via their ndarray views.",3
"Data loader script crashes trying to create chunk table
Vaikunth discovered a bug in data loader when trying to load a data into Object table: {noformat} [CRITICAL] root: Exception occured: Table 'Object_7480' already exists Traceback (most recent call last):   File ""/usr/local/home/vaikunth/src/qserv/bin/qserv-data-loader.py"", line 318, in <module>     sys.exit(loader.run())   File ""/usr/local/home/vaikunth/src/qserv/bin/qserv-data-loader.py"", line 254, in run     self.loader.load(self.args.database, self.args.table, self.args.schema, self.args.data)   File ""/usr/local/home/vaikunth/src/qserv/lib/python/lsst/qserv/admin/dataLoader.py"", line 171, in load     return self._run(database, table, schema, data)   File ""/usr/local/home/vaikunth/src/qserv/lib/python/lsst/qserv/admin/dataLoader.py"", line 209, in _run     self._loadData(database, table, files)   File ""/usr/local/home/vaikunth/src/qserv/lib/python/lsst/qserv/admin/dataLoader.py"", line 586, in _loadData     self._loadChunkedData(database, table)   File ""/usr/local/home/vaikunth/src/qserv/lib/python/lsst/qserv/admin/dataLoader.py"", line 653, in _loadChunkedData     self._makeChunkAndOverlapTable(conn, database, table, chunkId)   File ""/usr/local/home/vaikunth/src/qserv/lib/python/lsst/qserv/admin/dataLoader.py"", line 727, in _makeChunkAndOverlapTable     cursor.execute(q)   File ""build/bdist.linux-x86_64/egg/MySQLdb/cursors.py"", line 176, in execute     if not self._defer_warnings: self._warning_check()   File ""build/bdist.linux-x86_64/egg/MySQLdb/cursors.py"", line 92, in _warning_check     warn(w[-1], self.Warning, 3) Warning: Table 'Object_7480' already exists {noformat} It looks like I did not do enough testing after my recent improvement in creating chunk tables. It tries to create the chunk table with ""CREATE TABLE IF NOT EXISTS ..."" but that actually generates ""warning exception"" on mysql side when table is already there. Need to catch this exception and ignore it.",1
"Implement authentication mechanism for worker management service
We need some reasonable security for access to new worker management service. It should be lightweight and not depend on complex things that require infrastructure. Something based on a shared secret should be adequate for our immediate needs and likely for the long term.",4
"Document API for worker management service
New worker management service exposes its API as an interface to RESTful web service. Many or all ""methods"" will be wrapped into some sort of Python API, but it would still be useful to document every web service ""methods"" independently. There is basic documentation in the design document (https://dev.lsstcorp.org/trac/wiki/db/Qserv/WMGRDesign), this needs to be extended with detailed description of what those methods do and what kind of data they accept and return.  This story involves selecting the right tool.",4
"Improve support for Python modules in Scons
it seems we have two tools to manage python modules:  - site_scons/pytarget.py and - site_scons/site_tools/pymod.py (grep for InstallPythonModule) used by admin tools. We could unify this, isn't it?",1
"Weighting in photometric calibration is incorrect
Dominique points out that the zero point calibration uses errors not inverse errors to calculate the zero point.  git annotate reveals: bq. 24c9149f python/lsst/meas/photocal/PhotoCal.py (Robert Lupton the Good 2010-12-13 05:03:12 +0000 353)     return np.average(dmag, weights=dmagErr), np.std(dmag, ddof=1), len(dmag)  Please fix this.  At the same time, we should add a config parameter to soften the errors. ",1
"Create transitional duplicate of Span
One challenge in switching from {{PTR(Span)}} to {{Span}} in {{Footprint}} is that Swig won't generate wrappers for {{std::vector<Span>}} (or any other container) if {{%shared_ptr(Span)}} is used anywhere else in the codebase.  So, to allow both the old {{Footprint}} class and the new {{SpanRegion}} to coexist (temporily), we need to have two {{Span}} classes, one wrapped with {{%shared_ptr}} and one wrapped without it.  Since we don't want to disrupt the old {{Footprint}} class yet, we should call the new Span something else, and make it the one that's wrapped without {{%shared_ptr}}.  This ticket can be considered complete once we have a unit test demostrating a usable Swig-wrapped {{std::vector<NewSpan>}} while all old {{Footprint}} tests continue to pass.",1
"Implement SpanRegion core functionality
Implement the core of the SpanRegion class, as prototyped in RFC-37.  This includes the following:  - The private implementation object and copy-on-write utilities (see Schema for an example of copy-on-write, but note that SpanRegion's implementation object can be private, while Schema's is not).  - All STL container methods and typedefs, and their Pythonic counterparts.  - All constructors and assignment operators, except for SpanRegionBuilder.  This includes the ability to detect and fix overlapping Spans.  - All simple accessors.  - {{isContiguous()}}  - The shift and clip methods.",6
"Implement SpanRegion+ellipse operations
Implement the following SpanRegion operations:  - Construct from an ellipse - note geom::ellipses::PixelRegion; this should do most of the work.  - Compute centroid - see old Footprint implementation  - Compute shape (quadrupole moments) - see old Footprint implementation  One complication here is that this will introduce a circular dependency between afw::geom and afw::geom::ellipses.  That's easy to address at the C++ level, but it's tricky in Python (which package imports the other?)  I'll be emailing dm-devel shortly to start a discussion on how to address this problem.",2
"Implement SpanRegion applyFunctor methods
Implement methods that apply arbitrary functors to pixels within a SpanRegion, as described on RFC-37.  The only tricky part of this implementation will be the ""traits"" classes that allow different target objects to interpreted differently.  I'd be happy to consult on this; I have a rough idea in my head, but it needs to be fleshed out.",3
"Add aperture corrections to meas_extensions_photometryKron
When transitioning {{meas_extensions_photometryKron}} to the new measurement framework, aperture correction was omitted pending the completion of DM-85. It needs to be re-enabled when that epic is complete.",1
"Make qserv server-side log messages more standard
Qserv server-side Python logging appears to mostly use a common format: ""{{%(asctime)s %(name)s %(levelname)s: %(message)s}}"".  It also mostly uses a common date format: ""{{%m/%d/%Y %I:%M:%S}}"".  But I see instances of: * ""{{%(asctime)s %(levelname)s %(message)s}}"" * ""{{%(asctime)s - %(name)s - %(levelname)s - %(message)s}}"" *  ""{{%(asctime)s \{%(pathname)s:%(lineno)d\} %(levelname)s %(message)s}}"" * and now, after DM-2176, ""{{%(asctime)s \[PID:%(process)d\] \[%(levelname)s\] (%(funcName)s() at %(filename)s:%(lineno)d) %(name)s: %(message)s}}""  Unless these are used in very different contexts, it will aid automated log processing for them to be more standardized.  In addition, the date format is unacceptable as it does not use RFC 3339 (ISO8601) format and does not include a timezone indicator (which means the default {{datefmt}} is insufficient).  This must be fixed.  See also DM-1203.",1
"Fork GREAT3 sim code and integrate with LSST stack
Get the GREAT3 simulation code running with LSST-provided third-party packages of Python, GalSim, etc, and figure out where we're going to put our modified scripts on GitHub:  - Do we just put things in a fork of the great3 repo, or do we have other repos layered on top of a fork of the great3 repo?  (I think probably the latter, but we should determine how many repos, and for what purposes.)  - Where in GitHub space do we put them (lsst?  lsst-dm? user spaces?)  This is one of several issues that together will replace DM-1132 (which was just a planning stand-in for these more detailed issues).",4
"Increase postage stamp size in simulation scripts
The GREAT3 simulations have a fixed postage stamp size (though this may differ between branches).  A first step at modifying the simulation scripts to meet our needs would be to try to change the postage stamp.  This is one of several issues that together will replace DM-1132 (which was just a planning stand-in for these more detailed issues).",2
"Create simulation script with different constant PSF per galaxy.
Modify the GREAT3 simulation scripts to create a branch in which each galaxy gets a different constant PSF, rather than one constant PSF per subfield or a spatially-varying PSF that spans multiple subfields.  This could be done by modifying the control/ground/constant branch or the variable-psf/ground/constant branch, or creating an entirely new branch, or anything else (since we don't actually need multiple branches in our simulations).  At this point, the source of the PSFs doesn't really matter - as long as we have a class that can provide a different one to every image.  This is one of several issues that together will replace DM-1132 (which was just a planning stand-in for these more detailed issues).",6
"Draw simulated PSFs from a library of on-disk files
Modify the simulation code to draw PSFs at random from a library of on-disk files (whose format and on-disk layout should be specified here).  The PSFs chosen should be deterministic via a random number generator seed specified via config.  This is one of several issues that together will replace DM-1132 (which was just a planning stand-in for these more detailed issues).",4
"Reading an Exposure from disk aborts if the Psf is of an unknown type
Attempting to read an Exposure (in this case via the butler) fails if the PSF class isn't available.  An exception would be reasonable, but an assertion failure is not.  Running the attached script on tiger-sumire with bq. setup python anaconda; setup -T v10_1_rc2 lsst_apps; setup -j distEst -t HSC; setup -j -r ~/LSST/obs/subaru  {code}  WARNING: Could not read PSF; setting to null: PersistableFactory with name 'PsfexPsf' not found, and import of module 'lsst.meas.extensions.psfex' failed (possibly because Python calls were not available from C++). {0}; loading object with id=4, name='PsfexPsf' {1}; loading object with id=28, name='CoaddPsf' {2} python: src/table/io/InputArchive.cc:109: boost::shared_ptr<lsst::afw::table::io::Persistable> lsst::afw::table::io::InputArchive::Impl::get(int, const lsst::afw::table::io::InputArchive&): Assertion `r.first->second' failed. Aborted {code}",1
"Cherry-pick ""fix makeRGB so it can replace saturated pixels and produce an image"" from HSC
HSC-1196 includes fixes and test cases for {{afw}}. After review on HSC, they should be checked/merged to LSST.",1
"Port HSC-side functionality to allow showCamera to display real data via the butler
One of the things that exists on the HSC side of things but not LSST is the ability to use showCamera to create full-focal-plane mosaics.  Please convert the code to run with the new cameraGeom    Not only is this generically useful, but it's part of the effort required to make the DM-side visualisation work for the Camera group  ",4
"iRODS test: Replicate data between servers
A fundamental feature of using iRODS would be to prevent file loss/corruption incidents by replicating data to different physical servers, possibly in geographically disparate locations. We verify that we can replicate data within out test zone/grid.",2
"iRODS test:  Virtual collection 
iROD manages data as a 'virtual collection', that is, one can have a single logical/virtual view of a collection of files (the appearance of a single file system/tree) while the data with the collection is stored on separate physical servers. We demonstrate this by creating a collection with data targeted/uploaded to different physical resources.",2
"iRODS test: Register data in place
In our first tests of iRODS, we have used ""iput"" to load data into iRODS cache spaces (the iRODS Vault).  For large collections already in a well known location on a server, one may want to leave the data in place but still manage it with iRODS. To do this one can use ""ireg"" to register the data with IRODS without the upload process.",2
"iRODS usage, devel survey
Read up on current IRODS usage and development track. ",3
"Fix and test CheckAggregation
{code:C++} class CheckAggregation { public:  CheckAggregation(bool& hasAgg_) : hasAgg(hasAgg_) {}  inline void operator()(query::ValueExpr::FactorOp const& fo) { if(!fo.factor.get()); {code}  - return is missing here. .get() is not needed, shared_ptr is like regular pointer which is convertible to bool, so whole thing should probably be: if (! fo.factor) return;  - We should have a unit test to show us we have problem here ",4
"Fix query ""SELECT * FROM Object o, Source s WHERE  o.objectId = s.objectId AND o.objectId = 390034570102582 AND    o.latestObsTime = s.taiMidPoint""
Next query fails:  {code:bash}  mysql --host=127.0.0.1 --port=4040 --user=qsmaster qservTest_case01_qserv -e   ""SELECT *   FROM Object o, Source s   WHERE  o.objectId = s.objectId   AND    o.objectId = 390034570102582   AND    o.latestObsTime = s.taiMidPoint""  {code}    It seems there's several problems here:    * objectId field is duplicated, zookeeper could be used to know all the fields involved by * in a query, but then it has to know each columns.  * subChunkId and chunkId are also duplicated, this isn't the case in the plain-mysql query.    This duplicated columns prevent the creation of the result table on the czar.  ",6
"v10_1_rc2 build test
Test v10_1_rc2 + tickets/DM-2303 on el6, el7, fedora 21, ubuntu 12.04, & ubuntu 14.04.  Results to be reported in http://ls.st/faq .",1
"Write additional test for duplicate fields check
Alongside:  {code:C++} BOOST_AUTO_TEST_CASE(getDuplicateAndPosition) {code}  Add test for: - no duplicate strings - triplicate - more than one string duplicated  and alongside:  {code:C++} BOOST_AUTO_TEST_CASE(SameNameDifferentTable) {code}  test more than one duplicated column, and a column duplicated more than twice.",3
"Fix cmsd-server logger configuration
cmsd-server logger configuration is incorrect:  see cmsd.log on Qserv worker:  {code:bash} Plugin loaded unreleased QservOssGeneric unknown from osslib libxrdoss.so log4cxx: Could not instantiate class [org.apache.log4j.XrootdAppender]. log4cxx: Class not found: org.apache.log4j.XrootdAppender log4cxx: Could not instantiate appender named ""XrdLog"". log4cxx: No appender could be found for logger (QservOss). log4cxx: Please initialize the log4cxx system properly. QservOss (Qserv Oss for server cmsd) ""worker"" {code}",4
"Fix interface between QservOss and new cmsd version
QservOSS gives an error when attempting to run queries on the worker from the czar. Error log snippet:  {code} QservOss (Qserv Oss for server cmsd) ""worker"" 150331 16:06:17 9904 Meter: Unable to calculate file system space; operation not supported 150331 16:06:17 9904 Meter: Write access and staging prohibited. ------ worker@lsst-dbdev3.ncsa.illinois.edu phase 2 server initialization completed. ------ cmsd worker@lsst-dbdev3.ncsa.illinois.edu:36050 initialization completed. {code} ",1
"Replace toString() function
See [~salnikov] comment:    Fabrice, anything is possible in C++, if you can define toString() for vectors it should also be possible to define some other construct to format vector into a stream :)  My objection to toString() is based on couple of of observations:      most of the time in our code converting complex objects to string is done to push them to streams or to logging system (logging is also usually based of streams)     methods like toString() are usually implemented using temporary streams.  So if you write code like cout << toString(vector) or LOGF_DEBUG(""vector: %1%"" % toString(vector)) it is very inefficient because it creates temporary stream and temporary string(s).  To make it more efficient you have to define operator<<() which is implemented without using toString(). Then you could implement toString() based on operator<<() but I'd argue that you should avoid it. In case you really need to convert to string there semi-standard tools which already do the same for types that have operator<< defined (like boost::lexical_cast), but again most of the time you only need operator<< as you don't want to mess with strings.  If you want to know how to implement operator<< for vector (or any container) here is the sketch of what I would do (there might be simpler ways):  {code:c++} namespace detail {     template <typename Cont> struct _ContInserterHelper {         const Cont& cont;     };     template <typename Cont> std::ostream& operator<<(std::ostream& out, const _ContInserterHelper<Cont>& cins) {         out << ""["";         const Cont& cont = cins.cont;   // this is container itself         // print container elements with separators         return out << ""]"";     } } template <typename Cont> detail::_ContInserterHelper<Cont> ContInserter(const Cont& cont) {     return detail::_ContInserterHelper<Cont>{cont}; } {code}  And after that you can do:  {code:c++} std::vector<int> v; std::cout << ContInserter(v); {code}  And this has no overhead or any temporary objects created. ",3
"investigate github oauth integration for jenkins 
We need a means of authenticating and authorizing users to interact with the CI system.  The current seem of using an htpasswd file with buildbot is a hassel both for end user and administratively.  Jenkin's has support for ldap and there is a plugin available for github oauth.  Administratively, and it terms of reliability, it may make more sense to be coupled with github than a a new DM or the exist LSST LDAP instance.",7
"uncaught exceptions in GaussianFlux
{{SdssShapeAlgorithm::computeFixedMomentsFlux}}, which is used to implement {{GaussianFlux}}, now throws an exception when the moments it is given are singular.  That shouldn't have affected the behavior of {{GaussianFlux}}, as it contains an earlier check that should have detected all such bad input shapes.  But that doesn't seem to be the case: we now see that exception being thrown and propagating up until it is caught and logged by the measurement framework, resulting in noisy logs.  We need to investigate what's going wrong with these objects, and fix them, which may be in {{SdssShape}} or in the {{SafeShapeExtractor}} {{GaussianFlux}} uses to sanitize its inputs.",1
"Participate in April design process
Most work here was with designing firefly tools API related details.",8
"Prepare firefly for GitHub
nan",8
"Finsh pushing firefly to GitHub
nan",3
"Begin actual conversion of parts of firefly to pure javascript
nan",10
"Develop next gen Firefly JavaScript API Tools
nan",10
"Develop external http api that can control Firefly viewer
nan",14
"Implement client side of mask layers in FITS image Viewer
nan",20
"Prepare v10_1 release candidate
Candidate is v10_1_rc2 based on EUPS tag b949",6
"lsstsw ./bin/deploy needs LSSTSW set to install products in the right place
I  cloned lsstsw into ~/Desktop/templsstsw and cd'd into it and typed ./bin/deploy and was shocked to find it installed everything into ~/lsstsw, leaving an unsable mess: some files were in templsstsw and some in ~/lsstsw.  The short-term workaround is to manually set LSSTSW before running ./bin/deploy, but this should not be necessary; bin/deploy should either set LSSTSW or not rely on it. I don't recall this problem with earlier versions of lsstsw; I think this is a regression.  For now I updated the instructions at https://confluence.lsstcorp.org/display/LDMDG/The+LSST+Software+Build+Tool but I look forward to being able to revert that change.",1
"Implement stitching multiple patches across tract boundaries in a coadd v2
* Find region that returns multiple tractPatchLists for testing.  * Request region via central point (RA, Dec) with width and height definable in arcseconds and pixels.  * May be extend web interface to other data sets, and/or good seeing SkyMaps. ",8
"Qsev Documentation
nan",10
"Turn on C++ 11 flag for Qserv
nan",4
"Handle all exceptions coming from worker
nan",7
"Build 2015_04 Qserv release
See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.",1
"Design API and RFC design
Use the HSC implementation of the base class as a point of reference for designing an integrated Approximate and Interpolate class.  The design take into account Chebyshev, spline, and Gaussian process mechanisms.  Want to take into consideration client code.  I.e. it shouldn't make current consumers more complicated (background and aperture correction to name two).  RFC the designed API.",8
"Edit background class
Make fixes to background class to use new approximate/interpolate class.",10
"Fix-up any code that uses approximate/interpolate
The background matcher is one area where the approximate/interpolate class will be used.  This story will find all places (including examples and unit tests) where the old approximate/interpolate mechanisms are used and update them to use the new interface.",4
"Delete old approximate/interpolate classes
Once all updates are done to code and unit tests pass with the new approximate/interpolate interface, the old ones should be completely removed.",2
"Justify level of staff at La Serena, to the level of justifying for office space
help with the specifications for the buildings in La Serena. The request si enough prove to justify office space for the DM administrators. and for  other staffing needed for DM.  This work will span two weeks and is due this Thursday  April 9.    This story is for the orienting work - -kickoff phone call.",1
"recieve and begin to process document from SET about scalability of CEPH
IN the context of ISL investigations into stogie systems the SET group has produced a document  that goes into the scaling of the meta data services.  The concern is that there is a central meta data service ins CEPH.   Began to process this analysis and to think about feasibility of testing program.",1
"management for week March 30.
Investigated invoicing fro storage condo -- appears to be annual fee, OK by Jeff. Investigated attaching  effort breakdown to invoke -- this seems hard as U of I invoicing occurs at quite a distance (procedural) distance from the NCSA business office.  Decided to look at improvements in recording effort in Jira so as to be able to generate report. -- Capture all actuals.  Business office transition  support is transitioning from Matt S. to new person.  Review AMCL sides,  kept tradition generating exponentially more comments, but reduced the exponent.  Process to bill out effort applied to project, but not in standing assignments in the staffing plan.  Internal strategy meeting about agenda items w.r.t VAO given Rap Plante is leaving NCSA. Prep for DM leadership meeting --  synergies at NCSA.  ",4
"security weekly meeting 
met with the ISO, looking for ways to more actively engage.  Idea was to focus on the SCADA enclave, and the need was to engage with  German Etc",1
"Draft SCADA security plan
nan",10
"Initial survey of Datacat for LSST 
Jacek, Brian Van Klaveren have sent along some initial overview/description of their work on Datacat;      https://confluence.slac.stanford.edu/display/~bvan/LSST+Datacat+Overview  We start examining this in the context of our studies of managing data collections at NCSA.",1
"shapelet unit tests attempts to access display on failure
When tests/profiles.py tests fail, they attempt to create live plots without checking for any variables that indicate that the display should be used.  These plots should be disabled, as they obscure the real error when the display is not available.",1
"prepare v10_1_rc3 release candidate
Need for rc3 identified ",10
"Fix g++ 4.9 return value implicit conversion incompato
g++ 4.9 enforces the ""explicit"" keyword on type conversion operators in return value context.  This mean bool checkers along the lines of  bool isValidFoo() { return _smartPtrFoo; }  require an explicit cast to compile under g++ 4.9 with -std=c++0x.  There were a handful of these in our code; found and fixed.",1
"run jenkins builds on multiple platforms
Demonstrate a Jenkins build matrix running lsstswBuild.sh on a number of platforms including; el6, el6, f21, u12.04, & u14.04.",28
"Mountain - Base fiber path design and installation method
Design path and installation method for Mountain - Base fiber cable.  Path will run from Cerro Pachon to Cerro Tololo to AURA gate.  Installation method will define where the fiber cable will be on poles or underground.",40
"Improve db.createTable
DM-2417 revealed that the current implementation of createTable in db module behaves differently that mysql: mysql will issue a warning if table exists, and db module will fail with an error. We should make the db behave similarly to how mysql behaves. ",2
"Doxygenize db
The db module needs to be doxygenized.",1
"Optimize support for many identical database schemas - design
It is likely we will have 100s or 1000s of identical databases (identical in terms of schema). It'd be good to not repeat the schema information in metaserv. This ticket include coming up with a plan how to implement it.",1
"Optimize support for many identical database schemas - impl
It is likely we will have 100s or 1000s of identical databases (identical in terms of schema). It'd be good to not repeat the schema information in metaserv. This ticket include implementing a clean solution, proposed through DM-2504",2
"Document structure of our custom ddl ascii schema
Need to better document what is supported / accepted by schemaToMeta.py. We are currently relying on cat/sql/baselineSchema.sql as the guide.",2
"Information exchange between processes - research
We need to identify a reliable and fast way to exchange information between processes (for example, cmsd and xrootd).   This story involves understanding key requirements (structures, scale), and researching what mechanism would be best).   Deliverable: short narrative describing key requirements, and proposed mechanism, including a sketch of the design.",4
"Information exchange between processes - implementation
Implement system for information exchange between cmsd and xrootd, per instructions in DM-2507",8
"Research feasibility of using SQLite as backend to the db module
This story involves plugging in SQLite and dealing with issues that arise as a result of using SQLite in places that depend on the db module.",10
"The distance field of match lists should be set
The meas_astrom AstrometryTask returns a match list that has distance = 0 for all elements. Neither the matcher nor the WCS fitter are setting this field, and both ought to.",2
"FY17 Integrate Web Services with NCSA Authentication System
We need to integrate Data Access Web Services with Authentication mechanisms used by NCSA.",40
"W16 Improvements to db
Improvements to Db wrapper.",31
"Migrate to new WBS for 02C.06
Migrate to the new WBS structure for 02C.06. Work include: * revisiting wbs assignment for all epics * updating [S15 planning 4 DB team|https://confluence.lsstcorp.org/display/DM/S15+planning+4+DB+team] * updating ldm-240 spreadsheet * updating associated budget accounts * tweaking [build-ldm240.py|https://github.com/jbecla/experimental/blob/master/build-LDM-240.py]",1
"Catch ""address in use""
I noticed when running integration tests, it failed with the error pasted below. It'd be good to catch it and print something useful. I am not entire sure what port number is in use, and what to kill...   {code}   File ""/usr/local/home/becla/stack_201502/repo/qserv/bin/qservWmgr.py"", line 89, in <module>     sys.exit(main())   File ""/usr/local/home/becla/stack_201502/repo/qserv/bin/qservWmgr.py"", line 85, in main     app.run(host)   File ""/usr/local/home/becla/stack_201502/Linux64/anaconda/2.1.0/lib/python2.7/site-packages/flask/app.py"", line 772, in run     run_simple(host, port, self, **options)   File ""/usr/local/home/becla/stack_201502/Linux64/anaconda/2.1.0/lib/python2.7/site-packages/werkzeug/serving.py"", line 710, in ru n_simple     inner()   File ""/usr/local/home/becla/stack_201502/Linux64/anaconda/2.1.0/lib/python2.7/site-packages/werkzeug/serving.py"", line 692, in in ner     passthrough_errors, ssl_context).serve_forever()   File ""/usr/local/home/becla/stack_201502/Linux64/anaconda/2.1.0/lib/python2.7/site-packages/werkzeug/serving.py"", line 486, in ma ke_server     passthrough_errors, ssl_context)   File ""/usr/local/home/becla/stack_201502/Linux64/anaconda/2.1.0/lib/python2.7/site-packages/werkzeug/serving.py"", line 410, in __ init__     HTTPServer.__init__(self, (host, int(port)), handler)   File ""/usr/local/home/becla/stack_201502/Linux64/anaconda/2.1.0/lib/python2.7/SocketServer.py"", line 419, in __init__     self.server_bind()   File ""/usr/local/home/becla/stack_201502/Linux64/anaconda/2.1.0/lib/python2.7/BaseHTTPServer.py"", line 108, in server_bind     SocketServer.TCPServer.server_bind(self)   File ""/usr/local/home/becla/stack_201502/Linux64/anaconda/2.1.0/lib/python2.7/SocketServer.py"", line 430, in server_bind     self.socket.bind(self.server_address)   File ""/usr/local/home/becla/stack_201502/Linux64/anaconda/2.1.0/lib/python2.7/socket.py"", line 224, in meth     return getattr(self._sock,name)(*args) socket.error: [Errno 98] Address already in use {code}",1
"Add a CFHT-based post-build integration test to the sandbox build
From [~boutigny]    I have installed some simple stack validation tools working on CFHT data in {{/lsst8/boutigny/valid_cfht}}    Here is the content of the README file :    ------------------------------------------------------------------------------------------------------------------------  This directory contains a set of utilities to validate a stack release with CFHT data    At the moment, only validation plots for the astrometry are produced    Directories :  -------------  rawDownload     : contain raw CFHT images (flat, dark, bias, fringe,... corrected)  reference_plots : contain reference plots corresponding to the best results obtain so far.    Files :  -------  setup.cfht       : stack environment setup  valid_cfht.sh    : run processCcd taks on the cfht images     valid_cfht.sh init : create the input/ouput directories, ingest raw images and run processCcd     valid_cfht.sh      : without the ""init"" argument, runs processCcd assuming that the directory structure exists and that the raw images have been ingested.  valid_cfht.py    : run some analysis on the output data produced by valid_cfht.sh  processConfig.py : configuration parameters for processCcd  run.list         : list of vistits / ccd to be processed by processCcd    Requirements :  --------------  obs_cfht : tickets/DM-1593  astrometry_net_data : SDSS_DR9 reference catalog corresponding for CFHT Deep Field #3  ------------------------------------------------------------------------------------------------------------------------    Basically it produces a set of plots stored in a png image that can be compared to a reference plot corresponding to the best results obtained so far with stack_v10_0    I hope that this is useful. Just be careful that I wrote these scripts with my own ""fat hand full of fingers"" and that it is just basic code from a non expert. If it is useful, I can certainly add more plots to validate the psf determination, photometry, etc.    Comments, suggestions and criticisms are very welcome.",1
"Check for Qserv processes at configuration tool startup
Configuration tool has to check for Qserv processes before removing configuration directory (which may contains init.d scripts for these running processes)",4
"Proof of concept Python APIs to access Firefly components
The pipeline needs to visualize the images using Firefly. We want to provide a few Python APIs for proof of concept that we could do this in Python and IPython notebook. ",6
"Update repo.yaml for first set of Sims Stash repo moves
The repos.yaml file needs to be updated with correct repository locations once SIM-1074 is completed.",1
"Implement distributed database deletion
Implement database deletion based on the process defined in DM-1396. Need to deal with situations like worker is offline - might need some infrastructure e.g., running something in background to act when affected workers come back online.  Deliverable: a demonstration of system that deletes a distributed database: user issues ""drop database x"" and all copies of that database on all workers, all replicas of all chunks are deleted. It should be possible to ""create database x"" at any time later.",6
"Ensure we can delete/create table with the same name
Test / ensure that we can create a table with the same name as the table we just deleted.",4
"Margaret's mgmt. activities in March
Catch-all story for LOE activities in March 2015.  20 working days - 7 days vacation = 13 actual days  13 days * 2 SP/day * .5 FTE = 13 SP.",13
"Tom Durbin on board as a consultant for the Base site data center.
Prep/attend/follow through for meeting with Tom Durbin, the facility manager of the National Petascale Computing Facility, to discuss participating as  consultant to the project as it finds a design contractor, and as the design matures.    ",1
"Mgt activity summary for week of April 6
- Made inquires about the status of materials contracting - ball in AURA's court. - Prepare for visit to Lyon.  Consult with atoll, researched collaborative structures,  articulated and vetted hardware process, made some slides for the visit. - Spent time thinking about VO protocols and such in prep for the DM F2F discussion. - Edited Job descriptions for the ADS department ,who will recruit for our systems engineers to include LSST, and LSST concerns. - Management / leading by walking around  N.B. Margaret in CAM training (or associated travel)  Tu-F. N.B Don was off 1 1/2 days  ",3
"Resolve outgoing port issues on Blue Waters/Cray systems 
pro data system scaling tests on cray system were limited by the number of outgoing ports on a cray node. The limitation had been  ~20 ports, participated in Tests of new system software,limit relaxed to at least ~2000 in tests. Likely greater.",3
"discussed the request from the SUI group for authentication guidance 
responded to ticket from Jacek, on behalf, I think  of the SUI group asking for guidance on authentication at NCSA.   So far, consulted with Alex Withers,  contemplating the extent of policies so far (not much) an authentication mechanism worth investigtaing and likely policies.  Drew figure for discussion, wrote up in hip chat.",1
"Remove version attribute from Schema
Remove the Schema attribute and its getters and setters.  This change won't be something we can merge to master on its own, as it doesn't provide backwards-compatible FITS reading that will added in future tasks.",1
"Rewrite afw::table FITS reading to be more flexible
In order to support backwards-compatible FITS table reading, we need to break the current assumption that everything we need to know about how to read a Record from a FITS file is contained in the Record's Schema.  This issue involves that refactoring, without actually adding the backwards compatibility support.",4
"Backwards compatibility for reading compound fields from FITS
Read old-style afw::table compound fields in as scalar fields, using the new FunctorKey conventions.",2
"Backwards compatibility for reading slots and measurements from FITS
Rename fields to match the new slot and measurement naming conventions.",2
"Contextual error handling
There are cases when an empty result might have different errors than the top error, and it would be good to unwrap the context in which the error occured. Example: GET /meta/v0/db/L3/joe_myDb/tables/Object, the result might be empty because the database does not exist, or the v0 is not a supported version, etc.",4
"RESTful python client
Develop basic abstractions for restful apis in a python client",3
"Research Ceph file system
Research Ceph as possible networked filesystem for LSST usage to replace NFS. Estimate spending 10-20 hours of work with result being a wiki page of suggestions, limitations, etc.  (Implementation will be a different task, presuming we want to implement.)",6
"Python APIs for Firefly 
We need Python APIs to interface with Firefly visualization components.  This is the first set of many functions.  ",8
"ctrl_events build issue
Had a problem where ctrl_events was having build issues.",1
"LaTeX support in Doxygen broken
LaTeX markup in Doxygen documentation ought to be rendered properly for display in HTML. It isn't: it's just dumped to the page as raw text. See, for example, [the documentation for {{AffineTransform}}|https://lsst-web.ncsa.illinois.edu/doxygen/xlink_master_2015_04_15_07.01.28/classlsst_1_1afw_1_1geom_1_1_affine_transform.html#details].",1
"Host.cc doesn't find gethostname and HOST_NAME_MAX under el7
el7 gives an error that it can't find HOST_NAME_MAX.",1
"Fix again interface between QservOss and new cmsd version
QservOSS gives an error when attempting to run queries on the worker from the czar. Error log snippet:  {code} QservOss (Qserv Oss for server cmsd) ""worker"" 150331 16:06:17 9904 Meter: Unable to calculate file system space; operation not supported 150331 16:06:17 9904 Meter: Write access and staging prohibited. ------ worker@lsst-dbdev3.ncsa.illinois.edu phase 2 server initialization completed. ------ cmsd worker@lsst-dbdev3.ncsa.illinois.edu:36050 initialization completed. {code} ",8
"The string repr of Coord should show the coordsys and angles in degrees
The default string representation of Coord (e.g. std::cout << coord in C++ and str(coord) in Python) is to show class name and a pair of angles in radians.  It would be much more useful if the default display showed the angles in degrees, as that is what people are used to. Also, it would be very helpful if the display included the name of the coordinate system. This is especially needed for the base class, as it is quite common to get shared_ptr to Coord and have no idea what coordinate system it is.  At present there is a lot of code that unpacks the angles and explicitly displays them as degrees to get around this problem. But it seems silly to have to do that.",2
"ANetAstrometryTask's debug doesn't fully work
{{ANetAstrometryTask}}'s debug code calls (deprecated) method {{Task.display}}, which raises an AttributeError on this coce:  {code}  try:      sources[0][0]  except IndexError:              # empty list      pass  except (TypeError, NotImplementedError): # not a list of sets of sources  {code}  ",1
"xrootd can't be started via ssh
{code:bash} qserv@clrinfopc04:~/src/qserv$ ssh localhost -vvv ""~qserv/qserv-run/2015_02/etc/init.d/xrootd start"" ... debug3: Ignored env _ debug1: Sending command: ~qserv/qserv-run/2015_02/etc/init.d/xrootd start debug2: channel 0: request exec confirm 1 debug2: callback done debug2: channel 0: open confirm rwindow 0 rmax 32768 debug2: channel 0: rcvd adjust 2097152 debug2: channel_input_status_confirm: type 99 id 0 debug2: exec request accepted on channel 0 Starting xrootd.. debug1: client_input_channel_req: channel 0 rtype exit-status reply 0 debug1: client_input_channel_req: channel 0 rtype eow@openssh.com reply 0 debug2: channel 0: rcvd eow debug2: channel 0: close_read debug2: channel 0: input open -> closed {code}  Here ssh command freeze, it is possible to lauch xrootd with this (example) script: {code:bash} set -e set -x  . /qserv/run/etc/sysconfig/qserv export QSW_XRDQUERYPATH=""/q"" export QSW_DBSOCK=""${MYSQLD_SOCK}"" export QSW_MYSQLDUMP=`which mysqldump` QSW_SCRATCHPATH=""${QSERV_RUN_DIR}/tmp"" QSW_SCRATCHDB=""qservScratch"" export QSW_RESULTPATH=""${XROOTD_RUN_DIR}/result"" export LSST_LOG_CONFIG=""${QSERV_RUN_DIR}/etc/log4xrootd.properties""  eval '/qserv/stack/Linux64/xrootd/xssi-1.0.0/bin/xrootd -c /qserv/run/etc/lsp.cf -l /qserv/run/var/log/xrootd.log -n worker -I v4 &'  echo ""SCRIPT STARTED"" {code} and the same problem occurs. So the problem seems to be with xrootd, and not the startup scripts.   ",5
"Remove most compound fields from afw::table
Remove all Point, Moment, Coord, and Covariance compound fields.  Array fields should be retained for now; it's not clear if we want to remove it or not, or how to handle variable-length arrays if we do.",2
"Create and advertise Firefly mailing list
Create an IPAC mailing list for all users of Firefly.  Advertise it to the interested communities (including the LSST Camera group) and through the Github site.  The mailing list firefly@ipac.caltech.edu has been created and all the interested partied have been subscribed to the list.",1
"Make dbserv async
nan",5
"Vectorize methods for locating objects on detectors
vectorize _transformSingleSys and _findDetectors in afw.cameraGeom so that the sims_coordUtils method findChipName (which finds the chips that an object lands on) runs faster.",2
"Migrate qserv code to reworked db/dbPool
Migrate code to the new implementation of SQLAlchemy-based Db module, including removal of DbPool.",11
"Chilean Network LOE 
nan",40
"Addressing File corruption in iRODS 3.3.1
In this issue we examine how file corruption would be detected and repaired with iRODS tools and rules/microservices.",16
"read and understood proposal to consider CAS/crowd system 
the FERMI telescope has an authentication system based on CAS/Crowd. The  benefit of the system is that it can be use as an authentication system for both web and command line.      Download materials, and acquire the  understanding from a review of documentation . Discuss with the ISO,  propose discussion for vTony's visit to NCSA (may 21).",1
"management activities for week of April 13
Read proposed  ""Hardware"" contract amendment, sent marked up comments to Julie Robinson, U of I contract negotiator.  Major points are that Hardware is not descriptive of all purchases  needed to fulfill SOW.  The procurement approval process needs spelling out. Detailed guidance in comments inserted into contract.  Along with M. Gelman met with the NCSA business people to fully understand the U  of I invoicing process, and the information in the existing business processes. prior to inventing processes for the  supplementing the U of I invoice with the more detailed annotations (hours by WBS) agreed to in the LSST contract.  Obtained help from the NSCS IT group. Documented in tow page note.   Met concerning seemingly large amount of effort to respond to hip chat take about slowness in the NCSA development system.     Miscellaneous and meetings.  ",5
"Research BeeGFS file system
Research BeeGFS as possible networked filesystem for LSST usage to replace parts of NFS. Estimate spending 10 hours of work with result being a wiki page of suggestions, limitations, etc. (Any implementation will be a different task, presuming we want to implement.)  http://www.beegfs.com/content/  BeeGFS (formerly FhGFS) is a parallel cluster file system, developed with a strong focus on performance and designed for very easy installation and management. If I/O intensive workloads are your problem, BeeGFS is the solution.  Likely not good replacement for formal/managed data, but perhaps great option for shared scratch file systems.",5
"Calling AliasMap::get("""") can return incorrect results
It looks like empty string arguments can cause AliasMap to produce some incorrect results, probably due to the partial-match logic being overzealous.",1
"Implement user-friendly template customization
Qserv configuration tool has to be improved to allow developers/sysadmin to easily use their custom configuration files (with custom log level, ...) for each Qserv services.    An optional custom/ config file directory will be added, and configuration files templates which will be here will override the ones in the install directory.    This should be thinked alongside configuration management inside Docker container.",5
"log4cxx build failure on OS X
[~frossie] writes:  {quote} I have a log4cxx failure on a Macp while building lsst_distrib. Attaching file in case someone has any bright ideas for me in the morning {quote}",1
"Research MaxScale as a mysql-proxy replacement
We have been told by Monty that MaxScale is the replacement of the mysql-proxy. Based on DM-2057 the sentiment is that it won't work for our needs. We should very briefly document what our needs are, how we use the proxy now, and if we think MaxScale is not good-enough, say it why, and discuss with Monty and his team.",5
"Purchase of network equipment for use in Chile
jkantor, rlambert",40
"Base LAN Network Design 
Design of the network at the Base to to provide services for the ""tenants"" Telescope, Camera and DM",20
"Base Network LOE
nan",40
"Design the Network from NCSA to Ampath in Florida
nan",20
"Comparison of ALMA and Reuna/AURA costs on National link
nan",4
"Comparison of ALMA summit to base link with LSST
nan",2
"Remove obsolete hinting code in proxy
Remove now dead code related to sending hints from proxy to czar",1
"Client API for new worker management service
We have new worker management service which has HTTP interface, now we need to provide simple way to access it from Python basically wrapping all HTTP details into simple Python API. ",8
"Change repos.yaml for next set of Simulations Stash repos
The next set of Simulations Stash repository migrations is laid out in SIM-1121.",1
"Symlink data directory at configuration
We decided to introduce symlinks in order to protect data. This is in particular useful when we need to reinstall qserv, but we have valuable, large data set that we want to preserve. This story introduces symlinks to data: when Qserv is reinstalled, only the symlink is destroyed, and the data stay untouched.",5
"Fiber installation on AURA property from Gate to Pachon
AURA and Reuna oversee Telefonica in installing the fiber from the AURA Gate to Cerro Tololo to Cerro Pachon.",20
"afw.Image.ExposureF('file.fits.fz[i]') returns the image in 'file.fits.fz[1]' 
It seems that afwImage.ExposureF ignores the extension number when this is passed on as part of the filename and uses the image in extension number 1. This is not the case with afwImage.MaskedImageF which correctly uses the input extension number passed in the same way.  The problem has been checked on OSX Yosemite 10.10.3 with  the is illustrated in  the following code https://gist.github.com/anonymous/d10c4a79d94c1393a493  which also requires the following image in the working directory: http://www.astro.washington.edu/users/krughoff/data/c4d_130830_040651_ooi_g_d1.fits.fz ",3
"FY18 Integrate DRP with Data Provenance
Integrate the Data Provenance system with the DRP. This includes capturing hardware and software configuration, as well as dependencies between data sets.  Deliverable: System capable of capturing provenance for DRP.",54
"'eups distrib install flask -t qserv' fails on Ubuntu 14.04
Qserv now depends on Flask, so this blocks all Qserv install which rely on eups.  Comman below works with system-python but not with anaconda:  {code} qserv@clrinfoport09:~/stack/EupsBuildDir/Linux64/flask-0.10.1/flask-0.10.1⟫ python setup.py install --home /home/qserv/stack/Linux64/flask/0.10.1                                                                   running install Traceback (most recent call last):   File ""setup.py"", line 110, in <module>     test_suite='flask.testsuite.suite'   File ""/home/qserv/stack/Linux64/anaconda/master-g68783b1848/lib/python2.7/distutils/core.py"", line 151, in setup     dist.run_commands()   File ""/home/qserv/stack/Linux64/anaconda/master-g68783b1848/lib/python2.7/distutils/dist.py"", line 953, in run_commands     self.run_command(cmd)   File ""/home/qserv/stack/Linux64/anaconda/master-g68783b1848/lib/python2.7/distutils/dist.py"", line 972, in run_command     cmd_obj.run()   File ""/usr/lib/python2.7/dist-packages/setuptools/command/install.py"", line 73, in run     self.do_egg_install()   File ""/usr/lib/python2.7/dist-packages/setuptools/command/install.py"", line 82, in do_egg_install     cmd.ensure_finalized()  # finalize before bdist_egg munges install cmd   File ""/home/qserv/stack/Linux64/anaconda/master-g68783b1848/lib/python2.7/distutils/cmd.py"", line 109, in ensure_finalized     self.finalize_options()   File ""/usr/lib/python2.7/dist-packages/setuptools/command/easy_install.py"", line 274, in finalize_options     ('install_dir','install_dir')   File ""/home/qserv/stack/Linux64/anaconda/master-g68783b1848/lib/python2.7/distutils/cmd.py"", line 298, in set_undefined_options     src_cmd_obj.ensure_finalized()   File ""/home/qserv/stack/Linux64/anaconda/master-g68783b1848/lib/python2.7/distutils/cmd.py"", line 109, in ensure_finalized     self.finalize_options()   File ""/usr/lib/python2.7/dist-packages/setuptools/command/install_lib.py"", line 13, in finalize_options     self.set_undefined_options('install',('install_layout','install_layout'))   File ""/home/qserv/stack/Linux64/anaconda/master-g68783b1848/lib/python2.7/distutils/cmd.py"", line 302, in set_undefined_options     getattr(src_cmd_obj, src_option))   File ""/home/qserv/stack/Linux64/anaconda/master-g68783b1848/lib/python2.7/distutils/cmd.py"", line 105, in __getattr__     raise AttributeError, attr AttributeError: install_layout {code}",5
"FY18 Integrate Calibration Pipe with Data Provenance
nan",26
"FY19 Integrate L3 with Data Provenance
Integrate L3 (images and databases) with Data Provenance.",80
"Package flask dependencies
We packaged flask (see dm-1797) and we are using it via eups, but we have not packaged flask dependencies, and we are still relying on anaconda to get them. This story involve packaging the dependencies.",3
"HSC backport: recent Footprint fixes
This is a backport issue to capture subsequent HSC-side work on features already backported to afw.  It includes (so far) the following HSC issues:   - [HSC-1135|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1135]   - [HSC-1129|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1129]   - [HSC-1215|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1215]",2
"W16 Qserv Refactoring
Refactoring of Qserv as found necessary in W16.",60
"FY17 Qserv Refactoring
nan",100
"FY18 Qserv Refactoring
nan",100
"API key case study
nan",1
"Margaret's mgmt. activities in April
- Weekly DMLT phone meetings - Weekly security meetings - Weekly local group meetings - T/CAM training meeting - 1 week w/ travel - Attended leadership meetings about NCSA/UIUC receiving and inventory policies and procedures and grant management for project managers  - Prepared slides for AMCL - Updated LDM-240 milestones for FY16+ - Created invoice breakdown (incorrectly!) - Prepared March Technical Progress Report - Prepared Travel Expense Report for T/CAM meeting - Attempted to update S15 Staff Plan in PMCS - Reviewed risk registry with Don - Met with Julie Robinson & AURA to work on/discuss procurement contract amendment   - Met with Kaylyn and Alan to discuss invoicing, billing process and timeline, WBS activity code breakdown in MIS - Met with Jay and Kaylyn about staff planning and budget - Met with Nathan to figure out how to track a lot of this EV stuff by downloading to a local database and integrating MIS information - Developed swimlane diagram to understand roles and responsibilities of reporting - Started working on Gantt chart to wrap my head around tracking resource loading and activity progress",31
"weekly liaison with ISO
Discussed ""should we piggyback signing of the LSST AUP with a capability offered by OSG""? with Alex.  Additional understanding of Authorization/Authentication.",1
"Management activities for week of april 21
Met with Julie Robinson, the Illinois contract negotiator, w.r.t. aura ""hardware"" contract amendment.  re-drafted long paragraph i AURA section, breaking it down into separate items for each party, and addressed what I see a grave flaws so that a discussion could be held.  Did work w Margaret do arrange for business proscesst discussion relating to monthly reporting to LSST appended to invoices, basic   Interviewed Martin Paegert (one day visit).  Further work on other other matters relating to open requisitions of people. Other work on personal matters  Responded to comments about the NCSA WBS and overall project WBS not being aligned.  on LLDM-240 -- provided example of working one case -- scheduled for next wee'k LT.  Misc.",5
"LOE - Week ending 5/1/15
- patch maintenance (kernel, zfs, Intel NIC, esxi NICs) on Thursday",12
"Drop PK on overlap tables in data loader
nan",2
"Reimplement Data Loader Using Worker Mgmt Service
Current loader depends on ssh, need to switch to the new service, http based.",8
"Add version stamping in czar and ssi service
DM-2547 will introduce compile-time version generation of a header file that has macros defining version strings. Ideally, each running process using qserv code (e.g., czar, cmsd, xrootd, and mysql-proxy), and perhaps one-shot binaries (loader?) would print version information when logging to improve debuggability.  DM-2547 focused on the osslib plugin (libxrdoss) for the cmsd. The next important processes are the czar and xrootd (libxrdsvc). This story covers inclusion of version identifiers (w/ commit hash) in the czar and xrootd logs. Hopefully this will end any confusion about versions when reading log files sent from colleagues.",3
"Modify czar to support table deletion
Czar needs to handle table deletion. In practice that means mysql proxy should let DROP TABLE queries through, and czar should modify appropriate table-related metadata structures in CSS. This is part of work proposed in  DM-1896.  ",6
"Design Basic Watcher
Design watcher, including its interactions with other components (mysql, css, etc). In the near term, the watcher will handle deleting tables and databases.",2
"Implement DROP table in watcher
Implement DROP table using the watcher designed in DM-2623.",1
"Create service for managing watcher
We need to be able to start/stop the watcher implemented through DM-2624. This story involves extending our scripts for starting various qserv services to manage watcher.",1
"Add support for configuring multi-node integration tests
The multi-node integration test software produced through DM-2175 has hardcoded node names. This story will allow user to configure it. Current plan is to pre-set integration test for several different configurations, e.g., single-node, 2-node, 8-node (and maybe eg 24-node), and user would supply node names through a configuration file.",5
"Integration test succeeds when individual tests fail
Integration test behaves strangely, it always succeeds even though there may be tests that fail. Here is what I ge when I run individual case: {noformat} [salnikov@lsst-dbdev4 dm-2617]$ qserv-check-integration.py -i 01 -l ............... 2015-04-28 11:26:12,137 - lsst.qserv.tests.benchmark - ERROR - MySQL/Qserv differs for 4 queries: 2015-04-28 11:26:12,138 - lsst.qserv.tests.benchmark - ERROR - Broken queries list in /usr/local/home/salnikov/qserv-run/2015_04/tmp/qservTest_case01/outputs/qserv: ['0001_fetchObjectById.txt', '0003_selectMetadataForOneGalaxy_withUSING.txt', '0003_selectMetadataForOneGalaxy_classicJOIN.txt', '0003_selectMetadataForOneGalaxy.txt'] 2015-04-28 11:26:12,138 - root - CRITICAL - Test case #01 failed {noformat}  But if I run integration test it says everything is OK: {noformat} [salnikov@lsst-dbdev4 dm-2617]$ qserv-test-integration.py ................... ok  ---------------------------------------------------------------------- Ran 5 tests in 160.058s  OK {noformat}  There are actually messages about failed test in the output but you have to look very closely not to miss them. ",1
"Fix build for gcc 4.7.2 and gcc 4.8.2
#include <condition_variable> is missing in threadSafe.h",1
"Document configuration tool main use cases
- Document main use case for qserv-configure.py: install Qserv master/worker node with externalized data directory  - Hide complex configuration options?  {code} Configuration steps:   General configuration steps    -d, --directory-tree  Create directory tree in QSERV_RUN_DIR, eventually                         create symbolic link from QSERV_RUN_DIR/var/lib to                         QSERV_DATA_DIR.   -e, --etc             Create Qserv configuration files in QSERV_RUN_DIR                         using values issued from meta-config file                         QSERV_RUN_DIR/qserv-meta.conf   -c, --client          Create client configuration file (used by integration                         tests for example)  Components configuration:   Configuration of external components    -X, --xrootd          Create xrootd query and result directories   -C, --css-watcher     Configure CSS-watcher (i.e. MySQL credentials)  Database components configuration:   Configuration of external components impacting data,   launched if and only if QSERV_DATA_DIR is empty    -M, --mysql           Remove MySQL previous data, install db and set                         password   -Q, --qserv-czar      Initialize Qserv master database   -W, --qserv-worker    Initialize Qserv worker database   -S, --scisql          Install and configure SciSQL {code}  ",3
"Use WebSocket for communication between client and web server, proof of concept
Research and proof of concept code to use Web Socket for two-way communication between client and web server. ",10
"implementation of Web Socket for two-way communication between client and Web server 
Implementation of web socket to be used as the two-way communication method between client and web server. ",10
"refactor the image stretch code for better, simplified  organization 
nan",8
"add new image stretch algorithm to Firefly visualization 
There is a need to include two new stretch algorithms, which are asinh and power law gamma.  The algorithm is as follow: * asinh ## input        zp: zero point of data        mp: maximum point of data        dr:  dynamic range scaling factor of data.  It ranges from 1-100,000        bp: black point for image display        wp: white point for image display ## calculate rescaled data value        rd = dr *(xPix - zp)/mp ## calculate normalized stretch data value         nsd = asinh(rd)/asinh(mp-zp) ## calculate display pixel value        dPix = 255 * (nsd-bp)/wp       Note: The bp, wp values specify how far outside of the scale data one wants the image to display.  By default, setting bp=0 and wp=dr.    * power law gamma ## input \br        zp: zero point of data        mp: maximum point of data        gamma: gamma value for exponent ## calculate rescaled data value        rd = xPix - zp ## calculate normalized stretch data value         nsd =  rd^(1/gamma) / (mp0zp)^(1/gamma) ##  calculate display pixel data value         dPix = 255 * nsd       ",8
"Provide a function to return the path to a package, given its name
As per RFC-44 we want a simple function in utils that returns the path to a package given a package name. This has the same API as eups.getProductDir, but hides our dependence on eups, as per the RFC.",2
"Update code to use the function provided in DM-2635
As per RFC-44: update existing code that finds packages using eups.getProductDir or by using environment variables to use the function added in DM-2635",3
"Run large scale tests
Coordinate running large scale tests.",6
"missing dependencies in scons builds
ndarray and afw have some headers generated via m4, and while those are built when the package is installed, if someone just tries to build other targets, they aren't - leading to build failures.  We also need to add a dependency from the ""lib"" target to the ""python"" target, because we can't link the Python libraries against the C++ library until it's built.  That needs to be changed in sconsUtils. ",1
"Migrate Qserv to ssi v2
ssi v2 including comple objectification of the interface. Need to migrate qserv to the new interface.",6
"Switch to using shpgeom and remove duplicate code
Qserv is currently relying on a copy of the spherical geometry code (in core/modules/sg) instead of relying on the sphgeom module. This needs to be cleaned once we sort out the build issues with sphgeom (DM-2262).",1
"TOWG attendance 
remote operations discussion. ",1
"MGT for balance of April 
recruiting for open positions Work on accounting infrastrucutre. ""hardware"" contact - -   outline to Jeff over the phone what is coming  work through  inventory infrastructure for materials for La Sereba  review budget and effort projections.  hear file system invesitigations meeting. ",3
"Histogram options
We have studied histogram options supported by Exoplanet Archive (http://exoplanetarchive.ipac.caltech.edu/cgi-bin/TblView/nph-tblView?app=ExoTbls&config=planets)    We'd like to support similar options for our Histogram plot. The options are:    - Column selection  - Axes options:         Linear, Log, Linear reversed, Log reversed        Range selection (Auto, Manual)  - Binning options        Min, Max, Number of Bins        (These can be assigned automatically: nBins = sqrt(nPoints))  ",12
"Prepare external http api for Firefly viewer for beta use
nan",18
"Fix thread leak in Qserv
Qserv is currently leaking a thread per query. Executing a simple query list select count(*) from Object in a loop results in everything hanging after qserv is up to 67 threads.",6
"Prepare next gen Firefly JavaScript API Tools for beta
nan",8
"Look into current transient alert event systems.
- Research how prior and current alert systems work, Skyalert in particular. - Install and try current working transient event alert system - Catalina. What can we learn from this? - Look at new technologies that might help - ZeroMQ messaging, message formats, forms of distribution and archiving, etc.",10
"Configuration mechanism for GalSim galaxy generation
This is an additional script for great3sims to allow simple configuration of the great3sims.run().  Most of the parameters which need to be set are in great3sims.constants.py, though some additional command line parameters may be needed for the run method.",2
"Build Psf Libraries from PhoSim Images
Takes output provided by Debbie from PhoSim runs and use them to create libraries of Psfs.  Warp to remove camera distortion if necessary.  This issue does not include figuring out what different categories of Psfs are required, but all of the process issues should be covered in this issue.",4
"Categorize Psfs and Distributions Required from PhoSim
Request a full focal plane of Psf images. Write code to allow them to be stored in a way which allows us to sample randomly from a full focal plane.  There will be multiple such focal planes, so we also need to be able to pass the information to the measurement algorithm which will allow us to categorize measurements by visit.  This will be done in the Psf Library building code, and will then be passes to the measurement algorithm through the great3sims code which constructs the data for the measurement algorithm.",2
"Produce HSC Psf sample for use in algorithm testing
Produce a set of well distributed Psfs from HSC data.  As long as the Wcs info is also provided, the code to warp them should have been done in a separate issue.",4
"Alternative parameterized Psfs from PhoSim
Michael Schneider has suggested that he can do a better job of creating realistic Psfs from Psf models which he is working on, and which he intends to integrate into GalSim.  These are intriguing, but depend on work which hasn't been done yet.  When these models are fully available in GalSim and supported through the yaml configuration interface, we should work with them.  But this is currently an ""as time permits"" issue.",6
"Prototype test harness for testing measurement algorithms
This is a relatively simple task, which will take the Galaxy images from the great3sims modifications and run measurement algorithms on the individual postage stamps.  The result will be a catalog of the measurement outputs, cross-references against the galaxy and psf parmeters used for a given postage stamp.  To do this, we need to combine information from the galaxy catalog and psf catalog into an input catalog for the algorithm.  A source needs to be created for each galaxy which will contain at least the galaxy centroid and footprint relative to the postage stamp.  The postage stamp with Psf appended and the above source much be fed to the measurement algorithm",4
"Do time tests running measurement algorithms against sample galaxies
Jim has suggest that we use cmodel to run these tests, since he is not committing to completing a complete shape measurement algorithm during the next sprint.  So we will do our timing test using cmodel and shapelet approximation, and switch to the new algorithm from Jim when it is available.",5
"Find an adequate process platform for shape measurement tests
This issue requires an estimate of how many measurements  we will need to run during S 15.  And it also needs an estimate of how long it will take to measure a single galaxy.  We should be able to guess how many galaxies are required to do an accurate assessment of a single parameterization of the shape measurement algorithm.  We probably cannot accurately estimate how much of the parameter space of the shape measurement algorithm we will have to explore.   The total amount of processing required should tell us whether this can be done with simple multi-core systems, or if a more sophisticated parallel process environment is required.  There is a large additional task if ordinary multi-core processing isn't adequate, so this task may spawn a rather large additional issue.",4
"proof of concept types and providers for managing jenkins security settings
Proof of concept level implementation of native puppet types and providers for managing jenkins users, security realm, and authorization strategy.",20
"Create Analysis code for Constant Shear Tests
For any test of shear measurement vs. input shear (where input shear is constant), plot the measured shear vs. input shear and fit the multiplicative bias m and additive bias c.",8
"puppet types & providers for puppet security management
The current puppet-jenkins (and ansible, and chef) can not fully control jenkins users and security realm / authorization strategy settings.  We should develop a proof of concept level limitation of naive puppet types and providers for users, security realm, and authorization strategy.",20
"Analyze bias vs. postage stamp size of galaxies
Vary the postage stamp size of simulated galaxies and access the effect of that size on the shear bias.  This task will not require additional galaxy image generation, as the intent is to generate all the galaxies at a size which is liberally larger than the likely point where bias does not change with size.  James Jee has indicated that 48 pixels on the LSST scale is not large enough enough for this bias to converge.  It seems likely that we will need to generate our images at 64 or 96 pixels to get beyond this limit.",10
"resolve communication between JavaScript component and java server
 We are writing the web application client side code in JavaScript. JS interop will make it much easier to use GWT with JavaScript libraries. This task is to resolve the issues that may rise with this technology since it is new in GWT2.7.  ",10
"Build 2015_05 Qserv Release
See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.",1
"Build 2015_06 Qserv Release
See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.",1
"Get meas_mosaic working on HSC data with LSST stack
We have an old, bitrotted version of meas_mosaic on the LSST side, created in a failed attempt to get it running on LSST PhoSim data.  Now that we're making a serious effort to get HSC data running through the LSST pipeline, we'll need to get it running with the LSST pipeline at least on HSC data, which will probably involve just merging everything from the HSC side over, and then fixing it until it builds and runs.  For this issue, we'll assume that we're going to use the Eigen backend for the matrix solver, rather than the MKL one we use on the HSC side.  That will make it much slower (since MKL is multithreaded and we can't make Eigen multithreaded for just meas_mosaic), but hopefully still usable.",10
"Research Serf and Consul
Serf: https://serfdom.io  Consul: https://www.consul.io  ",4
"Investigate loading of binary data
The binary entries in qserv_testdata are stored as binary values in text files and there is no reason to believe that they are being read into the database correctly, see qserv_testdata/datasets/case01/data/Science_Ccd_Exposure.tsv.gz.     Binary data from text files needs to be in hex format along with whatever other changes need to be made to reliably load the data into the database. Note that this story involves just investigating, it is not yet clear how much work will be needed to properly implement it. This story will help up understand the effort needed.",5
"Fix default LOAD DATA options
Integration tests in multi-node produced the following error during data loading: {code} 2015-05-01 17:03:03,030 - lsst.qserv.admin.dataLoader - CRITICAL - Failed to load data into non-partitioned table: Data truncated for column 'poly' at row 60 2015-05-01 17:03:03,031 - root - CRITICAL - Exception occured: Data truncated for column 'poly' at row 60 {code}  The default options for MySQL LOAD DATA need to be fixed for this.",1
"v11.0 release
nan",20
"Fix race condition in userQueryProxy
In UserQuery_kill, depending on timing, the call ""uqManager.get(session)->kill()"" can fail if kill is called more than once by user, because the session might get deleted by the earlier kill. To simulate this, I modified the code to delay the second kill as follows:  {code} void UserQuery_kill(int session) {     static int killNo = 0;     killNo ++;     LOGF_INFO(""EXECUTING UserQuery_kill(%1%), %2%"" % session % killNo);     if (killNo > 1) {         sleep(10);     }     uqManager.get(session)->kill(); } {code}  We need to revisit if other functions in this class might suffer in similar way.",4
"Add missing empty-chunk-path on Ubuntu 14.04
QSERV_DATA_DIR/var/lib/qserv wasn't created on Ubuntu 14.04 and this was breaking loader script. It was working on SL7 for unknown reason. Creation of the directory has been added to qserv-czar config script.",1
"Fix case05 3009_countObjectInRegionWithZFlux freeze
This prevents 2014_05 release to pass integration tests.",1
"F17 Experiment with Non-Partitioned Tables
Test sharing of unpartitioned tables between worker nodes. This is something we claimed would work if we simply stuck them on a SAN, but never tested. Now is a good time to find out whether it actually works. If it fails, we need to re-think that part of the design. Shall we put the unpartitioned on a set of std replicated mysql nodes and attach them to the worker mysqld via the connect engine? Probably worth it to reconsider the overall architecture so that this can be integrated more elegantly (and maintainably) than just bolting on more moving parts.    This epic involves:  a) revisiting the numbers and checking if we could simply replicate non-partitioned tables on all nodes  b) estimating realistic load on the non-partitioned tables  c) playing with bringing the non-partitioned tables through mechanisms such as [connect engine|https://mariadb.com/kb/en/mariadb/connect-table-types-mysql-table-type-accessing-mysqlmariadb-tables/]    If replicating on all nodes turns out to be too costly, we will arrange appropriate test bed (at NCSA?) and do the testing in S16 cycle.    ",20
"Clean up FITS binary table writing
FITS binary table is being refactored by necessity on DM-2534, and while there's no similarly urgent need to clean up the writing code, we should do it at some point, as the refactoring of the read code broke some symmetries and made it even harder to follow the writing code that it already was.",4
"Review with Telefonica revised path Tololo - Pachon
A meeting in Santiago  with Reuna and Telefonica to discuss the difference in price for the new path from Tololo to Pachon",2
"May 1 management
Met with A. Withers first cut read of scada plan. Contact session AURA -- explain gross contract changes accepted i principle)",1
"Rule for automatic replication in iRODS
Maintaining extra copies/replicas on separate resources is an important tenet in iRODS, with this practice considered key for prevention of data loss. The automatic replication of files upon ingest can be encoded via a system rule, so that data is preserved as a inherent part of storing in iRODS.",2
"Revisit mysql connections usage in integration tests
Recent changes in integration tests require too many connections. We need to understand what changes that is now requiring so many connections, and fix it.",4
"Revisit mysql connections from worker
Revisit the code that handles mysql connections in qserv. At the moment Qserv will maintain a connection per chunk-query, up to a hardcoded limit (GroupScheduler: 4, ScanScheduler:32).  Also, we have to gracefully handle connection issues (such as dropped connection, or if we hit the max_connections limit).",8
"Prototype Ceph Deployment
Deploy Ceph on spare storage servers, with particular emphasis on deploying Ceph FS. This should likely take 8-15 story points over a period of about 3 weeks. This does not include a production deployment of Ceph for LSST. It is intended to help us gain insight into requirements for the initial production deployment. The story will primarily consist of effort from M. Elliott & W. Glick, with secondary effort from M. Freemon and B. Mather. ",20
"Research GPFS Server for Performant Access to Condo Storage
Work with NCSA SET to figure out requirements for LSST GPFS Server access to our Condo storage. Implementation will be a different story. Expect this to take 3-6 story points over the next couple of weeks.",1
"Fix connection leak
Fix connection leak: 1 connection is leaking per chunk-query, in practice ~30+ connections for a query that touches many chunks.  It is a real blocker, and we need to fix it asap.",6
"Final cleanup of Query cancellation code
The query cancellation code that went in through DM-1716 works fine, however we feel it'd be good to do another pass and double check we are applying the cancellation consistently. Some potential places to clean: 1. in ccontrol/UserQuery.cc we changed the semantics of discard() 2. QueryRequest needs some cleanup: it'd be better to call Finished() from one place  More regarding the former (from DM-1716 PR): ""if a query is cancelled, none of the cleanup below happens in discard() anymore -- presumably we are now waiting for object deletion to do the cleanup.  If object deletion is sufficient to do this cleanup, do we need discard() at all anymore? It would be best if cleanup always occured in the same place rather than having two different control paths for it?""  Regarding the latter - see comment in https://jira.lsstcorp.org/browse/DM-1716",4
"Fix memory leak in Executive
There is a memory leak, most likely in Executive, related to _requesters. It looks like the ~MergingRequester() is never during normal operations (it is called when there are abnormal conditions and different parts of the code are triggered).   As a result _infileMerger kept inside MergineRequester is not called either, which results in 2 connection leaks per query.",6
"Latin America Infinera rep to give presentation of equipment
nan",4
"FY17 Research technologies potentially useful for Data Access
nan",26
"Understand race condition in Executive::_dispatchQuery
Inserting a log (presumably just a delay) in Executive::_dispatchQuery after the new QueryResource but before the Provision call causes queries to fail.  The particular test query was ""select count(*) from Object"" on test case 01.",2
"Convert the ds9 interface to follow RFC-42
RFC-42 (provide a backend-agnostic interface to displays) being accepted, please implement it.    For now, provide compatibility code so that the old way (import lsst.afw.display.ds9) still works.  ",6
"Mutex use before creation
qana/QueryPlugin.cc contains a static boost::mutex, that is used by static class member functions to register plugin implementations. Its constructor is not guaranteed to be called before the static registerXXXPlugin (see e.g. qana/AggregatePlugin.cc) instances use it to register plugin classes.",1
"Migrate boost:thread to std::thread
We are mixing boost and std threading libraries. This should be cleaned up - use std:thread consistently everywhere.",5
"Migrate boost::shared_ptr to std::shared_ptr
We are mixing boost and std shared_ptrs. This should be cleaned up - use std:shared_ptr consistently everywhere. In a few places we have other types of pointers, (e.g weak_ptr). Migrate these too.",2
"Add missing includes unistd.h for gcc 4.9.2
nan",1
"Fix connection leak (2nd iteration)
Fix connection leak (and memory leak and thread leak) -- we are leaking 2 per query.",2
"Add test involving many chunks
It might be useful to add a test to the integration test suite that involves a large number of chunks per node. I think I'd try something like 200-300 chunks. I'd 1. add case06 2. get one table, say Object from case05 and configure partitioning to ensure we have 200-300 chunks. 3. Run several queries that touch all chunks.",5
"Upgrade EUPS used by lsstsw
As discussed, bump it up when you get a chance please. ",1
"Migrate boost::scoped_ptr to std
We have a few places where we are using boost::scoped_ptr. Given we migrated shared_ptrs, we might want to move scoped_ptrs too (most likely to std::unique_ptr).",1
"Revisit design of query poisoner
As we discovered through DM-2698, poisoner tends to hold onto query resources even after the query completes. We should revisit whether than can be redesigned and improved, so that when query finishes, all resources related to that query are immediately automatically released. This story involves just the planning part, implementation will be done through separate stories.",1
"LOE - Week ending 5/8/15
nan",12
"LOE - Week ending 5/15/15
nan",3
"LOE - Week ending 5/22/15
nan",3
"LOE - Week ending 5/29/15
nan",12
"Package Python requests package
To complete DM-2593 we need to package and install `requests` as a separate package instead using one from anaconda.",1
"Build should fail if node.js is not present
Problem: I built Firefly by mistake w/o having node on my path. The build didn't signal any errors, but generated an unusable webapp that wouldn't load.  Expected behavior: the build should have failed and warned the user that node.js is missing.",2
"Fix a few more g++ 4.9.2 compatos
Some of the recent boost -> std changes don't compile/link under gcc 4.9.2, because of some poor #include hygiene (including <thread> when we should include <condition_variable>, not explicitly including <unistd.h>, etc.)  Also, -pthread linker option is required when using std::thread under gcc 4.9.2. ",1
"Generalize / Simplify Facade 
Daniel started thinking about simplifying Facade, here is some unfinished code from him  {code} /// Unfinished. Planned to be a re-thinking of Facade that collapses some /// genericity and simplifies things using the assumption of running on a /// snapshot. class FacadeSnapshot : public Facade { public:     StringMap _map; // Path --> key      FacadeSnapshot() {     }      virtual bool containsDb(std::string const& dbName) const {         if (dbName.empty()) {             LOGF_DEBUG(""Empty database name passed."");             throw NoSuchDb(""<empty>"");         }         string p = _prefix + ""/DBS/"" + dbName;         bool ret =  (_map.find(p) != _map.end());         LOGF_DEBUG(""containsDb(%1%): %2%"" % dbName % ret);         return ret;     }     virtual bool containsTable(std::string const& dbName,                                std::string const& tableName) const {         if (!containsDb(dbName)) {             throw NoSuchDb(dbName);         }         if (tableName.empty()) {             LOGF_DEBUG(""Empty table name passed."");             throw NoSuchTable(""<empty>"");         }         string p = _prefix + ""/DBS/"" + dbName + ""/TABLES/"" + tableName;         bool ret =  (_map.find(p) != _map.end());         LOGF_DEBUG(""containsTable returns: %1%"" % ret);         return ret;     }     virtual bool tableIsChunked(std::string const& dbName,                                 std::string const& tableName) const {         if (!containsTable(dbName, tableName)) {             throw NoSuchTable(dbName + ""."" + tableName);         }         string p = _prefix + ""/DBS/"" + dbName + ""/TABLES/"" +                tableName + ""/partitioning"";         bool ret =  (_map.find(p) != _map.end());         LOGF_DEBUG(""%1%.%2% %3% chunked.""                    % dbName % tableName % (ret?""is"":""is NOT""));         return ret;     }     virtual bool tableIsSubChunked(std::string const& dbName,                                    std::string const& tableName) const {         string p = _prefix + ""/DBS/"" + dbName + ""/TABLES/"" +             tableName + ""/partitioning/"" + ""subChunks"";         StringMap::const_iterator m = _map.find(p);         bool ret = (m != _map.end()) && (m->second == ""1"");         LOGF_DEBUG(""%1%.%2% %3% subChunked.""                    % dbName % tableName % (ret ? ""is"" : ""is NOT""));         return ret;     }     virtual bool isMatchTable(std::string const& dbName,                               std::string const& tableName) const {         LOGF_DEBUG(""isMatchTable(%1%.%2%)"" % dbName % tableName);         if (!containsTable(dbName, tableName)) {                 throw NoSuchTable(dbName + ""."" + tableName);         }         string p = _prefix + ""/DBS/"" + dbName + ""/TABLES/"" + tableName + ""/match"";         StringMap::const_iterator m = _map.find(p);         bool ret = (m != _map.end()) && (m->second == ""1"");         LOGF_DEBUG(""%1%.%2% is %3% a match table""                    % dbName % tableName % (ret ? """" : ""not ""));             return ret;     } #if 0     virtual std::vector<std::string> getAllowedDbs() const {     };     virtual std::vector<std::string> getChunkedTables(std::string const& dbName) const;     virtual std::vector<std::string> getSubChunkedTables(std::string const& dbName) const;     virtual std::vector<std::string> getPartitionCols(std::string const& dbName,                                                       std::string const& tableName) const;     virtual int getChunkLevel(std::string const& dbName,                               std::string const& tableName) const;     virtual std::string getDirTable(std::string const& dbName,                                     std::string const& tableName) const;     virtual std::string getDirColName(std::string const& dbName,                                       std::string const& tableName) const;     virtual std::vector<std::string> getSecIndexColNames(std::string const& dbName,                                                          std::string const& tableName) const;     virtual StripingParams getDbStriping(std::string const& dbName) const;     virtual double getOverlap(std::string const& dbName) const;     virtual MatchTableParams getMatchTableParams(std::string const& dbName,                                                  std::string const& tableName) const;   private: #endif }; {code}",14
"Add config file for test dataset 04 tables
Following the changes to default LOAD DATA settings in DM-2679, two tables in test case 04 need to have a config file to include their in.csv format.",1
"optimistic matcher may match the same reference object to more than one source
The optimistic pattern matcher in meas_astrom, adapted from hscAstrom, does not check if reference objects have been used before when finding the reference object nearest to each source. As a result the same reference object may be matched to more than one source. This should not happen.",4
"Log xrootd client debug messages in Qserv czar
xrootd client print it's debug messages to stdout. This ticket aims at redirecting them to Qserv logger, if possible.",4
"Build a DiscreteSkyMap that covers a collection of input exposures
This is essentially a rehash of the old trac Ticket #[2702| https://dev.lsstcorp.org/trac/ticket/2702], originally reported by [~jbosch], which reads:  ""I'd like to add a Task and bin script to create a DiscreteSkyMap that bounds a set of calexps specified by their data IDs. This makeDiscreteSkyMap.py could be used instead of makeSkyMap.py when the user would rather compute the pointing and size of the skymap from the input data than decide it manually.""  The work was done by [~jbosch] & [~price] and exists on branch {{u/price/2702}} in {{pipe_tasks}}, but it was never merged to master.  I plan to simply rebase the commits in that branch onto master.",1
"Remove #include ""XrdOuc/XrdOucTrace.hh"" from Qserv code
See next emails:  Hi Fabrice,  Absolutely!  Andy  On Wed, 13 May 2015, Fabrice Jammes wrote:  > Hi Andy, > > Thanks, > > In my understanding, you're ok if I remove the existing > #include ""XrdOuc/XrdOucTrace.hh"" > from Qserv source code. I'll do it soon. > > Have a nice day, > > Fabrice > > Le 12/05/2015 23:41, Andrew Hanushevsky a écrit : >> Hi Fabrice, >> >> Well, no. We have a long-standing approach that qserv should not depend on anything outside of XrdSsi public interfaces. This is the only way to easily protect sqserv code from infrastructure changes. So, I would not. If you want to copy something like that for >> >> qserv please do, it's simple enough. But in the end qserv needs to be self-contained in that it does not depend on xrootd code just the public ssi interfaces. >> >> Andy >> >> -----Original Message----- From: Fabrice Jammes >> Sent: Tuesday, May 12, 2015 9:06 AM >> To: Andrew Hanushevsky >> Subject: About xrdssi client logging >> >> Hi Andy, >> >> Hope you're doing well. >> Could you please tell me if its usefull to include >> #include ""XrdOuc/XrdOucTrace.hh"" >> in our xrdssi client code? >> >> Indeed client seems to only print DBG macro output, that's why I was >> wondering if XrdOucTrace was only use on the server side. >> If yes, I will remove it from our client. >> >> Thanks, and have a nice day, >> >> Fabrice ",1
"Make ANetAstrometryTask more configurable
The current ANetAstrometryTask has a solver that is not easy to retarget. This makes testing with hscAstrom needlessly difficult. My suggestion is to make the solver a true Task instead of a task-like object, and make it retargetable using a ConfigurableField instead of a ConfigField. This is very easy to do because the solver is already a task in all but name. ",2
"sandbox selection of newinstall.sh source url
Frossie would like the ability to control the source URL for the newinstall.sh script in sandbox-stackbuild.  The newinstall.sh installation logic needs to be migration to the puppet-lsststack module, converted into a defined type, and have unit+ acceptance tests written for it.",1
"Second Review with Chris Smith AURA head
Went over the process relating to AURA and NSF",4
"Design of the summit network computer facility
jeff ",40
"Add clear message when integration test fails
Integration test fails without printing a clear message at the end, and for now a query is broken: 0011_selectDeepCoadd.txt but it isn't printed at the end of tet output.",2
"Fix case04/0011_selectDeepCoadd.txt
It seems --config=/path/to/table.cfg param can be duplicated (see dbLoader l.77 and QservDbLoader l. 87)    Futthermore there is an enclosing pb and it can be solved for this query by passing correct cfg table (which in.csv.enclose correct parameter), but then next query fails: it seems some cfg parameters of table.cfg aren't managed correctly by the loader in plain MySQL mode.     This need further investigations.",6
"Allow lsst/log library to log PID on the C++ side
lsst/log should be able to log application PID",3
"db 10.1+4 tests randomly fail with python egg installation error
The unit tests for DB seem to fail at random and always pass on a second build attempt.  My hunch is that multiple tests are running in parallel all attempting to install the mysql module but I haven't investigated.  {code}                   db: 10.1+4 ERROR (0 sec). *** error building product db. *** exit code = 2 *** log is in /home/build0/lsstsw/build/db/_build.log *** last few lines: :::::  [2015-05-15T19:12:35.557258Z] scons: done reading SConscript files. :::::  [2015-05-15T19:12:35.558276Z] scons: Building targets ... :::::  [2015-05-15T19:12:35.558409Z] scons: Nothing to be done for `python'. :::::  [2015-05-15T19:12:35.570007Z] makeVersionModule([""python/lsst/db/version.py""], []) :::::  [2015-05-15T19:12:35.686733Z] running tests/testDbLocal.py... running tests/testDbRemote.py... running tests/testDbPool.py... failed :::::  [2015-05-15T19:12:35.695011Z] passed :::::  [2015-05-15T19:12:35.698811Z] passed :::::  [2015-05-15T19:12:35.706360Z] 1 tests failed :::::  [2015-05-15T19:12:35.706703Z] scons: *** [checkTestStatus] Error 1 :::::  [2015-05-15T19:12:35.708443Z] scons: building terminated because of errors. {code}  {code} [root@ip-192-168-123-151 .tests]# cat * tests/testDbLocal.py  Traceback (most recent call last):   File ""tests/testDbLocal.py"", line 53, in <module>     from lsst.db.db import Db, DbException   File ""/home/build0/lsstsw/build/db/python/lsst/db/db.py"", line 49, in <module>     import MySQLdb   File ""build/bdist.linux-x86_64/egg/MySQLdb/__init__.py"", line 19, in <module>        File ""build/bdist.linux-x86_64/egg/_mysql.py"", line 7, in <module>   File ""build/bdist.linux-x86_64/egg/_mysql.py"", line 4, in __bootstrap__   File ""/home/build0/lsstsw/anaconda/lib/python2.7/site-packages/setuptools-5.8-py2.7.egg/pkg_resources.py"", line 937, in resource_filename   File ""/home/build0/lsstsw/anaconda/lib/python2.7/site-packages/setuptools-5.8-py2.7.egg/pkg_resources.py"", line 1632, in get_resource_filename   File ""/home/build0/lsstsw/anaconda/lib/python2.7/site-packages/setuptools-5.8-py2.7.egg/pkg_resources.py"", line 1662, in _extract_resource   File ""/home/build0/lsstsw/anaconda/lib/python2.7/site-packages/setuptools-5.8-py2.7.egg/pkg_resources.py"", line 1003, in get_cache_path   File ""/home/build0/lsstsw/anaconda/lib/python2.7/site-packages/setuptools-5.8-py2.7.egg/pkg_resources.py"", line 983, in extraction_error pkg_resources.ExtractionError: Can't extract file(s) to egg cache  The following error occurred while trying to extract file(s) to the Python egg cache:    [Errno 17] File exists: '/home/build0/.python-eggs'  The Python egg cache directory is currently set to:    /home/build0/.python-eggs  Perhaps your account does not have write access to this directory?  You can change the cache directory by setting the PYTHON_EGG_CACHE environment variable to point to an accessible directory.  tests/testDbPool.py  /home/build0/lsstsw/anaconda/lib/python2.7/site-packages/setuptools-5.8-py2.7.egg/pkg_resources.py:1032: UserWarning: /home/build0/.python-eggs is writable by group/others and vulnerable to attack when used with get_resource_filename. Consider a more secure location (set with .set_extraction_path or the PYTHON_EGG_CACHE environment variable). 05/15/2015 07:12:35 root WARNING: Required file with credentials '/home/build0/.lsst/dbAuth-test.txt' not found. tests/testDbRemote.py  /home/build0/lsstsw/anaconda/lib/python2.7/site-packages/setuptools-5.8-py2.7.egg/pkg_resources.py:1032: UserWarning: /home/build0/.python-eggs is writable by group/others and vulnerable to attack when used with get_resource_filename. Consider a more secure location (set with .set_extraction_path or the PYTHON_EGG_CACHE environment variable). 05/15/2015 07:12:35 root WARNING: Required file with credentials '/home/build0/.lsst/dbAuth-testRemote.txt' not found. {code}",1
"Write example-based documentation for multiband processing
The multi-band coadd processing tasks we're porting over from the HSC side don't have the high-quality example-based documentation we typically provide for Tasks on the LSST side, so we need to write it from scratch.",6
"Improve selection criteria for sources
Dominique Boutigny has demonstrated that one reason the new astrometry task is working so poorly is that it is not selective enough about which sources it uses. This ticket is to be used to improve that situation.  Another problem Dominique discovered is that the TAN-SIP WCS fitter needs to be iterated to work properly, and that work may also be done on this ticket. ",4
"Configure NCSA LSST Perfsonar Host
nan",3
"Administrative - 6-2015
Meetings and reporting and such",2
"Create LSST wiki documentation for LHN effort
nan",3
"Onboarding Humberto
Efforts in helping new employee Humberto come up to speed in his role as lead on perfsonar deployments",1
"Avoid leaking memory allocated by mysql_thread_init
mysql/MySqlConnection.cc contains the following comment: {code}     // Dangerous to use mysql_thread_end(), because caller may belong to a     // different thread other than the one that called mysql_init(). Suggest     // using thread-local-storage to track users of mysql_init(), and to call     // mysql_thread_end() appropriately. Not an easy thing to do right now, and     // shouldn't be a big deal because we thread-pool anyway. {code}  The comment is not really correct with regards to thread pooling. Instead, each rproc::InfileMerger has an rproc::InfileMerger::Mgr which contains a util::WorkQueue that spawns a thread, and so we are failing to call mysql_thread_end at least once per user query. This has been verified using the memcheck valgrind tool. ",3
"Mountain - Base fiber implementation
Aquire, install, and test fiber connecting Mountain - Base.   The fiber will follow a path along roads from Cerro Pachon to Cerro Tololo and down to the AURA gate, where it will connect with Telefonica fiber bundle to La Serena.",40
"Improve management of ColSchema.hasDefault and ColSchema.defaultValue
Managing default values in protobuf and result table isn't optimized for now. Indeed all values are packed in protobuf, whereas default values could be removed from protobuf messages (in .QueryAction::Impl::_fillRows())  It is interesting to monitor performance when packing default values, and when not, and then improve the code related to default value management, or completely remove it.",10
"AURA Traffic Utilizing the Fiber Link at 10Gbps
Obtain equipment in order to light the fibers up to 10Gbps and run live traffic for Tololo and Pachon over the link",30
"Fix ORDER BY in integration test query case03 0019.1.0 
ORDER BY fails sometimes for unknown reason, see  datasets/case03/queries/0019.1.0_selectRunDeepSourceDeepcoaddDeepsrcmatchRefobject.sql.FIXME",4
"investigate decomposition of stack build into independent packages
In order to obtain per package build time, test time, coverage, or virtually any per component metric ,the CI build needs to be decomposed from a single large integrated build into per package jobs with an overall work flow representing the dependency graph.    This is also needed for binary artifacts to be passed between builds step and/or binary packages.",10
"sconsUtil has a hard dependency on EUPS for both tests and installation
After some discussion on Data Management, its clear that sconsUtils is a hard requirement on EUPS for both tests and installation.  It was decided by RFC-44 that tests should not depend on EUPS.  However, I'd argue that sconsUtils should also not depend on EUPS as any package that uses sconsUtils (the virtual entirety of the stack) can not build or run tests without the presence of EUPS.  The current situation is that the complete stack has a hard dependency on EUPS.    Attempting to build sconUtils without the presence of EUPS.  The tests fail attempting to import the eups module.  {code}  $ SCONSUTILS_DIR=. scons -Q  Unable to import eups; guessing flavor  CC is gcc version 4.8.3  Checking for C++11 support  C++11 supported with '-std=c++11'  Unable to import eups; guessing flavor  Doxygen is not setup; skipping documentation build.  ImportError: No module named eups:    File ""/home/vagrant/sconsUtils/SConstruct"", line 9:      scripts.BasicSConstruct.initialize(packageName=""sconsUtils"")    File ""python/lsst/sconsUtils/scripts.py"", line 106:      SCons.Script.SConscript(os.path.join(root, ""SConscript""))    File ""/usr/lib/scons/SCons/Script/SConscript.py"", line 609:      return method(*args, **kw)    File ""/usr/lib/scons/SCons/Script/SConscript.py"", line 546:      return _SConscript(self.fs, *files, **subst_kw)    File ""/usr/lib/scons/SCons/Script/SConscript.py"", line 260:      exec _file_ in call_stack[-1].globals    File ""/home/vagrant/sconsUtils/tests/SConscript"", line 5:      import eups  {code}    Attempting to bypass the test failures:  {code}  [vagrant@jenkins-el7-1 sconsUtils]$ rm -rf tests  [vagrant@jenkins-el7-1 sconsUtils]$ SCONSUTILS_DIR=. scons -Q install  Unable to import eups; guessing flavor  CC is gcc version 4.8.3  Checking for C++11 support  C++11 supported with '-std=c++11'  Error with git version: uncommitted changes  Found problem with version number; update or specify force=True to proceed  {code}",6
"sconsUtil install target does not respond to either force=True or --force
I've been unable to figure out how to bypass the install 'force' check, but have confirmed that this is the correct expression by commenting it out:    https://github.com/lsst/sconsUtils/blob/54c983ffe9714a33657c4388de3506fe7a40518d/python/lsst/sconsUtils/installation.py#L92    {code}  $ SCONSUTILS_DIR=. scons -Q force=True install   Unable to import eups; guessing flavor  CC is gcc version 4.8.3  Checking for C++11 support  C++11 supported with '-std=c++11'  Error with git version: uncommitted changes  Found problem with version number; update or specify force=True to proceed  {code}  ",1
"Improve SIP fitting
Dominique Boutigny says ""the tan-sip fitter is very sensitive to bad matches. This is a weakness of the fitter and I think that it could (should) be rewritten in such a way to reject the outliers internally.""  This has resulted in iteration in the matching (DM-2755), which should be unnecessary (or at least minimised).    Additionally, it seems the SIP fitter fits for x and y in subsequent iterations, which can confuse users.    We should:  1. Make the SIP fitter fit for x and y concurrently.  2. Add rejection iterations in the SIP fitter.  3. Remove or minimise the iterations in the matching.",6
"Fix races in BlendScheduler
_integrityHelper() from wsched/BlendScheduler inspects a map of tasks and is sometimes called without holding the corresponding mutex. My theory is that it is observing the map in an inconsistent state, leading to assert failure and hence worker death, and finally to hangs/timeouts on the czar.",2
"HSC backport: allow for use of Approximate model in background estimation
This issue involves transferring changesets from the following HSC issues:    - [HSC-145|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-145]  Investigate approximating rather than interpolating backgrounds  - [HSC-1047|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1047] Background object cannot be loaded with butler  - [HSC-1213|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1213] Set background 'approximate' control settings when background control is created.  - [HSC-1221|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1221] tests failing in ip_diffim  - [HSC-1217|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1217] Verify backgroundList IO works properly when Approximate is enabled in background control - HSC JIRA    The Approximate (Chebyshev) approach greatly improves the background subtraction around bright objects compared with the interpolation scheme currently in use (which over-subtracts near bright objects).",6
"Fix race in Foreman
The Foreman implementation passes a TaskQueue pointer corresponding to running tasks down to the task scheduler without holding a lock. This means that the scheduler can inspect the running task list (usually to determine its size) while it is being mutated.",2
"Document and test how to log PID via lsst/log
nan",2
"push PID in lsst/log MDC in a C++ plugin (for xrootd)
nan",2
"Firefly Tools API: Add advance region support
Firefly Tools API: Add advance region support  Improve firefly's region functionality to support a ""dynamic region"".  Data can be added or removed from this region by API calls.  Allow any amount of region lines to be added or removed.  Make sure performance is good.  Also, document the current Firefly region support.",2
"Control firefly viewer tri-view mode
When table data is add to Firefly Viewer, control whether it goes into tri-view or just overlay data on FITS, or just does an XYPlot, etc",6
"Add Firelfy Tools API controlled Pan and Zoom
nan",2
"FFTools python wrapper: make launch Browser smarter. 
FireflyClient.launchBrowser() needs to send an event to the server who will attempt to guess if there is an existing connection.  It will not be launch in that case.  This way it can be called every time without creating tons of tabs, who are all talking to the same channel.    Also, launchBrowser really should not return until the tab is ready to receive events from the websocket channel.  Both of these feature are going to take some thought on how to do.  This is a multi-threaded problem on both the client and the server.    ",4
"FFTools api, wrapper: upload region file from memory like fits file
nan",1
"Footprint dilation performance regression
In DM-1128 we implemented span-based dilation for footprints. A brief test on synthetic data indicated that this was a performance win over the previous version of the code.    In May 2015, this code was merged to HSC and applied to significant quantities of real data for the first time. A major performance regression was identified:    {quote}  [May-9 00:26] Paul Price: processCcd is now crazy slow.  [May-9 00:29] Paul Price: Profiling...  [May-9 00:40] Paul Price: I'm thinking it's the Footprint grow code...  [May-9 00:44] Paul Price: And the winner is…. Footprint construction:  [May-9 00:44] Paul Price: 2    0.000    0.000  702.280  351.140 /home/astro/hsc/products/Linux64/meas_algorithms/HSC-3.8.0/python/lsst/meas/algorithms/detection.py:191(makeSourceCatalog)         2    0.005    0.002  702.274  351.137 /home/astro/hsc/products/Linux64/meas_algorithms/HSC-3.8.0/python/lsst/meas/algorithms/detection.py:228(detectFootprints)       15    0.001    0.000  698.597  46.573 /home/pprice/hsc/afw/python/lsst/afw/detection/detectionLib.py:3448(__init__)       15  698.596  46.573  698.596  46.573 {_detectionLib.new_FootprintSet}  [May-9 00:53] Paul Price: If I revert HSC-1243 (""Port better Footprint-grow code from LSST""), then the performance regression goes away.  @jbosch @jds may be interested...  {quote}    The source of the regression must be identified and resolved for both HSC and LSST.",5
"rename CameraMapper.getEupsProductName() to getPackageName() and convert to abstract method
Per discussion on this PR related to DM-2636:  https://github.com/lsst/daf_butlerUtils/pull/1#issuecomment-104785055    The CameraMapper.getEupsProductName() should be renamed to getPackageName() and converted to an abstract method.  This will eliminates a runtime, and thus ""test time"", dependency on EUPS.  As part of the rename/conversion, all subclasses that are not already overriding getEupsProductName() will concurrently need to have getPackageName() implemented.",3
"meas_modelfit not in full-stack doxygen build
I'm fairly certain meas_modelfit is included in lsst_apps and hence in CI, but id doesn't seem to be included in the LSST Doxygen build:    http://lsst-web.ncsa.illinois.edu/doxygen/x_masterDoxyDoc/search.php?query=modelfit",1
"Make the new astrometry task the default task
The new astrometry task should be the default astrometry task, but we need to make sure it is good enough first.",1
"Improve behavior of new matcher on highly distorted fields
The optimistic matcher used by the new astrometry task probably does not handle highly distorted fields well. The issue is that it tries to match in X-Y space, and if that has significant curvature then the match is not optimal.    I suggest matching in RA/Dec space, as per Tabur's original algorithm (on which this code is based). This is simple and easy to understand    An alternative is to use the old technique of ""undistorting"" source and reference object positions before matching. This works, but is complicated, difficult to understand and adds an unnecessary step.  ",8
"Fiber from Tololo to Pachon
Preparation for fiber cable from Cerro Tololo to Cerro Pachon",5
"Various fixes for broken code within display=True clauses and/or using --debug
Running with the display and/or debug options turned on is revealing many instances of code that is now suffering from bit rot.  This ticket will be used to track those encountered while trying to debug issues arising while porting HSC code and running processing tasks on real data.",4
"Implement HSC improvements to Colorterm
Paul Price recommended some HSC changes for the Colorterm class. To quote Paul: changed Colorterms so it's not a global, and it can now be configured using Config. See https://github.com/HyperSuprime-Cam/meas_astrom/blob/master/python/lsst/meas/photocal/colorterms.py    This sounds useful. Note that the HSC colorterms.py is in meas_astrom but as of DM-1578 the LSST version is in pipe_tasks.",6
"Tests for daf_butlerUtils should not depend on obs_lsstSim
Currently two of the tests in {{daf_butlerUtils}} depend on {{obs_lsstSim}}. They will never run in a normal build because {{obs_}} packages can not be a dependency on {{daf_butlerUtils}}.    After discussing the options with [~ktl] the feeling is that {{ticket1640}} should be rewritten to remove the dependency and {{ticket1580}} can probably be removed.",2
"Review with Contractors preparing fiber path
Hold conversations with the two major fiber laying contractors to prepare the path from Tololo to Pachon with a trench",2
"Document NCSA Wide Area Network status now and in the near future
Write up a document explaining how the wide area network is evolving at NCSA.",2
"W16 Finish Implementing Database & Table Mgmt
Implement table and database deletion.",29
"Adapt multi-node tests to latest version of qserv / loader
The multi-node integration tests have to be updated to work with the latest changes to qserv, in particular the loader, which broke already working tests lately.",8
"Implement query metadata skeleton
Skeleton implementation of the Query Metadata - including the APIs and core functionality (accepting long running query and saving the info about it)",8
"Complete Query Metadata Implementation
Including query abort",10
"Run large scale tests
nan",6
"Chile National Links Contracts Negotiation
The negotiations and conversations that have occurred with firstly Entel and finally Reuna and Telefonica for dark fiber between La Serena and Santiago",60
"La Serena - Santiago Dark Fiber Acquisition
Acquiring the fiber between La Serena and Santiago and testing the segments. This will be carried out by Reuna with some collaboration with LSST personnel",15
"La Serena - Santiago Link Equipment installation
Reuna will install the amplifiers and the DWDM equipment for the link between La Serena and Santiago with collaboration with LSST personnel ",20
"La Serena - Santiago LInk with Live Traffic flowing
The link is established and tested with Live LSST data running over the 100G Lambda",10
"Mountain to Base Implementation Plan Feasibility Check/Reevaulation
The Installation of the fiber on the AURA property from gate to Pachon via Tololo. Testing that portion of the link.  Connecting the 4 dark fibers supplied by Telefonica and testing end to end.",80
"Fiber lay between Gate to Tololo
Telefonica will install the fiber cable from the Gatehouse to Tololo. This will be tested once terminated. We will oversee and monitor the installation and be present for testing.",40
"Fiber Install from Tololo to Pachon 
Telefonica will install the cable from Tololo to Pachon Sahred Infrastructure building and terminate. Testing of this portion will be carried out.",20
"Connection of Telefonica 4 Fibers at AURA Gate 
The connection will made with the Telefonica 4 dark fibres to the 24 fiber cable from the gate to Pachon.  Tests will then take place over the whole mountain to base link.",10
"Creation of chunked views in wmgr
Current implementation of the creating chunks for views in wmgr is likely not doing right thing. Need to find an example of the partitioned views and implement correct procedure.",3
"Document architecture of the data loader
Fabrice requested documentation for the overall architecture of the data loader.",2
"Install 10Gbs Transceivers at the ends of the fibers and test
Purchase 2x10Gbs Transceivers and install at Pachon and La Serena.  Run tests to confirm integrity of the link  Utilize the link for AURA live traffic on the fiber backbone",20
"AURA traffic utilizing 100Gbs Lambda
AURA acquires their DWDM end nodes and installs. Tests and live traffic flows.",30
"Contract Negotiations for Chilean links
Defining the contracts between AURA and Reuna and Reuna and Telefonica who is supplying and installing the fibers for the La Serena to Santiago link",20
"Mountain - Base Contract and Execution
Defining the contracts and execution between AURA and Reuna and Reuna and Telefonica who is supplying and installing the fibers for the mountain to base link",30
"La Serena - Santiago Early Diverse Path
Reuna will provide a 4Gbps path via the legacy fiber path to Santiago",10
"La Serena - Santiago Diverse Path Final Capacity
Reuna will upgrade the capacity from 4Gbps to a minimum of 40Gbps over the legacy fiber route to Santiago",10
"La Serena - Santiago Fiber Tests over 3T cable
The 3T cable from La Serena to Santiago is expected to be completed September 2015 at which time Reuna can test the segments along the route.  ",20
"Mountain - Base AURA link upgrade to 100Gbps
AURA will obtain their DWDM equipment end nodes and install on the backbone from the summits to La Serena.",30
"Implement RESTful interfaces for Database (POST)
Implement RESTful interfaces for Database (see all D* in https://confluence.lsstcorp.org/display/DM/API), based on the first prototype developed through DM-1695. The work includes adding support for returning appropriately formatted results (support the most common formats). This covers ""POST"" type requests only, ""GET"" will be handled separately.",8
"orphan threads in archive DMCS
The archive DMCS can experience orphaned threads if a connection is made from external processes waiting for data to arrive.   If the external process goes away, the thread that was created to handle that connection will be waiting on a data structure to be updated.   If the data doesn't arrive, the thread remains alive when it should be checking to see if the connection that created it is still viable, and die if it isn't viable.",17
"LOE - Week ending 6/5/15
nan",5
"LOE - Week ending 6/12/15
nan",3
"LOE - Week ending 6/19/15
nan",3
"LOE - Week ending 6/26/15
nan",1
"Add unit tests for the new colorterms code
The new colorterms code that we adopted from HSC may not have complete unit tests. The existing colorterms test is pretty good, but may have holes. I'm more concerned about the unit test for PhotoCalTask, which does not apply colorterms at all (likely an existing issue).    Also, be sure to test that the obs_cfht config override loads correctly (presumably with a unit test in obs_cfht) and similarly for obs_subaru.",4
"Documentation and testing for Firefly Javascript and Python API
Document, polish and test Firefly Javascript and Python APIs    - Proofread and polished all the documentation, added missing docs  - Tested all the examples and API methods. Updated test cases as needed.",10
"sconsUtils should notice when SWIG python file has been modified
Currently {{scons}} will not rerun tests if a {{.i}} file has been modified if the only outcome of that modification was a change to the {{.py}} file. {{sconsUtils}} should be modified to look for changes in both SWIG output files.",2
"Add support for listing async queries
Modify mysql proxy and implement ""show processlist"" command, which should display list of currently running queries.",4
"Write Qserv User Guide
It'd be useful to write a document about Qserv geared towards users, describing what queries Qserv supports now, what will be supported in the future, what restrictions we are imposing and such. ",8
"Distributed Hash Table prototyping
nan",20
"reserved
nan",10
"Near Neighbor Optimizations
To optimize near neighbor queries we are maintaining overlap tables and subchunking. It'd be useful to revisit that. Getting rid of subchunks would simplify qserv code. This epic involves   * testing speed of in-database near neighbor queries without subchunking, including how sensitive the optimizer is for these types of queries   * exploring possibility of precalculating and storing near neighbors, perhaps per subchunk  ",15
"SUI Firefly server side Python job management
In order to support Camera team needs and L3 data production, Firefly server needs to be able to start a Python job with proper input data and get the output data as a result of running the Python job. This will make the future integration of Firefly and DM pipeline stack much easier. ",40
"Tweaks to OO display interface
When I wrote the initial version of display_firefly I found a few minor issues in the way I'd designed the Display class; at the same time, [~lauren] found some missing functions in the backward-compatibility support for ds9.    Please fix these;  note that this implies changes to afw, display_ds9, and display_firefly.  ",2
"getSchemaCatalogs() breaks Task encapsulation
The {{getSchemaCatalogs()}} method was added to {{Task}} to allow {{CmdLineTasks}} to introspect their subtasks for schemas they produce, but it requires the subtasks to report the schemas by butler dataset.  This limits subtask reusability by locking them into producing a particular Butler dataset (or, as in DM-2191, requiring additional arguments from their parent task that they wouldn't need with a better design).    Instead, we should have per-subtask-slot interfaces (i.e. an interface for all subtasks that could fill a particular role in a CmdLineTask) for how the parent tasks should retrieve their schemas.  This will require `CmdLineTask` subclasses to implement the `writeSchemas` method themselves, instead of inheriting an implementation from `CmdLineTask` itself.",2
"Build the recent 10.1 release  & Gather strace logs for file system testing
We build the recent Version 10.1 stack release in a Centos 6.6 docker container. As we do so, we also gather strace logs for candidate packages  (for example, afw) for analysis within an effort to create load simulators for file system testing/profiling.   As another product of the effort,  I will make a docker image of the latest release installed on Centos 6.6 and push to  docker hub.",4
"AAA requirements document
nan",2
"Put together a few slides for NCSA-IN2P3 meeting
 I put together a few slides for the NCSA-IN2P3 meeting describing previous scaling, middleware, and processing efforts of LSST DM. ",1
"Fix Qserv SsiSession worker race
The worker SsiSession implementation calls ReleaseRequestBuffer after handing the bound request to the foreman for processing. It therefore becomes possible for request processing to finish before ReleaseRequestBuffer is called by the submitting thread, resulting in a memory leak.",2
"Margaret's mgmt. activities in May
Weekly DMLT meeting  Weekly ISO meeting  Weekly NCSA local group meeting    Met with NCSA networking person (Paul) to discuss progress and plans  Attended End-to-End networking meeting    Attended remotely the CCS-DAQ-OCS-DM Workshop for SCADA presentation by ISO    Worked on/discussed AURA procurement contract amendment and to understand property management procedures at AURA and NCSA    Discussed/planned agenda, made travel arrangements, prepared slides for trip to CC-IN2P3  2-day meeting in France with CC-IN2P3 group    4-day DMLT face-to-face meeting and T/CAM day    Interview with candidate for systems lead    Cleaned up Jira tickets from April  Reviewed reporting requirements and Jira procedures with NCSA employee (Bruce) and managers (Doug, Brett)    Attended several NCSA leadership development and training courses",22
"Multi-processing capability for shear test measurements
A suitable multi-cpu capability must be created for measurement tests.  We are hoping to just use a pipe_task, but to do so, the butler must be customized to allow it to read our cutouts and psfs from galaxies and psfs generated from GalSim and PhoSim.    This will be a relatively simple story if pipe_tasks running on 2 or 3 machines at UC Davis proves to be an adequate solution for running our shear experiments.",6
"Add support for ""ORDER BY f1, f2"" for has-chunks query
{code}   QuerySession description:    original: SELECT objectId, taiMidPoint FROM   Source ORDER BY objectId, taiMidPoint ASC;  has chunks: 1    needs merge: 1    1st parallel statement: SELECT objectId,taiMidPoint FROM LSST.Source_%CC% AS QST_1_    merge statement: SELECT objectId,taiMidPoint ORDER BY objectId,,taiMidPoint ASC    ScanTable: LSST.Source  {code}    Merge statement has syntax error",4
"Return error for ""SELECT a FROM T ORDER BY b"" for has-chunks query
ORDER BY field has to be in result table => it has to be in select list.  Return clear error message to user if not.",4
"Enquiry into MiniSub FO cable
Obtaining a quote from a company in Canada for a special clad cable for the Tololo-Pachon link",2
"RFI with vendors in Vina
Open day with all interested vendors  to layout the projects for equipment on Mountain-Base and La Serena-Santiago links.",8
"Validate wmgr client / server versions 
If the client and server are on different versions, unexpected things can happen. (example: we run old version of the server, and use latest client). We need to check the version on both sides. ",4
"Fix bug related to selecting rows by objectId from non-director table
The following example illustrates the problem:    Let's select one raw from qservTest_case01_qserv    {code}  select sourceId, objectId FROM Source LIMIT 1;  +-------------------+-----------------+  | sourceId          | objectId        |  +-------------------+-----------------+  | 29763859300222250 | 386942193651348 |  +-------------------+-----------------+  {code}    Then select it, but use ""sourceId"" in the query, all good here:  {code}  select sourceId, objectId FROM Source WHERE sourceId=29763859300222250;  +-------------------+-----------------+  | sourceId          | objectId        |  +-------------------+-----------------+  | 29763859300222250 | 386942193651348 |  +-------------------+-----------------+  {code}    But if we add ""objectId"", the row is not found:    {code}  select sourceId, objectId FROM Source WHERE sourceId=29763859300222250 and objectId=386942193651348;  Empty set (0.09 sec)  {code}    Similarly, even without sourceId constraint, the query fails:  {code}  select sourceId, objectId FROM Source WHERE objectId=386942193651348;  Empty set (0.09 sec)  {code}    ",8
"Merge BoundedField from HSC as is
To make headway on aperture corrections, we are bringing the HSC implementation of BoundedField over.",2
"Learn about Butler
Transferring knowledge from K-T to the DB team.",2
"Learn about Butler
Transferring knowledge from K-T to the DB team.",2
"Learn about Butler
Transferring knowledge from K-T to the DB team.",2
"Learn about Butler
Transferring knowledge from K-T to the DB team.",2
"Learn about Butler
Transferring knowledge from K-T to the DB team.",2
"Document butler v2 and transfer knowledge to Nate
Clean up and release prototype implementation of Butler v2.",9
"Setting with CoordKey doesn't support non-IcrsCoord arguments
Something in the {{FunctorKey}} template resolution doesn't allow {{Coord}} arguments to be used when setting record values with a {{CoordKey}} (only {{IcrsCoord}} arguments work.",1
"Handle ""where objectId between""
Query in a form:    {code}  select objectId   from Object   where objectId between 386942193651347 and 386942193651349  {code}    currently fails with  {code}  ERROR 4120 (Proxy): Error during execution:  -1 Ref=1 Resource(/chk/qservTest_case01_qserv/6630): 20150602-20:41:43, Complete (success), 0,   Ref=2 Resource(/chk/qservTest_case01_qserv/6631): 20150602-20:41:43, State error (unrecognized), 0,   Ref=3 Resource(/chk/qservTest_case01_qserv/6800): 20150602-20 (-1)  {code}    We already documented that such queries are not advised, but nethertheless qserv should handle it better, e.g., return a message ""not supported"" ",6
"demo package should contain the same comparison script used by CI
The lsst_dm_stack_demo package currently used in the CI system contains a {{bin/compare}} script that doesn't do all of the checks done by the numdiff script that buildbot runs.  These need to be unified, so users can anticipate buildbot results and reproduce buildbot failures locally, especially when making changes to the expected-results file.    While the {{numdiff}} script currently checks more columns than the {{compare}} script, I believe the compare script follows a much better approach and should be extended to be used in both places ({{numdiff}} converts everything to ascii then compares text files; {{compare}} works directly from the binary results and uses NumPy to do the comparisons).",2
"Add transformation tasks for new Butler dataset types
The new Butler dataset types created as part of the HSC deblender merge will need transformation tasks so they can be ingested to the database.    See also DM-2191.",2
"Revisit Parser / IR
Revisit the existing parser code   * consider reusing the code from maxscale or (antrl3) mysql parser from mysqlworkbench, or maybe reuse http://savage.net.au/SQL/sql-92.bnf.html and wrap in bison   * separate IR node productions from grammar",94
"S15 Qserv CSS v2
Revisit Qserv Common State System. Implement mysql-based KV interface, and add support for updates. Implement ""locking"" mechanism. ",36
"wcslib is unable to read PTF headers with PV1_{1..16} cards
SCAMP writes distortion headers in form of PVi_nn (i=1..x, nn=5..16) cards, but this is rejected (correctly) by wcslib 4.14;  there is a discussion at https://github.com/astropy/astropy/issues/299    The simplest ""solution"" is to strip the values PV1_nn (nn=5..16) in makeWcs()  for CTYPEs of TAN or TAN-SIP and this certainly works.    I propose that we adopt this solution for now.  ",1
"LOG() macro fails if message is a simple std::string
lsst:log LOG() macro crash with fatal error if message is a simple string.",2
"Improve confusing error message
Selecting a column that does not exist results in confusing error. Example:    {code}  SELECT badColumnName  FROM qservTest_case01_qserv.Object   WHERE objectId=386942193651348;  {code}    ERROR 4120 (Proxy): Error during execution:  -1 Ref=1 Resource(/chk/qservTest_case01_qserv/6630): 20150605-16:23:42, Error in result data., 1, (-1)    Similarly,     {code}  select whatever   FROM qservTest_case01_qserv.Object;  {code}    prints  ERROR 4120 (Proxy): Error during execution:  -1 Ref=1 Resource(/chk/qservTest_case01_qserv/6630): 20150605-16:23:52, Error in result data., 1,   Ref=2 Resource(/chk/qservTest_case01_qserv/6631): 20150605-16:23:52, Error merging result, 1990, Cancellation requested  Ref=3 Resource(/chk/qservTest_case01_qs (-1)    (note, sourceId does not exist in Object table)      ",5
"Fix broken IN - it now takes first element only
IN is broken - it only uses the first element from the list. Here is the proof:    {code}  select COUNT(*) AS N FROM qservTest_case01_qserv.Source   WHERE objectId=386950783579546;  +------+  | N    |  +------+  |   56 |  +------+  1 row in set (0.10 sec)    mysql> select count(*) AS N FROM qservTest_case01_qserv.Source  WHERE objectId=386942193651348;  +------+  | N    |  +------+  |   39 |  +------+  1 row in set (0.09 sec)    mysql> select COUNT(*) AS N FROM qservTest_case01_qserv.Source  WHERE objectId IN(386942193651348, 386950783579546);  +------+  | N    |  +------+  |   39 |  +------+  1 row in set (0.09 sec)    mysql> select COUNT(*) AS N FROM qservTest_case01_qserv.Source  WHERE objectId IN(386950783579546, 386942193651348);  +------+  | N    |  +------+  |   56 |  +------+  1 row in set (0.11 sec)  {code}",8
"isrTask assumes that the Exposure has a Detector
While trying to use the isrTask to interpolate over bad columns in PTF data I discovered that the code assumes that the Exposure has a Detector attached.    Please remove this restriction.  ",1
"Keep track of database of the director table
An L3 child table might very well have an LSST data release Object table as its director, while almost certainly not living in the DR database. To support it, we should keep track of the database name holding director's table. Note, this is related to DM-2864 - the code touched in that ticket should be checking the director's db name.    Don't forget to add a unit test that will exercise it!",1
"Improve qproc unit testing framework
qproc unit testing framework allow to test the whole query analysis pipeline, it has grow and should be re-organized to be easilly understandable, maintainable.",5
"treat lsst_apps, lsst_libs and lsst_thirdparty as top level products not required by lsst_distrib
Per discussion on RFC-55, it was determined that  lsst_apps and lsst_libs and lsst_thirdparty maybe be treated as separate top level products that lsst_distrib need not depend on them nor do they need to be included as part of CI builds.",1
"Travel to CCIN2P3 to establish realtionship
Travel to CC IN2P3.  of establish operatal coordination between the sites",10
"Prototype file system loading tools for file system studies.  
worked on a tool chain to   -- Extract file IO patterns from program strike.  -- Represent in a flat file.  Generate codes --   -- To make many dependent copies dummy files and directories   -- To generate a python code to read and write files like the original application.  -- To generate a c-code to read and write files like the original application.    a driver program to run the pseudo codes in parallel.    The system runs on toys and needs to be show to work on traces from       ",10
"Host/Attend DM LT meeting 
host/attend DM LT meeting ",7
"Add queries that exercise non-box spatial constraints
Qserv has code to support:   * qserv_areaspec_box   * qserv_areaspec_circle   * qserv_areaspec_ellipse   * qserv_areaspec_poly    but only the first one (box) is exercised in our integration tests. This story involves adding queries to test the other 3.",2
"Clean up gitolite
We need to clean up gitolite and cgit:  * Repositories that have moved to GitHub should be removed (or, possibly, mirrored back from GitHub).  * Empty repositories (like contrib/eups.git) and obsolete repositories (like LSST/DMS/afw_extensions_rgb.git) should be removed altogether.  * contrib/data_products.git (the Data Products Definition Document source) should be moved to GitHub in the lsst org.  * contrib/processFile.git should be moved to GitHub in the lsst-dm org unless the author adds some test cases and it can be integrated into the CI system as a top-level product (in which case it can go into the lsst org).  * The primary authors of other contrib repositories should be contacted to see if they should be moved to GitHub in the lsst-dm or another org (possibly a new lsst-contrib org).  In particular, contrib/plotz/* (Paul Lotz of the Telescope and Site subsystem) and contrib/pyreb (IN2P3 work for the Camera subsystem) contain current work that should be moved.",4
"Update Scons to v2.3.4
Scons has not been updated in over a year. RFC-61 agreed that we should upgrade it now before tackling some other {{scons}} issues.",1
"Add Gaussian PSF example to measurement task documentation
I see some documentation on how to add a placeholder Gaussian Psf to an image (to work around the fact that some algorithms require a Psf) was recently added to the release notes.  I don't think that's actually appropriate, as the same algorithms also required Psfs in the framework - the failure mode was just different (previously, it'd just result in all objects being flagged and the peak position used for the centroid, so it may have been easy to miss - hence the change to a fatal error).    I propose moving the example to the documentation for SingleFrameMeasurementTask, and taking it out of the release notes.  I'll also make sure there's a link from the Measurement Framework Overhaul Release Notes page to the Doxygen for SingleFrameMeasurementTask - I'm not sure if that's sufficient to make up the visibility gap between the Doxygen docs and the release notes in Confluence, but I don't have any other short-term ideas.",1
"Remove unused code from sconsUtils
The code in {{deprecated.py}} in {{sconsUtils}} is not used by anything anywhere. [~jbosch] has indicated that the file can simply be removed.",1
"obs_cfht is broken with the current stack
obs_cfht's camera mapper is missing the new packageName class variable, so it is not compatible with the current stack.    I suggest fixing obs_sdss and obs_subaru as well, if they need it.",1
"Build 2015_07 Qserv Release
See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.",1
"Port HSC Curve-of-Growth code
Port content from HSC-1144, HSC-1236, HSC-1223, HSC-1219, HSC-1203, HSC-1153.",6
"Improve handling of extremely large blends
Port HSC code from issues:  * [HSC-1250|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1250]  * [HSC-1245|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1245]  * [HSC-1237|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1237]  * [HSC-1268|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1268]  * [HSC-1274|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1274]  * [HSC-1265|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1265]  * [HSC-1228|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1228]",10
"Port safe coadd clipping from HSC
We have an algorithm on the HSC fork that modifies AssembleCoaddTask to clip outliers in a much safer way, based on detecting contiguous regions in the difference between a non-clipped coadd and an aggressively-clipped coadd, and only rejecting pixels that are outliers in a single epoch.    One complication for this code transfer is that some of the coadd code has been refactored on the HSC side, and there may be code in hscPipe that duplicates much of what's in pipe_tasks.  We may need help from [~price] to resolve those inconsistencies before tackling this issue.    Once that's done, HSC code can be transferred from the following issues:   - [HSC-1166|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1166]   - [HSC-1202|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1202]",18
"obs_cfht unit tests are broken
obs_cfht has one unit test ""testButler"" that uses git://git.lsstcorp.org/contrib/price/testdata_cfht. 4 of the tests fail, as shown below.    In addition, testdata_cfht is huge, and the tests barely use any of it. It's worth considering making a new test repo that is smaller, or if the amount of data is small enough, move it into afwdata or obs_cfht itself.    {code}  localhost$ tests/testButler.py   CameraMapper: Loading registry registry from /Users/rowen/LSST/code/testdata/testdata_cfht/DATA/registry.sqlite3  CameraMapper: Loading calibRegistry registry from /Users/rowen/LSST/code/testdata/testdata_cfht/CALIB/calibRegistry.sqlite3  ECameraMapper: Loading registry registry from /Users/rowen/LSST/code/testdata/testdata_cfht/DATA/registry.sqlite3  CameraMapper: Loading calibRegistry registry from /Users/rowen/LSST/code/testdata/testdata_cfht/CALIB/calibRegistry.sqlite3  ECameraMapper: Loading registry registry from /Users/rowen/LSST/code/testdata/testdata_cfht/DATA/registry.sqlite3  CameraMapper: Loading calibRegistry registry from /Users/rowen/LSST/code/testdata/testdata_cfht/CALIB/calibRegistry.sqlite3  ECameraMapper: Loading registry registry from /Users/rowen/LSST/code/testdata/testdata_cfht/DATA/registry.sqlite3  CameraMapper: Loading calibRegistry registry from /Users/rowen/LSST/code/testdata/testdata_cfht/CALIB/calibRegistry.sqlite3  .CameraMapper: Loading registry registry from /Users/rowen/LSST/code/testdata/testdata_cfht/DATA/registry.sqlite3  CameraMapper: Loading calibRegistry registry from /Users/rowen/LSST/code/testdata/testdata_cfht/CALIB/calibRegistry.sqlite3  E.  ======================================================================  ERROR: testBias (__main__.GetRawTestCase)  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""tests/testButler.py"", line 122, in testBias      self.getDetrend(""bias"")    File ""tests/testButler.py"", line 110, in getDetrend      flat = self.butler.get(detrend, self.dataId, ccd=ccd)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_persistence/10.1-1-g6edbc00+1/python/lsst/daf/persistence/butler.py"", line 218, in get      location = self.mapper.map(datasetType, dataId)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_persistence/10.1-1-g6edbc00+1/python/lsst/daf/persistence/mapper.py"", line 116, in map      return func(self.validate(dataId), write)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/cameraMapper.py"", line 287, in mapClosure      return mapping.map(mapper, dataId, write)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/mapping.py"", line 118, in map      actualId = self.need(self.keyDict.iterkeys(), dataId)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/mapping.py"", line 199, in need      lookups = self.lookup(newProps, newId)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/mapping.py"", line 345, in lookup      return Mapping.lookup(self, properties, newId)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/mapping.py"", line 168, in lookup      where, self.range, values)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/registries.py"", line 120, in executeQuery      c = self.conn.execute(cmd, values)  OperationalError: no such column: extension    ======================================================================  ERROR: testFlat (__main__.GetRawTestCase)  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""tests/testButler.py"", line 117, in testFlat      self.getDetrend(""flat"")    File ""tests/testButler.py"", line 110, in getDetrend      flat = self.butler.get(detrend, self.dataId, ccd=ccd)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_persistence/10.1-1-g6edbc00+1/python/lsst/daf/persistence/butler.py"", line 218, in get      location = self.mapper.map(datasetType, dataId)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_persistence/10.1-1-g6edbc00+1/python/lsst/daf/persistence/mapper.py"", line 116, in map      return func(self.validate(dataId), write)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/cameraMapper.py"", line 287, in mapClosure      return mapping.map(mapper, dataId, write)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/mapping.py"", line 118, in map      actualId = self.need(self.keyDict.iterkeys(), dataId)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/mapping.py"", line 199, in need      lookups = self.lookup(newProps, newId)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/mapping.py"", line 345, in lookup      return Mapping.lookup(self, properties, newId)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/mapping.py"", line 168, in lookup      where, self.range, values)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/registries.py"", line 120, in executeQuery      c = self.conn.execute(cmd, values)  OperationalError: no such column: extension    ======================================================================  ERROR: testFringe (__main__.GetRawTestCase)  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""tests/testButler.py"", line 127, in testFringe      self.getDetrend(""fringe"")    File ""tests/testButler.py"", line 110, in getDetrend      flat = self.butler.get(detrend, self.dataId, ccd=ccd)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_persistence/10.1-1-g6edbc00+1/python/lsst/daf/persistence/butler.py"", line 218, in get      location = self.mapper.map(datasetType, dataId)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_persistence/10.1-1-g6edbc00+1/python/lsst/daf/persistence/mapper.py"", line 116, in map      return func(self.validate(dataId), write)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/cameraMapper.py"", line 287, in mapClosure      return mapping.map(mapper, dataId, write)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/mapping.py"", line 118, in map      actualId = self.need(self.keyDict.iterkeys(), dataId)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/mapping.py"", line 199, in need      lookups = self.lookup(newProps, newId)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/mapping.py"", line 345, in lookup      return Mapping.lookup(self, properties, newId)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/mapping.py"", line 168, in lookup      where, self.range, values)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/registries.py"", line 120, in executeQuery      c = self.conn.execute(cmd, values)  OperationalError: no such column: extension    ======================================================================  ERROR: testRaw (__main__.GetRawTestCase)  Test retrieval of raw image  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""tests/testButler.py"", line 101, in testRaw      raw = self.butler.get(""raw"", self.dataId, ccd=ccd, immediate=True)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_persistence/10.1-1-g6edbc00+1/python/lsst/daf/persistence/butler.py"", line 244, in get      return callback()    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_persistence/10.1-1-g6edbc00+1/python/lsst/daf/persistence/butler.py"", line 242, in <lambda>      innerCallback(), dataId)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_persistence/10.1-1-g6edbc00+1/python/lsst/daf/persistence/butler.py"", line 238, in <lambda>      callback = lambda: self._read(pythonType, location)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_persistence/10.1-1-g6edbc00+1/python/lsst/daf/persistence/butler.py"", line 426, in _read      location.getCppType(), storageList, additionalData)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_persistence/10.1-1-g6edbc00+1/python/lsst/daf/persistence/persistenceLib.py"", line 1430, in unsafeRetrieve      return _persistenceLib.Persistence_unsafeRetrieve(self, *args)  FitsError:     File ""src/fits.cc"", line 1064, in lsst::afw::fits::Fits::Fits(const std::string &, const std::string &, int)      cfitsio error: could not open the named file (104) : Opening file '/Users/rowen/LSST/code/testdata/testdata_cfht/DATA/raw/08BL05/w2.+2+2/2008-11-01/i2/1038843o.fits.fz[1]' with mode 'r' {0}  lsst::afw::fits::FitsError: 'cfitsio error: could not open the named file (104) : Opening file '/Users/rowen/LSST/code/testdata/testdata_cfht/DATA/raw/08BL05/w2.+2+2/2008-11-01/i2/1038843o.fits.fz[1]' with mode 'r''      ----------------------------------------------------------------------  Ran 6 tests in 3.544s    FAILED (errors=4)  {code}",1
"PhotoCalTask mis-calling Colorterm methods
When I implemented DM-2797 I made a few errors in pipe_tasks:  - PhotoCalTask mis-calls two methods of Colorterm by providing filterName, which is not needed  - ColortermLibrary.getColorterm mis-handles glob expressions (the two arguments to fnmatch.fnmatch are swapped).    We also need a unit test for applying colorterms, but that will require enough work that I have made a separate ticket for it: DM-2918. Meanwhile I have tested my changes by running Dominique's CFHT demo. This proves that the colorterm code runs, but does not prove that the terms are correctly applied.",1
"Clean up code in afw for Approximate background estimation
The intention is to eventually set {{useApprox=True}} (i.e. Chebychev Approximation)  as the default for background estimation.  However, in looking into the relevant code in afw/math while working on DM-2778, there is some clean-up and restructuring that needs to be done before resetting the defaults (which may also require adjusting some defaults in the calibrate stage to be more appropriate for the approximation, as opposed to inperpolation, scheme).  This issue is to clean up the code and make sure it all operates coherently.  A seperate ticket will be to actually reset the defaults and make any other config default changes required.    In particular, the config setting of approxOrderX/binSize are not being assessed properly, nor is the behavior of the given undersampleStyle being executed.  The under-sampling checks are currently only being done against the interpoation settings, which is not appropriate when useApprox=True.  A temporary check was added in {{meas.algoritms.detection.getBackground()}} in DM-2778 so that it is currently ""safe"" to run with useApprox=True and any other user overridden config setting (binSize, approxOrder, undersampleStyle) (and there currently exists similar checks in {{pipe.tasks.matchBackground}}), but these should be removed once this issue has been implemented. ",3
"Set Approximation as default for background subtraction
Once the Approximate code in {{afw.math}} has been cleaned up (see DM-2920), set the default for background subtraction to be the Chebychev Approximation (i.e. useApprox=True).  Ensure any other relevant config defaults (e.g. binSize, approxOrderX) are adjusted appropriately.  This will change the outputs of the {{lsst_dm_stack_demo}}, so the ""expected"" files will need to be replaced along with this change in default settings (see DM-2778 for some comparisons of the demo outputs using the interpolation vs. approximation background estimation schemes).",1
"Initial DC Base dseign
Prepare a Document for the Initial Design of the Base and Summit Networks",6
"Port HSC-1199 to LSST (UNMASKEDNAN mask propagates to all amplifiers)
Port issue HSC-1199 to LSST stack to address UNMASKEDNAN mask propagates to all amplifiers",4
"Add RFD issue type to RFC project
To support the RFD process adopted in [RFC-53], an RFD issue type in the RFC project is required.  While we could add RFD-specific fields to it, I think it's simplest if it's just generic with details provided in the Description.",1
"Modernize sconsUtils code to python 2.7 standard
As part of the work investigating DM-2839 I modernized the sconsUtils code to meet current coding standards (using {{in}} rather than {{has_key}}, using {{items()}} rather than {{iteritems}} etc). Since I'm highly doubtful that DM-2839 is going to be closed any time soon I will separate out the modernization patches into this ticket.",1
"move old ingest scripts into and retire old packages
This ticket implements RFC-57, by:   - renaming datarel to daf_ingest (there is already a daf_ingest package, but it's *completely* empty, so I'll just force-push it all away)   - removing everything from the renamed package that doesn't relate to ingest (including pruning dependencies)   - removing ap and testing_endToEnd from the CI system  ",1
"Some AFW tests are not enabled with no explanation
Running {{coverage.py}} on the AFW test suite indicated that two test classes in {{tests/wcs1.py}} are disabled. {{WCSTestCaseCFHT}} was added by [~rhl] in 2007 but disabled during a merge a long time ago by [~jbosch] in 2010 but with no indication as to why. {{WCSRotateFlip}} appeared in 2012 (added by [~krughoff]) but doesn't appear in the {{suite}} list at the end and so does not execute.    Similarly {{testSchema.py}} has two tests that are not run: {{xtestSchema}} and {{testJoin}}. I assume {{xtestSchema}} is deliberately disabled but could there at least be a comment in the test explaining why?    My feeling is that we should either run the tests or they should be removed. Having them their gives the impression they are doing something useful.    Less importantly, {{warpExposure.py}} has some support code for comparing masked images that was written in 2009 by [~rowen] but which is not used anywhere in the test.",2
"Fix problem with Qserv related to restarting mysql
I noticed some strange (reproducible!) behavior: if I run:    {code}qserv-check-integration.py --case=01{code}    then restart mysqld    {code}<runDir>/etc/init.d/mysqld restart{code}    then the query:  {code}mysql --host=127.0.0.1 --port=4040 --user=qsmaster   qservTest_case01_qserv -e   ""SELECT COUNT(*) as OBJ_COUNT   FROM Object   WHERE qserv_areaspec_box(0.1, -6, 4, 6)""{code}    consistently fails every single time.    To fix it, it is enough to restart xrootd.",5
"We write truncated Wcs data to  extended HDU tables in Exposures
When we write Wcs to extra HDUs in Exposures they are truncated if other than TAN/TAN-SIP.  Please don't write them.    A better long term solution is needed.  In particular, we shouldn't be duplicating this information unnecessarily, and we need to be able to persist e.g. TPV to the tables so as to support CoaddPsf.  These issues are not included here.",1
"Test install of OCS software on CentOS VMs
Install current version of the OCS software onto two VMs",24
"qserv-admin CREATE NODE fails
{noformat}  qserv > CREATE NODE worker1 type=worker host=worker-1 port=5012 runDir=1;  06/15/2015 05:59:52 QADM ERROR: Missing parameter. (mysqlConn)  ERROR:  Missing parameter. (mysqlConn)  {noformat}  ",1
"Refactor Histogram in edu.caltech.ipac.visualize.plot package.
The Histogram has 6 constructors to handle 6 bitpixel data types which are byte, short integer,  integer, long integer, float and double.  Since FitsRead has now only works on float, there the  Histogram should be refactored accordingly.",3
"CalibrateTask has an unwanted ""raise"" in it
On 2014-06-30 commit 696b641 a developer added a bare ""raise"" as a debugging aid to the CalibrateTask in pipe_tasks. That change was accidentally merged to master. I confirmed it was an accident and am filing this ticket as a way to remove the raise and run buildbot before merging to master.",1
"fix usage of obsolete astrometry interfaces in ProcessImageTask
As discussed recently on HipChat (Science Pipelines Standup), there's code in {{ProocessImageTask}} that assumes an ""astrometer"" attribute on a {{CalibrateTask}} instance.  Since this is just needed to match a new set of sources against the reference catalog here, we should probably be using one of the new matcher objects, either by getting one from {{CalibrateTask}} via a documented interface, or by constructing a new one.",4
"DS9 tests fail if DS9 not running in some configurations
There are a few issues with the robustness of the {{testDs9.py}} tests in AFW.    * The tests are skipped if the {{display_ds9}} package can not be loaded but they should also skip if {{ds9}} is missing or if {{ds9}} can not be loaded. The latter is especially important during builds that unset {{$DISPLAY}}.  * The launching code in {{initDS9}} can not notice the simple case of {{ds9}} immediately failing to load. It simply assumes that there are delays in launch. The reason for this is that {{os.system}} does not return bad status if the command has been started in the background. Another scheme for starting {{ds9}} should be considered. Maybe a different exception could be raised specifically for failing to start it.  * At the moment each test independently has a go at starting {{ds9}}. This makes the tests take a very long time (made worse by {{_mtv}} also trying multiple times) despite it being clear pretty quickly that {{ds9}} is never going to work.  * Currently the {{mtv}} tests must run early as they are the only tests that attempt to start {{ds9}} if it is not running. If the two tests that call {{mtv}} are disabled two other tests fail. Ideally the {{initDS9}} code should be called in all cases.",1
"Wmgr refuses to serve queries from remote interface
Vaikunth discovered that wmgr returns 404 for all operations. It looks like wmgr can serve requests coming from 127.0.0.1 interface but returns 404 for queries from non-local interface.",1
"Remove explicit buildbot dependency on datarel
The buildbot scripts have an explicit dependency on the {{datarel}} package, which we'd like to remove from the stack.  It uses {{datarel}} as the top-level product when building the cross-linked HTML documentation; {{lsstDoxygen}}'s {{makeDocs}} script takes a single package, and generates the list of packages to include in the Doxygen build by finding all dependencies of that package.    So, to remove the explicit dependency on {{datarel}}, we need to either:   - find a new top-level product with a Doxygen build to pass to {{makeDocs}} (e.g. by adding a trivial Doxygen build to {{lsst_distrib}})   - modify the argument parsing in {{lsstDoxygen}} to take a list of multiple products (it *looks* like the limitation to one package is only in the argument parsing), and pass it a list of top-level products in the buildbot scripts.    This is currently a blocker for DM-2928, which itself a blocker for DM-1766, which has now been lingering for a few weeks now.  I'm going to look for other ways to remove the block on the latter, but I don't have a solution yet.",3
"remove dead code and dependencies from datarel
Removing the {{datarel}} package entirely has proved to be difficult (DM-2928, DM-2948), so instead I'm simply going to remove non-ingest code (and dead ingest code) from the package, along with its dependencies on {{ap}} and {{testing_endToEnd}}.  Other dependencies will be retained even if they aren't necessary for the code that will remain in {{datarel}}, to support {{lsstDoxygen}}'s use of {{datarel}} as a top-level package for documentation generation.",1
"Refactoring the class CropAndCenter
This class contains the codes which are not used.  It needs to be simplified and refactored. ",4
"Crop needs to be refactored
This class needs to be refactored to be in consist with FitsRead class which treats all data type as float.  Thus the bitpix in this class does not have to be treated based on its value.",3
"Qserv code cleanup and auto_ptr --> unique_ptr migration
Code cleanup, including migrating some parts to c++11 (in particular, auto_ptr --> unique_ptr)",4
"Add a unit test for aperture corrections in measurement task
DM-436 adds code to meas_base that allows one to run a subset of measurement algorithms based on execution order. This addition should have a unit test.    DM-436 also tasks to measure and apply aperture correction. Those tasks should have unit tests.",6
"Setting up and running PhoSim for Psf Library
Debbie Bard leaving created some new work creating the Psf Libraries we need.  While this od not a major task except for computer time, there is some setup required.  I will get an account at SLAC and learn to run the PhoSim utilities she and Michael have developed.    In the short term, Simon is going to do some runs for me.  Meanwhile, I will get into Debbie's account and run her configuration at SLAC.    The outputs then need to be checked to be sure that the Psfs are reasonable.",4
"Migrate Qserv code to nullptr
nan",3
"Review ITIL V3.0 as prep for input to IT use case
ITIL is a standard breakdown of processes used in an IT system.  While full ITIL may very well be too heavy LSST operations, it provides a useful checklist for he use cases being developed in the TOWG.   I created an ITIL type spreadsheet to check against the dump of the workflows in the EA  tool  ",3
"add metric to  application specific IO benchmarking tool
iosim is an application-specific benchmarking tool that is developed to be responsive to the request from LSST to select and investigate file systems ahead of actual benchmarks, and in advance of having workflow and other infrastructures needed to investigate file systems under realistic load.  The week the software was    -- Optimized to allow for faster test cycles.   -- Threads were supported by squashing all thread IO into a single simulation process.  -- Information from the original ""model"" program is propagated to the simulation program, allowing for comparison to the model.  -- initial matplotlib plots allowing a degree of visualization of the performance of a simulated run was added    The goals to deliver this capability in a few week when common systems at NCSA are available for testing.",4
"Management for Don for week of June 15
On boarded Mattais Carrasco-Kind to work in the process execution in the context of Level 3 processing.   Misc.",4
"Port the psfextractor external library from HSC to LSST
nan",14
"Prototype iRODS tiered resource with NERSC HPSS
iRODS can support access to a tape archive with the use of a ""tiered resource"" where one resource has the role of the cache, and a second has the role of archive.    Use of such a tiered resource construct could be valuable to data management.   Because a iRODS plugin for HPSS is readily available, we examine the set up and use of the tiered resource testing against NERSC HPSS.",4
"Read through and comment on latest version of LSE-78
nan",1
"Design CSS that supports updates
Design how to redesign CSS, we currently take a snapshot when char starts. It is too static. ",2
"Fix to DM-2883 isn't quite right
The fix to DM-2883 (remove illegal PVi_j cards) isn't quite right, and the error was masked by a piece of code elsewhere that duplicated the functionality.    The issues is that while PV1_[1-4] cards are indeed valid, the ones that SCAMP writes are not.  So we should remove them too, if there are any other SCAMP TPV coefficients.    The masking code was a unilateral removal of PVi_j cards dating back years.  ",1
"Discourse evaluation (Part 1)
Work in support of evaluation Discourse as a DM platform for internal and external interactions.",3
"Quantify how much objects are blended
It would be useful to have a parameter that indicates how much any given galaxy is blended. This will be useful for testing how photometry or shears are affected by blending effects.    Ports code from HSC-1260.",2
"SourceCatalog.getChildren requires preconditions but does not check them
This is a code transfer from HSC-1247.",2
"Miscellaneous CModel improvements from HSC
This improves handling of several edge case failure modes, tweaks the configuration to improve performance, and adds some introspection useful for Jose Garmilla's tests.    Includes HSC-1288, HSC-1284, HSC-1228, HSC-1250, HSC-1264, HSC-1273, HSC-1240, HSC-1249, HSC-1238, HSC-990, HSC-1155, HSC-1191",2
"FootprintMerge: fix bug when identifying existing peaks in a merge.
If two separate footprints from the same catalog happen to be merged because an existing merged object overlaps both of them, the flags of which peaks are being detected in which bands is not being propagated. This is causing the apparent dropout of some sources in a merged catalog which were detected in single frame processing.    Taken from ticket HSC-1270",1
"refactor coaddition code
The HSC fork has coaddition code in two places: pipe_tasks and hscPipe.  The code in hscPipe is what we use (though that depends on the code in pipe_tasks in places), while the code in pipe_tasks is more similar to what's currently on the LSST side.    We want to bring the refactored version in hscPipe back to LSST, but we want to put it directly in pipe_tasks to remove the code duplication that currently exists on the HSC side.    Work on this issue should begin with an RFC that details the proposed changes.    Note that this should not bring over the ""safe coadd clipping"" code, which is DM-2915.",5
"polygon masking in CoaddPsf
We need to create polygon-based masks of the usable area of the focal plane, persist them with exposure, and include them in coaddition of PSFs and aperture corrections.    This includes HSC issues HSC-972, HSC-973, HSC-974, HSC-975, HSC-976.    At least some of this will be blocked by DM-833, which is the port issue for coaddition of aperture corrections.",8
"Updating node status in qserv-admin to INACTIVE fails
In qserv-admin.py when attempting to update a node status from ACTIVE to INACTIVE the following error is produced:    {code}  > update node worker2 state=INACTIVE;  Traceback (most recent call last):  File ""/usr/local/home/vaikunth/src/qserv/bin/qserv-admin.py"", line 650, in <module>  main()  File ""/usr/local/home/vaikunth/src/qserv/bin/qserv-admin.py"", line 645, in main  parser.receiveCommands()  File ""/usr/local/home/vaikunth/src/qserv/bin/qserv-admin.py"", line 163, in receiveCommands  self.parse(cmd[:pos])  File ""/usr/l  ocal/home/vaikunth/src/qserv/bin/qserv-admin.py"", line 180, in parse      self._funcMap[t](tokens[1:])    File ""/usr/local/home/vaikunth/src/qserv/bin/qserv-admin.py"", line 380, in _parseUpdate      self._parseUpdateNode(tokens[1:])    File ""/usr/local/home/vaikunth/src/qserv/bin/qserv-admin.py"", line 405, in _parseUpdateNode      self._impl.setNodeState(**options)    File ""/usr/local/home/vaikunth/src/qserv/lib/python/lsst/qserv/admin/qservAdmin.py"", line 660, in setNodeState      self._kvI.set(nodeKey, state)    File ""/usr/local/home/vaikunth/src/qserv/lib/python/lsst/qserv/css/kvInterface.py"", line 415, in set      self._zk.set(k, v)    File ""/usr/local/home/vaikunth/qserv/Linux64/kazoo/2.0b1+1/lib/python/kazoo-2.0b1-py2.7.egg/kazoo/client.py"", line 1170, in set      return self.set_async(path, value, version).get()    File ""/usr/local/home/vaikunth/qserv/Linux64/kazoo/2.0b1+1/lib/python/kazoo-2.0b1-py2.7.egg/kazoo/client.py"", line 1182, in set_async      raise TypeError(""value must be a byte string"")  {code}",1
"Backport HSC parallelization code
Assuming RFC-68 is approved, transfer the HSC code to LSST as described there.",4
"Compare LSST and HSC pipelines through ISR
Run both the HSC and LSST ISR routines on two to three visits worth of HSC engineering data. Compare the results. Where differences exist, either:    * Create and work other tickets to resolve them;  * Explain their origin and why we don't think they are a problem.",10
"Integrate javascript build with gradle
Integrate javascript build tools webpack with gradle.",2
"Conversion of FITS binary table extension to IPAC table format. 
FITS binary table contains data types and structures that cannot map directly to IPAC table.  We need to define ways to handle these cases.",6
"Modify IpacTableParser to support extra wide table.
IpacTableParser fail to load IPAC table with extra wide headers and columns.  Replace the logic for reading headers and columns information so that it will support any file/size.",2
"XY plot need to be able to handle multiple tables with the same name
XY plot was relying on a table request object to cache previously loaded tables.  This was done for performance reason.  However, table request is not reliable since the same request may be submitted multiple times.",2
"Add XYPlot to Python interface
Make it possible to add plots (not connected to a displayed table) to StandaloneUI.  Add showXYPlot to python API.",6
"Firefly External Task Launcher
Implement and external task launcher, which forks a [python] process and gets back the results. The results can be a table, an image, or a JSON.    ",10
"Search processors to get image, table, or json from an external task
Implement three search processors, which use the External Task Launcher (DM-2991):    - to get a table (possibly in binary FITS format)  - to get an image  - to get JSON",8
"Products must not depend on anaconda
{{setupRequired(anaconda)}} should be removed from webservcommon.table.    We want to keep the stack buildable with any python 2.7, and should not explicitly depend on anaconda.",1
"Understand and improve error code management
It seems there is several constants to store Qserv error code (for example see msgCode.h and util::ErrorCode, or MsgState::RESULT_ERR,JobStatus::RESULT_ERROR). This could certainly be simplified and clarified?    Furthermore in util::Error it seems there's a confusion between code and statuses",8
"Bump eups anaconda package to 2.2
By popular request. ",1
"Begin to write a note for the TOWG using ITIL as a checklist
Began to work use cases, and found that they were long, considering the number r of aspects that need to be considered,  Presented to the TOWG,  got guidance to think in terms of processes, but to take that the effort estimates would  have a reasonable basis.  Got guidance to think about the sites's needs but to congress a central approach.    Agree that this would be the struggle for the week.",4
"Management work for Don in the week of June 22
Internal and external recruiting.   Input on NCSA re-organizaiton to ensure proper placement of LSST activities in the NCSA organization.  Meetings.",3
"Whitepaper submission to NSF Cyber Summit
Working on drafting whitepaper and abstract for SCADA security challenges faced by LSST.",1
"ISO presentation to all-hands meeting
Presentation giving overview of ISO work, esp. w.r.t AUP and master security plan.",1
"Extend the Process Execution Framework to accomodate changes needed by SUI and others
Extend the Process Execution Framework to accomodate changes needed by SUI and others by changing the task and configuration classes.    Covers effort from July 1st - August 31st at 0.25 FTE.",22
"Preliminary Process Execution Framework work
Preliminary work to extend the Process Execution Framework to accomodate changes needed by SUI and others.    This story captures work done in June, prior to incorporating the activity into the baseline plan. Work will be logged under DM-3003 starting July 1st.",4
"prepare jenkins ""demo"" for usage as an interim CI system
We have a working plan of putting the buildbot-scripts under jenkins demo into usage as a production CI system as an intermediate step towards a fully decomposed build.",15
"complete puppet jenkins native type implimentation and merge upstream
nan",20
"Meeting with CTSC at CLHS Portland OR
Discussed LSST security plan going forward.  Specifically work on SCADA security plan.  Meeting held at conference in Portland OR, June 14th, ACM CLHS.",1
"Do more research into Flux modules and bring one in 
nan",16
"expose stretch to python API
nan",2
"Improve region support
Some parts of the region support has been more testing because of the python interface.  It is now clear what we should do.",4
"Convert Color Stretch dialog to React/flux/JavaScript
nan",4
"Review LSE-78
Review either the current version #26 and/or the newest version when it become available.",2
"Discuss US WAN options with NCSA ESnet representative
nan",1
"Provide network support for ceph and openstack lsst storage server efforts
work done to provide network connectivity, troubleshoot and monitor connections for the above efforts",1
"DLP/LDM-240 support chages
  - JIRA changes to create DLP project    - lsst-sqre/sqre-jirakit to generate LDM-240-like display     - iterate with T/CAMs, Kevin, Jeff",6
"Early access user onboarding and feedback 
Getting comments, testing, hipchat/JIRA changes",2
"Display stories in JIRA epic table display
  Solved with Issue Matrix plugin; unfortunately this removed the ""create issue in this epic"" functionality, so that needs to be a new ticket.",1
"Set up Slack for evaluation
  Free account procured and tested by various volunteers; next step is to apply for non-profit status which gives us the first paid tier free to 100 users. ",1
"Set up Discourse for evaluation.
  Server up on DO at community.lsst.org. Email needs fixing before volunteer users can be invited. ",1
"Addressing File corruption in iRODS 4.1.x
We examine solutions for repairing corrupt files within an iRODS 4.1.x zone.",2
"Read revised LSE-209 and LSE-70
Read over the revised LSE-209 and LSE-70 documents",4
"Add Sdss3Mapper to ingest, convert and map SDSS-III ""frame"" files
SDSS-III does not use the fpC file format for science images.  Science images are now released as [""frame"" files. | http://data.sdss3.org/datamodel/files/BOSS_PHOTOOBJ/frames/RERUN/RUN/CAMCOL/frame.html]  The primary science image (hdu0) comes background subtracted and calibrated to units of nanomaggies, with the backgrounds and flat-field conversions included as extensions. The astrometric information is in hdu3 instead of a separate asTrans file.     obs_sdss should be able to ingest frame files and map them to load as dataset ""raw."" It should also optionally replace the backgrounds and de-calibrate to convert the units back from nanomaggies to counts.     This will be implemented as lsst.obs.sdss.sdssMapper.Sdss3Mapper.",5
"Margaret's mgmt. activities in June
nan",14
"Check czar->proxy messages size
These messages are stored in VARCHAR(255) (FYI, MEMORY tables can't contain TEXT). We just need to make sure we have a reasonable fixed size CHAR (and maybe check whether we are hitting the limit, and log it somewhere)",4
"Move Qserv code comment to LSST documentation standards
LSST documentation standards: https://confluence.lsstcorp.org/display/LDMDG/Documentation+Standards#DocumentationStandards-RequiredDocumentationStyle  is different from the previous standards used by Qserv (i.e. /// text).     We should convert everything to LSST documentation standards.",6
"remove lsst/log wrapper from Qserv
lsst/log API looks stable now, so removing the wrapper would simplify the code.",1
"Debug problems with Qserv at scale
nan",20
"Implement test suite for new class SqlTransaction
Some test that shows that transactions are properly committed/aborted would be nice to have.",1
"Remove unused function populateState() 
Qserv doesn't seem to relaunch no more chunk query in case it fails (see DM-2643)    And this function is now unused:  {code:bash}  qserv@clrinfopc04:~/src/qserv (master)$ grep -r populateState core/  core/modules/qdisp/Executive.cc:void populateState(lsst::qserv::qdisp::ExecStatus& es,  {code}  ",1
"LOE - Week ending 7/10/15
nan",1
"LOE - Week ending 7/17/15
nan",1
"LOE - Week ending 7/24/15
nan",2
"LOE - Week ending 7/31/15
nan",2
"Bi-weekly meeting with Victor and Iain.
nan",2
"Incident response report template
nan",1
"Incident response security work plan document
nan",2
"Creation of XML descriptions of messages sent to OCS
Create XML descriptions of messages sent to the OCS. Upload these to a new github repository.",4
"Resolve segmentation fault in LoggingEvent destructor
There seems to be a possible race condition in log4cxx::spi::LoggingEvent::~LoggingEvent. I've had multiple segmentation faults in that function. In all cases, another thread was involved in writing. In at least 2 cases, the second thread was in XrdCl::LogOutFile::Write.  ",5
"RFI with prospective DWDM vendors for Chile National Networks
Hold Request for Information meetings in Chile with equipment vendors. ",12
"Add ""ORDER BY"" clause to lua SQL query on result table
If user query has ""ORDER BY"", then lua  can't just execute ""SELECT * FROM result"" because the order for such query is not guaranteed. To fix that, we need to add ""ORDER BY"" clause to the ""SELECT * FROM result"" query on the lua side.    Once we have the above, we might want to remove ""ORDER BY"" from the query class which runs a merge step on the czar (this has to be done in query analysis step).",8
"Add assertXNearlyEqual methods for image-like classes
Presently one can compare two image-like objects using free functions imagesDiffer, masksDiffer and maskedImagesDiffer in lsst.afw.image.testUtils. These should be replaced by assertXNearlyEqual methods that afw adds to lsst.utils.tests, as per DM-2193.    If necessary, we could leave the old functions around for awhile. But I would prefer to simply get rid of them if we can.    One subtlety is that the current functions take numpy arrays, not afw image-like class instances. Examine the existing users of the code to determine how best to deal with that.",4
"Add slot for calibration flux
This is a port of [HSC-1005|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1005].",2
"Use aperture flux for photometric calibration
This is a port of work performed on HSC but without a ticket. Relevant commits are:    * [05bef6|https://github.com/HyperSuprime-Cam/meas_astrom/commit/05bef629adc37e44ea8482aab88e2eb38a47e3a0]  * [4a6be5|https://github.com/HyperSuprime-Cam/meas_astrom/commit/4a6be51c53f61e70f151de7f29863cb723197a99]  * [69d35a|https://github.com/HyperSuprime-Cam/obs_subaru/commit/69d35a890234e37c1142ddbeff43e62fe36e6c45]  * [9c996d|https://github.com/HyperSuprime-Cam/obs_subaru/commit/9c996d75c423ce03fb54c4300d9c7561b5c1ea99]",1
"Add support for accessing schema from QueryContext
When we are analyzing a query, sometimes there are situations where we need to know the schema of tables involved in a query. It will also be useful for checking if user is authorized to run query, and for queries like ""SHOW CREATE TABLE"". This story involves writing code that will provide access to schema.",3
"qserv code cleanup
I made some random cleanup of the qserv code while playing with css v2. I want to push these changes to master, thus I am creating this story for this. It involves improvements to logging in UserQueryFactory and Facade (both are now per-module), removing unnecessary namespace qualifiers, and whitspace cleanup.",1
"Convert major portion of GWT in Firefly to pure JavaScript (W16)
Continue to convert GWT portion of Firefly to pure JavaScript",100
"Enable aperture correction in the integration test
The present integration test does not enable aperture correction. This should be enabled and the results sanity-checked.    This is a separate ticket rather than DM-436 at Jim Bosch's suggestion, to avoid ticket bloat.    It requires two separate changes:  - update obs_sdss's SdssCalibrateTask to measure and apply aperture correction  - update the expected results from the integration test lsst_dm_stack_demo",4
"Attend CCS-DAQ-OCS-DM Workshop IV
nan",6
"whitepaper CFP for nsf cyber summit
nan",2
"Execution Framework prototype
nan",8
"Graphical communication interface
Creating a graphical representation of execution framework",2
"basic monitoring of jenkins nodes with notification
This last weekend, the build slaves el6-2 and el7-2 ran out of disk space and were causing stack-os-matrix build failures.  We should have an active monitoring system that sends notifications via at least one of hipchat/email/pagerduty.  There is disk utilitization information present in jenkins itself, aws cloudwatch, and ganglia (as of v0.2.x of the demo).  However, it may make be more convenient (read: expedient) to use a dedicated monitoring system such as sensu instead of mining existing data sources.    We should also investigate if we can configure jenkins to not schedule jobs on a slaves with low disk space.",6
"gcc 4.8 package does not create a symlink bin/cc
I created a new lsst package named ""gcc"" that contains Mario's gcc 4.8 package. I used it to build lsst_distrib on lsst-dev and it worked just fine. Unfortunately the package does not include bin/cc (which should be a symlink to bin/gcc), and this is wanted because the LSST build system uses cc to build C code.    The desired fix is to modify the installer to make a symlink bin/cc that points to bin/gcc.",2
"add ""dax_"" prefix to data access related packages
As agreed at [Data Access Mtg 2015/07/13|https://confluence.lsstcorp.org/display/DM/Data+Access+Meeting+2015-07-13], add dax_ prefix towebserv, webservcommon, webserv_client, dbserv, imgserv, metaserv",1
"Implement Result Streaming in Qserv
Qserv supporting streaming results while the query is running to client application.",60
"FY19 Handling unexpected conditions during query execution
Detect and handle unexpected conditions during query execution (e.g. bad chunk, hit per user resource limit)",53
"Add & use new mask plane for out-of-bounds regions
Add a new mask plane for regions with no data - fully vignetted, edge patches in coadd.    This is a port of [HSC-669|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-669].",2
"Handle bad pixels in image stacker
We currently OR together all mask bits, but we need to be cleverer about how we handle pixels that are bad in some but not all inputs.    This is a port of work carried out on [HSC-152|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-152].",1
"Open day for RFP from Equipment vendors
Open day with vendors to describe the needs and requirements of the mountain to base and La Serena to Santiago networks. This is the first formal meeting in the procurement process. Vendors will be invited to propose their solution along with cost.",8
"HSC backport: extra ""refColumn"" class attributes in multiband
This is a transfer for changesets for [HSC-1283|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1283].  ",1
"add gcc to list of packages in lsstsw
Add gcc to the list of packages in etc/repos.yaml in lsstsw",1
"Reduce verbosity of astrometry
The astrometry.net solver that runs by default in meas_astrom 10.1 is very verbose.  Here's an example running HSC data with an SDSS reference catalog:  {code}  $ processCcd.py /tigress/HSC/HSC --output /tigress/pprice/lsst --id visit=904020 ccd=49 --clobber-config  : Loading config overrride file '/home/pprice/LSST/obs/subaru/config/processCcd.py'  WARNING: Unable to use psfex: No module named extensions.psfex.psfexPsfDeterminer  hscAstrom is not setup; using LSST's meas_astrom instead  Cannot import lsst.meas.multifit: disabling CModel measurements  Cannot import lsst.meas.extensions.photometryKron: disabling Kron measurements  Cannot enable shapeHSM ('MEAS_EXTENSIONS_SHAPEHSM_DIR'): disabling HSM shape measurements  Cannot import lsst.meas.extensions.photometryKron: disabling Kron measurements  Cannot enable shapeHSM ('MEAS_EXTENSIONS_SHAPEHSM_DIR'): disabling HSM shape measurements  : Loading config overrride file '/home/pprice/LSST/obs/subaru/config/hsc/processCcd.py'  : input=/tigress/HSC/HSC  : calib=None  : output=/tigress/pprice/lsst  CameraMapper: Loading registry registry from /tigress/pprice/lsst/_parent/registry.sqlite3  CameraMapper: Loading calibRegistry registry from /tigress/HSC/HSC/CALIB/calibRegistry.sqlite3  processCcd: Processing {'taiObs': '2013-11-02', 'pointing': 671, 'visit': 904020, 'dateObs': '2013-11-02', 'filter': 'HSC-I', 'field': 'STRIPE82L', 'ccd': 49, 'expTime': 30.0}  processCcd.isr: Performing ISR on sensor {'taiObs': '2013-11-02', 'pointing': 671, 'visit': 904020, 'dateObs': '2013-11-02', 'filter': 'HSC-I', 'field': 'STRIPE82L', 'ccd': 49, 'expTime': 30.0}  processCcd.isr WARNING: Cannot write thumbnail image; hsc.fitsthumb could not be imported.  afw.image.MaskedImage WARNING: Expected extension type not found: IMAGE  processCcd.isr: Applying linearity corrections to Ccd 49  processCcd.isr.crosstalk: Applying crosstalk correction  afw.image.MaskedImage WARNING: Expected extension type not found: IMAGE  : Empty WCS extension, using FITS header  processCcd.isr: Set 0 BAD pixels to 647.04  processCcd.isr WARNING: There were 6192 unmasked NaNs  processCcd.isr WARNING: Cannot write thumbnail image; hsc.fitsthumb could not be imported.  processCcd.isr: Flattened sky level: 647.130493 +/- 12.733898  processCcd.isr: Measuring sky levels in 8x16 grids: 648.106765  processCcd.isr: Sky flatness in 8x16 grids - pp: 0.024087 rms: 0.006057  processCcd.calibrate: installInitialPsf fwhm=5.88235294312 pixels; size=15 pixels  processCcd.calibrate.repair: Identified 80 cosmic rays.  processCcd.calibrate.detection: Detected 303 positive sources to 5 sigma.  processCcd.calibrate.detection: Resubtracting the background after object detection  processCcd.calibrate.initialMeasurement: Measuring 303 sources (303 parents, 0 children)   processCcd.calibrate.astrometry: Applying distortion correction  processCcd.calibrate.astrometry: Solving astrometry  LoadReferenceObjects: read index files  processCcd.calibrate.astrometry.solver: Number of selected sources for astrometry : 258  processCcd.calibrate.astrometry.solver: Got astrometric solution from Astrometry.net  LoadReferenceObjects: getting reference objects using center (1023.5, 2084.5) pix = Fk5Coord(320.3431396, 0.5002365, 2000.00) sky and radius 0.00194896 rad  LoadReferenceObjects: search for objects at Fk5Coord(320.3431396, 0.5002365, 2000.00) with radius 0.111667372351 deg  LoadReferenceObjects: found 495 objects  LoadReferenceObjects: trimmed 257 out-of-bbox objects, leaving 238  processCcd.calibrate.astrometry.solver: Fit WCS: use iter 2 because it had less linear scatter than the next iter: 0.307471 vs. 0.320229 pixels  processCcd.calibrate.astrometry: 186 astrometric matches  processCcd.calibrate.astrometry: Refitting WCS  processCcd.calibrate.astrometry: Astrometric scatter: 0.047945 arcsec (with non-linear terms, 174 matches, 12 rejected)  processCcd.calibrate.measurePsf: Measuring PSF  /tigress/HSC/LSST/stack10_1/Linux64/anaconda/2.1.0-4-g35ca374/lib/python2.7/site-packages/numpy/core/_methods.py:59: RuntimeWarning: Mean of empty slice.    warnings.warn(""Mean of empty slice."", RuntimeWarning)  /tigress/HSC/LSST/stack10_1/Linux64/anaconda/2.1.0-4-g35ca374/lib/python2.7/site-packages/numpy/core/_methods.py:71: RuntimeWarning: invalid value encountered in double_scalars    ret = ret.dtype.type(ret / rcount)  /home/pprice/LSST/meas/algorithms/python/lsst/meas/algorithms/objectSizeStarSelector.py:143: RuntimeWarning: invalid value encountered in less    update = dist < minDist  processCcd.calibrate.measurePsf: PSF star selector found 163 candidates  processCcd.calibrate.measurePsf: PSF determination using 114/163 stars.  processCcd.calibrate.repair: Identified 92 cosmic rays.  processCcd.calibrate: Fit and subtracted background  processCcd.calibrate.measurement: Measuring 303 sources (303 parents, 0 children)   processCcd.calibrate.astrometry: Applying distortion correction  processCcd.calibrate.astrometry: Solving astrometry  processCcd.calibrate.astrometry.solver: Number of selected sources for astrometry : 258  Solver:    Arcsec per pix range: 0.153025, 0.18516    Image size: 2054 x 4186    Quad size range: 205.4, 4662.78    Objs: 0, 50    Parity: 0, normal    Use_radec? yes, (320.343, 0.500178), radius 1 deg    Verify_pix: 1    Code tol: 0.01    Dist from quad bonus: yes    Distractor ratio: 0.25    Log tune-up threshold: inf    Log bail threshold: -230.259    Log stoplooking threshold: inf    Maxquads 0    Maxmatches 0    Set CRPIX? no    Tweak? no    Indexes: 3      /tigress/HSC/astrometry_net_data/sdss-dr9-fink-v5b/sdss-dr9-fink-v5b_and_263_0.fits      /tigress/HSC/astrometry_net_data/sdss-dr9-fink-v5b/sdss-dr9-fink-v5b_and_263_1.fits      /tigress/HSC/astrometry_net_data/sdss-dr9-fink-v5b/sdss-dr9-fink-v5b_and_263_2.fits    Field: 258 stars  Quad scale range: [641.674, 2208.56] pixels  object 1 of 50: 0 quads tried, 0 matched.  object 2 of 50: 0 quads tried, 0 matched.  object 3 of 50: 0 quads tried, 0 matched.  object 4 of 50: 0 quads tried, 0 matched.  object 5 of 50: 0 quads tried, 0 matched.  object 6 of 50: 0 quads tried, 0 matched.  Got a new best match: logodds 787.099.    log-odds ratio 787.099 (inf), 178 match, 1 conflict, 75 distractors, 220 index.    RA,Dec = (320.343,0.500213), pixel scale 0.167612 arcsec/pix.    Hit/miss:   Hit/miss: ++-+++++-++++++++++++--++-+--+++++-+-+++++++++-+++++++-+++++-+++++++++++++-++++++-++++++-+++++++-+++  Pixel scale: 0.167612 arcsec/pix.  Parity: pos.  processCcd.calibrate.astrometry.solver: Got astrometric solution from Astrometry.net  LoadReferenceObjects: getting reference objects using center (1023.5, 2084.5) pix = Fk5Coord(320.3431396, 0.5002365, 2000.00) sky and radius 0.00194896 rad  LoadReferenceObjects: search for objects at Fk5Coord(320.3431396, 0.5002365, 2000.00) with radius 0.111667328272 deg  LoadReferenceObjects: found 495 objects  LoadReferenceObjects: trimmed 257 out-of-bbox objects, leaving 238  processCcd.calibrate.astrometry.solver: Fit WCS: use iter 2 because it had less linear scatter than the next iter: 0.306732 vs. 0.320115 pixels  processCcd.calibrate.astrometry: 186 astrometric matches  processCcd.calibrate.astrometry: Refitting WCS  processCcd.calibrate.astrometry: Astrometric scatter: 0.048271 arcsec (with non-linear terms, 174 matches, 12 rejected)  processCcd.calibrate.photocal: Not applying color terms because config.applyColorTerms is False  processCcd.calibrate.photocal: Magnitude zero point: 30.685281 +/- 0.058711 from 173 stars  processCcd.calibrate: Photometric zero-point: 30.685281  processCcd.detection: Detected 1194 positive sources to 5 sigma.  processCcd.detection: Resubtracting the background after object detection  processCcd.deblend: Deblending 1194 sources  processCcd.deblend: Deblended: of 1194 sources, 143 were deblended, creating 358 children, total 1552 sources  processCcd.measurement: Measuring 1552 sources (1194 parents, 358 children)   processCcd WARNING: Persisting background models  processCcd: Matching icSource and Source catalogs to propagate flags.  processCcd: Matching src to reference catalogue  LoadReferenceObjects: getting reference objects using center (1023.5, 2087.5) pix = Fk5Coord(320.3429016, 0.5001781, 2000.00) sky and radius 0.00195667 rad  LoadReferenceObjects: search for objects at Fk5Coord(320.3429016, 0.5001781, 2000.00) with radius 0.112109149864 deg  LoadReferenceObjects: found 499 objects  LoadReferenceObjects: trimmed 261 out-of-bbox objects, leaving 238  processCcd.calibrate.astrometry.solver: Fit WCS: use iter 1 because it had less linear scatter than the next iter: 0.300624 vs. 0.300652 pixels  {code}    The verbosity of the astrometry module is out of proportion with the rest of the modules, which makes it difficult to follow the processing.    This is a pull request for fixes I have made.",1
"Port HSC optimisations for reading astrometry.net catalog
Some astrometry.net catalogs used in production can be quite large, and currently all of the catalog must be read in order to determine bounds for each component.  This can make the loading of the catalog quite slow (e.g., 144 sec out of 177 sec to process an HSC image, using an SDSS DR9 catalog).  We have HSC code that caches the required information, making the catalog load much faster.  The code is from the following HSC issues:    * [HSC-1087: Make astrometry faster|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1087]  * [HSC-1143: Floating point exception in astrometry|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1143]  * [HSC-1178: Faster construction of Astrometry.net catalog|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1178]  * [HSC-1179: Assertion failure in astrometry.net|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1178]    While there have been some changes to the LSST astrometry code that will mean we can't directly cherry-pick the HSC code, yet I think the main structure remains, so the approach can be copied without much effort.",3
"Audit and improve warm-start configuration options
Many of our command-line tasks - particularly the high-level MPI-based drivers we're moving over from the HSC side - typically reuse intermediate data products they find on disk rather than regenerate them by default, and have a suite of configuration options to control this behavior.    While this aids in faster reprocessing of aborted or failed jobs, it can produce results users would consider surprising (""I re-ran with a new version of the pipeline and nothing changed""), and (IMO) should not be the default behavior for any task.    We also need to guarantee that any warm-start reprocessing always produces the exact same results as a single consistent run.  I do not believe this is currently the case for some of the options in ProcessCcdTask.    Finally, we should put these configuration options somewhere other than the main task config, because they control how the processing is done, not what the results are, and hence should not be checked against existing config files in an output data repo before running.  This will probably require a new mechanism in pipe_base; perhaps a second config class associated with each Task, containing only options that affect the ""how"" of processing without affecting the ""what"".    The first step of implementing this issue should be an RFC.",8
"Purchase transceivers for use by AURA
Cisco Xenpack for extra long distance 10Gbs",5
"Install switches and transceivers on Pachon and Tololo
Buy switches to deploy MPLS over the fibers. ",8
"Configure switches for AURA tenants
Switch configuration for use by individual tenants of AURA",8
"CI validation of lsstsw's repos.yaml
Having some sort of automatic ""lint check"" of the repos.yaml file is desirable due to the length of time required to do a full up test of lsstsw.  It should be possible to cobble a sanity checker together that can be run from travis-ci.",1
"meas_base still uses eups in tests
{{tests/centroid.py}} uses EUPS to determine the location of the data file used by the test. This needs to be fixed to use a location relative to the test file.",1
"meas_astrom still using eups in tests
In DM-2636 we modified the tests to be skipped if EUPS is not available. I've had a closer look and all the ones I have glanced at seem to be easily fixable to run without EUPS. The tests seem to be using EUPS to locate the {{meas_astrom}} (effectively asking EUPS for the location of the test file), then a path to the astrometry.net test data within the {{tests/}} directory is located and then EUPS is asked to setup {{astrometry_net_data}} using that path. Since the table files are all empty this is the equivalent to simply assigning the {{ASTROMETRY_NET_DATA_DIR}} environment variable directly to the path in the tests sub-directory.    Making this change to one of the tests seems to work so I will change the rest.",2
"W16 Qserv Release and Testing
This epic captures stories related to building, testing and maintaining Qserv releases, along with related documentation. Note that testing involve running larger scale tests more or less monthly to ensure we haven't broken anything. 3 SPs per month.",23
"Data Distribution + Qserv
nan",75
"Improve name and default value of MeasureApCorrConfig.refFluxAlg
The config name refFluxAlg should be refFluxField (since it is a flux field name prefix) and the default should be  base_CircularApertureFlux_5 instead of base_CircularApertureFlux_0 (thus giving a reasonable radius instead of one that is ridiculously too small).    I should have handled it on DM-436 but it slipped through.",1
"Implement MySQL-based KVInterface
This story covers adding mysql-based implementation of KVInterface. The implementation will be done in C++, and it will be exposed to the python layer.",12
"Extend KVInterface - add support for updates
The CSS Facade and KVInterface currently do not support updates. This story covers adding support for basic updates.",4
"CSS/QMeta interaction in czar
CSS currently does not have any notion of locks. The snapshots of CSS should be taken per query, for each query and they should be done in coordination with Query Metadata. This will ensure tables used by a running query never gets altered or deleted while the query is running.",10
"S17 Fine-tune Data Access Interfaces
nan",15
"Install the LSST Stack on loaned laptop 
nan",5
"Qserv Release and Testing
This epic captures stories related to building, testing and maintaining Qserv releases, along with related documentation. Note that testing involve running larger scale tests more or less monthly to ensure we haven't broken anything. 3 SPs per month.",9
"X16 Qserv Refactoring
nan",29
"Accessing the current obs_decam package 
Installing the non-official obs_decam package from Simon Krughoff's Github   and processing some DECam data blindly     ",14
"In CalibrateTask if one disables psf determination then aperture correction will fail
In pipe_tasks CalibrateTask, by default aperture correction uses source flag ""calib_psfUsed"" to decide if a source is acceptable to use for measuring aperture correction. If PSF determination is disabled then this flag is never set and aperture correction will fail with a complaint that there are 0 sources.  ",1
"CalibrateTask instantiates measureApCorr, applyApCorr and photocal subtasks using the wrong schema
CalibrateTask instantiates measureApCorr, applyApCorr and photocal subtasks using the initial schema ""schema1"" instead of the final schema. Normally this would not matter since most of the fields are shared, but aperture correction wants aperture flux at a larger radius than the narrowest option, and schema1 may only provide the narrowest option.    In any case it is safer to instantiate those three subtasks using the final schema, since they are only ever run on the final schema. (Several other subtasks are run on both the initial and final schema, and should continue to be instantiated using schema1).",1
"Build 2015_08 Qserv Release
See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.",1
"Build and Test 2015_09 Qserv Release
See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.",3
"Build and Test 2015_10 Qserv Release
See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.",3
"Build and Test 2015_11 Qserv Release
See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.",3
"Build and Test 2015_12 Qserv Release
See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.",3
"Build and Test 2016_01 Qserv Release
See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.",3
"Build and Test 2016_02 Qserv Release
See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.",1
"Aperture correction not applied for some measurements
Aperture correction needs to be applied every time a measurement is run after it is first measured in CalibrateTask. As of DM-436 aperture correction is only being applied in CalibrateTask, which for example means the information is overwritten during the final measurement of ProcessImageTask.run.    This is probably best done by adding code to apply aperture correction to BaseMeasurementTask, so it is inherited by SingleFrameMeasurementTask and ForcedMeasurementTask.",5
"CalibrateTask instantiates some subtasks with the wrong schema
CalibrateTask instantiates some subtasks with the wrong schema, in particular:  - astrometry is instantiated with the final schema but run on schema1  - measureApCorr, applyApCorr and photocal are instantiated with schema1 but run on the final schema    One way this can cause problems is that schema1 may not have the data needed to measure aperture correction (e.g. it may contain only one tiny radius of aperture flux), as came to light when running the lsst stack demo.",1
"Investigate workflow for OpenStack via Python scripts
We investigate the use of Python scripts that work against OpenStack APIs to start up VMs and configure them for use, for example, in processing, build & test scenarios, etc.   We are initially working against  the ISL OpenStack, and intend to test against the ""nebula"" system when it becomes available.  (This type if work was initiated in issues DM-1787, DM-1788 in a previous Epic, and we continue within the context of DM-1273, )    ",12
"Add PT.12 Filter/Science_Ccd_Exposure tables to extend test query coverage
Filter table is missing from case02, case05 data, so next query can't be tested:  {code:sql}  -- datasets/case02/queries/3023_joinObjectSourceFilter.sql.FIXME  --- Join on Source and Filter and select specific filter in region  --- https://dev.lsstcorp.org/trac/wiki/db/Qserv/IN2P3/BenchmarkMarch2013  --- https://dev.lsstcorp.org/trac/wiki/db/queries/007  -  SELECT objectId, taiMidPoint, fluxToAbMag(psfMag)  FROM   Source  JOIN   Object USING(objectId)  JOIN   Filter USING(filterId)  -WHERE   ra_PS BETWEEN 1 AND 2 -- noQserv  AND decl_PS BETWEEN 3 AND 4 -- noQserv  --- withQserv  qserv_areaspec_box(1,3,2,4)  AND  filterName = 'u'  AND  variability BETWEEN 0 AND 2    {code}    Same thing for case02:1011_objectsForExposure and case02:1030_timeSeries.sql",4
"Fix UDF for case01 query: 3005_orderByRA.sql
query    {code:bash}  mysql --host=127.0.0.1 --port=4040 --user=qsmaster --batch qservTest_case01_qserv -e ""SELECT * FROM Object WHERE qserv_areaspec_box(0.,1.,0.,1.)""  {code}  returns nothing whereas   {code}  SELECT *   FROM Object  WHERE ra_PS BETWEEN 0. AND 1.   -- noQserv  AND decl_PS BETWEEN 0. AND 1.  {code} does (but doesn't use geom index)",6
"Remove _chunkId, _subChunkId column from case02:Object table
This columns are Qserv internal and shouldn't be in input data. For example, this prevents case02:3021_selectObjectSortedByRA to work.    Check also that these columns aren't in other test data set and remove FIXME suffix from related broken query.",4
"Document deprecation of DecoratedImage
According to discussion on Hipchat (20 July 2015)    {quote}  Jim Bosch: [...] DecoratedImage is strongly deprecated, though  {quote}    This was news to me, and certainly isn't reflected [in (at least the obvious place) in Doxygen|https://lsst-web.ncsa.illinois.edu/doxygen/x_masterDoxyDoc/afw_sec_image.html]. It should be.",1
"Re-implement watcher based on new CSS implementation
Current watcher implementation (in {{admin/bin/watcher.py}}) is based on direct watching of zookeeper updates via kazoo. If we are to re-implement CSS based on mysql then watcher needs to be updated to support it. Mysql does not have watch mechanism, so it has to be done via polling or using some other mechanism if synchronous notifications are needed.",8
"Audit existing test and development system
Document in the wiki how the test and development systems are connected and configured.",3
"Fix cluster install procedure and improve docker support
Document how-to update cluster from Qserv release:    See  http://www.slac.stanford.edu/exp/lsst/qserv/2015_07/HOW-TO/cluster-deployment.html",1
"makeWcs() chokes on decam images in 10.1
In 10.0, processCcdDecam.py could process decam images to completion (whether the WCS was read correctly is a different question). Now it fails on makeWcs() (see traceback below), and I suspect this change in behavior is related to DM-2883 and DM-2967.    Repository with both data and code to reproduce:  http://www.astro.washington.edu/users/yusra/reproduce/reproduceMakeWcsErr.tar.gz  (apologies for the size)    The attachment is a document describing the WCS representation in the images from the community pipeline, courtesy of Francisco Forster.    Please advise. This ticket captures any changes made to afw.     {code}  D-108-179-166-118:decam yusra$ processCcdDecam.py newTestRepo/ --id visit=0232847 ccdnum=10 --config calibrate.doPhotoCal=False calibrate.doAstrometry=False calibrate.measurePsf.starSelector.name=""secondMoment"" doWriteCalibrateMatches=False --clobber-config  : Loading config overrride file '/Users/yusra/lsst_devel/LSST/repos/obs_decam_ya/config/processCcdDecam.py'  : Config override file does not exist: '/Users/yusra/lsst_devel/LSST/repos/obs_decam_ya/config/decam/processCcdDecam.py'  : input=/Users/yusra/decam/newTestRepo  : calib=None  : output=None  CameraMapper: Loading registry registry from /Users/yusra/decam/newTestRepo/registry.sqlite3  processCcdDecam: Processing {'visit': 232847, 'ccdnum': 10}  makeWcs WARNING: Stripping PVi_j keys from projection RA---TPV/DEC--TPV  processCcdDecam FATAL: Failed on dataId={'visit': 232847, 'ccdnum': 10}:     File ""src/image/Wcs.cc"", line 130, in void lsst::afw::image::Wcs::_initWcs()      Failed to setup wcs structure with wcsset. Status 5: Invalid parameter value {0}  lsst::pex::exceptions::RuntimeError: 'Failed to setup wcs structure with wcsset. Status 5: Invalid parameter value'    Traceback (most recent call last):    File ""/Users/yusra/lsst_devel/LSST/DMS5/DarwinX86/pipe_base/10.1-3-g18c2ba7+49/python/lsst/pipe/base/cmdLineTask.py"", line 320, in __call__      result = task.run(dataRef, **kwargs)    File ""/Users/yusra/lsst_devel/LSST/DMS5/DarwinX86/pipe_base/10.1-3-g18c2ba7+49/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/Users/yusra/lsst_devel/LSST/repos/obs_decam_ya/python/lsst/obs/decam/processCcdDecam.py"", line 77, in run      mi = exp.getMaskedImage()    File ""/Users/yusra/lsst_devel/LSST/DMS5/DarwinX86/daf_persistence/10.1-1-g6edbc00+28/python/lsst/daf/persistence/readProxy.py"", line 41, in __getattribute__      subject = oga(self, '__subject__')    File ""/Users/yusra/lsst_devel/LSST/DMS5/DarwinX86/daf_persistence/10.1-1-g6edbc00+28/python/lsst/daf/persistence/readProxy.py"", line 136, in __subject__      set_cache(self, get_callback(self)())    File ""/Users/yusra/lsst_devel/LSST/DMS5/DarwinX86/daf_persistence/10.1-1-g6edbc00+28/python/lsst/daf/persistence/butler.py"", line 242, in <lambda>      innerCallback(), dataId)    File ""/Users/yusra/lsst_devel/LSST/DMS5/DarwinX86/daf_persistence/10.1-1-g6edbc00+28/python/lsst/daf/persistence/butler.py"", line 236, in <lambda>      location, dataId)    File ""/Users/yusra/lsst_devel/LSST/repos/obs_decam_ya/python/lsst/obs/decam/decamMapper.py"", line 118, in bypass_instcal      wcs         = afwImage.makeWcs(md)    File ""/Users/yusra/lsst_devel/LSST/DMS5/DarwinX86/afw/10.1-26-g9124caf+1/python/lsst/afw/image/imageLib.py"", line 8706, in makeWcs      return _imageLib.makeWcs(*args)  RuntimeError:     File ""src/image/Wcs.cc"", line 130, in void lsst::afw::image::Wcs::_initWcs()      Failed to setup wcs structure with wcsset. Status 5: Invalid parameter value {0}  lsst::pex::exceptions::RuntimeError: 'Failed to setup wcs structure with wcsset. Status 5: Invalid parameter value'  {code}    ",2
"Standardize Qserv install procedure: step 1 build docker container for master/worker instance and development version 
- shmux could be used for parallel ssh (remove Qserv builtin one)  - look at ""serf and consul"" (See Confluence pages)  - improve doc: http://www.slac.stanford.edu/exp/lsst/qserv/2015_07/HOW-TO/index.html  - run multiple instances/versions of Qserv using different run dir/ports and the same data",8
"Include reference magnitude errors in PhotoCal
PhotoCal task currently ignores the uncertainties on reference sources, which can lead to problems when the reference and measured catalogs have relatively little overlap or otherwise disagree on how trustworthy a source is.",3
"W16 Data Access and Db Release Documentation
Write Release documentation covering Data Access and Database work.",5
"Revisit cost of replicating non-partitioned tables on all nodes
Revisit size of all non-partitioned tables, and cost of replicating them on all worker nodes.",4
"Estimate I/O load for non-partitioned tables
Estimate realistic IO load from user queries on non-partitioned tables. Consider whether there might be hot spots (eg., maybe a small subset of columns from exposure is used very often. If it is it, maybe it'd be worth replicating only these columns across all worker nodes and serve the rest from one shared file system).",6
"Experiment with CONNECT engine for non-partitioned tables
Idea: store non-partitioned tables in a dedicated mysql server, and bring them to the worker nodes using connect engine.    This story involves exploring if that would work, and uncovering potential pitfalls.",10
"Add debugging for astrometry.net solver
To be able to debug astrometric matching, it helps to be able to visualise the source positions, the distorted source positions, and the reference positions.  This is a pull request to add these.",1
"Query Coverage
nan",28
"Get ImageDifferenceTask running again
ImageDifferenceTask doesn't run. The issues I've seen so far are related to the measurement overhaul. This ticket will capture the one-off updates needed to get this running again.     Appropriate Bugs and Papercuts epic?",10
"ChebyshevBoundedField should use _ not . as field separators for persistence
ChebyshevBoundedField uses ""."" instead of ""\_"" as field separators in its afw table persistence. This is the old way of doing things, and unfortunately causes errors when reading in older versions of tables, becaus afw converts ""."" to ""_"" in that situation.    This shows up as a unit test failure in DM-2981 (brought over from HSC) when an older version table is read in.    It is an open question whether to fix this as part of DM-2981 (which conveniently has a test that shows the problem, though not intentionally so) or separately, in which case a new test is wanted. In the former case I'm happy to do the work so I can finish DM-2981.    Many thanks to Jim Bosch for diagnosing the problem.",1
"Begin drafting specification document for the Level 1 System
Don Petravick (.5 FTE) + Jason Alt (.8 FTE) + Paul Wefel (.125 FTE)  July 2015 - August 2015",50
"Initial work to process DECam data with LSST stack
Hsin-Fang Chiang (1 FTE)  July 2015 - August 2015",80
"Extrapolate to the current document 
Discussed use  cases in the context of the TOWG, and also with the site and telescope group.  A picture of the  structure of IT operations has (I believe consensus emerged) that is the  four layers ITIL cake    Service Design (cataloged, budget, availability, etc).  Service Transition (The work of inserting Charge into the system).  Service Delivery  (the work of running the as-is system of servinces)  ITC -- The work of providing the Facility, Hardware and networking.     I also obtained the ability to interact with EA. (but still working to master it and its concepts)",8
"unable to create public images
Errors are returned when attempting to upload an image marked as public.",1
"Improve czar-worker communication debugging
Add features to make it easier to debug communication problems. Particularly, record the source of a message, and remove extraneous messages.",2
"Document setting up multi-node Qserv and running integration test
nan",4
"openstack API endpoint is broken
Similar to what was observed in DM-3226, the referral endspoint returned by   {code:java}  https://nebulous.ncsa.illinois.edu:5000  {code}  are not FQDNs.  This fundamentally breaks any attempt to use the API one step past authenticating with keystone.    This is an example HTTP response:    {code:java}  HTTP/1.1 200 OK  Date: Mon, 27 Jul 2015 23:11:02 GMT  Server: Apache/2.4.10 (Ubuntu)  Vary: X-Auth-Token  X-Distribution: Ubuntu  x-openstack-request-id: req-ac7bb613-86ef-43ab-a663-75c2ed3fb124  Content-Length: 1656  Content-Type: application/json    {""access"": {""token"": {""issued_at"": ""2015-07-27T23:11:02.342216"", ""expires"": ""2015-07-28T00:11:02Z"", ""id"": ""99b843d4baf94569a0d34ca4fecb470c"", ""tenant"": {""description"": null, ""enabled"": true, ""id"": ""d1f16653856540d386224fb057b5b00c"", ""name"": ""LSST""}, ""audit_ids"": [""fAP8851vTQi1n5pYmNoIjw""]}, ""serviceCatalog"": [{""endpoints"": [{""adminURL"": ""http://nebula:9292"", ""region"": ""RegionOne"", ""internalURL"": ""http://nebula:9292"", ""id"": ""49365a8e8fe743af9d517e84a98e3ee9"", ""publicURL"": ""http://nebula:9292""}], ""endpoints_links"": [], ""type"": ""image"", ""name"": ""glance""}, {""endpoints"": [{""adminURL"": ""http://nebula:8774/v2/d1f16653856540d386224fb057b5b00c"", ""region"": ""RegionOne"", ""internalURL"": ""http://nebula:8774/v2/d1f16653856540d386224fb057b5b00c"", ""id"": ""c1e31df3656042ef9c5502efd7d574f2"", ""publicURL"": ""http://nebula:8774/v2/d1f16653856540d386224fb057b5b00c""}], ""endpoints_links"": [], ""type"": ""compute"", ""name"": ""nova""}, {""endpoints"": [{""adminURL"": ""http://nebula:9696"", ""region"": ""RegionOne"", ""internalURL"": ""http://nebula:9696"", ""id"": ""266c9dc8e0344f8fa3f078652e868443"", ""publicURL"": ""http://nebula:9696""}], ""endpoints_links"": [], ""type"": ""network"", ""name"": ""neutron""}, {""endpoints"": [{""adminURL"": ""http://nebula:35357/v2.0"", ""region"": ""RegionOne"", ""internalURL"": ""http://nebula:5000/v2.0"", ""id"": ""5d474008bcee4f44800546e3f3302404"", ""publicURL"": ""http://nebula:5000/v2.0""}], ""endpoints_links"": [], ""type"": ""identity"", ""name"": ""keystone""}], ""user"": {""username"": ""jhoblitt"", ""roles_links"": [], ""id"": ""6ea0c8e153b04ae29572c5fd877b6ac3"", ""roles"": [{""name"": ""user""}], ""name"": ""jhoblitt""}, ""metadata"": {""is_admin"": 0, ""roles"": [""142761bd922e453294e9b7086a227cbc""]}}}    {code}  ",1
"evaluate NCSA OpenStack against SQRE requirements and provide feedback - part 1
See also https://confluence.lsstcorp.org/pages/viewpage.action?spaceKey=LDMDG&title=NCSA+Nebula+OpenStack+Issues",2
"Refinement, restatement of DM facilites and functions 
Began detailed refinement.  The initial version was a word document with omnigraffle figures.  Began moving this to LSST Confluence to make the model more accessible for scrutiny.    The top level  functional  - physical  breakdown diagrams now exist and transferring the functional breakdown from word to confluence is in place.   I have request that the simple citation package be installed in  the LSST lira (not perfect, but helps).   ",10
"Potential talk for All-Hands
nan",1
"Potential talk for nsf cyber summit
nan",1
"Collab. with Ron Lambert and Oliver W.
nan",2
"Fix problems with no-result queries on multi-node setup
For queries like:        select * from Object where id = <non existent id>    qserv can't map it to any chunk, and it ends up executing      SELECT *     FROM qservTest_case01_qserv.Object_1234567890 AS QST_1_     WHERE objectId=<non existent id>    the chunk 1234567890 is a special chunk and it exists on all nodes.    And that fails with:    (build/qdisp/QueryResource.cc:61) - Error provisioning, msg=Unable to  write  file; multiple files exist. code=2 ",1
"Build a demo system for camera team to use the Firefly external task launcher
We have the code to launch ab external task from Firefly server. ( see DM-2991)  IN order to facilitate the development by the UIUC group for Camera team, we need to have a simple example to show how to connect the front extension to the external task at the server side.  ",10
"Support to the camera team development
We need someone to attend the weekly meeting at UIUC  group for camera team. To discover issues and answer questions. This is an on-going effort.   ",10
"Create images for the mask bits at server side
LSST FITS images will have a extension that indicate the mask bits. In order to overlay the masks on the primary image, we need to turn the mask bits into a set of images. This task is to take the requested bits and FITS as input, output a set of images for each requested bit. Each bit will have different color. ",20
"a simple demo version to use Firefly in iPython notebook
We want to build a simple demo version of Firefly  that works in the iPython notebook.  It will make it easier for users to try out Firefly visualization capabilities  with Python APIs. ",10
"Include polygon bounds in CoaddPsf logic
This is a port of [HSC-974|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-974]. Original description:    The {{CoaddPsf}} class should use the polygon bounding areas that were added to {{Exposure}} and {{ExposureRecord}} in DM-2981 (was: HSC-973) when determining which PSF images to coadd.",1
"Add support for passing query classification info from user to czar
We need to be able to pass information from user about query type (sync/async). This will require tweaking the parser.  ",8
"Add support for async query results
Modify Qserv to support async queries: send query results to the right place instead of to mysql proxy. In this story, we can simply use some reasonable default location and send the results there. Later on we will extend qserv to make it configurable.",10
"Add support for configuring async queries
Extend Qserv configuration to allow a DBA to specify (a) where results from async queries should be stored and (b) what rules to apply when purging old results.    Note that we need to think about the purging rules, it is not immediately obvious what would make most sense.",5
"Revisit and document user-facing aspects of async queries
Outline all aspects of async queries that are affecting users, discuss with the DM team, and document. This includes things like:   * managing async queries (checking status, terminating)   * retrieving results from async queries   * managing query results (purging policies etc)   * probably more, need to think about it...",8
"Unify KVInterface python and c++ interfaces
Swig the C++ mysql-based KvInterface implementation.   ",8
"FY18 Qserv Health Verification Tool
Need a tool for verifying if all the services are up and running, including things like whether udfs are loaded",79
"Prepare and implement RFP for DWDM devices
Prepare and implement RFP",10
"Fiber path Gate to pachon
Discussion on fiber path",5
"Port flux.scaled from HSC
[HSC-1295|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1295] introduces {{flux.scaled}}, which measures the flux within a circular aperture that is set from the size of the PSF, scaled by some factor.  Stephen Gwyn recommends using this as our fiducial calibration flux.",2
"CoaddPsf.getAveragePosition() is not a valid position
This is a port of [HSC-1138|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1138] to LSST. That is an aggregate of two related minor fixes:    * {{CoaddInputRecorder}} should default to {{saveVisitGoodPix=True}} so that average positions in the {{CoaddPsf}} can be properly weighted;  * {{computeAveragePosition}} and {{doComputeKernelImage}} should be consistent about the data included when determining whether a source is off image.",1
"Define polygon bounds for CCDs based on vignetted regions
This is a port of [HSC-976|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-976] to LSST. The original issue description was:    We should set the polygon bounds (added in DM-2981 [was HSC-973]) for HSC CCD exposures to cover the non-vignetted regions. This should probably be done in ISR or some other camera-specific location.    Note that, contrary to the description in DM-2981, this functionality was not included there.",1
"Fix problems in xrootd discovered in multi-node qserv tests
nan",15
"Explore how to run multi-node tests
Not testing qserv code often enough in multi-node environment led to introducing many problems over the past two years since we last run large scale test. It should be simple for developer to run a multi-node test. This story covers work related to understanding how to run integration test on multi-node.",4
"W16 Make Query Cancellation Robust
It is clear that the code responsible for query cancellation needs some more thoughts and refactoring (it was prematurely rushed when Daniel was about to leave slac). In particular, the code seems to have some subtle problems. We need to debug these problems and solve them.",49
"Audit & cherry-pick HSC-1126 fixes
[HSC-1126|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1126] contains a number of unrelated bug fixes. Given the nature of that ticket, it's not immediately clear which might already have been ported to LSST, which don't apply, and, of the others, what dependencies they have on code which might still be in the queue for merger.    We need to dig through that ticket and ensure that everything is properly merged.",2
"add task to meas_astrom to fit an aribtrary WCS with a TAN-SIP WCS
Sometime in the past, Russell Owen wrote a method to take an arbitrary WCS and approximate it as a TanWcs (our implementation of FITS' TAN-SIP WCS formalism).  That method is currently just a utility function in the unit test testFitTanSipWcsHighOrder.py.  This issue will promote that method to a full-fledge task in meas_astrom.",2
"Reconfigure Openstack systems
1) Configured the IPMI on 13 systems.",22
"Administrative - 7-2015
nan",2
"Networking support of Openstack efforts
nan",2
"assertWcsNearlyEqualOverBBox and friends is too hard to use as a free function
assertWcsNearlyEqualOverBBox and similar functions elsewhere in afw were written to be methods of lsst.utils.tests.TestCase, so their first argument is a testCase. This is fine for use in unit tests, but a hassle to use as free functions because the user must provide a testCase argument (though it need only be a trivial class with a fail(self, msgStr) method). Worse, that minimal requirement is not documented, so technically providing a simple mock test case is unsafe.    I have two proposals:  - Document the fact that testCase need only support fail(self, msgStr). This makes it clear how to safely use these functions as free functions.  - Allow testCase to be None, in which case RuntimeError is raised. That makes these functions even easier to use as free functions.  ",1
"Include translation of aliases in measurement calibration
Tasks that calibrate measurement outputs should include transferring (and translating, as needed) the aliases in the original measurement catalog.  This should include transferring slots, which may involve ""renaming"" the original slots, as those names refer explicitly to raw measurements (i.e. ""slot_PsfFlux"" might become ""slot_PsfMag"").",3
"Add test case for ExposureRecord::contains
In DM-3243 we ported from HSC the ability to take account of the associated {{validPolygon}} when checking whether a point falls within an {{Exposure}}. This functionality was not accompanied by an adequate unit test.",2
"Reproducing errors of the current obs_decam package
Learning the stack and development worlflow by reproducing errors in obs_decam as DM-3196.  Changes from the stack need to be incorporated into obs_decam package to keep the package up to date, hence the errors. As a learning process I reproduced the errors and used her fix (afw branch u/yusra/DM-3196) to move on.  ",2
"Study basic afw 
Learn the basic operation of the aft package about handling images, tables, etc.  ",6
"Management until end july 215
Finsh system development lead and on Onboard Jason Alt.  Deal with management of group providing NCSA openstack.  Review and re-review procurement contract prose and directions.  Coordinate  TOWG input with NCSA management.    Gain acume about exiting design documents, decide and prototype details refinement/ analysis /restatement needed to manage project at NCSA.  Management of people.",10
"Read in the FITS cube that Herschel project produced
IRSA needs to be able to read in the FITS cube generated by Herschel project. We need to support and guide the effort so the code is generic enough for non-Herschel data. ",2
"Support the FITS cube reader
RSA needs to be able to read in the FITS cube generated by Herschel project. We need to guide the effort so the code is generic enough for non-Herschel data.",1
"Fix Firefly build script so it'll work with latest version of gradle
Firefly build was failing when using gradle version 2.5.  Minor changes to the dependencies declaration fixed it.",1
"Margaret's mgmt. activities in July
nan",30
"Add mysql-based test to multi-node integration test
At the moment multi-node integration test runs only on multi-node using Qserv, it does not run on plain mysql, and thus we can't validate results. The story involves tweaking qserv_testdata such that we can run mysql test on the czar, and compare results from mysql and qserv.",5
"Investigate jenkins creating a container per job
nan",2
"Run large scale tests
nan",8
"Debug problem with joins in multi-node tests
We seen to have problems with joins:  {quote}  SELECT o.deepSourceId, s.objectId, s.id, o.ra, o.decl FROM Object o, Source s WHERE o.deepSourceId=s.objectId;  {quote}    cluster seems to have hung. I can send new queries to the czar, and they show up in the czar's log, but they don't get answered (they can be cancelled).  Cancelling the join works(at least for the czar) but no further queries work.",4
"Analyze qserv performance / KPIs
nan",5
"Explore Qserv authentication and authorization
nan",8
"Produce Data Access & DB team S15 Release docs
Complete these documents:  * https://confluence.lsstcorp.org/display/DM/Summer+2015+Qserv+Release  * https://confluence.lsstcorp.org/display/DM/Summer+2015+WebServ+Release",4
"Add multi-process python runner script for Galaxy Shear Experiments
The current runner scripts are in tcsh and bash.  There is no good excuse for this, except that it was easy to implement.  Since we need both multi-threading and better parameter parsing, this will be replaced with a python script.",2
"Port HSC MPI driver for single-visit processing
Transfer the {{reduceFrames.py}} script and the {{ProcessExposureTask}} it utilizes from hscPipe to a new package in the LSST stack (RFC-68 proposes calling this new package {{pool_tasks}}, but this isn't set in stone).    We should probably rename either the driver script or the task (or both), so they agree; the lack of consistency is a historical artifact on the HSC side, and I think it's time to change that.",6
"Port HSC MPI driver for coaddition
Port the HSC driver for coaddition, {{stack.py}} from hscPipe to a new LSST package (the same as DM-3368).    In the process, we should remove the inclusion of {{ProcessCoaddTask}}, and instead run detection and background subtraction only.    I think it might be time to consider renaming this task as well; I find it a little unfortunate we use ""coadd"" everywhere else but ""stack"" here.",4
"Port HSC MPI driver for multi-band coadd processing
Port the HSC MPI driver of multi-band coadd processing, multiBand.py, from hscPipe to a new LSST package (the same as in DM-3368).",4
"Port HSC --rerun option for CmdLineTask
Port the HSC side's {{--rerun}} option for specifying processing inputs and outputs.    This work should be preceded by an RFC; we've proposed implementing this option on the LSST side in the past, and it was met with some resistance as it isn't strictly necessary.  We've since found it extremely convenience on the HSC side, and I think it's very much worth porting.",4
"Port, replace, or defer HSC-side provenance of EUPS products
The HSC pipeline checks that setup EUPS products are identical between runs with the same output directory, in the same way configuration is checked in both the LSST and HSC pipelines.    The implementation is a bit messy, and it's not strictly necessary, so it's not clear we should port this over as-is, or just wait for a better implementation to be provided by the Process Middleware team.  We should at least RFC this question now.",6
"Port HSC code for generation of calibration products
Port HSC code for building calibration products (flats, bias frames, etc.).",6
"add realistic Footprints to measurement code
The current measurement code for the galaxy shear simulations uses the full postage stamp bounding box for the Footprint.  We need to use more realistic Footprints for some of the tests we want to run.  That probably involves running {{SourceDetectionTask}} and somehow combining that with the input-catalog based iterating already in the measurement code.",4
"Test shear bias vs. CModel region.nGrowFootprint
One piece of how the CModel code chooses its fit region size is via nGrowFootprint, which is used to grow the original detection Footprint.  We should test how changing this parameter affects the _m_ and _c_ shear biases between input and recovered shear.  They should decrease for larger nGrowFootprint values, and eventually plateau.  We want to find the point where this happens, and see how the parameter affects both the fit region area and the shear biases before this threshold.    It may be necessary to make a small modification to the CModel code to output the fit region area to complete this test.",4
"Test shear bias vs. CModel region.nInitialRadii
Like DM-3375, but testing the region.nInitialRadii parameter instead.  This parameter sets the fit region using a multiple of the half-light ellipse from an initial approximate fit.  The full fit region is formed as the union of this with the grown detection Footprint, so it may be necessary to set nGrowFootprint to a negative number to see any affect from this parameter.",4
"Initial issue investigation for the nebula openstack
    The nebula openstack system at NSCA first became available ~Fri Jul 24 and  the week of Jul 27 -- 31 was spent testing and debugging issues that the                 LSST team identified within, for example, DM-3225, DM-3219, DM-3227 and others.  ",20
"NSF Cyber Summit talk
NSF Cyber Security Summit talk:  a case study of LSST cyber security.  Talk goes over challenges and successes with LSST's security program.  Talk is divided into four sections:  security plan, data security, user access, and security for the observation site.",7
"Port HSC hooks for simulated source injection
Port HSC hooks injecting simulated sources into real images to test processing.    This includes the code in {{fakes.py}} in pipe_tasks and its callers.  The pipeline does not include code for actually adding the fake sources; it just provides a callback interface that is implemented by third-party plugins such as https://github.com/clackner2007/fake-sources.",4
"Add test cases for thresholding
In DM-3136 changes were made to the way thresholds are handled in detection ([{{a4b011d}}|https://github.com/lsst/meas_algorithms/commit/a4b011dd0775908c925ad9f40f802f9ed8723ef9] and [{{74c2ed0}}|https://github.com/lsst/meas_algorithms/commit/74c2ed0b79afce4c94b0db5f1e168c28ba1aa15b]). These were not accompanied by test cases, but they should be.",2
"security playbook
Practical document for handling and responding to incidents.",1
"Meeting with on HTCondor
Attended meeting with Miron Livny.",1
"Port HSC improvements to HSM moments code
The HSM shear estimation has received several improvements and important bugfixes on the HSC side that need to be ported to LSST.  This is complicated by the fact that much of the code has been entirely rewritten on the LSST side to work within the new measurement framework, but we've also synchronized this package with the HSC side much more frequently than with other packages.",4
"Administrative - 8-2015
nan",1
"Parallelism Framework migration
nan",2
"Make use of good pixel count when building CoaddPsfs
When building a CoaddPsf we have the ability to take account of the number of pixels contributed by the inputs (see http://ls.st/paj and DM-3258). However, the {{CoaddPsf}} constructor fails to use this information. It should copy this field when copying the provided {{ExposureCatalog}}, so that {{computeAveragePosition}} can use it.",1
"Edit end-to-end test plan to reflect current DM plans
nan",2
"Re-generate data for large scale tests at in2p3
Sources were incorrectly duplicated, need to be redone",3
"Refactor Zscale.java class 
In early this year, the decision all data types would be converted to float in FitsRead.  Thus,the bitpixel is not relevant.  In Zscale, it still uses bitpixel to test the data type.  It should be refactored in the same manner as FitsRead etc. ",2
"Fix precision related problem in UDFs
SciSQL udfs seem to have a subtle precision problem. The following query that is not relying on scisql returns one row:    {quote}  select ra, decl, deepSourceId   FROM Object o   WHERE decl between 0.992 and 0.993 and ra between 19.171 and 19.172;  +------------------+-------------------+------------------+  | ra               | decl              | deepSourceId     |  +------------------+-------------------+------------------+  | 19.1719166801441 | 0.992087616433663 | 4368217963236477 |  +------------------+-------------------+------------------+  1 row in set (2 min 9.49 sec)  {quote}    But equivalent scisql-based query:    {quote}  select ra, decl, deepSourceId   FROM Object o   WHERE qserv_areaspec_box(0.992, 19.171, 0.993, 19.172);  {quote}    will fail to find that row.    If we relax the search criteria just a little bit, it finds some other row, but still not the one with decl = 0.992087616433663    {quote}  select ra, decl, deepSourceId FROM Object o WHERE qserv_areaspec_box(0.99, 19.171, 0.999, 19.172);  +-------------------+-----------------+------------------+  | ra                | decl            | deepSourceId     |  +-------------------+-----------------+------------------+  | 0.994098536926311 | 19.171425377618 | 4372684729224984 |  +-------------------+-----------------+------------------+  {quote}  ",4
"Fix column names in query result
The following shows the problem (See the column names in the results, they are not what user will expect). It happens for all aggregates: min, max, avg, count etc    {code}  select min(ra_PS), min(decl_PS), max(ra_PS), max(decl_PS), avg(ra_PS) from Object;  +----------------+---------------+---------------+---------------+-------------------------------+  | MIN(QS1_MIN)   | MIN(QS2_MIN)  | MAX(QS3_MAX)  | MAX(QS4_MAX)  | (SUM(QS6_SUM)/SUM(QS5_COUNT)) |  +-----------------+--------------+---------------+---------------+-------------------------------+  | 0.041714119635 | -6.1011707745 | 359.938579891 | 3.89870649736 |                 112.537203939 |  +----------------+---------------+---------------+---------------+-------------------------------+  {code}",4
"Cost Model Discovery
Learn about the cost model (LDM-144) and related documents in preparation for updating it per the contract",9
"Discourse evaluation (Part 2)
Work in support of evaluation Discourse as a DM platform for internal and external interactions.",7
"Find and evaluate multi-user password wallet for SQuaRE
Work to find and evaluate an off-the-shelf solution for sharing web services passwords between the SQuaRE group.",1
"Fix problem with default_engine
Fix the problem:    {quote}  08/04/2015 05:39:47 werkzeug INFO: 141.142.237.30 - - [04/Aug/2015 17:39:47] ""GET /meta/v0/ HTTP/1.1"" 200 -  08/04/2015 05:39:49 __main__ ERROR: Exception on /meta/v0/db [GET]  Traceback (most recent call last):    File ""/home/becla/stack/Linux64/flask/0.10.1+8/lib/python/Flask-0.10.1-py2.7.egg/flask/app.py"", line 1817, in wsgi_app      response = self.full_dispatch_request()    File ""/home/becla/stack/Linux64/flask/0.10.1+8/lib/python/Flask-0.10.1-py2.7.egg/flask/app.py"", line 1477, in full_dispatch_request      rv = self.handle_user_exception(e)    File ""/home/becla/stack/Linux64/flask/0.10.1+8/lib/python/Flask-0.10.1-py2.7.egg/flask/app.py"", line 1381, in handle_user_exception      reraise(exc_type, exc_value, tb)    File ""/home/becla/stack/Linux64/flask/0.10.1+8/lib/python/Flask-0.10.1-py2.7.egg/flask/app.py"", line 1475, in full_dispatch_request      rv = self.dispatch_request()    File ""/home/becla/stack/Linux64/flask/0.10.1+8/lib/python/Flask-0.10.1-py2.7.egg/flask/app.py"", line 1461, in dispatch_request      return self.view_functions[rule.endpoint](**req.view_args)    File ""/nfs/home/becla/stack/repos/dax_metaserv/python/lsst/dax/metaserv/metaREST_v0.py"", line 59, in getDb      return _resultsOf(text(query), scalar=True)    File ""/nfs/home/becla/stack/repos/dax_metaserv/python/lsst/dax/metaserv/metaREST_v0.py"", line 122, in _resultsOf      engine = current_app.config[""default_engine""]  KeyError: 'default_engine'    {quote}",1
"Research and Documenting the L1 System
nan",4
"Eliminate circular aliases in slot centroid definition
[~smonkewitz] has discovered that our schema aliases for even the default configuration of measurement algorithms involve cycles, because the slot centroid algorithm contains a reference to its own flag.  Fixing this should just involve an extra check in {{SafeCentroidExtractor}}.",1
"Explicitly disallow alias cycles in Schemas
The current guard against cycles is lazy and incomplete, as it seemed unlikely we'd ever have them.  That's already been disproven (DM-3400), so it seems prudent to fix the guard code now.",6
"Port HSC updates to ingestImages.py
ingestImages.py provides a camera-agnostic manner of creating a data repository (including a registry).  The HSC fork contains multiple improvements not present on the LSST side.  We need these in order to ingest the HSC data.",2
"organize workspace functions discussion 
nan",4
"workspace functions discussion
nan",4
"workspace functions specification document
The first version of the document is here https://confluence.lsstcorp.org/pages/viewpage.action?pageId=41783931",20
"Review and edit import of Level 2 ICD milestones into DLP
nan",2
"Clarify status of LSE-77
Work on LSE-77 _per se_ appears not to have kept up with the status of the substance of the interface requirements, as represented in, for instance, LSE-239.  The action here is to see what change request actions may be appropriate for LSE-77 at this point.",1
"Review all DM ICDs for open issues and work with partner subsystems to clarify schedules for completion
nan",4
" workspace functions discussion
nan",2
"Fix overestimation of aperture correction error
We're overestimating the aperture correction errors by including photon noise twice: both in the aperture correction errors and the original measurement errors.    This is a migration of [HSC-1277|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1277] to LSST, which is where it should be fixed.",8
"Evaluate changes to LSE-209 and LSE-70
Action item from    https://confluence.lsstcorp.org/display/SYSENG/2015+July+08-10+CCS-DAQ-OCS-DM+Workshop+IV    Send initial feedback on LSE-70 and LSE-209",4
"obs_decam unit test for reading data 
The unit test wasn't working before and I edited the unit test of reading raw data. This got included with DM-3462.   This unit test needs testdata_decam to be setup.      The test fails with the stack b1597 at makeWcs (DM-3196).   The afw branch u/yusra/DM-3196 is a temporary fix before DM-3196 is resolved.      ",2
"Create testdata_decam 
Create a new testdata_decam repo with public instrument calibrated data.        The 435MB file can be downloaded from http://uofi.box.com/testdata-decam",2
"S15 Qserv Release and Testing
This epic captures stories related to building, testing and maintaining Qserv releases, along with related documentation.",10
" CI, Deploy and Distribution Improvements Part II
[Retitled epic to better capture current plan]    This epic is an umbrella for Jenkins improvements, OpenStack and AWS  automatic deployment, binary distribution, developer requests.    Docker items in particular should go to a different epic. DM-2053    [JH 100%]     ",49
"DLP/LDM-240 support chages - Part II
Service feature requests and bugfixes from JK, T/CAMs including format changes on sqre-jirakit.",2
"Meetings - Aug 2015 
8/3: local group meeting  8/3: UIUC postdoc orientation  8/5: LSST2015 NCSA coordination meeting  8/10: local group meeting  8/17-22: LSST2015 Bremerton All Hands Meeting  8/24: local group meeting  8/28: Astro postdoc meeting",12
"Set up new desktop and install the stack
- Set up software on the new iMac  - Installed the lsst stack with eups and lsstsw  - Noticed a compiler requirement DM-3405  ",2
"Learn the development workflow and obs_decam status update 
- Learn git workflow and branching of the stack.    - Had a long chat with Yusra about obs_decam   - Reproduced obs_decam issues with different builds of the stack. There was a measurement failures that got solved with the newer builds of the stack. ",4
"Debug problem with timeout
If I run 4 simultaneous large queries: 3 object scans and 1 source scan, Xrootd silently died on 2 machines. Below I pasted the tail of the log files      ccqserv108    0808 00:22:33.824 [0x7fb739583700] WARN  Foreman (build/wdb/QueryAction.cc:109) - QueryAction overriding dbName with LSST  0808 00:22:33.824 [0x7fb74a00e700] INFO  root (build/xrdsvc/ChannelStream.cc:122) - returning buffer (256, (more))  0808 00:22:33.826 [0x7fb74a00e700] INFO  root (build/xrdsvc/ChannelStream.cc:122) - returning buffer (62, (last))  0808 00:22:33.826 [0x7fb74a00e700] INFO  root (build/xrdsvc/SsiSession.cc:153) - RequestFinished type=isStream  0808 00:22:34.468 [0x7fb739583700] INFO  root (build/wdb/QueryAction.cc:261) - &&& _fillRows size=8  0808 00:22:34.469 [0x7fb739583700] DEBUG root (build/wdb/QueryAction.cc:288) - _transmit last=1  0808 00:22:34.469 [0x7fb739583700] DEBUG root (build/wdb/QueryAction.cc:307) - _transmitHeader  0808 00:22:34.469 [0x7fb739583700] INFO  root (build/proto/ProtoHeaderWrap.cc:52) - msgBuf size=256 -> [[0]=40, [1]=13, [2]=2, [3]=0, [4]=0, ..., [251]=48, [252]=48, [253]=48, [254]=48, [255]=48]  0808 00:22:34.469 [0x7fb739583700] INFO  root (build/xrdsvc/SsiSession_ReplyChannel.cc:85) - sendStream, checking stream 0 len=256 last=0    -----    ccqserv124    0808 00:22:25.329 [0x7f25a67bf700] DEBUG ScanSched (build/wsched/ChunkDisk.cc:234) - ChunkDisk registering for 2044 : SELECT MIN(ra) AS QS1_MIN,MAX(ra) AS QS2_MAX,MIN(decl) AS QS3_MIN,MAX(decl) AS QS4_MAX FROM LSST.Object_2044 AS QST_1_ p=0x7f257003d0b8  0808 00:22:25.329 [0x7f25a67bf700] INFO  Foreman (build/wcontrol/Foreman.cc:296) - Runner running Task: msg: session=7 chunk=2044 db=LSST entry time=Fri Aug  7 23:52:06 2015   frag: q=SELECT MIN(ra) AS QS1_MIN,MAX(ra) AS QS2_MAX,MIN(decl) AS QS3_MIN,MAX(decl) AS QS4_MAX FROM LSST.Object_2044 AS QST_1_, sc= rt=r_7bff268d0e369dda8fa314132538a96ad_2044_0  0808 00:22:25.329 [0x7f25a67bf700] INFO  Foreman (build/wdb/QueryAction.cc:177) - Exec in flight for Db = q_b762c96f418726ae3457c74c0350d0c4  0808 00:22:25.329 [0x7f25a67bf700] WARN  Foreman (build/wdb/QueryAction.cc:109) - QueryAction overriding dbName with LSST  0808 00:22:25.330 [0x7f25a50b6700] INFO  root (build/xrdsvc/ChannelStream.cc:122) - returning buffer (44, (last))  0808 00:22:25.330 [0x7f25a50b6700] INFO  root (build/xrdsvc/SsiSession.cc:153) - RequestFinished type=isStream  0808 00:22:26.218 [0x7f25a67bf700] INFO  root (build/wdb/QueryAction.cc:261) - &&& _fillRows size=81  0808 00:22:26.218 [0x7f25a67bf700] DEBUG root (build/wdb/QueryAction.cc:288) - _transmit last=1  0808 00:22:26.218 [0x7f25a67bf700] DEBUG root (build/wdb/QueryAction.cc:307) - _transmitHeader  0808 00:22:26.218 [0x7f25a67bf700] INFO  root (build/proto/ProtoHeaderWrap.cc:52) - msgBuf size=256 -> [[0]=40, [1]=13, [2]=2, [3]=0, [4]=0, ..., [251]=48, [252]=48, [253]=48, [254]=48, [255]=48]  0808 00:22:26.218 [0x7f25a67bf700] INFO  root (build/xrdsvc/SsiSession_ReplyChannel.cc:85) - sendStream, checking stream 0 len=256 last=0 ",4
"Fix socket timeout problem in xrootd framework
When we run a query that take long time, client times out, it closes the socket, which triggers cancellation on the server side.",4
"W16 End-to-End Integration (Data Access portion)
End to end system integration. This epic covers work related to  # loading data generated by pipelines into Qserv  # fully integrating Qserv with SUI",22
"Qserv - webserv integration
Setup Qserv and configure webserv to talk to Qserv. Verify all works, and fix discovered problems.",2
"Add column names metadata to db query results
Per discussion at data access meeting Aug 10, it'd be good to send column names with the query results.",2
"Revisit KPIs for Qserv
Need to come up with KPIs for Qserv  ",2
"Rename ingest.py to reduce confusion with database
The {{ingestImages.py}} bin script provides a camera-agnostic manner of creating a data repository (including a registry).  The back-end code resides in pipe_tasks under the name {{ingest.py}}, and the {{IngestTask._DefaultName = ""ingest""}}, which means that configuration files in obs packages are also named {{ingest.py}}.  This choice of name was unfortunate, as it may be confused with ingest of sources into the database.  We should change the name to reduce this confusion, perhaps {{ingestImages.py}} like the bin script.",2
"add meas_extensions_photometryKron to lsstsw, lsst_distrib
meas_extensions_photometryKron should be added to the CI system, since we are trying to keep it updated.    This is blocked by DM-2429 because that includes a fix for a unit test (which the CI system would have caught).",1
"Processing y-band HSC data fails in loading reference sources
{code}  processCcd.py /lsst3/HSC/data/ --output /raid/price/test --id visit=904400 ccd=50  [...]  processCcd.calibrate.astrometry.solver.loadAN: Loading reference objects using center (1023.5, 2091) pix = Fk5Coord(319.8934727, -0.0006943, 2000.00) sky and radius 0.111920792477 deg  processCcd FATAL: Failed on dataId={'taiObs': '2013-11-03', 'pointing': 672, 'visit': 904400, 'dateObs': '2013-11-03', 'filter': 'HSC-Y', 'field': 'STRIPE82L', 'ccd': 50, 'expTime': 30.0}: Could not find flux field(s) y_camFlux, y_flux  Traceback (most recent call last):    File ""/home/lsstsw/stack/Linux64/pipe_base/10.1-4-g6ba0cc7+15/python/lsst/pipe/base/cmdLineTask.py"", line 320, in __call__      result = task.run(dataRef, **kwargs)    File ""/home/lsstsw/stack/Linux64/pipe_base/10.1-4-g6ba0cc7+15/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/home/lsstsw/stack/Linux64/pipe_tasks/10.1-28-gf9582e4+2/python/lsst/pipe/tasks/processCcd.py"", line 85, in run      result = self.process(sensorRef, postIsrExposure)    File ""/home/lsstsw/stack/Linux64/pipe_base/10.1-4-g6ba0cc7+15/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/home/lsstsw/stack/Linux64/pipe_tasks/10.1-28-gf9582e4+2/python/lsst/pipe/tasks/processImage.py"", line 160, in process      calib = self.calibrate.run(inputExposure, idFactory=idFactory)    File ""/home/lsstsw/stack/Linux64/pipe_base/10.1-4-g6ba0cc7+15/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/home/lsstsw/stack/Linux64/pipe_tasks/10.1-28-gf9582e4+2/python/lsst/pipe/tasks/calibrate.py"", line 457, in run      astromRet = self.astrometry.run(exposure, sources1)    File ""/home/lsstsw/stack/Linux64/pipe_base/10.1-4-g6ba0cc7+15/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/home/lsstsw/stack/Linux64/meas_astrom/10.1-19-g6e01b25+5/python/lsst/meas/astrom/anetAstrometry.py"", line 177, in run      results = self.astrometry(sourceCat=sourceCat, exposure=exposure, bbox=bbox)    File ""/home/lsstsw/stack/Linux64/pipe_base/10.1-4-g6ba0cc7+15/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/home/lsstsw/stack/Linux64/meas_astrom/10.1-19-g6e01b25+5/python/lsst/meas/astrom/anetAstrometry.py"", line 292, in astrometry      astrom = self.solver.determineWcs(sourceCat=sourceCat, exposure=exposure, bbox=bbox)    File ""/home/lsstsw/stack/Linux64/meas_astrom/10.1-19-g6e01b25+5/python/lsst/meas/astrom/anetBasicAstrometry.py"", line 409, in determineWcs      return self.determineWcs2(sourceCat=sourceCat, **margs)    File ""/home/lsstsw/stack/Linux64/meas_astrom/10.1-19-g6e01b25+5/python/lsst/meas/astrom/anetBasicAstrometry.py"", line 437, in determineWcs2      astrom = self.useKnownWcs(sourceCat, wcs=wcs, **kw)    File ""/home/lsstsw/stack/Linux64/meas_astrom/10.1-19-g6e01b25+5/python/lsst/meas/astrom/anetBasicAstrometry.py"", line 308, in useKnownWcs      calib = None,    File ""/home/lsstsw/stack/Linux64/pipe_base/10.1-4-g6ba0cc7+15/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/home/lsstsw/stack/Linux64/meas_algorithms/10.1-15-g0d3ecf6/python/lsst/meas/algorithms/loadReferenceObjects.py"", line 173, in loadPixelBox      loadRes = self.loadSkyCircle(ctrCoord, maxRadius, filterName)    File ""/home/lsstsw/stack/Linux64/pipe_base/10.1-4-g6ba0cc7+15/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/home/lsstsw/stack/Linux64/meas_astrom/10.1-19-g6e01b25+5/python/lsst/meas/astrom/loadAstrometryNetObjects.py"", line 141, in loadSkyCircle      fluxField = getRefFluxField(schema=refCat.schema, filterName=filterName)    File ""/home/lsstsw/stack/Linux64/meas_algorithms/10.1-15-g0d3ecf6/python/lsst/meas/algorithms/loadReferenceObjects.py"", line 40, in getRefFluxField      raise RuntimeError(""Could not find flux field(s) %s"" % ("", "".join(fluxFieldList)))  RuntimeError: Could not find flux field(s) y_camFlux, y_flux  {code}    We should be able to fix this by setting config parameters (e.g., {{calibrate.astrometry.solver.defaultFilter}} or {{calibrate.astrometry.solver.filterMap}}), but how do we keep that synched with the choice of reference catalog?  And once we get past astrometry, we also have the same problem in photocal.",2
"Add support for parsing user log files
In order to get timings for jobs executed, add support for non-dagman generated log files.  These are the user log files that HTCondor writes out for each individual job.",6
"Improve packaging of shared libraries in scons
As discovered through DM-3161, our swig-generated libraries are messy. Specifically, we are dumping everything we might need into the czarLib library. That includes mysql and mysql-client related things. CssLib needs mysql functions too. Given that czar imports both these libraries, we ended up with duplicate symbols. That is being patched in DM-3161, but it needs a further look / cleanup. We need to break things into smaller libraries. Difficulty: understanding dependencies and avoiding circular dependencies.",12
"S17 Long-running Query Optimizations
As discovered through DM-3432 when a query runs for long time, czar does not get response from worker for a long time, and times out. A quick patch we did during Summer 2015 tests was to increase the timeout to a large number.    The issue was discussed at [Qserv mtg Aug 12 2015|https://confluence.lsstcorp.org/display/DM/Database+Meeting+2015-08-12]. If we knew how long a query was going to take, we could set the timeout to appropriate value, but we can't always estimate well how long the query is going to take. The best solution we came up with: periodically check with the worker asynchronously what the status of the query is. Worker should respond with something like: queued, scheduled, working / started x sec ago. Note, this might require changes to the xrdssi API.",26
"Handle problems with connecting to mysql in czar
We observed in S15 tests that if we run too many queries, czar is running out of connections to mysql and as a result through exception that is uncaught (and dies). We triggered this by starting 110 queries. To ""fix"" this problem we increased the max_connections in etc/my.cnf from 256 to 512. So, most likely the easiest way to reproduce it would be to set the max_connections to a very small number.     This story involves handling the ""uncaught exception"" gracefully.",3
"Tweaks to configurations discovered during S15 tests
Apply tweaks we found useful when running large scale tests. This includes:  # etc/my.cnf: change max_connections to 512  # add"":  {quote}export XRD_REQUESTTIMEOUT=64000  export XRD_STREAMTIMEOUT=64000  export XRD_DATASERVERTTL=64000  export XRD_TIMEOUTRESOLUTION=64000{quote}  to init.d/qserv-czar  # add :  {quote}ulimit -c unlimited{quote}  to all startup scripts in init.d. This will make sure core file is always dumped when we have problems.",1
"Resolve problem with running many simultaneous queries
When we run with 110 simultaneous queries, czar fails with ""uncaught exception""",2
"Integrate pipelines with MySQL and Qserv
Load data produced by pipelines into MySQL (on lsst10), and Qserv",5
"AstrometryTask.run return not consistent with ANetAstrometryTask
ANetAstrometryTask.run returns matchMetadata but AstrometryTask.run returns matchMeta. The two must agree. It turns out that matchMeta is more widely used, so I'll standardize on that.",1
"ProcessImageTask.matchSources fails if using ANetAstrometryTask
ProcessImageTask.matchSources fails when using ANetAstrometryTask with the following error:  {code}  processCcd.calibrate.astrometry: Applying distortion correction  processCcd FATAL: Failed on dataId={'taiObs': '2013-11-03', 'pointing': 672, 'visit': 904400, 'dateObs': '2013-11-03', 'filter': 'HSC-Y', 'field': 'STRIPE82L', 'ccd': 50, 'expTime': 30.0}:     File ""src/table/Schema.cc"", line 239, in lsst::afw::table::SchemaItem<T> lsst::afw::table::detail::SchemaImpl::find(const string&) const [with T = double; std::string = std::basic_string<char>]      Field or subfield withname 'astrom_distorted_x' not found with type 'D'. {0}  lsst::pex::exceptions::NotFoundError: 'Field or subfield withname 'astrom_distorted_x' not found with type 'D'.'    Traceback (most recent call last):    File ""/ssd/rowen/lsstsw/stack/Linux64/pipe_base/10.1-4-g6ba0cc7+3/python/lsst/pipe/base/cmdLineTask.py"", line 320, in __call__      result = task.run(dataRef, **kwargs)    File ""/ssd/rowen/lsstsw/stack/Linux64/pipe_base/10.1-4-g6ba0cc7+3/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/ssd/rowen/lsstsw/stack/Linux64/pipe_tasks/tickets.DM-3453-g086c9ddd0a/python/lsst/pipe/tasks/processCcd.py"", line 85, in run      result = self.process(sensorRef, postIsrExposure)    File ""/ssd/rowen/lsstsw/stack/Linux64/pipe_base/10.1-4-g6ba0cc7+3/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/ssd/rowen/lsstsw/stack/Linux64/pipe_tasks/tickets.DM-3453-g086c9ddd0a/python/lsst/pipe/tasks/processImage.py"", line 219, in process      srcMatches, srcMatchMeta = self.matchSources(calExposure, sources)    File ""/ssd/rowen/lsstsw/stack/Linux64/pipe_tasks/tickets.DM-3453-g086c9ddd0a/python/lsst/pipe/tasks/processImage.py"", line 250, in matchSources      astromRet = astrometry.loadAndMatch(exposure=exposure, sourceCat=sources)    File ""/ssd/rowen/lsstsw/stack/Linux64/pipe_base/10.1-4-g6ba0cc7+3/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/ssd/rowen/lsstsw/stack/Linux64/meas_astrom/tickets.DM-3453-gbb2ad1f49c/python/lsst/meas/astrom/anetAstrometry.py"", line 321, in loadAndMatch      with self.distortionContext(sourceCat=sourceCat, exposure=exposure) as bbox:    File ""/ssd/rowen/lsstsw/anaconda/lib/python2.7/contextlib.py"", line 17, in __enter__      return self.gen.next()    File ""/ssd/rowen/lsstsw/stack/Linux64/meas_astrom/tickets.DM-3453-gbb2ad1f49c/python/lsst/meas/astrom/anetAstrometry.py"", line 295, in distortionContext      sourceCat.table.defineCentroid(self.distortedName)    File ""/ssd/rowen/lsstsw/stack/Linux64/afw/10.1-37-gaedf466/python/lsst/afw/table/tableLib.py"", line 8887, in defineCentroid      return _tableLib.SourceTable_defineCentroid(self, *args)  NotFoundError:     File ""src/table/Schema.cc"", line 239, in lsst::afw::table::SchemaItem<T> lsst::afw::table::detail::SchemaImpl::find(const string&) const [with T = double; std::string = std::basic_string<char>]      Field or subfield withname 'astrom_distorted_x' not found with type 'D'. {0}  lsst::pex::exceptions::NotFoundError: 'Field or subfield withname 'astrom_distorted_x' not found with type 'D'.'  {code}  This is probably a result of DM-2939. The basic problem is that the distortion context in ANetAstrometryTask should not be run at that point in processing. [~price] suggests that a simple clean fix is to make the distortion context a no-op if the WCS already contains distortion, if that works. This is what I will try first.",1
"Fix problems with talking from webserv to qserv
Flask or sqlalchemy which are part of webserv are producing some extra queries that are confusing qserv. So basically, at the moment even the simplest query run via webserv that is directed to qserv fails.",2
"Implement prototype stack documentation with Sphinx
Implement a minimally-viable Sphinx documentation repository for the LSST stack. The code is available at https://github.com/lsst-sqre/lsst_stack_docs",5
"Research existing sphinx doc implementations
Examine how python packages such as astropy structure and implement their sphinx docs.",2
"make forced and SFM interfaces more consistent
From [~rowen]:  {quote}  SimpleMeasurementTask.run and ForcedMeasurementTask.run now both take a source catalog, but the two use the opposite order for the first two arguments (one has the catalog first, the other has the exposure first)  {quote}",1
"applyApCorr mis-handles missing data
In ApplyApCorrTask.run the following lines do not behave as expected because get returns None if the data is missing, rather than raising an exception:  {code}              try:                  apCorrModel = apCorrMap.get(apCorrInfo.fluxName)                  apCorrSigmaModel = apCorrMap.get(apCorrInfo.fluxSigmaName)              except Exception:  {code}",1
"Make obs_decam handle raw data 
The current obs_decam expects instrument calibrated data from the community pipeline, i.e. it  requires matching instcal (Instrument Calibrated), dqmask (the associated mask file), and wtmap (weight map) data from the same visit.  This issue is to add functionality so that raw DECam images can be ingested into registry and retrieved by the data butler.     Practically, this will create new or expand existing sub-classes of CameraMapper and IngestTask.        A brief summary of changes:  - The unit test getRaw.py is updated and should pass, with DM-3196     - Working testdata_decam for the unit test is currently at lsst-dev /lsst8/testdata_decam and https://uofi.box.com/testdata-decam  - DecamInstcalMapper is renamed to DecamMapper, to reflect that Butler can also get ""raw"" now besides ""instcal"". Please update _mapper in your data repositories.  - To create a registry for raw data, run  {code:java}     ingestImagesDecam.py /path/to/repo --mode=link --filetype=""raw"" /path/*.fits.fz  {code}  - The default filetype is ""instcal"" for ingestImagesDecam.py, so previous use for instcal stays.  ",13
"psfex lapack symbols may collide with built in lapack
On my Mac meas_extensions_psfex fails to build due to the numpy config test failing. ""import numpy"" fails with:  {code}  dlopen(/Users/rowen/LSST/lsstsw/anaconda/lib/python2.7/site-packages/numpy/linalg/lapack_lite.so, 2): can't resolve symbol __NSConcreteStackBlock in /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libvMisc.dylib because dependent dylib #1 could not be loaded in /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libvMisc.dylib  {code}  Our best guess (see discussion in Data Management 2015-08-14 at approx. 1:57 PM Pacific time) is that the special lapack functions in psfex are colliding with the lapack that anaconda uses.    In case it helps I see this on OS X 10.9.5. I do not see it on lsst-dev.",2
"lsstswBuild.sh --print-fail should report config.log
nan",1
"Make async cancellation more flexible
nan",8
"drawing text to ds9 fails if size or the font family is set
Commands like  {code}  ds9.dot('xxxx', 100, 100, size=3)  ds9.dot('xxxx', 100, 120, fontFamily=""times"")  {code}  silently fail.  The problem is that commands like  {code}  xpaset -p ds9 regions command '{text 100 100 # text=xxxx color=red font=""times 12""}'  {code}  fail; you need to say {{font=times 12 normal}}",1
"Prepare for S15 end-to-end system and Firefly relesase
For S15, DM will try to have an end-to-end system in place, and work out all the issues during the process. SUI will try to a web application to  access some of dada access APIs (dax-) on a system at NCSA.  SUI will have a simple Firefly release to the community.",15
"Install/deploy SUI web application at NCSA
For summer 15 release,  we will deploya SUI web app on NCSA accessible to DM team.    - work with NCSA to have a server setup  - install necessary software packages  - install SUI software  - deploy the system and test ",5
"A simple Firefly release for S15
TO install and deploy a Firefly application from scratch will take a few hours, including getting all the necessary  software packages and install them all.  In order for users to get the system up and running in as little time as possible, we will provide a war file with embedded tomcat server. Users don;t need any third party software except Java1.8.  The time to get the system up and running after installing Java1.8 will be a minute after the download of war file.",10
"Implement Basic spatial lookups for the Butler 
Based on [this |https://confluence.lsstcorp.org/display/DM/Dependencies+on+02C.06+Science+Data+Archive+and+Application+Services] pipeline software will require spatial lookups via butler.    Current plan is:  Provide an ""ingest"" script that looks at a butler exposure dataset, pulls spatial information and data ID out of it, then shoves all of that into an sqlite3 table that can be used for accelerated spatial lookups.  Then provide a task that takes skymap parameters, and maps those to potentially overlapping exposures: the sqlite3 table is outside butler; and the will use a spatial location to get a dataId, and then pass that dataId into butler to get that file. ",10
"Debug problems with near neighbor queries
The following near neighbor query:    {quote}  select o1.ra as ra1, o2.ra as ra2, o1.decl as decl1, o2.decl as decl2,   scisql_angSep(o1.ra, o1.decl,o2.ra, o2.decl) AS theDistance   from Object o1, Object o2   where qserv_areaspec_box(90.299197, -66.468216, 98.762526, -56.412851)   and scisql_angSep(o1.ra, o1.decl, o2.ra, o2.decl) < 0.015  {quote}    fails on the IN2P3 cluster.",4
"Design SQL API for getting query type
When unsure, user should be able to check what type of query a given query is (for example, is the query ""select * from Object where qserv_area_spec(1, 2, 10, 5)"" considered interactive or async? This story involves deciding how the SQL API will look like.",2
"Design API for passing query type
User should be able to pass hint with a query indicating what query type it is. Based on that the return result will either be the query result, or queryId. This story involves designing the API  (sql and RESTful).",3
"Design RESTful APIs for async queries in WebServ
Need RESTul API for:   * retrieving partial results while query is running   * killing async queries",2
"Design SQL APIs for async queries
Need SQL API for:   * submitting async query, note that we should be able to specify where the results are going / what is the format of the results   * retrieving status of async query   * retrieving results of async query   * retrieving partial results of async query while it is running  ",5
"adapt sandbox-jenkins-demo to changes in jfryman/nginx 0.2.7
Changes in the way  jfryman/nginx 0.2.7 handles tls cert files since 0.2.6 have run awful of selinux permissions issues.",2
"Attending Cyber Security Summit
Attending NSF Cyber Security Summit in my capacity as LSST ISO.",3
"Calibration transformation should not fail on negative flux
Before database ingest, measured source fluxes are converted to magnitudes as per DM-2305. The default behaviour of {{afw::image::Calib}} is to throw when a negative flux is encountered, which derails the whole transformation procedure. Better is to return a NaN.",1
"Design RESTful API for getting query type
When unsure, user should be able to check what type of query a given query is (for example, is the query ""select * from Object where qserv_area_spec(1, 2, 10, 5)"" considered interactive or async? This story involves deciding how the RESTful API will look like.",2
"Debug problem with large results set
Query returning 2 billion rows causes problems for czar - czar is using nearly 16 GB or memory. Need to understand why RAM usage in czar is correlated with result size.",1
"replace Calib negative flux behavior methods with context manager
DM-3483 adds a nice context manager for handling the behavior of Calib objects when encountering negative fluxes.  This could be even nicer if we moved it into afw and integrated it with Calib itself, replacing the existing static methods (which are bad because they use global variables).    This will be an API change, and will require an RFC.",2
"Quick-and-dirty n-way spatial matching
This issue will add limited N-way spatial matching of multiple catalogs with identical schemas, sufficient for measuring FY15 KPMs.  It will be a simple wrapper on our existing 2-way matching code in afw, and will not be intended for long term use (as it won't be an efficient algorithm or an ideal interrface).",2
"Update ip_diffim to use the new NO_DATA flag instead of EDGE
In ip_diffImm some uses of EDGE were converted to or supplemented with NO_DATA, but others were not. This ticket handles the missing instances.",1
"Correct for distortion in matchOptimisticB astrometry matcher
matchOptimisticB does not correct for distortion, although an estimate of the distortion is available.  We suspect that doing the matching on the celestial sphere might be ideal, but matching on a tangent plane has worked for HSC.",5
"Fix crosstalk following ds9 interface changes
crosstalk.py in obs_subaru uses ds9 without actually displaying anything, which causes trouble if display_ds9 is not setup.",1
"lsst.afw.display.setMaskTransparency doc doesn't match code
The docstring for {{setMaskTransparency}} says ""Specify display's mask transparency (percent); or *None to not set it when loading masks*"", but (from inspection of the code), it doesn't do anything if {{transparency}} is {{None}}.  ",1
"X16 Large Results
As described in [LDM-135|http://ldm-135.readthedocs.org/en/master/#query-access-related-requirements] high volume queries can return large results (~6 GB per query). Current version of Qserv poorly supports large results. We have observed that a single query that returns ~40 million or two-column rows (~0.5 GB) requires nearly 16 GB of RAM in czar, and czar uses 100% of CPU for extended period of time. This is most likely related to aggregating results - results are currently cached in memory before they are returned to the proxy. This epic involves redesigning this part of Qserv. One reasonable approach would be to write to separate tables from each worker, and deliver the final result as UNION of these tables. Or, perhaps we could use a shared files system and each worker would write results directly there, without burdening czar.    Deliverable: demonstration involving 8 queries each returning 2 GB result handled through a single czar. Czar should not use excessive amount of RAM or CPU.",19
"Add support for images produced by pipelines in end-to-end integration test
We need to add support for images that are produced by pipelines which are run as part of the end-to-end integration tests - ImgServ should be able to serve these images.",6
"Add support for executing async queries through czar
nan",14
"S17 Async Queries in WebServ
Add support for async queries in webserv. Deliverable: webserv that allows to manage async queries (start, kill, retrieve results). Partial results are not covered here.",12
"Implement RESTful API for async queries in WebServ
nan",10
"Research mysql proxy alternatives
Research alternatives to mysql proxy, including things like maxScale.",9
"Improve spatial image search for butler
nan",11
"Modify czar to use per query CSS metadata
Czar is currently caching CSS information, the snapshot is taken when czar starts. Once we start dealing with more dynamic system where databases and tables can come and go anytime while the system is up (in particular, L3 databases and tables), the metadata in CSS would need to be refreshed per query to stay up to date",6
"W16 Support Dynamic CSS Metadata in Czar
Czar needs to support dynamic CSS metadata. This epic involves reworking Facade and related code so that czar can have up to date CSS metadata (per query) instead of relying on static snapshots",40
"Revisit Facade API
nan",6
"Adopt Webserv to work with reworked db module
nan",4
"Expose column metadata via metaserv
We need to expose via RESTful APIs information about columns such as units, ucds, column description etc. It will require querying information_schema with information from our DDT tables. This should include exposing information which columns are the ra/decl columns used for indexing.",6
"Add support for row counts in Metaserv
SUI software would like to frequently check what the row counts for our tables. This story involves caching the information about the row count information in the metaserv.     Note that for L3 tables, that might change, we will need to intercept queries that alter these tables, and update the cache.This is beyond the scope of this story.",4
"Add flag to MetaServ showing if qserv_area_spec is available
SUI software needs to know which tables support spatial queries. This story involves exposing this information via Metaserv (the information is available via CSS) ",3
"Refactor DipoleMeasurement: Dipole classification to plugin
DipoleMeasurementTask currently runs dipole measurement plugins and runs its own implemented dipole classification method. This ticket translates dipole classification to a plugin itself to simplify DipoleMeasurementTask.  Instead of inheriting from SingleFrameMeasurementTask, DipoleMeasurementTask will run SingleFrameMeasurementTask setting the appropriate default slots and plugins (including dipole classification plugin).      ",6
"Base DC network design
Design proposal for Base DC network",20
"prune stale obs_subaru dependencies
obs_subaru has some Eups dependencies that should be removed:   - meas_extensions_multiShapelet (removed from the LSST stack, content moved to meas_modelfit)   - meas_multifit (renamed to meas_modelfit)   - fitsthumb (need to cherry-pick code from HSC to replace it)   - pyfits (investigate what we use it for; it might not be needed)   - psycopg2 (I don't think we'll ever use this on the LSST side; we should add it back to the HSC side after we re-fork)",4
"Create flux environment for input panels
nan",20
"Organize React component to use new flux environment
nan",10
"Releasing un-acquired resources bug
Running a mix of queries: 75 low volume and 10 high volume that include near neighbor failed at some point with    {quote}  terminate called after throwing an instance of 'lsst::qserv::Bug'    what():  ChunkResource ChunkEntry::release: Error releasing un-acquired resource  {quote}    Stack trace:    {quote}  #0  0x00007fb6b0cce5e9 in raise () from /lib64/libc.so.6  Missing separate debuginfos, use: debuginfo-install expat-2.1.0-8.el7.x86_64 glibc-2.17-78.el7.x86_64 keyutils-libs-1.5.8-3.el7.x86_64 krb5-libs-1.12.2-14.el7.x86_64 libcom_err-1.42.9-7.el7.x86_64 libgcc-4.8.3-9.el7.x86_64 libicu-50.1.2-11.el7.x86_64 libselinux-2.2.2-6.el7.x86_64 libstdc++-4.8.3-9.el7.x86_64 nss-softokn-freebl-3.16.2.3-9.el7.x86_64 openssl-libs-1.0.1e-42.el7_1.9.x86_64 pcre-8.32-14.el7.x86_64 xz-libs-5.1.2-9alpha.el7.x86_64 zlib-1.2.7-13.el7.x86_64  (gdb) where  #0  0x00007fb6b0cce5e9 in raise () from /lib64/libc.so.6  #1  0x00007fb6b0ccfcf8 in abort () from /lib64/libc.so.6  #2  0x00007fb6b15d29b5 in __gnu_cxx::__verbose_terminate_handler() () from /lib64/libstdc++.so.6  #3  0x00007fb6b15d0926 in ?? () from /lib64/libstdc++.so.6  #4  0x00007fb6b15cf8e9 in ?? () from /lib64/libstdc++.so.6  #5  0x00007fb6b15d0554 in __gxx_personality_v0 () from /lib64/libstdc++.so.6  #6  0x00007fb6b1069913 in ?? () from /lib64/libgcc_s.so.1  #7  0x00007fb6b1069e47 in _Unwind_Resume () from /lib64/libgcc_s.so.1  #8  0x00007fb6ab554247 in lsst::qserv::wdb::ChunkResourceMgr::Impl::release (this=0x21d1cc0, i=...) at build/wdb/ChunkResource.cc:398  #9  0x00007fb6ab552696 in lsst::qserv::wdb::ChunkResource::~ChunkResource (this=0x7fb68a5f9b70, __in_chrg=<optimized out>)      at build/wdb/ChunkResource.cc:131  #10 0x00007fb6ab560f0f in lsst::qserv::wdb::QueryAction::Impl::_dispatchChannel (this=0x7fb65848c4d0) at build/wdb/QueryAction.cc:392  #11 0x00007fb6ab55f5ab in lsst::qserv::wdb::QueryAction::Impl::act (this=0x7fb65848c4d0) at build/wdb/QueryAction.cc:187  #12 0x00007fb6ab562084 in lsst::qserv::wdb::QueryAction::operator() (this=0x7fb658050548) at build/wdb/QueryAction.cc:450  #13 0x00007fb6ab544f46 in lsst::qserv::wcontrol::ForemanImpl::Runner::operator() (this=0x7fb67400fa20) at build/wcontrol/Foreman.cc:302  #14 0x00007fb6ab551cf0 in std::_Bind_simple<lsst::qserv::wcontrol::ForemanImpl::Runner ()>::_M_invoke<>(std::_Index_tuple<>) (      this=0x7fb67400fa20) at /usr/include/c++/4.8.2/functional:1732  #15 0x00007fb6ab551a8b in std::_Bind_simple<lsst::qserv::wcontrol::ForemanImpl::Runner ()>::operator()() (this=0x7fb67400fa20)      at /usr/include/c++/4.8.2/functional:1720  {quote}    Tail of log file from xrootd log:    {quote}  0821 19:08:58.530 [0x7fb68a6fb700] DEBUG GroupSched (build/wsched/GroupScheduler.cc:139) - _getNextTasks(1)>->->  0821 19:08:58.530 [0x7fb68a6fb700] DEBUG GroupSched (build/wsched/GroupScheduler.cc:151) - Returning 1 to launch  0821 19:08:58.530 [0x7fb68a6fb700] DEBUG GroupSched (build/wsched/GroupScheduler.cc:154) - _getNextTasks <<<<<  0821 19:08:58.530 [0x7fb68a6fb700] DEBUG ScanSched (build/wsched/ScanScheduler.cc:172) - _getNextTasks(29)>->->  0821 19:08:58.530 [0x7fb68a6fb700] DEBUG ScanSched (build/wsched/ChunkDisk.cc:199) - ChunkDisk busyness: yes  0821 19:08:58.530 [0x7fb68a6fb700] DEBUG ScanSched (build/wsched/ChunkDisk.cc:171) - ChunkDisk getNext: current= (scan=10436,  cached=8360,8259,) candidate=10301  0821 19:08:58.530 [0x7fb68a6fb700] DEBUG ScanSched (build/wsched/ChunkDisk.cc:184) - ChunkDisk denying task  0821 19:08:58.530 [0x7fb68a6fb700] DEBUG ScanSched (build/wsched/ScanScheduler.cc:196) - _getNextTasks <<<<<  0821 19:08:58.531 [0x7fb68a6fb700] INFO  root (build/xrdsvc/SsiSession.cc:120) - Enqueued TaskMsg for Resource(/chk/LSST/2732) in 0.001016 seconds  0821 19:08:58.531 [0x7fb6895f8700] DEBUG Foreman (build/wcontrol/Foreman.cc:175) - Registered runner 0x7fb66c141ab0  0821 19:08:58.531 [0x7fb6895f8700] DEBUG Foreman (build/wcontrol/Foreman.cc:209) - Started task Task: msg: session=434445 chunk=2732 db=LSST entry time= frag: q=SELECT o.deepSourceId,o.ra,o.decl,s.coord_ra,s.coord_decl,s.parent FROM LSST.Object_2732 AS o,LSST.Source_2732 AS s WHERE scisql_s2PtInBox(o.ra,o.decl,48.482655,-54.274507,48.555903,-54.196952)=1 AND scisql_s2PtInBox(s.coord_ra,s.coord_decl,48.482655,-54.274507,48.555903,-54.196952)=1 AND o.deepSourceId=s.objectId, sc= rt=r_4344458c9456ede5cbe0b5f42e1a1571d5dd73_2732_0   0821 19:08:58.531 [0x7fb6895f8700] INFO  Foreman (build/wcontrol/Foreman.cc:296) - Runner running Task: msg: session=434445 chunk=2732 db=LSST entry time= frag: q=SELECT o.deepSourceId,o.ra,o.decl,s.coord_ra,s.coord_decl,s.parent FROM LSST.Object_2732 AS o,LSST.Source_2732 AS s WHERE scisql_s2PtInBox(o.ra,o.decl,48.482655,-54.274507,48.555903,-54.196952)=1 AND scisql_s2PtInBox(s.coord_ra,s.coord_decl,48.482655,-54.274507,48.555903,-54.196952)=1 AND o.deepSourceId=s.objectId, sc= rt=r_4344458c9456ede5cbe0b5f42e1a1571d5dd73_2732_0   0821 19:08:58.531 [0x7fb6895f8700] INFO  Foreman (build/wdb/QueryAction.cc:177) - Exec in flight for Db = q_fd51ad249f62fb765e173d7b3cae5d94  0821 19:08:58.531 [0x7fb6895f8700] WARN  Foreman (build/wdb/QueryAction.cc:109) - QueryAction overriding dbName with LSST  0821 19:08:58.718 [0x7fb6a0d8e700] INFO  root (build/wdb/QueryAction.cc:261) - &&& _fillRows size=106  0821 19:08:58.718 [0x7fb6a0d8e700] INFO  root (build/wdb/QueryAction.cc:261) - &&& _fillRows size=210  0821 19:08:58.718 [0x7fb6a0d8e700] INFO  root (build/wdb/QueryAction.cc:261) - &&& _fillRows size=316    ...(thousands of _fillRows lines)    terminate called after throwing an instance of 'lsst::qserv::Bug'    what():  ChunkResource ChunkEntry::release: Error releasing un-acquired resource  {quote}  ",3
"data center requirements
nan",2
"Revise LDM-240
Revising LDM-240.",10
"Add ITIL model use cases to Enterprise Architect
nan",2
"Prepare use case diagrams for LSST 2015 Operations Concepts breakout session
nan",2
"FY19 Query Result Caching
Implement Query caching. Note that most likely we can't ""just"" rely on mysql caching, and we will need to do some custom tweaks, in particular for async queries.    I'd be useful to allow users to ""pin"" results from interesting queries. Typical usecase: user runs a query (it can be interactive) and then decides to keep the results for longer time. ",79
"LOE - Week ending 8/21/15
nan",3
"LOE - Week ending 8/28/15
nan",4
"Information categorization for docushare
Project started to categorize documents on docushare as per LSST's information categorization policy.",3
"Meet with Scott K for IdM project
Scott K will come to NCSA to discuss initial IdM requirements for LSST.",1
"NIST SP 800.82 investigation
NIST SP 800.82 might contain useful information for securing the scada enclave at the observatory site.  Or it might even be the model we should use in full.",2
"Czar Failover in xrdssi
In production we will need to recover from czar failures by automatically failing over to a different czar. This epic involves designing and implementing the interfaces in xrootd xrdssi that will be need to support czar fail over.",20
"Translate more mask pixel bits from instcal data quality mask of DECam data 
The mask pixel bits are defined differently in DECam instcal data from different pipelines, see http://community.lsst.org/t/decam-data-quality-masks/133  for a summary.      The current mapping in DecamInstcalMapper is for the community pipeline Pre-V3.5.0.  More mask bits were defined in DESDM y1 products. This issue provides full mapping for DESDM y1 products so all questionable pixels are translated into the MaskedImage.       ",2
"Fix bugs found in FitsRead 
There was a bug found in writeFitsFile(OutputStream stream, FitsRead[] fitsReadAry, Fits refFits) method where it saved the data from refFits into the output Fits file instead of the data from the FitsRead object.    There is another possible bug under investigation.  The output Fits file created by FitsRead.createNewFits() can not be opened as an image.   ",4
"Refactor ChunkResource for testability
While the wdb/testChunkResource.cc unit test appears to exercise the chunk resource management code, it does not seem to actually test it.    We should refactor the classes in the ChunkResource header and implementation files to make the sanity of the implementation checkable. In particular, I think that Backend from ChunkResource.cc should be a public interface (with a less generic name) that the unit test can implement, and that ChunkResourceMgr should be a concrete class implemented in terms of a user specifiable Backend.    This way the unit test can inject the dependency it wants (namely a mock backend that tracks sub-chunk tables as they are acquired and released), we don't pollute the actual implementation classes with ""I'm fake"" flags and alternate code paths for fake objects, and we can make the unit test actually perform validity tests.    The first cut at this should include a check for the problem described in DM-3522.",4
"Emergent Uncategorized Work
Epic to capture work that is not easy to categorize in other WBSs.",30
"Cleanup of initial astrometry improvements
The astrometry improvements are working, but some cleanup would be good to remove dependencies on A.net and to provide default reference catalog loaders.",20
"Improve aperture correction implemented in HSC
There is technical debt left over from the HSC-LSST merge of the aperture corrections.  This epic will take care of the debt noted during the port.",15
"Move LDM-151 to Sphinx/Read the Docs
Move the LDM-151 (DM applications design document) to restructuredText (built with Sphinx) and published automatically via readthedocs.org.    See discussion at http://community.lsst.org/t/requesting-comments-for-design-documentation-format-for-dm/132?u=jsick    This is an experiment.",1
"SsiService not being destroyed
SsiService::~SsiService is not being called. ",1
"Learn about IPython / Jupyter internals
In order to evaluate the suitability of IPython as a framework for Level 3 work, and its ability to be integrated with the SUI Tools, the internal structure of IPython and its communication protocols need to be understood.    Work on this story will include reading about IPython, experimenting with it, and reading the IPython code as needed.",14
"Further improve the TAN-SIP WCS fitter
lsst.meas.astrom.sip.makeCreateWcsWithSip, our C++ implementation of a TAN-SIP WCS fitter, fits for X and then Y. Thus it must be called several times (e.g. by FitTanSipWcsTask) in order to converge. Please make makeCreateWcsWithSip fit for X and Y simultaneously. This would simplify its use and speed up fitting.    In addition, please investigate whether outlier rejection can be added to makeCreateWcsWithSip. As of DM-3492 outlier rejection is implemented in FitTanSipWcsTask, but it would make makeCreateWcsWithSip easier to use and speed up fitting if makeCreateWcsWithSip did the outlier rejection itself. On the other hand, the current solution may be sufficient.    Once the above is implemented, please simplify FitTanSipWcsTask by removing the unneeded extra iterations used to work around these problems.",12
"Future measurement algorithm enhancements
nan",40
"Attend SciPy 2015 tutorials
Attend the tutorials at SciPy 2015 (July 6-7, 2015) in order to get hands-on experience with current scientific data analysis tools in the Python environment.    Planned attendance:   * Introduction to NumPy  * Building Python Data Applications with Blaze and Bokeh  * Efficient Python for High-Performance Parallel Computing  * Jupyter Advanced Topics Tutorial  ",4
"Discuss the QA visualization needs
Meet with SQAURE lead to discuss the QA needs of visualization tools.     The whole SUIT team of 8 people met with SQUARE lead Frossie for an all day discussion to outline the visualization needs that SQUARE team may need in support of the pipeline stack data processing verification. We agreed that the image visualization and XY 2D plot  components should be separately accessible through JavaScript API and Python API. SQUARE team could build its own web portal using the JavaScript API and build its own analysis tool in iPython notebook using the Python API. The resulting work work was captured in other Epic and stories.   ",18
"Access predefined catalogs via Data Access API
As a part of end-to-end exercise access predefined catalogs and their definitions from the new Data Access API.     We have been doing it via JDBC calls to QServ and queries to http://lsst-web.ncsa.illinois.edu/schema/index.php?sVer=S12_sdss    Even though we can not access QServ via Data Access API at the moment, it should be transparent to us in the future.    As for data definitions, for now we can only access column names and types. In future, more information (like units and field descriptions) should be available.",10
"Binary FITS table catalog upload
We need to be able to upload catalogs in binary FITS table format.    We'll do it by converting the first table in the provided FITS into an IPAC table. Our upload should be handling the conversion.    Later, we should figure out how to handle multiple tables in one FITS.",4
"Ignore ""SELECT @@tx_isolation"" queries
Looks like one of the queries we registered in webserv is: cursor.execute('SELECT @@tx_isolation') and that is bound to confuse Qserv. Need to suppress it at mysql proxy level.",1
"Attend SciPy 2015 conference
Attend the SciPy 2015 conference, July 8-10, 2015, to learn about current trends in this community.    Notably, the conference covers a variety of interactive data analysis tools, including Jupyter/IPython, interactive graphics packages such as VisPy and Bokeh, and astronomical data handling tools such as astropy.",6
"Document SciPy 2015 takeaways
Write up what was learned, and recommendations, from SciPy 2015.",4
"Experiment with Jupyter widget technology and Firefly Tools
Based on DM-2047 work to date, investigate the feasibility of using the Jupyter widget interface to wrap up Firefly tools.",8
"Implement improved Footprints
In S15/DM-1904 we began a redesign of the Footprint API. We need to follow through and convert that to code on disk.",60
"Complete HSC port: object characterization
Merge all functionality from HSC to LSST that is required so that the LSST stack can be used for all standard HSC processing functionality and the HSC fork is removed.",100
"Continued galaxy shear fitting experiments
Build on the test framework delivered in DM-1108 to arrive at conclusions regarding the number of pixels which must be included in galaxy fitting and the number of shapelet terms needed in the PSF.",60
"Refactor executor code
nan",18
"Add unit tests to exercise new scheduler
Add tests (unit test and/or extend the integration test) to test the new scheduler.",10
"Integrate Qserv code with cancellation-friendly xrdssi
nan",6
"Add support for type aliases
nan",6
"Complete HSC port: framework
Merge all framework functionality (afw, middleware, etc) from HSC to LSST that is required so that the LSST stack can be used for all standard HSC processing functionality and the HSC fork can be removed.",60
"Research/reading for PAST prototype C++
nan",6
"Make location of images more flexible
It'd be useful to be able to point imgserv to any location containing images (in particular for various random tests that we will be running over the next few years). Right now this requires changing imgserv code. An idea tossed around: pass the location via URI (optional  parameter)",4
"Collect requirements for pipeline developer visualization tools
This story covers a series of conversations with Robert Lupton, Jim Bosch, Paul Price, K-T Lim, and others going into details about how to make the visualization environment (based on Firefly tools) more useful for developers.",4
"Document AP simulator
Write up how the AP simulator current works, i.e. start the processes doing the AP simulator, sending messages to the base DMCS, how messages are sent from archive to jobs on the worker cluster, etc.",10
"Replace qservAdmin.py use with CssAccess
WE have new CSS interface which unifies C++ and Python and it is time to replace qservAdmin.py with CssAcces in places like qserv-admin.py and data loader.  ",8
"Research requirements for chromaticity
nan",30
"Redesign CalibrateTask
This covers design work only; an accompanying epic (perhaps in 02C.03?) will handle implementation.",10
"Refactor sub-task interfaces
nan",50
"Audit, update & integrate top level tasks
As part of the HSC merge, we've pulled in a number of high-level pipeline tasks, which may not be consistent with existing LSST tasks. Audit the overall pipeline flow, ensuring consistency. To include:    * Rewrite top-level pipeline tasks and Butler datasets.  * Remove ProcessCoaddTask.  * Ensure consistency between MPI drivers and individual CmdLineTasks.",25
"Investigate options for physically motivated PSF models
Rather than developing PSF models based on the optics and the atmosphere, this epic is focused on developing requirements and establishing what resources & expertise are available in other groups (e.g. DESC) which we can make use of. If necessary, it will flow down to another epic in which we actually produce working code.",30
"Develop improved galaxy model flux measurements
Hard thinking, cleaning up & optimizing the existing galaxy model flux measurement code.",50
"DRP W16 emergent work: object characterization
Catch all epic for emergent work in 02C.04.06 during W16.",20
"DRP W16 emergent work: framework
Catch all epic for emergent work in 02C.04.01 during W16.",30
"Firefly infrastructure improvement to support new functions (W16)
This epic will capture the necessary changes of Firefly infrastructure to support new functions needed. It does not include the changes caused by the the conversion from GWT infrastructure to pure JavaScript based system using React and FLUX platform. ",40
"Refactor the Firefly Java code (W16)
This epic will capture all the refactoring work related to Java code. We are converting portions of GWT code to pure JavaScript React based code. We will only refactor the Java code if it is not going to be converted. ",30
"Catalog transformation should pass through all non-measurement fields
We need to include fields added by other tasks (e.g. the deblender) or the source minimal schema (e.g. parent) in transformed catalogs.  I believe it should be safe to assume any such fields can simply be copied (i.e. they do not require any actual transformation - they're mostly flags), but we do need to make the list of fields to be copied by this mechanism dynamic.",3
"Add support for registry-free repository
Existing butler unit tests should run without an sqlite database registry.",12
"Refactor Backend to improve visibility
Backend is a class that connects to mysql and can cause the worker to terminate. It is buried in the Foreman and should be renamed and have the class defined in a header file so that it is more visible. The class should also be available to use for any other in memory tables. ",10
"Firefly support for pipeline visualization needs  (W16)
Data products pipeline needs visualization capabilities for display. Firefly needs to have new capabilities to support it. ",40
"New functions for XY 2D plot (W16)
Firefly should support histogram plot",50
"new algorithm and functions for 2D XY plot (F17)
nan",30
"use devtoolset-3 for Jenkins CI builds on EL6
nan",2
"Firefly support for Camera team visualization needs (W16)
Camera team plans to use Firefly visualization capabilities in camera test.  SUI/T team will continue to support this effort. ",30
"Workspace preliminary functional design  (W16)
Produce a preliminary functional design of user workspace. ",18
"Authentication and authorization system API requirement
Produce a requirement document on the authentication and authorization  API needed for the full SUIT. Work with other DM subsystems (DB, SQUARE, NCSA) closely to achieve this. ",10
"Prepare for system setup in NCSA for LSST web UI (X16)
We want to start preparing for the SUI system setup in Fall 2016 at NCSA to be able to do the following:    Set up a web UI for DM or even general public to access, so we can have a constant end-to-end system. It will be very useful for DM testing. SUIT will use it for simple test of accessing the DAX APIs.  Pipeline team can use it to see the processed data, search source table etc. The goal is to be able to access SDSS strip 82 data through SUIT web portal",10
"Expose more Firefly visualization functions through JavaScript API (W16)
Expose more Firefly visualization functions through JavaScript API to users so they can have more control in building their own web page. ",10
"Expose more Firefly visualization functions through Python API (F16)
Expose more Firefly visualization functions through Python API as needed.",22
"Data access (DAX) API design support (W16)
Continue to work with SLAC  in data access (DAX) API design and test. Participating the discussion and exercising the API as needed to help test and expose potential issues. ",10
"Submit change request for LSE-68, mid-phase-3 update
Collect changes and submit a change request to LSE-68 for a mid-phase-3 update to the document, covering clarifications on guider data and image identification, among others.    Includes preliminary work to prepare for change request.",4
"Prepare document for CCB review of LCR-357
LCR-357 was an outline of work to be done, based on discussions with the Camera DAQ staff.  This task is to generate an actual proposed document change for the CCB.",6
"provide detailed information needed to DAX meta API
SUIT needs certain specific information through DAX meta service when searching for meta data. For Example, what kind of table it is, does it have spatial index to search by position, which set of (ra, dec) columns is the primary one, etc?",1
"The Alert subscription system requirement gathering (F16)
Solidify the requirement for the alert subscription system. ",8
"CCB review and posting of final updated document
Carry out the CCB review, respond to questions, support final implementation of updated document.",2
"Prepare for Winter 2016 work on LSE-68
Use a session at the LSST 2015 all-hands meeting to prepare for LSE-68 work in the Winter 2016 cycle.",3
"option to plot error bars on XY plot
When there are error or uncertainty for a data point, there should be option to plot the error bars in the XY plot. ",6
"Time series plot
Time series plot",10
"capability to reverse the axis
Provide option to reverse the axis for XY plot, one example is the magnitude value convention. ",10
"expose region overlay on image function through JavaScript API
expose region overlay on image function through JavaScript API",5
"Expose image XY readout at cursor point function in JavaScript API
Expose image XY readout at cursor point function in JavaScript API",2
"Review risk register status
nan",3
"Fix bug related to restarting xrootd in wmgr
Changes from DM-2930 are failing integration tests because wmgr is restarting xrootd and now we need to also restart mysqld if xrootd pid changes.",1
"Move env variables related to xrootd/czar and unlimit in etc/sysconfig/qserv
nan",1
"OCS-DM-CCS-DAQ workshop
Prepare for and attend the OCS-DAQ-CCS-DM workshop in November 2014.  This is primarily intended to review the status of LSE-70 (the definition of the OCS interface to the subsystems) and therefore prepare for updates to LSE-72 in time.",6
"OCS-DM-CCS-DAQ workshop II
Prepare for and attend the OCS-DAQ-CCS-DM workshop in February 2015. This is primarily intended to continue to make progress on LSE-70 (the definition of the OCS interface to the subsystems) and therefore prepare for updates to LSE-72 in time.",6
"Overhaul deblender
Re-write the existing deblender code to use the refactored Footprints (DM-3559). Take the opportunity to resolve known issues and add new functionality from SDSS and elsewhere to make the deblender equal to the state of the art.",60
"LDM-144 costing model update
nan",6
"Investigate disk-only (no tape) data releases
The question that is being raised is how much it would cost to keep more than the last two Data Releases on readily-accessible storage (i.e. spinning disk). This will require changing several numbers and formulas in LDM-141, the storage sizing  model, observing the results as they propagate through LDM-144, the cost model,  and checking to make sure that everything makes sense and nothing has been overlooked. It looks like an answer to this question will be needed somewhere between a month and three months from now.",2
"Continue to refine the SUIT requirements (X16)
We did lots of work in FY15 to redefine the SUIT requirements. The picture is getting clear as we talk to others in DM system and science community.  We would like to have a better definition and provide a document for this. ",10
"a catch all epic for unexpected bug fixes  (W16)
This is an epic for unexpected bugs found and need to be fixed in this cycle. ",10
"implement jenkins support for running builds in docker containers
DM-3359 demonstrated the feasibility of running jenkins builds in one-off docker containers but did not cover investigation of user creation of containers nor did it include a puppetized deployment.    Per build containers are desirable for a number of reasons but the largest motivation is allowing DM developers to define their own jobs without disrupting the ""sanctity"" of the lsstsw build slaves.",7
"HSC port: verification
Compare the LSST and (old) HSC stacks for compatibility; identify and resolve any differences. Provide basic integration tests to SQuaRE to ensure the reliable and consistent operation of the LSST stack for processing HSC data. Cooperate with the SQuaRE team to develop more elaborate QA testing.    This work is timeboxed to consume the available effort in W16, and will be continued in the subsequent cycle.",50
"Change root to config in config override files
Implement RFC-62 by using {{config}} rather than {{root}} in config override files for the root of the config.    Note that I propose not modifying astrometry_net_data configs because those are numerous and hidden. They have their own special loader in LoadAstrometryNetObjectsTask._readIndexFiles which could easily be updated later. if desired. An obvious time to make such a transition would be when overhauling the way this data is unpersisted.",2
"Management for Aug 23-29
Hiring -- make offer to candidate, and also review candidates and strategy.  Review proposal for FY 2015 spend  Management meetings.  Openstack meeting.",3
"Study JavaScript, read up on React 
Spend about one hour per day to read up on JavaScript and React framework. Get ready to work on some of the JavaScript programming in Firefly",10
"Add readMatches back to meas_astrom
Recent changes to meas_astrom accidentally removed a function readMatches (copied below). Please restore it, preferably in its own module (though if someday we have more small python functions we may want a utils.py module).    Also please include a unit test.    {code}  def readMatches(butler, dataId, sourcesName='icSrc', matchesName='icMatch', config=MeasAstromConfig(), sourcesFlags=afwTable.SOURCE_IO_NO_FOOTPRINTS):      """"""Read matches, sources and catalogue; combine.      @param butler Data butler      @param dataId Data identifier for butler      @param sourcesName Name for sources from butler      @param matchesName Name for matches from butler      @param sourcesFlags Flags to pass for source retrieval      @returns Matches      """"""      sources = butler.get(sourcesName, dataId, flags=sourcesFlags)      packedMatches = butler.get(matchesName, dataId)      astrom = Astrometry(config)      return astrom.joinMatchListWithCatalog(packedMatches, sources)  {code}  ",4
"Preliminary design of Firefly core  using React and FLUX framework
Propose a preliminary design for Firefly core using React and FLUX framework. ",12
"configDictField.py has code that relies on an undefined variable
While taking a linter pass on {{pex_config}} I found that {{ConfigDict.\_\_setitem\_\_}} in {{configDictField.py}} has some code that uses an undefined variable {{value}}. See the else clause in:    {code}          if oldValue is None:                          if x == dtype:                  self._dict[k] = dtype(__name=name, __at=at, __label=label)              else:                  self._dict[k] = dtype(__name=name, __at=at, __label=label, **x._storage)              if setHistory:                  self.history.append((""Added item at key %s""%k, at, label))          else:              if value == dtype:                  value = dtype()              oldValue.update(__at=at, __label=label, **value._storage)              if setHistory:                  self.history.append((""Modified item at key %s""%k, at, label))  {code}",2
"continue L1 refined specifications 
Re-synchronize with prior work after vacation.    Write up page both engineering and facility database ingest  and use, incorporate suggestion and comments from KT and GDF.    Clean up (better name entities, and move for better narrative flow) pages about the general thing that is called ""level 1"",  -- the context diagram and supporting prose for further descriptions,  Review internally with Jason, Steve and Margaret.    Create a revised  diagram that would change LDM-230 specifications. (all non crosstalk corrected data flow into a disk buffer, and and an new archive tasks empties it)  to make specifications consigned with the refined specifications on the page above.  Yet to write the prose....    These pages are in my personal pages in the LSST confluence area.        ",4
"preliminary detailed content required for Authorization and Authentication system for SUIT
Provide the first draft  of detailed content required for authorization and authentication system from SUIT point of view to NCSA. ",6
"RangeField mis-handles max < min
RangeField contains the following bit of code to handle the case that max < min:  {code}           if min is not None and max is not None and min > max:              swap(min, max)  {code}    This is broken because there is no swap function and if there was it could not work in-place like this. However, rather than replace this with the standard {{min, max = max, min}} I suggest we raise an exception. If max < min then this probably indicates some kind of error or sloppiness that should not be silently ignored. If we insist on swapping the values then at least we should print a warning.    The fact that this bug has never been reported strongly suggests that we never do set min > max and thus that an exception will be fine.",1
"OCS-CCS-DAQ-DM teleconference, April 2015
Prepare for and attend a half-day teleconference on OCS issues.",2
"OCS-CCS-DAQ-DM workshop III, May 2015
Prepare for and attend an OCS-subsystems workshop at SLAC May 6-8, 2015.",6
"Firefly server side extensions using DM stack (F16)
Design and implement a control system to extend Firefly server side capabilities using task in DM stack.  This will make it easier to use DM stack for customized data processing. ",40
"Support OCS revision of LSE-70, LSE-209
Support the OCS efforts to update LSE-70 and create a new associated document, LSE-209.  Getting current versions of these under change control will allow us to complete a round of work on LSE-72.",20
"install DM stack, get familiar with the current DM task concept 
install DM stack, get familiar with the current DM task concept.   This is for getting ready to use task with Firefly server side extension capability.  Here is the link to the tutorial.  https://confluence.lsstcorp.org/display/DM/Getting+started+with+stack+development",6
"Support the design of Firefly core system using React and FLUX
Working with Loi on the design of Firefly core system based React and FLUX frameowrk",4
"Support the design of Firefly core system using React and FLUX 
Working with Loi on the design of Firefly core system based React and FLUX frameowork  ",4
"LSE-72: OCS-CCS-DAQ-DM workshop, July 2015
Work associated with Workshop IV in the series, held at NCSA July 8-10, 2015.",2
"Preliminary SUIT design 
Working with the suit-wg to produce a preliminary design of SUIT.",54
"SUIT design document outline
SUI/T design document outline.  ",2
"on-going support to Camera team in UIUC
Attend UIUC weekly meeting and give support as needed. ",2
"MakeDiscreteSkyMapRunner.__call__ mis-handled returning results
{{MakeDiscreteSkyMapRunner.\_\_call\_\_}} will fail if {{self.doReturnResults}} is {{True}} due to trying to reference undefined variables. This is at least approximately a copy of a problem that was fixed in pipe_base {{TaskRunner}}.    {{MakeDiscreteSkyMapRunner.\_\_call\_\_}} should be fixed in a similar way, and (like {{TaskRunner}}) changed to return a pipe_base {{Struct}}.  ",2
"SUIT design document outline
Work with Gregory on the SUIT design document outline    1.  Requirements flow down, making sure that we design the system satisfying the current requirements.  2.  Use cases collection. at least one typical use case in each major science theme  3.  Levels of different users  ** novice: treat the web portal as a archive to get some information, don't know much about LSST  ** novice expert: has some ideas of what special functions they would like, has some knowledge of LSST data  ** domain expert: knows LSST data very well and want some special functions ready to use  ** savvy expert: knows LSST data very well and like to use API to their own programming    4. functions for all different levels of users  5 system design   ** system diagram  ** details of the different parts  *** Firefly server  *** Firefly client  *** Firefly server extension  *** Firefly JavaScript API  *** Firefly Python API  *** Firefly Python API, Jupyter notebook, and other Python applications  *** workspace and level3 data  *** SUI web portal sketch, workflow  ** dependency on other capabilities of other institutes    6. development and test plan,  timeline  7. deployment plan                ",1
"SUIT design document outline
work with John Rector on SUIT design document outline",1
"Summarize current LSE-75 status as intro for new T&S personnel
With the arrival of new Telescope & Site personnel, especially the Telescope Scientist, [~sthomas], prepare a summary of the current state of LSE-75 and its open issues.",4
"Data loader doesn't work for match tables
qserv-data-loader.py fails to load match tables:   - it does not invoke the correct partitioner executable for them   - not all CSS parameters required for match tables are passed down to the CSS update code",1
"Create change request for LSE-75
Create a change request for LSE-75, the TCS - to - DM ICD.",2
"Discussions on LSE-75 with Telescope & Site personnel
Pursue interactions with Telescope and Site personnel regarding LSE-75, and in particular the issues surrounding calibration data products for the wavefront and guider data analysis pipelines.    Covers work through the end of August 2015.",3
"Initial discussions with Patrick Ingraham
This story is a catch-all for preliminary conversations about LSE-140 with the new Calibration Instrumentation Scientist, Patrick Ingraham.",1
"Review pending work, clean up related JIRA DM- and LIT- issues
nan",2
"Configure VMs to provide additional slots for task switching
We have to set up a new set of slots that will be used to execute the overlapping thread of execution for alert production.   Because of limited resources at this time, this means reconfiguring the worker nodes to provide additional slots, and to split the worker nodes into two sets.    ",3
"Add support for clang and OS X to qserv scons
nan",2
"Basis for HSC integration test
We need to assemble the basis for an integration test using HSC data, to protect HSC processing and obs_subaru from upstream changes.    This includes the assembly of the required data, a basic mechanism to test the mechanics of data release production, sufficient for the SQuaRE team to take it and incorporate it into the Jenkins system.  Addition of any scientific validation is deferred for now.",4
"rename parameter vector methods in afw.geom.ellipses
[~nlust] notes that the {{writeParameters}} and {{readParameters}} methods on the ellipse classes are confusingly named, especially when compared to similar methods on {{meas.modelfit.Model}}.",2
"improve test coverage of CModel failure modes
The CModel has a large number of failure modes, largely dealing with different kinds of problems in the inputs, and a correspondingly large number of flags.  It also has some fairly complex logic determining which flags can be set simultaneously.  All of these combinations need to be tested.    DM-1574 may be useful in capturing these conditions from runs on real data.",6
"Revisit Footprint design
nan",4
"PSFEX does not build if PLplot is installed
During the configure phase PSFEX checks for the presence of PLplot. If PLplot is found then the build fails (at least on a Mac using homebrew):  {code}  /bin/sh ../libtool  --tag=CC   --mode=link clang  -g -O2 -I/Users/timj/work/lsstsw/src/psfex/lapack_functions/include   -o psfex check.o context.o cplot.o diagnostic.o fft.o field.o field_utils.o fitswcs.o homo.o main.o makeit.o makeit2.o misc.o pca.o prefs.o psf.o sample.o sample_utils.o vignet.o wcs_utils.o xml.o ./fits/libfits.a ./levmar/liblevmar.a ./wcs/libwcs_c.a -L/Users/timj/work/lsstsw/stack/DarwinX86/fftw/3.3.3-1-g8fdba61+da39a3ee5e/lib -lfftw3f -lm  -L/Users/timj/work/lsstsw/src/psfex/lapack_functions/lib -llapackstub -lf2c -lm -lplplotd  libtool: link: clang -g -O2 -I/Users/timj/work/lsstsw/src/psfex/lapack_functions/include -o psfex check.o context.o cplot.o diagnostic.o fft.o field.o field_utils.o fitswcs.o homo.o main.o makeit.o makeit2.o misc.o pca.o prefs.o psf.o sample.o sample_utils.o vignet.o wcs_utils.o xml.o  ./fits/libfits.a ./levmar/liblevmar.a ./wcs/libwcs_c.a -L/Users/timj/work/lsstsw/stack/DarwinX86/fftw/3.3.3-1-g8fdba61+da39a3ee5e/lib /Users/timj/work/lsstsw/stack/DarwinX86/fftw/3.3.3-1-g8fdba61+da39a3ee5e/lib/libfftw3f.dylib -L/Users/timj/work/lsstsw/src/psfex/lapack_functions/lib -llapackstub -lf2c -lm -lplplotd  Undefined symbols for architecture x86_64:    ""_plwid"", referenced from:        _cplot_drawloccoordgrid in cplot.o        _cplot_fwhm in cplot.o        _cplot_ellipticity in cplot.o        _cplot_moffatresi in cplot.o        _cplot_asymresi in cplot.o        _cplot_counts in cplot.o        _cplot_countfrac in cplot.o        ...  ld: symbol(s) not found for architecture x86_64  {code}    This particular error is caused by PSFEX using a deprecated PLplot API ({{plwid}}) that is not enabled by default and whose name is not translated to {{c_plwid}}. This PLplot change occurred in version 5.9.10 released in 2013. I assume upstream PSFEX has a fix for this.    Given that LSST does not need the PLplot functionality I think the simplest fix may well be to disable the test for PLplot in our version.    It seems likely that there will be a reasonable number of systems ""in the wild"" who will have PLplot installed so I'm inclined to think that this should be a blocker for the v11.0 release.    If we are lucky people will have all upgraded their PLplot installs to v5.11.0 because in that version PLplot change the name of the library from {{libplplotd}} to {{libplplot}} and PSFEX has hard-wired the former rather than using pkg-config. This results in configure not finding PLplot. I don't think this eventuality is likely though.  ",1
"obs_test needs to override map_camera and std_camera
The Butler can't get a camera unless the map_camera and std_camera are defined correctly.  In most cases the camera can be built by the map_camera method.  In the case of obs_test, the camera is built in the constructor of the Mapper, so std_camera should just return the camera attribute.",1
"W16 Webserv Unit Tests
Implement unit testing across the webserv components. Use either a SQLite backend, or mock objects and mock results to emulate database interaction.",10
"Gather candidates for Verification Datasets and identify collaborators
  Identify candidate Verification sets (in the first instance, datasets with extant processed data used in one-off reductions with lsst_apps to allow a preliminary assessment of current quality from a science QA perspective).     Identify people within the project with effort, expertise and interest available to contribute to this effort. ",10
"Present Verification approach at AHM
  Present talk at AHM to seed Verification effort and seek feedback. ",10
"Resourcing Verification runs
  Identify required resources for Verification runs and communicate them to NCSA.   ",2
"HSC backport: Cleanup interpolation tasks and implement useFallbackValueAtEdge
This is a port of the changesets from:  [HSC-756|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-756]  ",4
"HSC backport: Standalone updates to star object selection
This involves pulling over the following standalone (i.e. non-ticket) HSC commits:  [Updated star selection algorithm.|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/071fcadc016908a10583c746f0a8e79df2a45ead]    [Appropriate config parameter for a unit test of testPsfDetermination.py.|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/e73c5e447ac0b8a71926d3e78fec30aad4beee91]    [Remove HSC specific codes.|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/15bb812578531766199e9a1ee41cc707fb3d9873]  (Note, the above reverts some unwanted camera-specific clauses added in the first commit.  May just squash them to only add the desired features)    [ObjectSizeStarSelector: push non-fatal errors to DEBUG level|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/44f75bc60b41c5f77b323a8d9981048ef7e5f3c4]    [We don't use focal plane coordinates anywhere, and detector may be None|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/4413db4610e4793727e591f395f5ad8cd0cb6030]    [Fixed axis labels|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/67efacaccf8346fdfa1b450617aebabddb2b7ec0]    [Improved PSF debugging plots|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/b1bc91ed1538607eb90e070881a82498fd551909]    [Worked on star selector|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/6b36f4d757187d30142a7e026754a07ffeb8dea2]",1
"Allow building/publishing components off branches other than master
Support of xrootd within the stack is currently complicated by the fact that qserv depends on features that are not available on upstream master (only available on an upstream non-master branch).  Since we can currently only publish packages from master, this means that our lsst fork of xrootd cannot be a ""pure"" fork -- we end up merging/rebasing from an upstream branch, then force-pushing the downstream master.  Upstream and downstream xrootd repos thus have completely different branch topologies, labels, etc., and history of master in the lsst fork is being continually rewritten to carry local patches forward.  The processes of both adopting upstream changes into the lsst fork and the pushing lsst changes back upstream are cumbersome, confusing, and labor intensive.    It is proposed that we extend our tools to allow publishing components from branches other than master.  This would allow us to have xrootd for example be a ""pure"" fork of upstream -- we could then create our own branch based off any upstream branch, carry our downstream patches there, and release off of that.    This functionality could be used similarly for any of our current ""t&p"" components where it would be convenient to track the upstream repo directly and/or carry changes in git instead of in an agglomerated patch file (e.g. when we might want to update frequently and/or contribute general purpose changes back upstream regularly with pr's, etc.)",2
"Jira PMCS EVM integration
This epic captures management support requests and non-SPed activities in FY15 covered by WBS 02C0102 that were not included in other Epics. ",100
"LDM-240 Long range planning
nan",100
"Jira Data Management long range planning project
nan",100
"Release engineering Part Two
This epic covers testing and co-ordination work associated with making  engineering and official releases, and code to support them.      [FE at 70%, JH at 20%, JS at 10%]",40
"Evaluate GitLFS
We want to try out GitLFS and evaluate its suitability as a solution to storing binary data in repos for CI. We want to know how it compares in usability terms to git-annex, git-fat etc. ",17
"Fix PATH and compiler version detection in qserv scons
In recently merged DM-3662 compiler version testing was done using OS tools with regular $PATH. This is inconsistent with other scons tools which reset PATH when executing actions.   We want to do two things:  - propagate PATH to the command execution  - Use scons tools to run ""$CXX --version"" instead of OS tools to keep things consistent",1
"Revisit KPIs for Image Access
Need to come up with KPIs for Image Query Access",4
"lsst_dm_stack_demo failure
Viz:    {code}  $ ./bin/demo.sh  [...]  $ bin/compare detected-sources.txt.expected detected-sources.txt  Failed (max difference 0.439326 over tolerance 0.004000) in column base_GaussianFlux_flux.  Failed (max difference 0.439326 over tolerance 0.004000) in column base_GaussianFlux_fluxSigma.  Failed (max difference 0.244707 over tolerance 0.004000) in column base_PsfFlux_flux.  Failed (max difference 0.244707 over tolerance 0.004000) in column base_PsfFlux_fluxSigma.  {code}    This is on OS X with {{lsst_apps}} {{w_2015_36}}.    ",2
"Forward community.lsst.org (Discourse) notifications to existing mailman lists
Setup a system to forward new post notifications from http://community.lsst.org categories to their appropriate legacy Mailman email list counterparts.    ||Discourse Source Category||Mailmain Forward List||  |DM Team||dm-staff|  |Announcements||dm-announce|  |DM Notifications||dm-devel|    Once this is implemented I will send a notice that all communications and replies should occur on discourse (these mailing lists should be read-only).    I will also send a notice that dm-users is deprecated.  ",7
"CalibrateTask has outdated, incorrect code for handling aperture corrections
The CFHT-specific CalibrateTask tries to apply aperture correction once just after measuring it (which is too early) and again later, at the right time. The error probably has no effect on the final results, but it is confusing and needlessly divergent from the standard CalibrateTask. The required changes are small. I plan to test by running [~boutigny]'s CFHT demo.",1
"HSC backport: Allow for some fraction of PSF Candidates to be reserved from the fitting
This is a port of the changesets from [HSC-966|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-966].    It provides the ability to reserve some fraction of PSF candidates from the PSF fitting in order to check for overfitting and do cross validation.",1
"HSC backport: allow photometric and astrometric calibrations to be required
This is a port of the standalone changesets:  [calibrate: make astrometry failures non-fatal|https://github.com/HyperSuprime-Cam/pipe_tasks/commit/e9db5c0dcdca20e8f7ba71f24f8b797e71699352]  [fixup! calibrate: make astrometry failures non-fatal|https://github.com/HyperSuprime-Cam/pipe_tasks/commit/c2d89396923f9d589822c043ed8753647e70f3f6]  (the above is a fixup, so will likely be squashed)  [make failure to match sources non-fatal|https://github.com/HyperSuprime-Cam/pipe_tasks/commit/cf5724b852937cfcef1b71b7a372552011fda670]  [calibrate: restore original Wcs after initial astrometry solution|https://github.com/HyperSuprime-Cam/pipe_tasks/commit/ab6cb9e206d0456dc764c5ef78ac80ece937c610]  [move CalibrateTask from ProcessImageTask into ProcessCcdTask|https://github.com/HyperSuprime-Cam/pipe_tasks/commit/08a8ec029dd52ac55e47b707a6905df061a40506]  [processCoadd: set detection to use the declared variances|https://github.com/HyperSuprime-Cam/pipe_tasks/commit/9e8563fd8d630dad967786387b1f27b6bc7ee039]  [adapt to removal of CalibrateTask from ProcessImageTask in pipe_tasks|https://github.com/HyperSuprime-Cam/obs_subaru/commit/52733a7ab1731a15cbb93151851f57cec276f928]  and HSC tickets:  [HSC-1085: background not saved in processCcd|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1085] and  [HSC-1086: psf - catalog scatter is very large in some coadds|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1086]",2
"Decrease buildbot  frequency
Buildbot frequency is now down to two builds, one at 19:42 machine time (NCSA) and one at 1:42. This is to stop people needing buildbot runs to eups publish to have to wait before a CI build, since they are now done on[ https://ci.lsst.codes ]/ Jenkins.     ",1
"Add unit tests for secondary index
qproc::testIndexMap.cc is very sketchy and doesn't perform any test for now (i.e. no BOOST_CHECK). It should be improved to really cover code related to secondary index. A mock secondary index is required here, i.e. qproc::FakeBacken should be strengthened.",8
"Install squid proxy on cc-in2p3 build node
Can we add this one to current sprint? It is required to access docker hub on in2p3 cluster.    I also need to automate/document it, test it on build nodes, and be reviewed by in2p3 sysadmins.    Cheers",4
"Replace --trace with --loglevel in pipe_base ArgumentParser
Replace the --trace argument with an enhanced version of --loglevel that supports named values and numeric log levels (which are the negative of trace levels). This simplifies the interface for users and potentially reduces the log level/trace level confusion, though that won't fully happen until we finish replacing use of pex_logging Trace and Debug with Log.    This work was already done as part of DM-3532; it just needs to be copied with minor changes (since there are no named trace levels in pex_logging).",1
"Literature search for DCR -- Sullivan
Go through the literature to find relevant seminal papers on DCR.    The outcome will be a bibliography and executive summary.  This should be posted on Discourse.",15
"Setup and conduct a conversation about DCR in the project
Advertise and conduct a broadly advertised videocon on DCR in the context of diffim.  The result of this should be minutes.  Ideally we would come out of this meeting with a list of possible techniques for dealing with DCR (preferentially sorted by priority).",4
"Assemble the report on DCR
Everything learned through the literature search and project wide meeting should be synthesized into a single readable report that details the expected effects of DCR on difference imaging as well as possible mitigation techniques.    This may involve some preliminary analysis work to measure effectiveness of various techniques.    Note that I expect this to be two weeks of work for two people, thus the 40 story points.  I don't know how to assign a story to two people.",40
"Port suspect pixel flags to meas_base
Pull HSC pixelFlags for suspect and suspect center over from {{meas_algorithms}} to {{meas_base}}. Additionally there are a few places in {{meas_base}} (& possibly in {{_algorithms}} as well) which set flags that have comments such as ""Set suspect flag if it was available"". Each of these places should be updated to use the ported bit. The relevant commit can be found at https://github.com/HyperSuprime-Cam/meas_algorithms/commit/21be65187c30302abb430d59fc5f67730ca7e0a1 and is discussed at https://dev.lsstcorp.org/trac/ticket/2838    It may also be necessary to add or update a unit test to make sure the flag is set appropriately.",4
"Refactor ImageDifferenceTask: Split into two tasks
This issue is to modernize the diffim task.  Once it is running it should be refactored to use the new measurement tasks and reference loading wherever possible.  Also remove any one off code used in past reports.    Measurement should be split into its own task.    The resulting task should be general, but not so general as to make it hard to run.    This issue is being split into three issues:  * DM-3704: split monolithic task into:  ** ImageDifferenceTask - creates image difference  ** ProcessDiffim - perform detection and measurement on image differences (and calexps)  * DM-5294 Refactor/Clean up new ImageDifferenceTask  * DM-5295 Refactor/Clean up new processDiffimTask",10
"fix EventLog references in ctrl_orca
There are a couple references to EventLog in ctrl_orca, which is an object that no longer exists.",4
"qserv scons - do not copy files to variant_dir
Some people are not happy with our current scons setup which copies source files from source directories to variant_dir, it makes it harder to trace errors using tools like eclipse or debug code. Would be nice to get rid of the extra copy, but we still want to have separate build directory (variant_dir). It should be simple enough, I think, but will need some testing of course.",2
"Scons build of lapack_functions in PSFex fails if SCONSFLAGS are set
The scons build system is unaware of extra flags which may be set in SCONSFLAGS environment variable, which are used from scons utils. This will cause the build to fail. The package needs to behave properly and build in the presence of these flags",2
"Prototype DRP sequence using DECam data
Learn the LSST stack and prototype a sequence for Data Release Production using DECam/DES data as inputs.    Assignees: Hsin-Fang Chiang, Robert Gruendl  Duration: September 2015 - February 2016",100
"Refine design specification and requirements analysis of Level 1 and Level 2 systems
Review of existing design documentation and gathering of requirements of all Level 1 & 2 (and 3) systems. Specify both a functional and physical breakdown of the systems for long-term planning and for building the production infrastructure needed at NCSA.    Assignees: Don Petravick, Jason Alt, Paul Wefel, Steve Pietrowicz, Margaret Gelman  Duration: September - December 2015",75
"Margaret's mgmt. activities in August
Local coordination meetings  DMLT meetings  LSST2015 workshop and conference  Hiring/interviewing  Budget review and FY16 prep  Invoice re-breakouts  July TPR  etc.",28
"Revisit shared scans design
nan",10
"Implement feature sets requested by SUI and DRP processing in Process Execution Framework
Extend the Process Execution Framework with feature sets requested by SUI and DRP processing.    Assignees: Matias Carrasco Kind",38
"Operations planning for Archive and US DAC in TOWG/TPWG
Develop use cases and plan for operations in the Technical Operations Working Group and Technical Proposal Working Group.    Assignees: Don Petravick, Margaret Gelman, Chris Pond, Robert Gruendl  Duration: September 2015 - January 2015",58
"Expose table metadata via metaserv
SUI team would find it useful to get counts of columns for a table, ideally, all counts of columns for all tables in a given database in one request. They'd also find it useful if we could send column description.",6
"Analyze Qserv KPIs
Improve script for analyzing KPIs, measure and document KPIs.",24
"Support operation of development infrastructure (lsst-dev and other)
Administrative support to operate the LSST development cluster and LSST's use of the NCSA OpenStack nebula. Includes planned maintenance activities and LOE/emergent work. Expect ~ 1FTE day per week.    Assignees: Bill Glick, Matt Elliot, Bruce Mather, Paul Wefel  Duration: September 2015 - February 2016",49
"Sizing model storage costing update
The question that is being raised is how much it would cost to keep more than the last two Data Releases on readily-accessible storage (i.e. spinning disk). This will require changing several numbers and formulas in LDM-141, the storage sizing model, observing the results as they propagate through LDM-144, the cost model, and checking to make sure that everything makes sense and nothing has been overlooked.    Assignees: Jason Alt  Duration: September 2015",6
"Refine file system policies and services
Refine data management policies and services (e.g., data retention, backup policies). Ideally we would have a draft of this by the November DMLT meeting.    Assignees: Jason Alt, James Parsons  Duration: October - November 2015",13
"Czar dies when parser throws exception
Running a query that mistakenly uses scisql_s2PtInBox instead of qserv_s2PtInBox    {code}select objectId, coord_ra, coord_dec   from smm_bremerton.deepCoadd_forced_src   where scisql_s2PtInBox(coord_ra, coord_dec, 320.05, 0.457, 320.06, 0.46){code}    kills czar, the error message in czar log file is:    {code}ERROR ccontrol.UserQueryFactory (build/ccontrol/UserQueryFactory.cc:117)   - Invalid query: ParseException:Parse error(ANTLR):unexpected token:  scisql_s2PtInBox:{code}    We need to change the code so that a random query does not kill czar. This story involves fixing czar so that it does not die when parser chokes on the syntax.  ",6
"For registry-free butler, look up information in related data type.
Butler reading information (in particular the observation time and length) out of an input dataset's file representation in order to provide rendezvous with calibration data in another repository (that does have a registry). Today, that read is handled by genInputRegistry.py so that the butler doesn't need to look into the dataset itself. If there's no registry, such a read will be necessary.    The ultimate test is to run processCcd.py (a CmdLineTask that runs Instrument Signature Removal) on a repository that contains raw frames that require more than one set of calibrations and have it pick up the correct ones.  That would have to be condensed down into a unit test.",12
"Add Butler access to calibration data in obs_decam
Goals:  - Have something to ingest calibration data and create a calibration registry (calibRegistry.sqlite3)  - Add mapping class calibration into DecamMapper and make butler able to get bias/flat/fringe.  - Include calib in testdata_decam and add to the unit test retrieving calib data.    Summary:  - A new task {{ingestCalibs.py}} is added to {{pipe_tasks}} for parsing through calibration data and creating a calibration registry.  The DECam customized configuration is added in {{DecamCalibsParseTask}} in {{obs_decam}}.   {{pipe_tasks/ingest.py}} is modified slightly for more general use while its original behaviors are kept by default.   - The calibration data are DECam Community-Pipeline products, including nightly-MasterCal bias/flat downloadable from NOAO Science Archive, and fringe of the latest version 56876 from  http://www.ctio.noao.edu/noao/content/decam-calibration-files or also on /lsst8/decam/cal/DECamMasterCal_56876/  Note that files from the two sources have different formats (MEF one HDU for each detector, or one single-HDU fits for each detector).  - New Butler dataset types bias/flat/fringe are added along with functions to standardize them to Exposures in {{DecamMapper}}.  - Tests of retrieving bias/flat/fringe by Butler are added.    - testdata_decam used by the unit tests is at lsst-dev:/lsst8/testdata_decam/    Known caveats for future users  - The task could be a bit noisy when ingesting DECam calibration data products.  But given the variety of data products on hand I might rather have those reminders than letting all pass silently.      - I wouldn't be too surprised if future DECam Community-Pipeline products appear in different formats. Depending on how different they become, {{DecamCalibsParseTask}} might or might not need future modifications.   ",21
"Resolve the issues found in the S15 end-to-end system exercise
There are a few items we need to take care to finish the end-to-end system for S15. ",8
"access the database created and populated for Bremerton end-to-end system
Collect the information for the tables populated for Bremerton end-to-end exercise. Use them in SUI/T so we can access them using the DAX API. ",2
"build the SUI system on NCSA to use the right database and tables
Due to the changes of the database and tables, the system has to be rebuilt.",1
"Resolve the issues accessing the newly populated tables
There are several issues need to be resolved for the system to work properly. ",5
"Fix compiler detection for non-default gcc/g++ compiler
{{scons CXX=g+\+-4.4}} launches {{g\+\+-4.4 --version}} which returns {{g++-4.4 (Debian 4.4.7-2) 4.4.7}}. Nevertheless the {{-4.4}} is not supported by Qserv compiler detection tool. Support will be added here",1
"add RUNID option to EventAppender
A RUNID needs to be added as an option to EventAppender to allow event logging selectors to receive only events for a particular run.",3
"lsst_build's default ref from repos.yaml support is broken when building multiple packages
A problem with the default ref in {{repos.yaml}} support implemented in DM-3679 was discovered last Friday, shortly after deploying this feature to the production CI systems.    The default ref for {{xrootd}} was changed/overridden in {{repos.yaml}} to {{legacy/master}}.  This worked as expected (and as was tested) when setting {{xrootd}} as the sole {{lsstswBuild.sh}} product or when running {{rebuild}} by hand.  However, when building any package that pulled in {{xrootd}} as a recursive dependency, the {{master}} branch was being used (this case had not been manually tested).",1
"HSC backport: updates to tract and patch finding
This is a port of the following HSC updates to how tracts and patches are found and listed given a set of coordinates.  These are all standalone commits (i.e. not associated with a ticket):  [Add findTract() and findTractPatchList() in ringsSkyMap.|https://github.com/HyperSuprime-Cam/skymap/commit/761e915dde25ce8ed5622c2d84b83793e9580fd7]  [move RingsSkyMap.findTractPatchList to BaseSkyMap.findClosestTractPatchlist|https://github.com/HyperSuprime-Cam/skymap/commit/56476142060bdb7d8c7fb59eacc383f0e0d5c85b]  [Small bug fix for RingsSkyMap.findTract().|https://github.com/HyperSuprime-Cam/skymap/commit/f202a7780ebb89166f03479d7447ace1555027c1]  [Add fast findTractPatchList() in RingsSkyMap.|https://github.com/HyperSuprime-Cam/skymap/commit/7e49c358501f95ce4c0e1aa8f48103a24391fc22]  [Fixed the problems regarding poles and RA wrap.|https://github.com/HyperSuprime-Cam/skymap/commit/841b0c9eda7462a7a4f182b7971d5e8e81478bfe]  [Add spaces around '+' and '-' to match LSST standard coding style.|https://github.com/HyperSuprime-Cam/skymap/commit/f7e2f036494afe382e653194c82bb15728c60fc3]",1
"LDM-144 Consistency Update
Due to the 2 year gap between the original authoring date of LDM-144 and the recent update,  the 'costing only' update caused the document to be less self consistent than desired. This work is to do more than 'costing' updates such that the document is useful and on target to be more than a costing umbrella.",3
"server side preparation for  histogram plot (2)
1. On the initialization, server needs to return the following summary table for all numeric columns:       a. column name / description / unit      b. min and max values      c. number of points    2. For a given column and binning options, return the table of bins:     a. first column - numInBin - number of points in a bin     b. second column - binMin - bin's lower bound     c. third column - binMax - bin's upper bound     [10-13-15] Added by LZ  Here is the detailed requirement from Tatiana:  The implementation of this ticket are two search processors (similar to IpacTableFromSource search processor). Both search processors should be located in edu.caltech.ipac.firefly.server.query package and produce an IPAC Table.     1.  Getting table statistics    INPUT:  The input to the first search processor should be TableServerRequest (treq) with ""searchRequest"" parameter set. The searchRequest parameter will be a serialized JSON string, which can represent a search request to any other search processor. This  parameter determines how the input IPAC table is  obtained. (The input IPAC table is produced similarly to how IpacTableFromSource produces the result with ""processor"" parameter set.)    To parse SearchRequest parameter into key-value pairs - the parameters for the search request - you can use json.simple library:    JSONObject searchRequestJSON = (JSONObject)JSONValue.parse(treq.get(""searchRequest""));  if (searchRequestJSON != null) {      for (Object param : searchRequestJSON.keySet()) {          String name = (String)param;          String value = (String)searchRequestJSON.get(param);          }     }  One of the parameters should be the ""id"" (ServerReuqest.ID_KEY) - which tells which search processor to use to obtain input IPAC table.    OUTPUT:  The output IPAC table produced by the first search processor should contain 5 columns: columnName, description, unit (unit or empty string), min, max, numPoints (the number of non-null values).    2.  Getting histogram data    INPUT:  The input to the second processor should be TableServerRequest (treq) with ""searchRequest"" parameter set (exactly the same as ""searchRequest"" parameter in item 1) plus binningOptions. The binningOptions parameter will be serialized JSON object with the following keys (object properties):  {  ""algorithm"" : ""fixedSizeBins"",          ""binSize"" : 20,          ""column"" : ""wavelength"",          ""scale"" : ""linear"",          ""min"" : 0,          ""max"" :100  }    Notes:   -the algorithm is simple fixed size binning for now, in future we'll need to add variable size bins algorithm described here: http://www.astroml.org/examples/algorithms/plot_bayesian_blocks.html  - at the beginning, you can assume that column is column name, but it can be an expression based on several columns.   - scale can be ""linear"" or ""log""  - min and max define filter on the points included into histogram calculation, any of them or both can be absent. If both are absent, no filter needs to be applied.    OUTPUT:  The output IPAC table produced by the second search processor should contain 3 columns:  numInBin, binMin, binMax in that order. The table should be sorted by binMin. The binMax of row i should be always less or equal to binMin of row i+1. If number of points in the bin is 0, it's OK to skip this bin.    ",12
"Fix compiler warns in protobuf clients
Google protobufs 2.6.1 includes a few unnecessary semicolons in some of its supplied header files; these generate a lot of compiler warnings when compiling client packages.    Proposed fix is to add a patch to our eups t&p protobufs package to remove the offending semicolons.",1
"clean up gcc and eclipse code analyzer warns
We've been ignoring some accumulating warns in the qserv build for some time now.  Now that it is possible to develop qserv in eclipse, it would be useful to address warns and analyzer issues so that we can start to notice when new ones pop up.",1
"Rationalize lsst/xrootd repo and maintenance procedures
The procedure for pulling/pushing xrootd changes from/to the upstream official xrootd repo is cumbersome, confusing, and error-prone.    Buildbot now has support for releasing packages from branches other than master.  Given this, we can now reasonably replace our lsst/xrootd repo with a fresh genuine fork (shared history) of upstream, then carry our lsst-specific work forward on a dev-branch.  This will make it much easier to track and contribute to the xrootd project moving forward.    Existing legacy branches and tags are to be migrated to the fresh fork, so historical builds will not be broken.",1
"Review, plan, procure development infrastructure (FY15)
nan",3
"Refining file system policies
nan",2
"W16 Operation of Joint Coordination Council
Activities associated with implementation of the MOA establishing CC-IN2P3 as a satellite processing center during operations. Includes preparation and execution of monthly JCC meetings, as well as a face-to-face meeting for intense coordination scheduled for early November.    Assignees: Don Petravick, Margaret Gelman, Jason Alt, Robert Gruendl  Duration: September 2015 - February 2016",56
"W16 processing control emergent work
Bucket epic for bug fixes and *minor* work that falls outside planned epics.    Assignees: Steve Pietrowicz, Greg Daues, Matias Carrasco Kind, Hsin-Fang Chiang, James Parsons  Duration: September 2015 - February 2016",30
"Update sizing model for February 2016 refresh - technology and costing
Biannual refresh of Sizing Model (LDM-144 et al.), including updates to both costing and technology. As it will be the first technology refresh in at least 2 years, we anticipate it will take a considerable amount of time. To prepare and gain insight on technology and costing trends, this activity includes attending SC2015 in mid-Nov.    Assignees: Jason Alt  Duration: November 2015 - January 2015",38
"Liaise all groups to commission OpenStack for LSST
Greg has been appointed the service manager of the NCSA nebula for LSST. Work with LSST developers and NCSA system engineers to commission the OpenStack for LSST's use.    Assignees: Greg Daues  Duration: September 2015 - February 2016",47
"FY16 Hardware Purchasing Plan
The Annual Acquisition Strategy Document describes the capabilities (hardware, compute cycles, software, licenses, etc.) planned for procurement during the fiscal year. We consider systems that will satisfy the needs of developers, systems for prototyping the production infrastructure, etc.    Assignees: Jason Alt, Bill Glick  Duration: September - October 2015",11
"Evaluate PASTRY DHT implementation
The David Keller kademlia implementation used in the earlier prototype has some bugs/limitations.  Try to find a better off-the-shelf DHT and integrate with prototype framework.",6
"obs_test data mis-assembled
obs_test images are mis-assembled and need to be regenerated. This may affect some existing unit tests that rely on the data.",2
"(FY16) Initial discussions and requirement consolidation
nan",2
"remove install_name_tool fix to libpython2.7.dylib from anaconda package
Now that SIM-1314 has been merged, we should be able to remove the    {code}  	if [[ $(uname -s) = Darwin* ]]; then  		#run install_name_tool on all of the libpythonX.X.dylib dynamic  		#libraries in anaconda  		for entry in $PREFIX/lib/libpython*.dylib  		do  			install_name_tool -id $entry $entry  		done  	fi  {code}    from eupspkg.cfg.sh in the EUPS anaconda package, and still have GalSim build correctly",1
"Enable SSL to community.lsst.org
Enable SSL (https) for the Discourse site at community.lsst.org",1
"Update flag names and config override files to current conventions
The {{deblend.masked}} and {{deblend.blendedness}} flag names in {{meas_deblender}} need to be updated to use underscores instead of periods.  Various flag names in the {{examples}} scripts also need updating to the underscore and camelCase format.    A search for these flags throughout the database revealed a number of config files that need updating to current conventions.  These are also included here.",1
"testProcessCcd.py computes values that are too different between MacOS and linux
tests/testProcessCcd.py runs processCcd on visit 1 of obs_test's data repository. The result on MacOS is surprisingly different than on linux in at least one case: psfShape.getIxx() computes 2.71 on MacOS X and 2.65 on linux. Iyy and Ixy are likely different. It's worth checking all other computed values, as well. These differences likely indicate that something is wrong, e.g. in obs_test, processCcd, or the way the test runs processCcd.    This showed up as part of fixing DM-3792, but it is not clear if the changes on DM-3792 actually caused or increased the difference between MacOS and linux, or if the difference was always too large, but was masked by an intentionally generous tolerance in the unit test.",2
"The gains in obs_test's amplifier table appear to be incorrect
As of DM-3792 the gains in obs_test's camera's amplifier table were set to the values reported in the headers of the lsstSim raw data used to generate obs_test's raw data. (Before that one nominal gain was used for all amplifiers).    However, [~rhl] reports that these gains are incorrect. He measured the following gains by scaling the nominal gains by the median values in the bias-subtracted data:  {code}  amp   meas      curr  name  gain      gain  00    1.7741    1.7741  01    1.8998    1.65881  10    1.8130    1.74151  11    1.8903    1.67073  {code}    We could simply adopt these values, but I would like to understand why the gains are so far off from those reported by phoSim in the raw data.",4
"The obs_test's sensor is shown 90 degrees rotated from that desired, in camera coords
When plotting the obs_test sensor, e.g. using lsst.afw.cameraGeom.utils.plotFocalPlane, the image is a short, wide rectangle. This suggests that the camera coordinate frame is rotated 90 degrees from the CCD coordinate frame (which has 1018 pixels in X and 2000 pixels in Y).    We would prefer that the camera frame and CCD frame have the same orientation.",1
"Fix Qserv compiler warnings with clang
Qserv triggers numerous warnings with clang on OS X. Full details are in the attached ticket, here we summarize the distinct warnings classes:    h5. Protobuf    {code}  /Users/timj/work/lsstsw/stack/DarwinX86/protobuf/2.6.1+fbf04ba888/include/google/protobuf/unknown_field_set.h:214:13: warning: anonymous types declared in an anonymous union        are an extension [-Wnested-anon-types]      mutable union {              ^  {code}    h5. Qserv    {code}  In file included from core/modules/sql/statement.cc:32:  core/modules/sql/Schema.h:74:1: warning: 'Schema' defined as a struct here but previously declared as a class [-Wmismatched-tags]  struct Schema {  ^  core/modules/sql/statement.h:35:1: note: did you mean struct here?  class Schema; // Forward  ^~~~~  struct  {code}    {code}  core/modules/proto/WorkerResponse.h:34:1: warning: 'WorkerResponse' defined as a struct here but previously declared as a class [-Wmismatched-tags]  struct WorkerResponse {  ^  core/modules/ccontrol/MergingRequester.h:38:3: note: did you mean struct here?    class WorkerResponse;    ^~~~~    struct  {code}    {code}  In file included from core/modules/qana/QueryMapping.cc:46:  core/modules/qproc/ChunkSpec.h:51:1: warning: 'ChunkSpec' defined as a struct here but previously declared as a class [-Wmismatched-tags]  struct ChunkSpec {  ^  core/modules/qana/QueryMapping.h:44:5: note: did you mean struct here?      class ChunkSpec;      ^~~~~      struct  {code}    {code}  core/modules/qana/TableInfo.h:186:1: warning: 'DirTableInfo' defined as a struct here but previously declared as a class [-Wmismatched-tags]  struct DirTableInfo : TableInfo {  ^  core/modules/qana/TableInfo.h:86:1: note: did you mean struct here?  class DirTableInfo;  ^~~~~  struct  core/modules/qana/TableInfo.h:221:1: warning: 'ChildTableInfo' defined as a struct here but previously declared as a class [-Wmismatched-tags]  struct ChildTableInfo : TableInfo {  ^  core/modules/qana/TableInfo.h:87:1: note: did you mean struct here?  class ChildTableInfo;  ^~~~~  struct  core/modules/qana/TableInfo.h:260:1: warning: 'MatchTableInfo' defined as a struct here but previously declared as a class [-Wmismatched-tags]  struct MatchTableInfo : TableInfo {  ^  core/modules/qana/TableInfo.h:88:1: note: did you mean struct here?  class MatchTableInfo;  ^~~~~  struct  In file included from core/modules/qana/ColumnVertexMap.cc:36:  core/modules/qana/RelationGraph.h:513:1: warning: struct 'Vertex' was previously declared as a class [-Wmismatched-tags]  struct Vertex;  ^  core/modules/qana/ColumnVertexMap.h:44:7: note: previous use is here  class Vertex;        ^  In file included from core/modules/qana/ColumnVertexMap.cc:36:  core/modules/qana/RelationGraph.h:547:1: warning: 'Vertex' defined as a struct here but previously declared as a class [-Wmismatched-tags]  struct Vertex {  ^  core/modules/qana/ColumnVertexMap.h:44:1: note: did you mean struct here?  class Vertex;  ^~~~~  struct  {code}    {code}  core/modules/wbase/Base.h:72:1: warning: 'ScriptMeta' defined as a struct here but previously declared as a class [-Wmismatched-tags]  struct ScriptMeta {  ^  core/modules/wbase/Task.h:41:5: note: did you mean struct here?      class ScriptMeta;      ^~~~~      struct  {code}    {code}  In file included from core/modules/parser/BoolTermFactory.cc:46:  core/modules/query/Predicate.h:86:27: warning: 'lsst::qserv::query::GenericPredicate::putStream' hides overloaded virtual function [-Woverloaded-virtual]      virtual std::ostream& putStream(std::ostream& os) = 0;                            ^  core/modules/query/Predicate.h:71:27: note: hidden overloaded virtual function 'lsst::qserv::query::Predicate::putStream' declared here: different qualifiers (const vs none)      virtual std::ostream& putStream(std::ostream& os) const = 0;                            ^  {code}    {code}  core/modules/parser/FromFactory.cc:62:15: warning: unused function 'walkToSiblingBefore' [-Wunused-function]  inline RefAST walkToSiblingBefore(RefAST node, int typeId) {                ^  core/modules/parser/FromFactory.cc:72:1: warning: unused function 'getSiblingStringBounded' [-Wunused-function]  getSiblingStringBounded(RefAST left, RefAST right) {  ^  {code}    {code}  In file included from core/modules/wsched/ChunkDisk.cc:25:  core/modules/wsched/ChunkDisk.h:130:10: warning: private field '_completed' is not used [-Wunused-private-field]      bool _completed;           ^  {code}    {code}  In file included from core/modules/parser/PredicateFactory.cc:45:  core/modules/query/Predicate.h:86:27: warning: 'lsst::qserv::query::GenericPredicate::putStream' hides overloaded virtual function [-Woverloaded-virtual]      virtual std::ostream& putStream(std::ostream& os) = 0;                            ^  core/modules/query/Predicate.h:71:27: note: hidden overloaded virtual function 'lsst::qserv::query::Predicate::putStream' declared here: different qualifiers (const vs none)      virtual std::ostream& putStream(std::ostream& os) const = 0;                            ^  {code}    {code}  core/modules/parser/WhereFactory.cc:265:31: warning: binding reference member 'c' to stack allocated parameter 'c_' [-Wdangling-field]      PrintExcept(Check c_) : c(c_) {}                                ^~  core/modules/parser/WhereFactory.cc:291:28: note: in instantiation of member function 'lsst::qserv::parser::PrintExcept<lsst::qserv::parser::MetaCheck>::PrintExcept' requested        here      PrintExcept<MetaCheck> p(mc);                             ^  core/modules/parser/WhereFactory.cc:269:12: note: reference member declared here      Check& c;             ^  {code}    {code}  core/modules/rproc/ProtoRowBuffer.cc:44:11: warning: unused variable 'largeRowThreshold' [-Wunused-const-variable]  int const largeRowThreshold = 500*1024;            ^  {code}    {code}  core/modules/util/testIterableFormatter.cc:85:43: warning: suggest braces around initialization of subobject [-Wmissing-braces]      std::array<std::string, 6> iterable { ""1"", ""2"", ""3"", ""4"", ""5"", ""6""};                                            ^~~~~~~~~~~~~~~~~~~~~~~~~~~~                                            {                           }  {code}    {code}  In file included from core/modules/qdisp/XrdSsiMocks.cc:37:  core/modules/qdisp/XrdSsiMocks.h:64:16: warning: private field '_executive' is not used [-Wunused-private-field]      Executive *_executive;                 ^  {code}    {code}  core/modules/xrdoss/QservOss.cc:77:1: warning: unused function 'print' [-Wunused-function]  print(std::ostream& os, lsst::qserv::xrdoss::QservOss::StringSet const& h) {  ^  {code}    h5. OS X    {code}  core/modules/qdisp/QueryRequest.h:54:25: warning: 'lsst::qserv::qdisp::BadResponseError::what' hides overloaded virtual function [-Woverloaded-virtual]      virtual char const* what() throw() {                          ^  /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1/exception:95:25: note: hidden overloaded virtual function        'std::exception::what' declared here: different qualifiers (const vs none)      virtual const char* what() const _NOEXCEPT;                          ^  In file included from core/modules/qdisp/Executive.cc:64:  In file included from core/modules/qdisp/XrdSsiMocks.h:37:  core/modules/qdisp/QueryRequest.h:67:25: warning: 'lsst::qserv::qdisp::RequestError::what' hides overloaded virtual function [-Woverloaded-virtual]      virtual char const* what() throw() {                          ^  /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1/exception:95:25: note: hidden overloaded virtual function        'std::exception::what' declared here: different qualifiers (const vs none)      virtual const char* what() const _NOEXCEPT;                          ^  {code}    {code}  core/modules/proto/TaskMsgDigest.cc:55:5: warning: 'MD5' is deprecated: first deprecated in OS X 10.7 [-Wdeprecated-declarations]      MD5(reinterpret_cast<unsigned char const*>(str.data()),      ^  /usr/include/openssl/md5.h:116:16: note: 'MD5' has been explicitly marked deprecated here  unsigned char *MD5(const unsigned char *d, size_t n, unsigned char *md) DEPRECATED_IN_MAC_OS_X_VERSION_10_7_AND_LATER;                 ^  {code}    {code}  core/modules/util/StringHash.cc:78:24: warning: 'SHA1' is deprecated: first deprecated in OS X 10.7 [-Wdeprecated-declarations]      return wrapHashHex<SHA1, SHA_DIGEST_LENGTH>(buffer, bufferSize);                         ^  /usr/include/openssl/sha.h:124:16: note: 'SHA1' has been explicitly marked deprecated here  unsigned char *SHA1(const unsigned char *d, size_t n, unsigned char *md) DEPRECATED_IN_MAC_OS_X_VERSION_10_7_AND_LATER;                 ^  core/modules/util/StringHash.cc:83:24: warning: 'SHA256' is deprecated: first deprecated in OS X 10.7 [-Wdeprecated-declarations]      return wrapHashHex<SHA256, SHA256_DIGEST_LENGTH>(buffer, bufferSize);                         ^  /usr/include/openssl/sha.h:150:16: note: 'SHA256' has been explicitly marked deprecated here  unsigned char *SHA256(const unsigned char *d, size_t n,unsigned char *md) DEPRECATED_IN_MAC_OS_X_VERSION_10_7_AND_LATER;                 ^  {code}    h5. Xrootd    {code}  In file included from core/modules/qdisp/Executive.cc:64:  In file included from core/modules/qdisp/XrdSsiMocks.h:33:  In file included from /Users/timj/work/lsstsw/stack/DarwinX86/xrootd/u.timj.DM-3584-ge22410fa7f+da39a3ee5e/include/xrootd/XrdSsi/XrdSsiRequest.hh:37:  /Users/timj/work/lsstsw/stack/DarwinX86/xrootd/u.timj.DM-3584-ge22410fa7f+da39a3ee5e/include/xrootd/XrdSsi/XrdSsiRespInfo.hh:43:1: warning: 'XrdSsiRespInfo' defined as a        struct here but previously declared as a class [-Wmismatched-tags]  struct  XrdSsiRespInfo  ^  /Users/timj/work/lsstsw/stack/DarwinX86/xrootd/u.timj.DM-3584-ge22410fa7f+da39a3ee5e/include/xrootd/XrdSsi/XrdSsiSession.hh:45:1: note: did you mean struct here?  class XrdSsiRespInfo;  ^~~~~  struct  {code}    {code}  core/modules/xrdoss/QservOss.h:64:17: warning: 'lsst::qserv::xrdoss::FakeOssDf::Opendir' hides overloaded virtual function [-Woverloaded-virtual]      virtual int Opendir(const char *) { return XrdOssOK; }                  ^  /Users/timj/work/lsstsw/stack/DarwinX86/xrootd/u.timj.DM-3584-ge22410fa7f+da39a3ee5e/include/xrootd/XrdOss/XrdOss.hh:63:17: note: hidden overloaded virtual function        'XrdOssDF::Opendir' declared here: different number of parameters (2 vs 1)  virtual int     Opendir(const char *, XrdOucEnv &)           {return -ENOTDIR;}                  ^  {code}    {code}  In file included from core/modules/xrdsvc/SsiSession.h:32:  /Users/timj/work/lsstsw/stack/DarwinX86/xrootd/u.timj.DM-3584-ge22410fa7f+da39a3ee5e/include/xrootd/XrdSsi/XrdSsiResponder.hh:177:27: warning: control may reach end of        non-void function [-Wreturn-type]                            }                            ^  {code}    h5. boost    {code}  /Users/timj/work/lsstsw/stack/DarwinX86/boost/1.55.0.1.lsst2+fbf04ba888/include/boost/regex/v4/regex_raw_buffer.hpp:132:7: warning: 'register' storage class specifier is        deprecated [-Wdeprecated-register]        register pointer result = end;        ^~~~~~~~~  {code}",1
"Fix order of arguments change in meas_base SingleFrameMeasurement
In sfm.py on line 271, a comment indicates that some code is a temporary work around until the switch from meas_algorithms to meas_base is complete. This work is complete, so this temporary workaround should be removed, or if it is decided it should be kept, the comment should be removed. See https://github.com/lsst/meas_base/blob/tickets/DM-2915/python/lsst/meas/base/sfm.py#L271",2
"convert newinstall.sh to use miniconda instead of anaconda
To match the conversion of lsstsw from anaconda -> miniconda to reduce the disk footprint and improve install times.",1
"Setup lsst_sphinx_kit package structure
Setup the lsst_sphinx_kit package, including    * setup.py  * unit tests, tox and Travis CI  * README stub  * Sphinx stub and readthedocs",1
"HSC backport: Include documentation strings for config parameters when they are dumped
This is a port of the following HSC tickets:  [HSC-1072|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1072]  and  [HSC-1175|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1175]",4
"Migrate LDM-152 to reST Design Doc Platform
nan",1
"Intermittent build failures on v11 candidate with eups distrib
We are seeing frequent intermittent failures on a variety of platforms when installing the v11 candidate with eups distrib install. Repeating the command works. It doesn't seem to be evenly distributed between packages: cfitsio, -meas_astrom-(?), meas_algorithms, and obs_lsstSim have been seen multiple times. It's been seen in the release verification CI (that uses the documented user facing process of eups distrib install instead of the factory CI which uses lsstsw) and in individual user ""manual"" builds on lsst-dev.     Test build key:  ||buildbot build # || eups tag || refs || comment ||  ||b1688 || t20150914-b1688 || tickets/DM-3829 v11_0_rc2 || ||  ||b1689 || t20150914-b1689 || tickets/DM-3829 tickets/DM-3815-intermittent-build-failure v11_0_rc2 || ||  ||b1690 || t20150914-b1690 || tickets/DM-3829 v11_0_rc2 master || modifications to tickets/DM-3829 ||  ||b1692 || t20150915-b1692 || tickets/DM-3829 ticket/DM-2752-egg-error v11_0_rc2 || ||",6
"levels in DecamMapper.paf is not quite right
When ccdnum is not given as part of the dataId, instead of iterating over it, an error like this happens    {code:java}    RuntimeError: No unique lookup for ['ccdnum'] from {'visit': 205344}: 61 matches  {code}    Likely a problem in policy/DecamMapper.paf",1
"Deploy docker images on ccqserv124/149
Creation of configured master and worker image will be improved here, and a deploymen tool (like swarm, of hand-made) will be used to deploy images over in2p3 cluster.",6
"Bi-weekly PO security meeting
Bi-weekly meeting with PO on cyber security.",1
"IdM work
Work for LSST Identity management and authentication.",2
"NIST SP 800.82 investigation
NIST SP 800.82 investigation for a more cohesive SCADA enclave security plan.",2
"Recent CModel bugfixes from HSC
I've just fixed two rather critical bugs in the CModel code on the HSC side (they would have been introduced on the LSST side in the last transfer, DM-2977):   - The {{minInitialRadius}} configuration parameter had a default that is too small, causing many galaxies to be fit with point source models, leading to bad star/galaxy classifications.  This is HSC-1306.   - There was a simple but important algebra error in the uncertainty calculation, making the uncertainty a strong function of magnitude.  This is HSC-1313.    On the LSST side, the transfer should be quite simple; we'll have to rewrite a bit of code due to the difference in measurement frameworks, but there was very little to begin with (most of the effort in the HSC issues was in debugging).",1
"Orchestration work to support verification dataset processing
Upgrades, modifications, fixes, etc. to orchestration framework for use in SQuaRE's verification dataset processing tests.    Assignees: Steve Pietrowicz, Greg Daues  Duration: September 2015 - February 2016",27
"W16 General Management Activities
nan",100
"meas_astrom bugs exposed by new Eigen
Trying a newer Eigen has exposed several issues in meas_astrom:  - tests/createWcsWithSip.py blindly uses sipWcs in the result returned by ANetBasicAstrometryTask.determineWcs2, but that attribute may be None  - ANetBasicAstrometryTask.determineWcs2 terminates iteration early if the # of matches goes down, even though the result may be improved. In the case in question the first fit iteration results in significantly better RMS error, but has one fewer matches, so the SIP fit is rejected, triggering the first bug mentioned.",6
"write meeting agenda, for Sept 14 meeting 
nan",1
"Further refinement 
added the image and engineering facility database.  and Observatory Operations Server.",10
"email discussion w.r.t Service separation for L1, and also some work on ITIL roles
email to German about the all the L1 stuff and clarified that the L1 system were a  derive provided to the Telescopes site (important for fitting this into the proper place in the who is worrying about what hierarchy.  Also, revised SA to see fi the EPO changes discusses affected the IT roles in th model (not apparently)  lastly attitude the TOWC ",2
"Management for week ending sept 11
Deal with Hiring James Parsons, and interfacing with the new NCSA organizations that will support LSST at NCSA, general group management issues",2
"Deploy FY16 Storage Expansion
Install, and deploy a prototype production GPFS cluster.",20
"Preparation work to process raw DECam data
Try to run processCcd.py with raw DECam data and see what are yet to be solved for it to run. ",7
"Deploy FY16 Nebula Expansion
* Install, test, and deploy additional capacity for the NCSA nebula.    (Note: this work occurs outside of LSST so we can only track it at a less granular detail than other deployments. We can include the final story for blockers however.)",18
"Decommission old development infrastructure
Decommission old hardware currently in the LSST development cluster when replacement equipment arrives and is provisioned.    Assignees: Bill Glick, Matt Elliot, Bruce Mather  Duration: November - December 2015",19
"Migrate LDM-230 to new docs platform
Convert LDM-230 from Word to restructuredText and deploy on readthedocs.org",2
"Migrate LDM-135 to new design docs platform
Convert LDM-135 from Word to restructuredText and deploy on readthedocs.org",5
"Migrate LDM-129 to new design docs platform
Convert LDM-129 from Word to restructuredText and deploy onto readthedocs.org",2
"SuperTask Redesign
Redesign pipe_base to allow the creation of supertasks which will be more flexible for  different execution applications",11
"Evaluate authentication and authorization services for user workspace
Develop an Identity and Access Management (IAM) program.    Deliverables (from SOW):  - LSST IAM Design Document: describe LSST's current and expected IAM needs and specify technical recommendations for the LSST IAM system architecture, including interface standards (e.g., LDAP, OAuth) and system components (with functional descriptions and implementation recommendations).  - LSST IAM Program Plan: specify future and ongoing IAM development and operational activities required to meet the LSST project mission.  - LSST IAM Technical Demonstration: provide an initial implementation of the LSST IAM design according to the philosophy of ""rough consensus and running code."" The project will prioritize technical implementation work based on the immediate needs identified by LSST developers for functional implementations of IAM system interfaces to ""fill the gaps"" needed for LSST development and integration work to proceed on schedule.    Assignees: Jim Basney, Terry Fleurry, Daniel Thayer, Alex Withers (ISO), Don Petravick, Jason Alt, Margaret Gelman  Duration: October 2015 - March 2016",88
"Graphical representation Example demo
nan",2
"LSE-72: Phase 3 in X16
Advance Phase 3 details as needed to eliminate obstacles to OCS and DM development during F16.",8
"LSE-75: Refine WCS and PSF requirements in W16
Clarify the data format and precision requirements of the TCS (or other Telescope and Site components) on the reporting of WCS and PSF information by DM on a per-image basis.    Depends on the ability of the T&S group to engage with this subject.    Current PMCS deadline for Phase 3 readiness of LSE-75 is 29-Sep-2015.",8
"LSE-68: ICD Details in X16
Bring ICD to phase 3 level of detail",6
"LSE-74: ICD Details in W16
""Bring ICD to phase 3 level of detail"" was the original specification, but actual work by the OCS group in the Winter 2016 period didn't quite reach Phase 3.  Nonetheless, a very useful revision was submitted to and recommended for approval by the CCB at the March meeting.",6
"Add tutorial-level documentation for ctrl_pool
The new ctrl_pool package (port of hscPipeBase) has no tutorial-level documentation, making it hard to figure out how to start using the package.    Unfortunately, I think only [~price] is qualified to write it directly, though it may make sense to have someone unfamiliar write it while bugging Paul a lot, both to transfer the knowledge and target the documentation better.",4
"Add unit tests for ctrl_pool
ctrl_pool (formerly hscPipeBase) is being ported with no unit tests - the only testing is an example script that can be run by hand to demonstrate a piece of the functionality.    Some functionality may simple not be amenable to tests (such as batch submission).  Other parts may be tricky to run via SCons because they're intrinsically parallel, and SCons naively wants to be able to run each test in a separate process.  Overcoming those problems is the reason this is challenging - there isn't really that much functionality to test.",8
"Read over LSE-70 and LSE-209 and discuss for meeting
Read over LSE-70 and LSE-209 for meeting on Friday 9/11.",2
"Grid overly bug
when using the grid overlay in galatic coordinate over an image that is around longitude = 0, the overlay doesn't work properly. In BOLOCAM, on fits image works but not the other one.    Reproducible:	  Steps to Reproduce:	  Go to Atlas search, and select BOLOCAM galactic plane survey (GPS).  Then enter single coordinate search on ""0 0 gal"" to find images taken around that.     See column ""FITS filename"" and search for the sharc-ii instrument images such as  images/sharc2/L000.15+0.00.fits [bad]   and   images/sharc2/L000.00+0.00.fits [good]     Then open it on irsaviewer by clicking on the icon link (first column).  On the image viewer, enable the grid overlay and click on the icon 'layer' on the toolbar.  Change the coordinate system to galactic to see the problem.     On the overlay for the associated Bolocam map there are three horizontal lines which exist only on the left hand side of the image. When the image coordinate system is set to ""Gal"", the reported GLON values range from -355.4 to -1.6 (from left to right), passing through -0.0 in the center.     Same problem can be seen for   images/v1/INNER_GALAXY/map/v1.0.2_super_gc_13pca_map50_crop.fits [bad]  ",4
"evaluate NCSA OpenStack against SQRE requirements and provide feedback - part 2
nan",1
"Nebula metadata service is intermittent
Upon restarting one of my nebula instances (ktl-test), I noticed a failure in the logs:  {quote}  Sep 14 18:07:29 ktl-test cloud-init: 2015-09-14 18:07:29,157 - util.py[WARNING]: Failed fetching metadata from url http://169.254.169.254/latest/meta-data  {quote}    Attempting to retrieve that URL seems to randomly vary between succeeding, which returns:  {quote}  ami-id  ami-launch-index  ami-manifest-path  [...]  {quote}    and failing, which returns:  {quote}  <html>   <head>    <title>500 Internal Server Error</title>   </head>   <body>    <h1>500 Internal Server Error</h1>    Remote metadata server experienced an internal server error.<br /><br />         </body>  </html>  {quote}  These failures may be contributing to observed sporadic {{ssh}} key injection failures.",2
"Liaise with Long-Haul Network group on Base Site to NCSA network
Coordinate with network group to establish Base Site-to-NCSA LHN.    Assignees: Paul Wefel  Duration: September 2015 - February 2016",4
"Evaluate file management technologies
Evaluate technologies for data management (e.g., iRODS).",100
"Consult on contracting and execution of Chilean Data Center contract
Continue to consult and review design documents for the construction of the Chilean DAC.    Assignees: Tom Durbin  Duration: September 2015 - February 2016",11
"Calibration products preparation
Develop the calibration products pipeline plan and begin initial implementation.",22
"Stack documentation Part I
Original documentation based on expressed team needs.     [JS 100%]    ",30
" Integration and test monitoring architecture Part II
Work to be assessed on the basis of the outcome of Part I   [DM-2050]    [100% JMP]        ",70
"Supertask design and prototyping - Part I
This epic covers design and prototyping work for an encapsulation of  tasks that allows for the chaining and interleaving of tasks with  QA/metrics tasks, intergration tests and/or KPM/analysis  afterburners. The emphasis at this point is on speed of development  and customisation; performance is assumed not to be an issue as it  will eventually be guaranteed by the process execution framework.     Only SQuaRE's effort is covered by this epic; additional effort on  this investigation is provided by the Middleware WBS and not captured  here.     The outcome of this epic is a proposal for moving forward. In the  event that it is acceptable to the Project Engineer, further epics  might be define, hence Part I.    [100% GPDF] ",42
"Communication Toolchain support
This epic covers support of communication tools primarily used by DM  and/or supported by DM on behalf of other parts of the project - JIRA,  Discourse, Hipchat, etc     The source of this work is primarily driven by short-term user  requests, and so the outcome is timeboxed rather than planned.     [JS 50% FE 50%]  ",20
"QA Architecture Documents
Design document outlining SQuaSH elements and implementation plan.    [FE 100%]",10
"Stack Build, Packaging and Testing Improvements Part II
This epic is an umbrella for RFC-69 work.     [JH 90% JS 10%]  ",60
"Web design fixes DM Design Documents on Sphinx/Read The Docs
Solve fit-and-finish issues with the stock readthedocs.org Sphinx template when rendering DM design documents. Issues include:    * Sections need to be numbered and those numbers need to appear in TOC  * RTD's TOC does not properly collapse sub-topics  * Appropriate styling for document title and author list  * Wrapping the changelog table  * Adapt section references so that just the section number can be referenced, independently of the section number and title in combination  * Section labels given explicitly in the reST markup are different from the anchors that Sphinx gives to the {{<hN>}}tags; the former are simply divs inserted in the HTML.    The solutions may involve    # reconfiguring the Sphinx installation of individual documents  # forking the RTD HTML template, and/or  # developing extensions for Sphinx in {{sphinxkit}}.",2
" Integration Dataset for metrics and regression tests - Part I 
  This epic covers the defintion of a compact but rich dataset to  support regression testing and surrogate metric development for  regular automatic integration tests. It also involes a definition of  interim/surrogate metrics where they can aid testing and development  and/or where KMPs cannot be calculated.    [DN 75% MWV 25%]",50
" Processing of DECAM and other Verification Datasets
This epic covers work to lead and co-ordinate the processing of  precursor datasets, DECAM in the first instance, through the stack and  produce an assessmment of progress and preliminary metrics. In W16  execution will be done using the orca/HTCondor setup previously used  at NCSA for Data Challenges.    [DN 50% AF 25% JS 25%]      ",100
"Do basic tests of CModel ellipticity measurements
I have seen enough anomalies that I have had to go back a bit and do some basic tests of CModel and its ellipticity bias.  This involves running the same pipeline as before, but with controlled galaxy profiles, ellipticities, and angles.  These are zero-shear, zero-seeing tests which I probably should have run first thing.    It I understand everything I see in these tests, I will be confident that the results I am seeing for CModel with varying footprint size, varying nInitialRadii, and varying stamp size are correct.",4
"Simultaneous astrometry requirements
The HSC group needs improvements to the simultaneous astrometry fitter delivered by Astier et al.  In particular we'll need to do simultaneous photometry as well.  This epic is to determine the superset requirements for such a system.",9
"Refactor Jointcal to use stack functionality
Jointcal currently has a lot of built-in features that already exist elsewhere in the stack (e.g. GTransfo, StarSelector, Points, etc.). These features should be removed and replaced with the equivalent stack functionality.",36
"Implement simultaneous photometry
The simultaneous astrometry framework should be able to be extended to also fit the photometry at the same time.  This task is to do just that.  The task to create a pluggable framework should help with this.",56
"Clean up Wcs classes
The Wcs classes as they currently exist are not easy to extend and also contains overrides that are a bit ad hoc.  This task is to clean up the existing classes so that there is a single abstract base class.  It should also be a priority of this task to determine whether an upgrade in wcslib helps with the special cases (e.g. TAN-SIP).",38
"Gather requirements for improved Wcs classes
The new Wcs class should be able to apply transforms in a stack so that many different distortions as different scales can be corrected for separately (rather than trying to correct the whole mess with a single 2D polynomial).  There is some work going on in the community around this.  This task should include conversations with community leaders in this area.",19
"Produce a design for the new Wcs classes
This will require generating a design as well as getting it reviewed via RFC.  The hope is that other work can be ongoing while the RFC process is carried out.",50
"Implement new Wcs classes
Once a design is accepted implement the new design. This epic covers the replacement of afw:image:wcs with afw:transform/afw:mapping (names currently in flux), but does not involve changes to XYTransform.",39
"Make Wcs persistable
Regardless of the mechanism used by the new Wcs classes, they will need to be persistable.  This will probably require some significant work unless classes that are already persistable are used in the design.",56
"Identify all corrections ISR needs to handle
This is informed by DECAM etc.  This is just for the corrections that do not require detection.  For example, we will push out CR rejection until the question of snaps is decided.",9
"Implement all ISR corrections for LSST
Most corrections have some implementation.  Perhaps the most difficult will be crosstalk since the implementation should allow for correction over multiple chips.",19
"Make up a test for dipole measurement
To facilitate work on the dipole measurement, a test will be helpful.  This could simply be an image with dipoles made with Gaussian PSFs with Poisson noise on top.  The important thing is to be able to determine whether the algorithm is returning the right answer.",9
"Improve the dipole measurement task
The current dipole measurement algorithm has some issues.  It also doesn't work in the current measurement framework.  This task is to improve the dipole measurement.  The result of this epic should be improved results when run on the test data.",38
"Gather requirements to inform a redesign of the CalibrateTask
The current calibrate task is fairly brittle and hard to extend.   This task is to gather the necessary requirements for a redesigned calibrate task.",2
"Implement the new CalibrateTask design
Implement the redesigned CalibrateTask.",28
"Create initial cluster design, send internally for feedback and planning
Gather feedback on initial designs for FY16 purchase plans.",2
"Create data products description
Addition of [https://confluence.lsstcorp.org/display/~petravick/Products+of+Image+Ingest+and+Processing] to understand more of the requirements necessary for the functional design  ",3
"LSE-78: W16 revisions, harmonization with existing design
Review LSE-78 for self-consistency and consistency with the current DM and overall system design.",6
"Revise early integration milestones, LCR-323 and beyond
Revise the list of early integration milestones with OCS, TCS, CCS, and DAQ to form a coherent plan.  Coordinate with NCSA and other interested parties in DM.",6
"Review ICD flowdown to DMSR and design documents
nan",8
"Add missing space after if in Qserv code to conform to standard
Replace ""if("" with ""if ("" to follow standard.    find core/modules/ -name ""*.cc"" |xargs grep ""if(""|wc -l  852    ",1
"Review LCR-323 proposal for integration milestones
Prepare for CCB action on LCR-323.  Ensure that DAQ integration is included (it's not in the original LCR proposal).",4
"Review current version of LSE-78, prepare for LCR
Do a comprehensive read-through of the previous released version of LSE-78.  Look for self-consistency and for consistency with the rest of the DM and overall system design.  Report issues to appropriate people.",3
"Research existing DHT-based FS approaches 
The previous prototype provided confidence that a DHT overlay could work for routing and placement at the scales and time constants needed for chunk distribution.  The next prototype will need actual data transport and storage management facilities layered on the node/key management provided by the DHT layer.  Explore existing works at this level to a greater depth pick from among proven approaches.",6
"Provide values for relative astrometry KPMs in FY15
Should produce the numeric values required (or an explanation of why they aren't available) together with a description of the process for generating them (incl. the data processed, scripts used for plotting, etc).",2
"Provide values for PSF ellipticity KPMs in FY15
Should produce the numeric values required (or an explanation of why they aren't available) together with a description of the process for generating them (incl. the data processed, scripts used for plotting, etc).",1
"Provide values for photometric repeatability KPMs in FY15
 Should produce the numeric values required (or an explanation of why they aren't available) together with a description of the process for generating them (incl. the data processed, scripts used for plotting, etc).",4
"Provide value for DRP computational budget KPM in FY15
Should produce the numeric values required (or an explanation of why they aren't available) together with a description of the process for generating them (incl. the data processed, scripts used for plotting, etc).",3
"Fix xrootd compiler warnings with clang
h5. Xrootd    {code}  In file included from core/modules/qdisp/Executive.cc:64:  In file included from core/modules/qdisp/XrdSsiMocks.h:33:  In file included from /Users/timj/work/lsstsw/stack/DarwinX86/xrootd/u.timj.DM-3584-ge22410fa7f+da39a3ee5e/include/xrootd/XrdSsi/XrdSsiRequest.hh:37:  /Users/timj/work/lsstsw/stack/DarwinX86/xrootd/u.timj.DM-3584-ge22410fa7f+da39a3ee5e/include/xrootd/XrdSsi/XrdSsiRespInfo.hh:43:1: warning: 'XrdSsiRespInfo' defined as a        struct here but previously declared as a class [-Wmismatched-tags]  struct  XrdSsiRespInfo  ^  /Users/timj/work/lsstsw/stack/DarwinX86/xrootd/u.timj.DM-3584-ge22410fa7f+da39a3ee5e/include/xrootd/XrdSsi/XrdSsiSession.hh:45:1: note: did you mean struct here?  class XrdSsiRespInfo;  ^~~~~  struct  {code}    {code}  core/modules/xrdoss/QservOss.h:64:17: warning: 'lsst::qserv::xrdoss::FakeOssDf::Opendir' hides overloaded virtual function [-Woverloaded-virtual]      virtual int Opendir(const char *) { return XrdOssOK; }                  ^  /Users/timj/work/lsstsw/stack/DarwinX86/xrootd/u.timj.DM-3584-ge22410fa7f+da39a3ee5e/include/xrootd/XrdOss/XrdOss.hh:63:17: note: hidden overloaded virtual function        'XrdOssDF::Opendir' declared here: different number of parameters (2 vs 1)  virtual int     Opendir(const char *, XrdOucEnv &)           {return -ENOTDIR;}                  ^  {code}    {code}  In file included from core/modules/xrdsvc/SsiSession.h:32:  /Users/timj/work/lsstsw/stack/DarwinX86/xrootd/u.timj.DM-3584-ge22410fa7f+da39a3ee5e/include/xrootd/XrdSsi/XrdSsiResponder.hh:177:27: warning: control may reach end of        non-void function [-Wreturn-type]                            }                            ^  {code}",1
"support shared_ptr<Statistics>
I would like to write some functions that return afw::math::Statistics objects and wrap them with SWIG. Unfortunately SWIG requires that any object returned by value must have a default constructor, and Statistics does not. Rather than try to add such an object I propose to make my functions return a shared_ptr to Statistics.    This ticket is a request to support that by adding the following to statistics.i:   {code}  %shared_ptr(lsst::afw::math::Statistics);  {code}  ",2
"Review of [DM-2983]
I was asked for a revision of [DM-2983] which is part of the Backport HSC parallelization code",4
"Update some tests to support nose and/or py.test
When {{sconsUtils}} is migrated to use {{nose}} or {{py.test}} some test scripts will need to be modified because test discovery will be slightly different and the namespace of test execution will change.    Two things to consider:  * People would still like the option of running a test as {{python tests/testMe.py}}.  * We have to work out how to run the memory test case.  ",4
"Fix protobuf compiler warning with clang
nan",1
"Base Site Data Access Center Description Page
https://confluence.lsstcorp.org/display/~petravick/Data+Access+Center",1
"Gathering use cases for verification data sets
Seeking out developer use cases of incoming data sets. Need to determine if datasets will be accessed for verification only or by developers and QA in general. Determine access methods. ",3
"Future infrastructure, common IT, and facility planning
Includes consulting on developing use cases for Base Site Commissioning Cluster    Jason Alt, Paul Wefel, Tom Durbin",13
"Specify FY15 Equipment Purchasing Plan
The hardware contract was finalized at the end of July 2015, leaving 2 months of FY15 for spending the fiscal year hardware budget. In lieu of an annual acquisition strategy document, we draft a one-off FY15 purchasing plan.    Assignees: Jason Alt, Bill Glick, Paul Wefel  Duration: September 2015",9
"W16 Work on Alert Production Simulator
Continued development of Alert Production simulator.    Assignees: Steve Pietrowicz  Duration: September 2015 - February 2016",71
"W16 Work on OCS Software Integration
Integrate the OCS Software, delivered from the Camera Team, into AP.    Assignees: Steve Pietrowicz  Duration: November 2015 - February 2016",38
"Run and document multi-node test with docker
In order to validate Docker setup on CC-IN2P3 cluster, it is required to launch some test on consistent data. S15 LargeScaleTest data doesn't seems to be compliant with latest Qserv version so running multi-node test would be interesting. Nevertheless the multi-node setup doesn't seems to be documented and, hence, is difficult to reproduce.",3
"HSC backport: avoid I/O race conditions config write out
This is a port of [HSC-1106|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1106]    When running tasks that write out config settings files ({{processCcd.py}}, for example), if multiple processes start simultaneously, an I/O race condition can occur in writing these files.  This is solved here by writing to temp files and then renaming them to the correct destination filename in a single operation.  Also, to avoid similar race conditions in the backup file creation (e.g. config.py~1, config.py~2, ...), a {{--no-backup-config}} option (to be used with --clobber-config) is added here to prevent the backup copies being made.  The outcome for this option is that the config that are still recorded are for the most recent run.",1
"some ctrl_events tests execute outside of execution domain
There are a couple of ctrl_events tests that attempt to execute outside of the valid domains acceptable by the tests, when they shouldn't be.  There's a check in place for tests to find this, but a couple of the tests do not have this check.",1
"transfer and update orchestration documentation
The self-service orchestration documentation needs to be transferred from Trac to Confluence, and updated.",2
"Misc work for this reporting week. 
Deal with comments,  coordinate with Jason Alt. Deal with comments on existing work from KT and GDF",2
"Begin thinking about governance aspects of DM operations
Thinking, (but not delivered use case)  -- what is the flow of tickets and the division of use cases between the Science and Data Operations?  What kind of tooling needs can be assumed to exist to support service management what does that mean for the support of processes?  (specifically  looked at a summary of the state the market, and took specifics look at what ITSM process supporting tools are open source. -- as may be useful to prototype processes before committing to something..  Looked at several, ITOP seemed to be mature/documented to a level that may be useable.    ",4
"Conduct, document, initial follow through on Sept JCC meeting 
Developed agenda items, considered JCC meeting.  Minutes are on the LSST confluence.  A major item of discussion was related to operations, since we are told this is a priority.  Befall followup on CCIN2P3 ISM practices. ",1
"General Management 
Hiring,  Internal relationships within the NCSA organization. LSST meetings, general management",4
"Replace boost::regex with std::regex
Boost 1.59 causes a ""keyword hidden by macro"" warn under clang in the regex package.  We should be using std::regex now anyway, so this is a good motivator to go ahead and convert.",1
"Create and deploy a git-lfs prototype
Create and deploy a git-lfs-s3 server.    High level requirements:    * The server should use github to authenticate users. Any github user who is a member of the lsst organization has write access. This means they can push objects to the git-lfs server.  * The server should allow for anonymous read access. This means anyone can clone, pull, fetch, etc.  * Use an S3 compliant API to store objects.  * Simple, well defined method to redeploy.  * Uses https.  * Backs up to Amazon Glacier (or similar) periodically. The data should be ""slow"" so backing up approximately once a day is OK.",64
"Update multi-node setup documentation
Workers in multi-node setup no longer require granting mysql permissions for test datasets since direct mysql connections are no longer used by the data loader.",1
"W16 ISO Work
Continuous work as LSST ISO.    Assignees: Alex Withers  Duration: September 2015 - February 2016",39
"Centralize Sphinx configuration for Design Documents
Centralize Sphinx configuration for design documents in {{documenteer}} and provide a facility for design document authors to use YAML files to store document metadata rather than editing {{conf.py}} files.",2
"Implement iostream-style formatting in log package
Implement proposed in RFC-96 change to log macros. This ticket only covers defining new set of macros (LOGS() and friends) which use ostringstream for formatting messages. Migration of all clients and removal of LOGF macros will be done in separate ticket.",1
"port dax_*serv to python3
nan",3
"Handle queries with no database
Sqlalchemy is generating some queries that are currently killing czar, the list is:    {code}  set autocommit=0  SHOW VARIABLES LIKE 'sql_mode'  SELECT DATABASE()  SELECT @@tx_isolation  show collation where `Charset` = 'utf8' and `Collation` = 'utf8_bin'  SELECT CAST('test plain returns' AS CHAR(60)) AS anon_1  SELECT CAST('test unicode returns' AS CHAR(60)) AS anon_1  SELECT CAST('test collated returns' AS CHAR CHARACTER SET utf8) COLLATE utf8_bin AS anon_1  SELECT 'x' AS some_label  select @@version_comment limit 1  {code}    Czar should survive unfriendly syntax, and this will be addressed through DM-3764.    In this story we will make sure that  sqlalchemy-generated queries are properly handled (not just these particular queries, but all queries that do not involve any database and any table). We should run such queries on a local mysql instance (alternatively, perhaps redirect to one of the workers?)",4
"AP, Co-add, Image cache definitions
Added physical breakdown for Alert Postage Stamp Images file system, co-add images file system and image cache file system",2
"sandbox-stackbuild issues
A number of issues with sandbox-stackbuild, or rather its infrastructure.    1. Problem with the librarian-puppet plugin and its mismatch with the puppet forge API ([~jhoblitt] has a PR open apparently)    2. As a workaround to above, one needs to    {code}  gem install librarian-puppet  librarian-puppet install  {code}    but that runs into an issue with swap_file needing a downgrade to work with Ubuntu 14.04. Working state as of the time of this bug report for the Puppetfile is:    {code}    forge 'https://forgeapi.puppetlabs.com'    mod 'puppetlabs/stdlib'  mod 'camptocamp/augeas', '~> 1.4'  mod 'stahnma/epel', '~> 1.1'  mod 'petems/swap_file', '1.0.1'  mod 'jhoblitt/sysstat', '~> 1.1'  mod 'maestrodev/wget', '~> 1.7'    mod 'jhoblitt/lsststack', :git => 'https://github.com/lsst-sqre/puppet-lsststack.git'  {code}    3. Which brings us to the fact that the Vagrant puppet-install plugin is broken with Puppet 4, and new platforms are not supported under Puppet 3. Ergo, as is, can't bring up Ubuntu 15.05 etc.     Ticket is to get PRs merged, fork and fix them ourselves, or find alternatives.   ",4
"Histogram calculation for image stretch has infinite loop 
When load the big.fits file, the image never came out.  It stopped at Histogram.  There was an infinity in Histogram.   ",4
"Measurement plugin errors
When doing measurements on coadds, several errors are thrown within the measurement plugins.    {code}  Error in base_GaussianFlux.measure on record 283467884979: Input shape is singular  {code}    {code}  Error in base_GaussianFlux.measure on record 283467883979:     File ""src/SdssShape.cc"", line 842, in static lsst::meas::base::FluxResult lsst::meas::base::SdssShapeAlgorithm::computeFixedMomentsFlux(const ImageT&, const lsst::afw::geom::ellipses::Quadrupole&, const Point2D&) [with ImageT = lsst::afw::image::MaskedImage<float, short unsigned int, float>; lsst::afw::geom::Point2D = lsst::afw::geom::Point<double, 2>]      Error from calcmom {0}  lsst::pex::exceptions::RuntimeError: 'Error from calcmom'  {code}    The measurements were done on tiger, and the command used was:  {code}  measureCoaddSources.py /tigress/HSC/HSC/rerun/nate/old_clip/ --output=/tigress/HSC/HSC/rerun/nate/old_clip/ -C /home/nlust/options_temp.py --id tract=0 patch=2,2 filter=HSC-I^HSC-R   {code}  The data can be found within the rerun directory specified as the input to the command. The data was created using the commands:  {code}  assembleCoadd.py  --legacyCoadd /tigress/HSC/HSC/rerun/nate/ --output=/tigress/HSC/HSC/rerun/nate/old_clip --id tract=0 patch=2,2 filter=HSC-R --selectId visit=1208 ccd=56^64^72 --selectId visit=1206 ccd=64^65^72^73^79^80 --selectId visit=1212 ccd=64^65^72^73^79^80 --selectId visit=23704 ccd=64^65^72^73 --selectId visit=23706 ccd=56^64^72 --selectId visit=23694 ccd=64^65^72^73^79^80 --selectId visit=1204 ccd=64^65^72^73^79^80 --selectId visit=1220 ccd=63^64^71^72^78^79 --selectId visit=1218 ccd=56^57^64^65^72^73 --selectId visit=23718 ccd=64^65^72^73^79^80 --selectId visit=23692 ccd=64^65^72^73^79^80 --selectId visit=1210 ccd=63^64^71^72^78^79 --selectId visit=1216 ccd=56^57^64^65^72^73 --selectId visit=1214 ccd=64^65^72^73^79^80 --selectId visit=23716 ccd=63^64^71^72^78^79 --selectId visit=1202 ccd=64^65^72^73^79^80  {code}  and   {code}  assembleCoadd.py --legacyCoadd /tigress/HSC/HSC/rerun/nate/ --output=/tigress/HSC/HSC/rerun/nate/old_clip --id tract=0 patch=2,2 filter=HSC-I --selectId visit=19658 ccd=64^65^72^73^79^80 --selectId visit=1248 ccd=56^64^72 --selectId visit=19696 ccd=65^66^73^74^80^81 --selectId visit=19684 ccd=64^65^72^73^79^80 --selectId visit=1238 ccd=64^65^72^73^79^80 --selectId visit=19710 ccd=56^64^72 --selectId visit=19680 ccd=56^64^72 --selectId visit=1230 ccd=64^65^72^73^79^80 --selectId visit=1236 ccd=63^64^71^72^78^79 --selectId visit=19694 ccd=64^65^72^73^79^80 --selectId visit=1232 ccd=64^65^72^73^79^80 --selectId visit=19698 ccd=64^65^72^73^79^80 --selectId visit=1228 ccd=64^65^72^73^79^80 --selectId visit=1246 ccd=63^64^71^72^78^79 --selectId visit=19682 ccd=63^64^71^72^78^79 --selectId visit=19708 ccd=64^65^72^73^79^80 --selectId visit=19662 ccd=64^65^72^73 --selectId visit=1240 ccd=64^65^72^73^79^80 --selectId visit=1244 ccd=56^57^64^65^72^73 --selectId visit=1242 ccd=56^57^64^65^72^73 --selectId visit=19660 ccd=64^65^72^73^79^80 --selectId visit=19712 ccd=56^57^64^65^72^73  {code}",6
"Fix ""Executive error executing job"" on the cluster
nan",1
"HSC backport: temporary file handling in butler
The HSC fork includes additional work to improve temporary file usage in the butler:  * [HSC-1275|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1275]: Probable resource leakage by butler  * [HSC-1285|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1285]: eups.version files ignore umask  * [HSC-1292|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1292]: Prevent opening files that are already open",1
"Pastry prototype C++
nan",22
"move camera factory methods from obs_lsstSim to afw
The methods defined in obs_lsstSim/bin/makeLsstCameraRepository.py can be easily adapted for use in generating arbitrary, non-LSST cameras.  This is useful for the sims stack, both for testing purposes, and because members of other projects have begun asking us to use our code.    This ticket will take those methods, make them fully LSST-agnostic, and place them in afw as utility functions.  The code in obs_lsstSim will refer back to these afw methods.",5
"NaiveDipoleCentroid/NaiveDipoleFlux algorithms should not require centroid slot
The {{NaiveDipoleCentroid}} and {{NaiveDipoleFlux}} algorithms in {{ip_diffim}} have members which are instances of {{meas::base::SafeCentroidExtractor}}. Due to the prerequisites that imposes, it is impossible to initialize these algorithms without first defining a {{centroid}} slot.    However, there is nothing in these algorithms which actually uses the {{SafeCentroidExtractor}} or any of the information stored in the slot; this seems to be an entirely arbitrary restriction which is likely a legacy of the port to the {{meas_base}} framework. We should remove the  use of {{SafeCentroidExtractor}} to simply the code and make it easier to run the test suite (since it will no longer be necessary to run a centroider).",1
"Split FY16 plan into audience specific documents
Fy16 purchase planning needs to be split for specific audiences (NCSA planning, Aura purchase approval)",2
"Preliminaries for the LSST vs. HSC pipeline comparison through single-frame processing
This ticket is in preparation for DM-2984, which is to run both the HSC and LSST pipelines on 2-3 visits of HSC data, and do a detailed comparison of the science quality and robustness for the single-frame processing (ProcessCcdTask) stage only.  Essentially, it is a detailed audit of all the functionality currently used in HSC single-frame processing and ensuring all relevant features have been pulled over to the LSST stack such that the comparisons to be done in DM-2984 will be meaningful and informative.",14
"QMeta thread safety
Initial QMeta implementation is not thread safe, it uses sql/mysql modules which also do not have any protection (there are some mutexes there but not used). Need an urgent fix to avoid crashes due to concurrent queries in czar.",1
"orchestration slide set for DM bootcamp
Create the ""Orchestration and Control"" slide set for DM Bootcamp, which is being held from Oct 5-7, 2015.  After review (and any revisions), the slide set will be uploaded to confluence, and a link to it will be put here.",5
"Simplify task queuing / Runner code
nan",8
"Cleanup cancellation-related worker code
nan",7
"Remove dependency on mysqldb in wmgr
Move remaining code that depends on mysqldb to db module",1
"Remove dependency on mysqldb in qserv
Remove remaining dependencies on mysqldb in qserv.:  {code}  ./core/modules/tests/MySqlUdf.py  ./core/modules/wmgr/python/config.py  {code}    and use the sqlalchemy from db module instead.",2
"Remove qserv_objectId restrictor
qserv_objectId restrictor can be replaced by the IN restrictor. This story involves checking if performance is acceptable if we use IN restrictor instead of qserv_objectId restictor, and if it is, doing the switch and removing the qserv_objectId restictor code.",3
"Cleanup lua miniParser
Maybe some cleanup can also be performed in lua code. Indeed ""objectId"" hint and parseObjectId() which seems useless.    Indeed miniParser.parseIt and miniParser.setAndNeeded seems useless.    Removing this code will ease maintenance of objectId management.",1
"E/I training and interview
Interviewed and attended training for E/I concerns.",1
"Investigate services for backups/data replication on Nebula openstack
Files generated on instances of the Nebula openstack  should be managed with some commensurate  level of data replication/backups.     We investigate services that might serve this task within the cloud context.",4
"Package SQLAlchemy in eups
Db module is expected to be used by science pipelines, and (per K-T, see qserv hipchat room) we have to package it through eups.",1
"Enable CModel in CalibrateTask prior to PhotoCal
CModel needs to run in CalibrateTask before PhotoCal in order to compute aperture corrections, but it also needs a Calib objects as input, and that isn't available until after PhotoCal is run.    On the HSC side, we dealt with this by adding preliminary PhotoCal run before CModel is run, but we could also deal with it by removing the need for a Calib as input, at least in some situations.",1
"Revisit provenance design / built proof-of-concept prototype
Discuss existing provenance design with the team, brainstorm and improve. Primary focus on capturing provenance information. Deliverable: proof-of-concept prototype + description.",16
"Revisit provenance sizing
Revisit estimates of the size of provenance information.",6
"First optimizations of provenance querying
Think through the issues of querying provenance. Deliverable: write up attached to this story. IT will be transferred to more official provenance documentation through DM-3961",14
"Document provenance design
nan",8
"Build prototype of provenance
Building a proof-of-concept prototype of provenance.    Deliverable: a working, standalone (not connected to any data producer) prototype of the Data Provenance. Not optimized / alpha version.",12
"Meetings Sep 2015
verification datasets weekly meetings and tech-talks",1
"Add SQLite-based v01. unit tests for dbserv
nan",6
"Meetings, institute events, or other LOE, Sep 2015
NCSA or Astronomy Department activities.   - NCSA All-hands  - Local LSST group meetings   - DES-Illinois meetings  - Colloquia  - other seminars, info sessions, or other local meetings",5
"detailed out alert processing/ transmission to event brokers. 
Detailed out the interactions with the event broker, after consulting with John Swinbank.  Dealt with outputs of alert processing.  Dealt with corrections and comments.   Began considering annual processing",2
"weekly management
Dealt with impending new hire.   Good deal of intergroup coordination, and deal with NCSA re organization.",13
"Package sqlalchemy in eups
Db module is expected to be used by science pipelines, and (per K-T, see qserv hipchat room) we have to package it through eups.",1
"FY pricing estimates
nan",1
"Network / Storage reviews
Initial (internal) review with networking and storage delegates",2
"research items needed for wan simulator procurement
locate part numbers and pricing information",1
"Consultation on system and network design
nan",4
"Prepare pricing estimates for networking infrastructure
provide switch quantities, weight, power and budgetary costs",4
"Recommend network infrastructure for Openstack expansion
nan",1
"Discussion of Base site network proposal
Email based discussion with Ron Lambert on his network infrastructure proposal for the Base site.",1
"Post SQLAlchemy-migration tweaks
Implement some minor tweaks take came in late through PR comments, mostly related to sqlalchemy related migration",1
"Improve the performance for making the image plot
Make FitsRead work better for multi-threads",1
"Larger Statistics needed for CModel Studies
The stampsize and nInitalRadius tests were not conclusive in the September Sprint. and the error estimates appeared to be overly large. The nGrowFootprints test was barely significant.  This is a continuation of work started on DM-1135 (DM-1135, 3375, 3376) , after a study of the sizes of our error estimates was conducted (DM-3984).    We started with a sample with 12 million galaxies (not all of which were used in DM-3375 and 3376.  They appeared to all be useful once we had new error estimates, so the studies were run against with this larger sample. ",6
"Errors for shear bias fits
DM-1135 mostly was inconclusive or at least not highly significant, due to the fact that the error bars were around the same size as the differences in most of the tests. This in spite of the fact that we ran 6x as many sample galaxies as great3sims.    Investigate the reason for this, and see if we can estimate the errors more accurately.  Investigate what the best way (or at least the way it is done in GREAT3) to estimate the errors on the bias parameters.     If it is not from the covariance matrix of the regression parameters, there could be some work here.",6
"Run multinode test using docker on one unique host
Qserv multinode test can be launched on n hosts using docker, but not on one unique host.  This ticket will allow developers to run multinode test on their workstation.",6
"Deploy developer code on in2p3 cluster in Docker images
Qserv latest release can be deployed easily on cc-in2p3 cluster using Docker. This ticket will allow developers to prepare worker and master containers using a specific Qserv version and deploy it on cc-in2p3 cluster.",6
"remove unnecessary 'psf' arg to SourceDeblendTask.run()
{{SourceDeblendTask.run}} takes both an {{Exposure}} and a {{Psf}}, even though it can get the latter from the former and always should.",1
"Update FY2015 hardware budget plan
Update the FY2015 hardware purchasing plan with new budget, equipment specifications, and general costs. ",3
"Week end 09/05/15
Support for lsst-dev cluster, OpenStack, and accounts  for week ending September 5, 2015.",5
"Provide detail specs to AURA
Provided Josh Hobblitt details specs for procurement request",1
"Week end 09/12/15
Support for lsst-dev cluster, OpenStack, and accounts  for week ending September 12, 2015.",5
"Display.dot origin swaps x and y
Correcting for xy0 in {{dot}} currently does:  {code:hide-linenum}  r -= x0  c -= y0  {code}  which is backwards.",1
"Backup Pugsley
Created backup of pugsley in anticipation of new hardware and shutdown of temp Mac OS solution",1
"Research using vSphere on Mac Pro
Researched setting up vSphere on Mac Pro.",1
"Week end 09/19/15
Support for lsst-dev cluster, OpenStack, and accounts  for week ending September 19, 2015.",5
"Learn about Lenovo storage and server options
Went to lunch with Lenovo to learn about systems that might be suitable for LSST deployment. Learned about server and storage options.",1
"Week end 09/26/15
Support for lsst-dev cluster, OpenStack, and accounts  for week ending September 26, 2015.",5
"Revamp FITS Viewer scrolling to stop using large div
The current scrolling system will not work as well with masking layer.  It is also suspected to use too much browser memory since is creates a very large div.  Firefly will scroll images manually now.",12
"Write next-generation stack doc writing guide
Write a guide in the prototype LSST Stack Docs (https://github.com/lsst-sqre/lsst_stack_docs) covering how to document the LSST Stack under the new doc infrastructure.    This exercise will implicitly involve designing how the new docs will work. Content includes:    # How to write a user guide to a package (both content wise and in terms of organizing a package's doc files)  # How to write python doc strings  # Coverage of reStructuredText and Sphinx as implemented by the LSST Stack Docs.  ",7
"Replace zookeeper CSS with mysql
To switch from QservAdmin to CssAccess interface in our Python tools we will need to replace zookeeper with mysql implementation because we do not have C++ KvInterface implementation for zookeeper.",2
"Investigate what is missing to run ISR with DECam raw data and processCcd
Currently raw DECam data can not be processed with the stack for Instrumental Signature Removal (ISR).  This issue includes efforts to understand how ISR is done in the stack, learn about processCcd, basic DECam ISR, and what is missing in the code stack to proceed.    ",13
"ctrl_execute templates still use ""root"" instead of ""config""
The templates that ctrl_execute fills in still use ""root"", instead of the new ""config"".  This causes extraneous warning messages to appear from pex_config when executing ""runOrca.py""",1
"Allow FlagHandler to be used from Python
The {{FlagHandler}} utility class makes it easier to manage the flags for a measurement algorithm, and using it also makes it possible to use the {{SafeCentroidExtractor}} and {{SafeShapeExtractor}} classes.  Unfortunately, its constructor requires arguments that can only be provided in C++.  A little extra Swig wrapper code should make it usable in Python as well.",3
"Rename forced photometry CmdLineTasks to match bin scripts
We use names like ""forcedPhotCcd.py"" for bin scripts but ""ProcessCcdForcedTask"" for class names; these need to be made consistent, and it's the former convention that was selected in an old (non-JIRA) RFC.",1
"Initial discussion EFD with Dave Mills
Review LTS-210 and discuss design of EFD cluster with Dave Mills.    Are there opensource clustering solutions?",1
"Replace boost::tuple with <tuple>
Replace boost::tuple with <tuple>    This ticket will be completed as part of the DM bootcamp at UW.",1
"Document Revision
Document cleanup. Added PDUs, networking estimates, power costs, vsphere licensing.",1
"Write developer workflow documentation
Write a developer workflow guide to walk through and document best practices for developing against the LSST Stack.",6
"Fix procedure for building docker image for 2010_09 release
This procedure should be straightforward but is currently failing due to gcc-4.9/boost problem (DM-4018)",2
"Replace boost::unordered_map with std::unordered_map
DM boot camp tutorial.    ",2
"forcedPhotCoadd.py fails on HSC data
When trying to run {{forcedPhotCoadd.py}} on HSC data, I see the following error:    {code}  $ forcedPhotCoadd.py /raid/swinbank/rerun/LSST/bootcamp --id filter='HSC-I' tract=0 patch=7,7  : Loading config overrride file '/nfs/home/swinbank/obs_subaru/config/forcedPhotCoadd.py'  Cannot import lsst.meas.extensions.photometryKron: disabling Kron measurements  Traceback (most recent call last):    File ""/home/lsstsw/stack/Linux64/meas_base/11.0+2/bin/forcedPhotCoadd.py"", line 24, in <module>      ForcedPhotCoaddTask.parseAndRun()    File ""/home/lsstsw/stack/Linux64/pipe_base/11.0+2/python/lsst/pipe/base/cmdLineTask.py"", line 433, in parseAndRun      parsedCmd = argumentParser.parse_args(config=config, args=args, log=log, override=cls.applyOverrides)    File ""/home/lsstsw/stack/Linux64/pipe_base/11.0+2/python/lsst/pipe/base/argumentParser.py"", line 360, in parse_args      self._applyInitialOverrides(namespace)    File ""/home/lsstsw/stack/Linux64/pipe_base/11.0+2/python/lsst/pipe/base/argumentParser.py"", line 475, in _applyInitialOverrides      namespace.config.load(filePath)    File ""/home/lsstsw/stack/Linux64/pex_config/11.0/python/lsst/pex/config/config.py"", line 529, in load      self.loadFromStream(stream=code, root=root)    File ""/home/lsstsw/stack/Linux64/pex_config/11.0/python/lsst/pex/config/config.py"", line 549, in loadFromStream      exec stream in {}, local    File ""/nfs/home/swinbank/obs_subaru/config/forcedPhotCoadd.py"", line 10, in <module>      config.deblend.load(os.path.join(os.environ[""OBS_SUBARU_DIR""], ""config"", ""deblend.py""))  AttributeError: 'ForcedPhotCoaddConfig' object has no attribute 'deblend'  {code}    This is with the stack version 11.0+3 and {{obs_subaru}} 5.0.0.1-676-g4ae362c.",1
"Local LSST IAM Meeting
Meeting notes:    * [10 minutes] Review LSST identity management statement of work    On the confluence wiki. Let me know if you have any follow-up questions/comments.    * [10 minutes] Plan initial project tasks    All: Review materials at https://confluence.lsstcorp.org/display/LAAIM. Jim to add updates from last week's meeting.    * [5 minutes] Schedule follow-on project meetings    Alex will schedule meeting with Bill Glick about LDAP at NCSA.  Terry and Daniel will attend https://community.lsst.org/t/dm-boot-camp-announcement/249 sessions of interest if available.",1
"Local LSST IAM Meeting
-Meeting attended by Jim Basney, Tim Fleury, Dan Thayer, and Alex Withers.  -Discussed upcoming DM bootcamp.  -Focused on SUIT diagram and determining what questions to ask at next bi-weekly meeting.  -First draft recommendations by end of October.",1
"First day activities
Meeting with HR at 8:30AM.  Taken around for introductions through 10AM.  Remainder of day was proceeding through HR punch list for new hires such as:  	NCSA Intranet account setup  	Gain familiarity with NCSA Intranet  	Kerberos and Enterprise ID setup  	Read and Sign-off  on Security and NCSA policy.  	Outlook mail and calendar setup  2pm DM meeting",2
"Second day start activities
Requisitioned used macbook and monitor from IT and set it up.  Located LSST stack site and read build and install documentation  More HR account items.  Requested confluence credentials from LSST.org  Learned about SDSS/Stripe 82",2
"groundwork for file management
Installed correct Xcode + misc. for stack version.  Downloaded LSST stack from GitHub repo and built/installed.  ",1
"Incidental items and jobs for file mgmt. groundwork
Installed and built LSST tutorials package.  Setup, fixed minor issues and ran First tutorial to check that initial stack  installation was successful.  Learned about CCD operation and how the LSST CCD is laid out.  Learned about raw CCD data from amplifiers, as well as other camera attributes  Studied stack code for these things.  Read description of Astronomy Associative relations as well as CoAdds.  Pulled PhoSim from repo. Built and Installed.  Had trouble getting PhoSim to run due to dynamic link library issues.  Instrumented PhoSim code with PDB commands.  Walked to bookstore and picked up new ID badge.",2
"Development for Developer activities!
Completed setup of confluence access.  Set up LSST HipChat.  Jumped into PhoSim a bit more and resolved errors by dropping in a few symlinks here and there;  not a solution for a proper execution environment, but a time-save just to see PhoSim work and  have a platform for tinkering with camera attar's.  PhoSim ran successfully.  Learned about FTTS format and how the file is built up. Learned about how Key/Val pairs   can proceed ANY data section throughout file by using data offset values.",2
"integration of confluence data into learning curve
Started studying LSST coding policies and best practices via confluence.  Colloquium.  Small meetings with other LSST team members throughout day.  Rebuilt PhoSim/LSST-Stack to take advantage of multiple cores when rendering portion of sky to a file. Built these commands into Stack code. They would have to be custom #defined by ./configure at build time depending on computer arch. which is too much to do when just gaining familiarity so stuck to MacBook  multi-core specs, where I was working.  Logged into JIRA and studied how tasks were proposed, realized, and checked as done.  Extensive talk about BBQ in group area of NCSA/LSST.  Close reading and note taking of LDM-230 and related docs.",5
"Add doc directory, and fix doxygen warnings
Add doc directory and fix doxygen warnings.",4
"obs_sdss should use pydl.yanny instead of it's own copy thereof
Inside obs_sdss there is a yanny.py that looks like it was copied from either sdss_python_module or pydl. We should just depend on pydl (https://github.com/weaverba137/pydl), so we can use whatever improvements it gets for free, and to prevent yet annother yanny reader floating around.",4
"Display DECam focal plane mosaics using showCamera
Try out some new functionalities of afw.cameraGeom.utils (from DM-2437) for DECam raw data. Raw data in {{testdata_decam}} are retrieved through Butler and displayed as a focal plane mosaic.  ",2
"Replace boost::array with std::array
Replace all use of boost::array with std::array in the DM stack    A quick search turned up use in 17 files spread over these packages:  ip_diffim, meas_base, meas_extensions_photometryKron and ndarray (which is presumably out of scope for this ticket)",4
"Change from boost::math
Most boost::math contents (not including pi) are now available in standard C++. Please convert the code accordingly.    In addition to the packages listed above, boost/math is used in ""partition"" a package I don't recognize and not a component JIRA accepts.",5
"New YAML config for community_mailbot
Per review comments to DM-3690, the configuration should move to YAML with the following goals    - Have a Configuration object that can be tested and passed around  - Redesign the configuration to allow for additional types of message handlers, such as twitter, hipchat/slack, etc.  - Move secret keys entirely into the configuration file  - Provide a configuration template  - Move any sort of hard-coded configuration to the expanded YAML file (e.g., Mandrill templates)",1
"Refactor Scripts and Discourse interface in community_mailbot
Per review comments for DM-3690, the community_mailbot can have slight code refactoring    - Refactor scripts into smaller testable units  - Refactor the Discourse feed classes around an ABC  - More testing",1
"Produce demo video for git lfs
Produce a screencast tutorial of the DM git-lfs implementation.",1
"update memory management in jointcal
jointcal currently uses a combination of raw pointers and a custom reference-counted smart pointer class, {{CountedRef}} (similar to {{boost::intrusive_ptr}}).  The code needs to be modified to use a combination of {{shared_ptr}} (most code), {{unique_ptr}} local-scope variables and factory functions, and {{weak_ptr}} (at least some will be necessary to avoid cycles in some of the more complex data structures).  As part of this work, we'll also have to remove a lot of inheritance from {{RefCounted}}, which is part of the {{CountedRef}} implementation.    This ticket looks like it will require a lot of work, because we'll have to be careful about every conversion to avoid cycles and memory leaks.  Nevertheless, I think it will be necessary to do this conversion before attempting any other major refactoring, as I'm worried that having a newcomer make changes to the codebase without first making the memory management less fragile could be very dangerous.",8
"integrate jointcal geometry primitives (Point, Frame, FatPoint) with afw
jointcal currently has three simple geometry classes that can be integrated relatively easily with existing classes in afw and meas_base:   - {{jointcal.Point}} is equivalent to {{afw.geom.Point2D}}.   - {{jointcal.Frame}} is equivalent to {{afw.geom.Box2D}}.   - {{jointcal.FatPoint}} is equivalent to {{meas.base.CentroidResult}}.  We should probably move {{CentroidResult}} to {{afw.geom}}, perhaps rename it ({{MeasuredPoint}}?), and reconsider its relationship with {{Point}}.  This will require a bit of refactoring in {{meas_base}}, but the usage in {{jointcal}} makes me think it's a sufficiently fundamental object to be included in afw.    We may find aspects of the interfaces in {{jointcal}} that we should add to {{afw.geom}}, but I think we'll mostly end up making trivial modifications to {{jointcal}} to use the {{afw}} interfaces.",8
"integrate Gtransfo functionality with XYTransform
{{meas_simastrom}} includes a {{Gtransfo}} class hierarchy that is similar to {{afw.geom.XYTransform}}, but with more functionality and some intentional differences, including:   - {{XYTransform}} objects are immutable; {{Gtransfo}} objects are not.   - {{Gtransfo}} objects expose their parametrization, and can compute various derivatives with respect to those parameters.  {{XYTransforms}} are essentially black-box functions, and expose no parameterization.    Unifying these classes is not entirely straightforward, and should include an RFC for the design prior to implementation.  Overall, I think {{XYTransform}}'s simpler, lower-functionality interface and immutability is worth mostly preserving somehow; I think it's a more fundamental interface than {{Gtransfo}} that can be used in more places.  But obviously we need to provide the more extensive {{Gtransfo}} interface somehow as well.    My initial thought is that we should have two parallel class hierarchies (with a concrete class for each type of transform, such as polynomial distortion, in both), and an ultimate base class shared by both hierarchies.  That ultimate base class would contain most of the current {{XYTransform}} interface but not require immutability, and one side of the tree would contain simple immutable objects while the other would contain the more extensive parameterized interface of {{Gtransfo}}.",8
"Redis cache for community_mailbot
Switch from a {{json}} cache to a redis cache from the community_mailbot.",1
"Add fake secondary index to testIndexMap.cc
qproc/testIndexMap.cc is sketchy and perform a very poor validation for now. It should at least use a minimal SQL secondary index, embedded in a simple database like SQLLite, inorder to validate secondary index lookup code.",8
"Replace QsRestrictor::PtrVector With std::vector<QsRestrictor> and use move constructor
Use of  QsRestrictor::PtrVector introduces a useless indirection. it maybe could be replace by std::vector<QsRestrictor> and use of move constructor. This would simplify code (currently a confusion exists between empty vector and nullptr) and ease maintenance.",5
"Security plan renewal
nan",3
"IaM work
nan",2
"bi-weekly IaM meeting
nan",1
"Meetings Oct 2015
- Verification datasets meetings  ",1
"Final additions and review
Add remaining components: power estimates, vSphere annual licensing, networking, PDUs, login nodes    review: misc expense fund, decommissioned services, financial targets",3
"Chasing down Pan Starrs Requirements
Pan Starrs data release (PS1) will be used in the integration QServ environment purchased as part of FY16. Catalog and file space requirements must be understood.",1
"Prepare Plan for ICI Leadership Review
Prepare plan and ICI-group-specific points of interest for hardware/service deployment plans. ",1
"Vendor Discussions
Discussions with vendors on planned procurement. Details of discussions will not be described here. This is for story point tracking only.",1
"Begin Approval Process
Approval process for FY16 procurement plan. This requires approval from Jeff K, Victor and NSF (due to the cost increment being greater than $250K).    Expected approval time frame: Dec 2015.",1
"Define policy based upon FY16 plans
Re evaluate previously proposed storage policies: https://wiki.ncsa.illinois.edu/display/LSST/Storage+Policy?src=contextnavpagetreemode    Plan new policies:  https://wiki.ncsa.illinois.edu/display/LSST/Changes+to+Storage+Policy+and+Design?src=contextnavpagetreemode",1
"Data access rights and retention policies
Added Data access center requirements, individual data access rights, data retention policies and other general cleanup.",1
"Support new casting requirements in NumPy 1.10
The function imagesDiffer() in testUtils attempts to OR an array of unit16s (LHS) against an array of bools(RHS) {{valSkipMaskArr |= skipMaskArr}} and errors with message  {code}  TypeError: ufunc 'bitwise_or' output (typecode 'H') could not be coerced to provided output parameter (typecode '?') according to the casting rule ''same_kind''  {code}  preventing afw from building correctly. ",1
"Revisit database compression trade-offs
As discussed at Qserv meeting Oct 7, it is not entire clear if it will be worth compressing data. Need to revisit baseline.",5
"Discuss with MySQL team
This story captures issues/topics that we want to bring up with mysql team.",2
"SuperTask structure implementation 
Starting to implement the structure of the Super Task framework for process execution",7
"SuperTask framework extension
Incorporate Configuration and data reference to implementation",4
"Bootcamp meeting
I've attended LSST DM bootcamp",3
"SuperTask framework documentation and  refactorization
While still prototyping I need to fill documentation on new code as well as do some clean up as well",7
"testPsfDetermination broken due to NumPy behaviour change
Old NumPy behaviour (tested on 1.6.2):  {code}  In [1]: import numpy    In [2]: a = numpy.array([])    In [3]: numpy.median(a)  /usr/lib64/python2.6/site-packages/numpy/core/fromnumeric.py:2374: RuntimeWarning: invalid value encountered in double_scalars    return mean(axis, dtype, out)    Out[3]: nan  {code}    New NumPy behaviour (1.10.0):  {code}  In [1]: import numpy    In [2]: a = numpy.array([])    In [3]: numpy.median(a)  [...]  IndexError: index -1 is out of bounds for axis 0 with size 0  {code}    This breaks {{testPsfDeterminer}} and {{testPsfDeterminerSubimage}}, e.g.:  {code}  ERROR: testPsfDeterminerSubimage (__main__.SpatialModelPsfTestCase)  Test the (PCA) psfDeterminer on subImages  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""./testPsfDetermination.py"", line 342, in testPsfDeterminerSubimage      trimCatalogToImage(subExp, self.catalog))    File ""/Users/jds/Projects/Astronomy/LSST/src/meas_algorithms/python/lsst/meas/algorithms/objectSizeStarSelector.py"", line 377, in selectStars      widthStdAllowed=self._widthStdAllowed)    File ""/Users/jds/Projects/Astronomy/LSST/src/meas_algorithms/python/lsst/meas/algorithms/objectSizeStarSelector.py"", line 195, in _kcenters      centers[i] = func(yvec[clusterId == i])    File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/numpy/lib/function_base.py"", line 3084, in median      overwrite_input=overwrite_input)    File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/numpy/lib/function_base.py"", line 2997, in _ureduce      r = func(a, **kwargs)    File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/numpy/lib/function_base.py"", line 3138, in _median      n = np.isnan(part[..., -1])  IndexError: index -1 is out of bounds for axis 0 with size 0  {code}",1
"S18 Improve Webserv
nan",79
"Assemble eslint rules for JavaScript code quality control
Review and assemble eslint rules, which enforce clean JavaScript and JSX code.    Code cleanup to avoid too many rule violations.",8
"JavaScript code cleanup - remove unused packages
Remove es6-promise, react-modal, other cleanup",2
"convert underscore to lodash
lodash has become a superset of underscore, providing more consistent API behavior, more features, and more thorough documentation. We'd like to convert our underscore package dependencies to lodash while we have only ~20 calls to underscore functions",2
"Create a React component which manages tabs
We need a React component, which manages tabs. ",6
"Shutdown mechanism doesn't work when logging process is disabled.
If the logging mechanism is turned off in ctrl_execute, the ctrl_orca Logger doesn't get launched.  The current shutdown mechanism waits for the last logging message to be transmitted before shutting down so it doesn't kill off that process.   If the logger.launch config file option is set to false, this process never get launched and ctrl_orca hangs after the shutdown waiting for the message to arrive.",8
"Lead  the Firefly conversion from GWT to React/FLUX design meeting
A focused week long design meeting on Firefly conversion from GWT to React/FLUX.   Agenda and notes here https://confluence.lsstcorp.org/pages/viewpage.action?pageId=41786446  Design document at https://confluence.lsstcorp.org/display/DM/Firefly+Client+Application+Architecture  ",6
"Firefly conversion from GWT to React/FLUX design meeting
A focused week long design meeting on Firefly conversion from GWT to React/FLUX.    Produce the first draft of design document   https://confluence.lsstcorp.org/display/DM/Firefly+Client+Application+Architecture",10
"Firefly conversion from GWT to React/FLUX design meeting
A focused week long design meeting on Firefly conversion from GWT to React/FLUX. Design document at  https://confluence.lsstcorp.org/display/DM/Firefly+Client+Application+Architecture  ",6
"Firefly conversion from GWT to React/FLUX design meeting
A focused week long design meeting on Firefly conversion from GWT to React/FLUX.  Design document at https://confluence.lsstcorp.org/display/DM/Firefly+Client+Application+Architecture.  ",4
"Attend DM boot camp
Attend DM boot camp to learn more about DM stack, butler, and task. ",2
"Attend DM boot camp
Attend DM boot camp to learn more about DM stack, butler, and task.     Most of the presentations are located at URL https://community.lsst.org/t/dm-boot-camp-announcement/249. Presentations like afw, eups, tasks, and butler are necessary to participate in LSST, so everyone on LSST must understand these concepts. Look at the list of presentations covering these topics and make sure your understand them. Some of the remaining talks go into more detail or cover more specialized topics. Those talks should be scanned to see if they are of interestß to you.",3
"Attend DM boot camp 
Attend DM boot camp to learn more about DM stack, butler, and task. ",3
"Consulting in September
nan",3
"update cat logging information
The ""cat"" package has a table which the Logger in ctrl_orca uses to insert information from logging messages.  The format of the log messages has changed, and therefore the table in ""cat"" needs to be changed as well.",1
"Update qserv for lastest xrootd
Small API change in latest xrootd, requires a parallel change to qserv.  Paves the way for DM-2334",1
"Update DECam camera geometry descriptions for raw data
The overscan and prescan regions of instcal data have been trimmed, but they are included for raw data.  The amplifier information in the current camera descriptions were made for instcal data and do not include overscan and prescan regions.      While processing raw data with the current camera descriptions, the bounding boxes from the camera object seem incorrect for raw data.     Code change summary:  - Use non-zero overscan and prescan regions  - Update the pixel array layout. My schematic of pixel array layout is attached in DecamAmpInfo.png    Screenshots are post-ISR images processed with bias and flat correction using the old or updated camGeom.  ",7
"Educational Activities for In-Depth Reusable Background
Bucket epic to capture effort spent in educational activities and meetings to gain in-depth reusable background knowledge for the LSST project.",10
"Please port showVisitSkyMap.py from HSC
The HSC documentation at http://hsca.ipmu.jp/public/scripts/showVisitSkyMap.html includes a useful script for displaying the skymap and CCDs from a set of visits. It would be convenient if a version of this script was available in the LSST stack.",1
"Update Trust Level of all LSST DM Staff to Level 4 via the API
It seems safe to update the Discourse trust level of all members of the LSSTDM group on community.lsst.org to Level 4 (full permissions). See https://meta.discourse.org/t/consequences-of-using-or-bypassing-trust-levels-for-company-organization-staff/34564?u=jsick    This should alleviate concerns that DM staff are being prevented from fully using the forum.    This ticket implements a small notebook to exercise the Discourse API to make this trust level migration possible.",1
"Provide upstream improvements to sphinx-prompt
Provide PRs to sphinx-prompt (or decide to own a fork of sphinx prompt in documenteer) that includes    - an actual package you can import  - better error reporting when you forget to include a class with the prompt directive",1
"Replace use of image <<= with [:] in python code
Replace all use of the afw image pixel copy operator {{<<=}} with {{\[:]}} in Python code.    See DM-4102 for the C++ version. These can be done independently.",2
"Remove use of <<= from C++ code in our stack
Replace usage of deprecated Image operator {{<<=}} in C++ code with {{assign(rhs, bbox=Box2I(), origin=PARENT)}} as per RFC-102    Switch from [:] to assign pixels in Python code where an image view is created for the sole purpose of assigning pixels (thus turning 2-4 lines of code to one and eliminating the need to make a view).",3
"Update user documentation
{{ORDER BY}}, {{objectId IN}} and {{objectId BETWEEN}} predicates support have been improved, this should be documented.    ",1
"doc & demostrate ceph setup
nan",3
"add lfs remote support to lsstsw/lsst_build
Support for cloning from lfs backed repos, when indicated via repos.yaml, is needed.  ",4
"Update cfitsio to 3.37 (adding bz2 support)
Per RFC-105, we should upgrade to cfitsio 3.37.",2
"evaluate git lfs prototype and provide feedback
nan",3
"Clean up lsst_stack_docs for preview
Improve the presentation of the New docs overall:    # Add a Creative Commons license  # Remove stub documents from the presentation  # Put READMEs in all doc directories to explain what content will go in them  # Clean up and update the source installation guide to reflect 11_0",1
"Local LSST IAM meeting
October 14, 2015 (local)    - Drawing identity access management architecture on whiteboard.    - DB access via kerberos       - mariaDB does pam but does it do kerberos tickets?            - could then simply access with a ticket   - if users are exporting VMs/containers, do they need keytabs?  how do we support this?   - what does the ID linking?   - do we need replication to base site so that base site can operate independent     - Can we get 2 LSST VMs?     - NCSA cyber-infrastructure standards?  MIT Kerberos?  OpenLDAP?    - Meeting with Iain Goodenow    ",2
"Security Meeting with LSST PO
October 8, 2015    - Refresh of security plan, sub-plan       - Talk about camera subsystem refusal to buy-in to security plan    - IaM work moving along       - Continue bi-weekly meetings with developers       - Most likely going with kerberos       - Iain meeting with Jim and Co on authn/z work    - Joint technical meeting in February       - when and where?  Santa Cruz, Feb 22-24th 2016    - security plan refresh:       - cover email with old document and instructions       - DM: Don P. and Jeff Kantor       - EPO       - PO: Iain       - Camera       - Incidents: all reports are collected and acknowledged",1
"Build instance and snapshot on Nebula for HTCondor worker with v11_0 LSST stack
Build instance and snapshot on Nebula for HTCondor worker with v11_0 LSST stack.",12
"Draft IaM Recommendations
LSST IaM draft recommendations from NCSA.  Group includes: Jim Basney, Terry Fleury, and Dan Thayer.",23
"ctrl_events test failures on CentOS7 VM on Nebula
I am seeing failure to build due to test  failures for ctrl_events on a CentOS7 instance on the Nebula Openstack.  Details to follow.",1
"Bootcamp meeting
Slides and attendance at DM Bootcamp",8
"Bootcamp meeting
Travel to Princeton and attend DM Boot Camp ",8
"pipe_tasks/examples/calibrateTask.py fails
The self contained example calibrateTask.py in pipe_tasks/examples/ fails when attempting to set field ""coord"" in refCat. Exact error message -     {code}  11:04:19-vish~/lsst/pipe_tasks (u/lauren/DM-3693)$ examples/calibrateTask.py --ds9  calibrate: installInitialPsf fwhm=5.40540540548 pixels; size=15 pixels  calibrate.repair: Identified 7 cosmic rays.  calibrate.detection: Detected 4 positive sources to 5 sigma.  calibrate.detection: Resubtracting the background after object detection  calibrate.initialMeasurement: Measuring 4 sources (4 parents, 0 children)   Traceback (most recent call last):    File ""examples/calibrateTask.py"", line 150, in <module>      run(display=args.ds9)    File ""examples/calibrateTask.py"", line 119, in run      result = calibrateTask.run(exposure)    File ""/home/vish/lsst/lsstsw/stack/Linux64/pipe_base/11.0-2-g8218aaa+5/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/home/vish/lsst/pipe_tasks/python/lsst/pipe/tasks/calibrate.py"", line 478, in run      astromRet = self.astrometry.run(exposure, sources1)    File ""examples/calibrateTask.py"", line 90, in run      m.set(""coord"", wcs.pixelToSky(s.getCentroid()))    File ""/home/vish/lsst/lsstsw/stack/Linux64/afw/11.0-5-g97168e0+1/python/lsst/afw/table/tableLib.py"", line 2372, in set      self.set(self.schema.find(key).key, value)    File ""/home/vish/lsst/lsstsw/stack/Linux64/afw/11.0-5-g97168e0+1/python/lsst/afw/table/tableLib.py"", line 1064, in find      raise KeyError(""Field '%s' not found in Schema."" % k)  KeyError: ""Field 'coord' not found in Schema.""  {code}    Note that {{wcs.pixelToSky(s.getCentroid())}} is set to {{Fk5Coord(15.007663073114244 * afwGeom.degrees, 1.0030133772819259 * afwGeom.degrees, 2000.0)}}",1
"Mask overlay does not work when the image is flipped
nan",2
"Database connection problems in daf_ingest
The DbAuth connection fallback in ingestCatalogTask passes the ""password"" keyword argument to {{MySQLdb.connect}} instead of ""passwd"", which fails. Also, the ""port"" command line argument isn't marked as an integer, causing port strings to be passed down to MySQLdb. This results in a type error. ",1
"Have a ""mark as current"" option in lsstsw
Russell explained to me the advantage of having a specific eups tag associated with a given lsstsw installation. However, it would be very handy to have a way to get all installed packages automatically tagged as current as part of the installation process.    I suggest a ""-c"" option to lsstsw which will tag everything as current after the full installation is complete. This way, partially installed packages won't get marked current, and people who do full installations can not have to deal with having to say ""-t bBLAH"" every time they setup things.",4
"Change type of LTV1/2 from int to float when writing afw images to FITS
The LTV1/2 problem is originally my bug.  I used integer LTV1/2 in  {code}  afw/src/image/ExposureInfo.cc:    data.imageMetadata->set(""LTV1"", -xy0.getX());  afw/src/image/ExposureInfo.cc:    data.imageMetadata->set(""LTV2"", -xy0.getY());  {code}  whereas a more careful reading of the NOAO page [http://iraf.noao.edu/projects/ccdmosaic/imagedef/imagedef.html] introducing them includes floating point examples.    The fix is to cast the XY0 values to float.  I'm not sure if there'll be any side effects of fixing this, but if so they'll be obvious and trivial.  ",1
"DHT prototype: HTTP server library refactor/cleanup
This experimental library turned out to be quite useful.  Next stage of prototyping will be making greater use of this, and I anticipate this library will also be used in production code.  Spend some time cleaning up and organizing this lib so it doesn't get off to a bad start, and prepare for general review/feedback from the team.",6
"DHT prototype: test fixture rework
The DHT test fixture, developed during the preliminary work with kademlia, needs some updates for the next stage of the prototype:  * adapt to reworked http library  * adapt DHT interface to be more generic  ",6
"Update DECam CCDs gain, read noise, and saturation values
The values of DECam gain, read noise, and saturation value need to be updated.     This ticket is to update them in the Detector amplifier information, which is used in IsrTask.     Talked to Robert Gruendl. These values should take precedence over the values in the fits header. They seem stable and do not seem to vary with time.   ",1
"Re-implement packed keys in CSS
Current implementation of JSON-packed keys in CSS has one complication - the name of the container for packed keys is the same as the key itself (plus .json suffix). This complicates handling of the subkeys because these keys need to be filtered out and these names are all different. It would be better to have easily identifiable packed key names.",4
"Discover how to create Doxygen XML in build system
We need XML output from Doxygen in order to import existing API documentation into Sphinx. In this story I find out how to achieve this within the stack build system.",2
"Demonstrate using Breathe for Python & C++ API reference in New Docs
Demonstrate use of breathe for utilizing the existing Doxygen API documentation in the new Sphinx-based doc platform.",3
"Reduce scons output in qserv
Yesterday AndyH expressed a valid concern that qserv prints too much info which makes it hard to find errors. By default scons prints whole command line for C++ compilation and linking which are quite long (~half screen depending on your screen size). Most of the time we don't need to see that, so it would be better to replace that with shorter messages like ""Compiling Something.cxx"" and have an option to print full command with --verbose option.",1
"Networking requirements for design
Meeting with Paul to discuss remaining network design.",1
"Finance Contract Discussions
Discussing updated contract hoops and game plan.",1
"ICI agenda and mtg
nan",1
"Investigating procurement of individual components
Component breakdown and explanation of components as part of FY16 purchases in order to plan optimal purchasing through vehicles available to NCSA.",1
"Search for uses of current afw.wcs in the stack
Search through the stack for all the uses of our Wcs implementation (Wcs, TanWcs, makeWcs, and any other hidden objects) and make a list of all of those uses (on Community for example). This list should note whether the usage is in C++ or python.",2
"Document detailing usage of Wcs in the stack
The information from DM-4151 should become a brief report on the kinds of usage of Wcs in the stack. This could be posted to Community or Confluence, or it could be a brief LaTeX document attached to the afw repo.    Included in this report should be whether each current usage requires C++, or whether it could be done with e.g. vectors returned from python.",4
"RFD to collect current and future use cases of Wcs
File an RFD requesting information about current and possible future use cases for a Wcs system in the stack. This should get feedback from, at minimum, the alerts pipeline, DRP, and Level III data producers. It should also get feedback from our resident wcs experts.    Whether those use cases are currently implemented in our code, or could be generalized from it, isn't important to this RFD as this information will feed into a subsequent requirements document and RFC about what needs to be written/changed.    Some use cases/buzzwords that will likely be included:   * Efficient x,y -> x',y'   * Stacking a sequence of transformations efficiently   * easy extensibility   * should color terms be included in the Wcs or dealt with elsewhere?   * pixel distortion effects (e.g. tree rings, edge roll off)   * Simultaneous Astrometry (i.e. from image stacks)",4
"Sever side Histogram for variable bin size
For any given column or columns (expression of columns such as col1+log(col2) ) of a IpacTable data, the variable bin histogram is needed.  The variable bin is based on ""Bayesian Blocks"" algorithm (http://jakevdp.github.io/blog/2012/09/12/dynamic-programming-in-python/).  The output is a new IpacTable (DataGroup) with three columns, numPoints, binMin and binMax.",6
"LSST Wcs requirements document
Based on the information compiled from DM-4153 and DM-4152, prepare a requirements document (LaTeX, with references) describing all of the known Wcs requirements for the various portions of the LSST stack. This document could live in afw or in its own repository, and could potentially become a published technical report/conference proceeding.",10
"Evaluate existing Wcs libraries and report on our options
As part of the requirements document in DM-4155, we need a report-likely section(s) of that same document-on the currently available Wcs libraries (including, but not limited to AST Starlink and astropy.Coordinates) and summaries and references from the current literature about how other surveys and projects have managed their Wcs.    For each of the currently existing options, the report should include at a minimum:   * implementation language.   * API and languages that can interface to that API.   * how well supported (including number of contributors) is the project and how active is ongoing development.   * its performance and optimization for both scalar and vector calculations, and its scalability to large data sets.    We also need to look for and/or request technical reports and other literature from existing and completed surveys, including DES (Robert Gruendl), Pan-STARRS (Paul Price), STScI (Erik Tollerud), and SDSS (Lupton and/or Blanton?).",30
"Provide a recommendation for how to manage Wcs in LSST
The technical report from DM-4155 and DM-4156 should have as its conclusion a recommendation for what Wcs system the LSST stack should adopt (either our own implementation, a third-party library, or some combination thereof). Beyond the conclusion section of that report, this will be provided as an RFC that includes:     * executive summary   * mock API   * links to the relevant documentation (beyond just the above technical report)   * a bullet list of the rationale for this decision    The conclusion of this RFC represents the end of this Epic.",4
"Unused variables in meas.algorithms.utils
Pyflakes 1.0.0 reports:  {code}  $ pyflakes-2.7 utils.py  utils.py:232: local variable 'chi2' is assigned to but never used  utils.py:481: local variable 'numCandidates' is assigned to but never used  utils.py:482: local variable 'numBasisFuncs' is assigned to but never used  utils.py:487: local variable 'ampGood' is assigned to but never used  utils.py:492: local variable 'ampBad' is assigned to but never used  {code}  In the best case, those variables are simply unnecessary, and they should be removed to simplify the code and avoid wasting time. Alternatively, it's possible that they ought to be used elsewhere in the calculation but have been omitted accidentally. Please establish this for each one, then either remove them or fix the rest of the code.",1
"Please implement a warper that works with a single XYTransform
At present we only warp images based on a pair of {{Wcs}}. This is needlessly restrictive. We should be able to define the transformation by function that computes {{f(x,y) -> x',y'}}, e.g. an {{XYTransform}}.    Note that reversibility, while not strictly necessary, is very desirable. Hence we might as well use {{XYTransform}}.     I suggest we have only one underlying implementation in order to avoid code duplication. This could easily be done by implementing an {{XYTransform}} that combines a pair of WCS.",6
"Create a skeleton framework for Firefly using redux and react.
As part of the GWT conversion to pure JavaScript, we've came up with a new design based on redux and react.  This task is to create all of the major components of the framework with minimal functionalities.  This will allow other developers to build upon this foundation core.",10
"Take upstream boost 1.59 patch to squelch warnings for gcc 5.2.1
Under gcc 5.2.1, use of boost 1.59.0 produces a torrent of compiler warns from within boost headers about use of deprecated std::auto_ptr (see https://svn.boost.org/trac/boost/ticket/11622).    A patch for this is already committed upstream in boost.  It is proposed that we take this patch into the lsst t&p in interim until the next official boost release.",1
"Internal documentation of procurement process
nan",1
"productize ""Data repository selection based on version""
finish & productize work from DM-5608",6
"Butler: move configuration (.paf) file into repository
We want to be able to keep the .paf file in the repository, with the data for which it provides configuration information.    At least for the time being it needs to be optional; it looks first in the repository and second in the ""old"" location for the paf.    In the case of a repository chain, the child should 'win'.",12
"Butler: change configuration from .paf to something else
for ""something else"" there are 2 major options: pex_config (dislike because the 'data' gets executed), and yaml (very well supported, is just data). 3rd option: we could use the sqlite registry, and define a schema for that.     (sqlite benefits: has support for write locks that will be necessary very soon. Except, need write-once-compare-same mechanism; so we'll need that 1. for normal files and 2. across n nodes. so maybe there's no benefit).    also, while we're there:  as noted in DM-4170, registries.py needs to not use astropy and instead should use pyfits. If it's quick, that change should be made here (or turned into a separate ticket)",12
"Build AL2S vlans from Miami to NCSA
To prototype the layer 2 circuit LSST will eventually have.",2
"document proposal for Base site to NCSA data transfer
With Steve and James distill the actual data movement requirements, the mechanism to broker those transfers and the network technology to ensure the real-time transfer of images",2
"Provide network infrastructure support to deployment of new nodes
nan",3
"Create baseline requirements for evaluating server hardware from different vendors
nan",1
"Refine server evaluation specifications into a usable quantifiable form
nan",1
"Discussions with Ron on Base site architecture
nan",1
"update LSE-78 once current updates are applied
There are several sections with inaccurate information about the North American portion of the LHN and the implementation of the networking into NCSA as well as the base site commissioning cluster architecture.",2
"Butler: provide API so that a task can define the output dataset type
The task needs to be able to specify everything that needs to be in the policy file so that the butler can put and get data for a new dataset type.    Consider that the policy data can be split between the camera-specific and the task-specific parts. (KT was thinking of calling the camera part the ""genre""), this potentially reduces the amount the task has to specify.   Another option is to hard-code some some of the policy in the butler itself:  * the path template (it could be assembled out of the data id components)  **  if it's hard coded the task must pass the component dataId keys  ** if it's not hard coded the task must provide a template   * python type  * storage type    there could be user overrides too.",20
"Butler: add support for skymap based dataIds
load the skymap from the repository and use it a la metadata lookup to complete lookups. for example if the script wants all the patches that are in tract x, look in the skymap to get that information.    will be something like:  create a skymap object given a configuration (in the repository) (it has its own dataset type)  create a registry with that skymap object  use that registry to lookup skymap related parameters. Might need to add to the policy file that a given configuration uses the skymap.    requires that data is already indexable by tract & patch, the work is adds iteration over tract/patch specified by the skymap.    example use case:  {code}  {      datasetType = 'coaddTempExp'      key = (?)      format = ['patch', 'visit']      dataId = {'tract':<some numberOrId>}      myButler.queryMetadata(datasetType, key, format, dataId)  }  {code}",15
"Experiment with memcached for secondary index
Test whether memcached could be used to serve objectId --> chunkId mapping, in particular from the performance perspective.",6
"Experiment with xrootd for secondary index
Test whether xrootd could be used to serve objectId --> chunkId mapping, in particular from the performance perspective.",6
"Experiment with c-style arrays for secondary index
Use simple C-style array (chunk[objID], allocated up to maximum number of objectIDs), or a single-layer set of arrays (chunk[block][objID%blksize], where second index runs from 0 to size of each block) to store index compactly and provide minimum overhead for lookups.",5
"Infrastructure Security at Site
nan",1
"Outline of data flow
Outline of data flow between Chile->NCSA and NCSA->Chile.",3
"L1 system functions and responsibilities 
Outline functions and responsibilities for all parts of the L1 system",35
"Initial code change to run ISR with DECam raw data
This ticket is for implementing changes in {{obs_decam}} in order to run {{processCcd}} with raw DECam data.    Changes are mostly in {{DecamMapper}} and a new class {{DecamIsrTask}} is added. A test to retrieve defects with Butler is also added. ({{testdata_decam}} is at lsst-dev /lsst8/testdata_decam/ )    The pixels with bit 1 (bad, hot/dead pixel/column) from the Community Pipeline Bad Pixel Masks are used as bad pixels.  The CP BPM fits files are directly used as defect files. Due to their large size, they probably should not go into {{obs_decam}} repository so they are treated similarly as other calibration products.     With the changes of this ticket, the following ingest defects files into {{calibRegistry}}:  {code:java}  ingestCalibs.py . --calibType defect path-to-bpm/*fits  {code}    and the following should run past ISR:  {code:java}  processCcd.py .  --config isr.doFringe=False isr.assembleCcd.setGain=False calibrate.doPhotoCal=False calibrate.doAstrometry=False calibrate.measurePsf.starSelector.name=""secondMoment""  {code}    Running fringe correction with DECam raw data will be in future tickets (DM-4223 and possibly more). Also this ticket does not cover implementing or porting new ISR functionalities that haven't yet been included in ip_isr (such as crosstalk).   ",14
"Other LOE -- Oct 2015
Local LSST group meetings, Ethics training, or other local meetings or tasks to comply with NCSA policies",2
"Automate LSST Firefly standalone releases using Jenkins
This task involves merging in feature branches, building Firefly standalone, tagging, push changes to github, generating changlog, and using github API to publish the release.  Release should be attached to the latest tag with downloading artifacts and changelog.",7
"Python LogHandler does not pass logger name to log4cxx
Not sure how or why it happened, but presently Python LogHandler for lsst.log does not pass logger name to log4cxx layer and all messages from Python logging end in root logger. ",1
"Build proof-of-concept package documentation for lsst.afw
Build package documentation for {{lsst.afw}} under the new doc platform as a demonstration.    - Install a Sphinx site in lsst.afw/doc  - Implement MVP documentation pages for lsst.afw packages (table, image, etc.)  - C++ API reference from doxygen+breathe  - Python API reference from numpydoc    This ticket *will not* attempt to add new documentation content; only to show how existing content can be re-organized.",2
"Build ltd-mason for running a multi-package software documentation build
LSST the Docs ([SQR-006|http://sqr-006.lsst.io]) is a system for extending our existing Jenkins build infrastructure to build Sphinx-based software documentation for our Eups-based packages. This is a ticket to implement the {{ltd-mason}} service, which runs on Jenkins after the {{buildlsstsw.sh}} step and compiles software documentation.    Specific outcomes of this ticket are    - Full specification of the YAML interface between {{ltd-mason}} and {{buildlsstsw.sh}} (including creating a mock YAML file for local testing)  - Demonstration of a Science Pipelines documentation build on a local lsstsw environment (in conjunction with content from DM-4195)  - Accommodations to the science pipelines documentation repo and documenteer for building sphinx packages are included in this ticket’s scope.    Next steps are    - Standing up the service on Jenkins and testing integration with {{buildlsstsw.sh}}  - Uploading to S3 (which involves building integration with {{ltd-keeper}}.",17
"Improve Qserv master robustness for queries like ""select @@max_allowed_packet""
This king of query crashes Qserv:    {code:bash}  mysql --host 127.0.0.1 --port 4040 --user qsmaster -e ""select @@max_allowed_packet""  {code}",1
"Documentation and technical debt in meas_base/PixelFlags.cc
The port from HSC of SafeClipAssembleCoaddTask has left some documentation and code quality changes to be made. Specifics include:    * In the process of porting, functionality was added which allowed users to specify additional mask planes to be converted to pixel flags. However this was fundamentally incompatible with the flag handler functionality that LSST was currently using. PixelFlags was thus modified to allow SafeClipAssemble coadd to work with the user defined masks, but that made it fundamentally different than the other plugins. In the future this plugin should be brought more in line with all the other measurement framework. This will most likely involve rewriting sections of the measurement framework, to add the ability for users to more directly set runtime behaviors of the measurement plugins (such as specifying non default mask planes to work with).  * Because PixelFlags could no longer use the the flag handler framework, sections of the SafeCentroidExtractor had to be duplicated within PixelFlags. This duplicated code is non-ideal and should be rectified. Possibly in the process of rewriting the measurement framework, the utils could be expanded to have convenience methods to access functionality when not using flag handlers  * As with all of the measurement framework, PixelFlags needs better documentation. This includes some line to line comments, but more importantly the over all functionality of the routine needs documentation. This includes: what the routine does; how it works; and why various design decisions were made",3
"Revert temporary disabling of CModel in config override files
Revert the temporary disabling of CModel that relates to a bug noted in DM-4033 that was causing too many failures to test that processCcd (etc.) would run all the way to completion (most of the other fixes/updates related to the initial disabling in the multiband tasks have now been completed in DM-2977 & DM-3821).     Relevant files:  {code}  config/processCcd.py   config/forcedPhotCcd.py   config/forcedPhotCoadd.py   config/measureCoaddSources.py  {code}  ",1
"wmgr should delete database from inventory when dropping it
When wmgr drops database it should also cleanup chunk inventory for that database.    ",2
"Research web authentication and authorization and gather usage stories
Research SUI, DAX, Butler, and Qserv authentication and authorization requirements and schemes. Document usage stories for all layers",6
"Create unit tests for SafeClipAssembleCoadd
Porting SafeClipAssembeCoadd from HSC to LSST left that functionality without a unit test. Currently AssembleCoadd is tested from within tests/testCoadds.py. This test does not call AssembleCoadd directly however. The actual code pertaining to assembling a coadd exists within python/lsst/pipe/tasks/mocks/mockCoadd.py. This called from testCoadds, and is used to build and coadd synthetic images from known psfs amongst other things. This should be expanded to test both AssembleCoadd and SafeClipAssembleCoadd, possibly with some sort of argument to the function call. It is important to keep testing both methods of generating a coadd.",3
"Create documentation and examples for SafeClipAssembleCoadd
SafeClipAssembleCoadd in HSC did not have adequate documentation, and thus neither does LSST post port. Documentation which details the functionality and usage of this function should be created, and should be available either through the [Doxygen task documentation|https://lsst-web.ncsa.illinois.edu/doxygen/x_masterDoxyDoc/group___l_s_s_t__task__documentation.html] or its successor . Examples of the usage and various options should also be included with the documentation.",4
"Investigate shear bias errors from DM-1135
DM-1135 mostly was inconclusive, due to the fact that the error bars were around the same size as the differences in many of the tests.  This in spite of the fact that we ran 6x as many sample galaxies as great3sims.    Investigate the reason for this, and see if we can estimate the errors more accurately.    ",6
"Rerun tests of DM-1135 with a larger number of galaxies
As a result of the new error estimation, it became apparent that a larger number of sample galaxies were required in DM-1135.  This is an expansion of that test to a larger number of galaxies.  Since the original  great3 tests had about 2 million galaxies, we should be able to do these test reasonably well with the 6 million galaxy pool created in DM-1135.  However, the measurements need to be rerun in some cases, and the error analysis done again.",6
"Initial tests of ShapeletApprox
This will be part of a wider set of test which I am hoping that Jim will fill in as additional stories under DM-1136    Basically, we hope to see how extensive a Shapelet Approximation must be done for a Psf in order to reach a stable result (stable meaning it would not markedly improve with increase in Shapelet order).    For this intial test, I propose to compare SingleGaussian, DoubleGaussian, Full, and 2 higher order models.  I will also add a model with inner and outer defined to see if those have a significant effect.  It will tell use how the existing models line up and which parameters matter.    This test will be done using the great3sims subfield organization, and with a single, randomly chosen Psf image from the 0.7 arcsec FWHM library (raw fwhm from PhoSim, the actual fwhm is closer to 0.8 arcsec)     This will probably be just the beginning of more extensive ShapeletApprox tests, so this story should be a sub-story of a larger test project which I am hoping Jim will define.",6
"Butler: SafeFile and SafeFileName can overwrite good with bad in some cases
A bad file can be written in Butler, in the case where 2 temp files that use SafeFile or SafeFileName to the same location are started, one closes, and then the other fails - and then closes and writes the bad file. Need to handle the exception in a way that does not write.    Also, in the case of no exception (failure), when closing B, need to compare to A and throw if different.",4
"Package capnproto for eups
Prototype is now getting to the point where a wire-protocol package like capnproto or protobuf is needed.  capnproto is the new hotness, and we're probably going to want to migrate qserv from protobuf->capnproto at some point.    This task is to go ahead and get capnproto packaged and published for use in the replication prototype.",2
"Convert copyright/license statements to one-liners for RFC-45
Refactor how we manage copyright and license information in stack repositories    # Identify a list of packages to process  # build and test an automated system of       - adding a global COPYRIGHT to each repo. Content will be “Copyright YYYY-YYYY The LSST DM Developers”. Years will be determined by git history.     - adding a GPLv3 license file to each repo.     - changing the boilerplate in all files to say ""See the COPYRIGHT and LICENSE files in the top-level directory of this package for notices and licensing terms.” Use https://gist.github.com/ktlim/fdaea18ab3d39afdfa8e     - automatically branch, commit, merge and push    And deploy this automated system.",8
"X16 Secondary Index - Implementation
Implement / productize optimizations to secondary index proposed through DM-2119",27
"IsrTask calls removeFringe in FringeTask but the method does not exist
The method {{removeFringe}} of {{FringeTask}} is called in {{IsrTask}} but there is no {{removeFringe}}.      Not sure if {{removeFringe}} was meant to be a place holder",1
"getExposureId not implemented in obs_lsstSim
There is no implementation of the getExposureId method in processEimage.py.  This causes it to fail using a modern stack.",1
"Collect single-host performance data for secondary index
Run production-scale (billions of entries) tests on different index options, collect performance statistics for allocation (CPU, memory) and for queries.",3
"Set up multi-host tests for secondary index technologies
For client-server technologies (memcached, xrootd, etc.), develop multihost test jobs to exercise production-scale indices (billions of entries, millions of queries).  Index should be allocated on single ""master"" machine, with queries generated from one or more separate machines, possibly with multiple threads/jobs per machine.",5
"Experiment with bulk updates to secondary index
The nightly data loader job may need to add new objectIDs, or change the chunks of existing objectIDs, _en masse_.  Develop test code (or add features to existing demonstrators) to handle bulk loading of new data to the index, or to overwrite collections of existing data.  This is significant for client-server technologies, where the bulk data may be transferred in a single transaction, with the server (possibly) handling the loop over individual elements.",7
"Collect multi-host and bulk-update performance data for secondary index
Run secondary index tests across multiple hosts (server and clients), collecting performance data for production-scale indices (billions of entries, millions of queries) with many parallel queries and bulk/block updates.",5
"Identify candidate technology for secondary index
Evaluate results of production-scale performance tests, both single and multiple host.  Identify the technology most likely to meet requirements, and estimate performance capability with respect to those requirements",3
"Port HSC-1355: Improved fringe subtraction
[HSC-1355|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1355]: ""with this fix, we get much  better fringe subtraction"".",2
"Data Distrib proto (nov)
nan",20
"Variance is set after dark subtraction
In the default {{IsrTask}}, the variance is currently set after dark subtraction.  This means that photon noise from the dark is not included in the variance plane, which is incorrect.  The variance should be set after bias subtraction and before dark subtraction.    [~hchiang2] also points out (DM-4191) that the {{AssembleCcdTask}} with default parameters requires amplifier images with variance planes, even though the variance cannot be set properly until after full-frame bias subtraction.  I believe that {{AssembleCcdTask}} only requires a variance plane in the amp images because it does an ""effective gain"" calculation, but I suggest that this isn't very useful (an approximation of an approximation, and you're never going to use that information anyway because it's embedded in the variance plane with better fidelity).  I therefore suggest that this effective gain calculation be stripped out and that {{AssembleCcdTask}} not require variance planes.",5
"HSC backport: Jacobian and focalplane algorithms
Add algorithms to compute the *Jacobian* correction for each object (calculable from the Wcs, but sometimes convenient)  and record the *focal plane* coordinates (instead of CCD coordinates) for sources (useful for plotting).    The standalone HSC commits to be cherry-picked are:    *Jacobian*  {{meas_algorithms}}  May 3, 2013  [Jacobian: add Algorithm to compute the Jacobian.|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/88d3bd3f32cf4d0138b80148e57bc275fc8c3454]  May 24, 2013  [Jacobian: fix up some cut/paste oversights.|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/ecad0d2559bb9815fc5560234f4502f35f50db73]  May 28, 2013  [Jacobian: fix calculation|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/7f3db53b56279929b9e416173ed09cf00dc81406]    {{obs_subaru}}  May 3, 2013  [config: enable jacobian calculation in processCcd|https://github.com/HyperSuprime-Cam/obs_subaru/commit/d0969911ee1a655fd82998f0b936fa90f443d2fd]  May 6, 2013  [config: set pixelScale for jacobian correction|https://github.com/HyperSuprime-Cam/obs_subaru/commit/e36bd1b4410812ca314f50c01f899d92acc0e7a5]      *focalplane*  {{meas_algorithms}}  May 24, 2013  [add algorithm to calculate position on the focal plane|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/dda3086f411d647e1a3e15451d7f093cd461873a]  May 25, 2013  [fix up building of focalplane algorithm|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/57d718bf51b255adf5789e389dfb776ecaa062d1]  Nov 21, 2014  [Adapt to removal of Point<float> from afw::table.|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/95627d55cb7d64718a42027954474df5c3661a65]    {{obs_subaru}}  May 24, 2013  [config: activate focalplane algorithm|https://github.com/HyperSuprime-Cam/obs_subaru/commit/d999a32e7e10b25cceccc94b61890486f96c0bfd]  ",4
"HSC backport: countInputs and per object variance functions
Back port of the following two HSC tickets:    *countInputs*  [HSC-1276|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1276]  {{meas_algorithms}}  Jul 1, 2015  [add measurement algorithm to count input images in coadd|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/51db0fd2624c7f9b641c93aa3cf6366539995d50]    {{obs_subaru}}  Sep 24, 2015  [config: activate countInputs for measureCoaddSources.|https://github.com/HyperSuprime-Cam/obs_subaru/commit/13ecd1317b05b5ff9e65fba41fe27a5cffcc2fda]    *variance*  [HSC-1259|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1259]  {{meas_algorithms}}  Jul 2, 2015  [add measurement algorithm to report background variance|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/86022f4381c3cec3f7f203b831b6a306596cfa3f#diff-7ae7aea69b58dbf075350ccfd3802cfb]    {{obs_subaru}}  Oct 19, 2015  [config: activate measurement of variance for coadds|https://github.com/HyperSuprime-Cam/obs_subaru/commit/cf1e80958bb9164dacf42d2d35a94dd366c78892]  ",6
"Specify default output location for CmdLineTasks
When neither {{\--output}} or {{\--rerun}} is specified as an argument to a {{CmdLineTask}}, any output from that task appears to be written back to the input repository. Note the use of the term ""appears"": from a preliminary inspection of the code and documentation, it's not clear if this behaviour can be overridden e.g. by environment variables.    The HSC stack behaves differently, using {{$INPUT/rerun/$USER}} as a default output location. A [brief discussion|https://community.lsst.org/t/new-argument-parser-behavior-rerun-flag-introduction-discussion/345] suggests that this is the preferred behaviour.    Please update the LSST stack to match the HSC behaviour.",2
"unable to upload images to nebula
I seem to be unable to upload an image to neblua from a URL via either horizon or the nova cli client.  The request seems to queue briefly and then reports a status of {{killed}}. Eg    {code}  glance image-create --name ""centos-7.1-vagrant"" --disk-format qcow2 --container-format bare --progress --copy-from http://sqre-kvm-images.s3.amazonaws.com/centos-7.1-x86_64 --is-public False --min-disk 8 --min-ram 1024  {code}",2
"Fix integer casting error in numpy version 1.10 in obs subaru
Fix type casting in obs_subaru in lates numpy in obs_subaru",1
"Identify Qserv areas affected by secondary index
Evaluate Qserv software for the Czars and workers to identify where an interface to the secondary index will be required for efficient operation.",5
"Implement secondary index service in Qserv
Implement the selected seconary index technology (see DM-2119) as a Qserv service, providing a client API to be used elsewhere in the Qserv system.  Depending on the chosen technology, this may including configuration of a separate lightweight server for the secondary index, creation of database tables, etc.  The client-side API should be technology independent.",10
"Implement bulk updating of secondary index in Qserv
Provide a service in Qserv to support creation or modification of secondary index objectID-chunk pairs in bulk, to support data loading.",4
"Implement secondary index query in Qserv
Implement query of secondary index in Qserv, using the technology selected in DM-2119.",4
"Draft of Configuration Solicitation
Draft of solicitation to be used for quote and configuration from multiple vendors for FY16 purchase. ",1
"Image Viewer memory leak
When reloading the same 500MB RAFT image into an image viewer (see the script below), it was discovered that single node Firefy server with 3G memory runs out of memory after ~15 reloads    Test case: keep reloading the html file with the following Javascript, creating an image viewer with 500MB image:    function onFireflyLoaded() {          var iv2= firefly.makeImageViewer(""plot"");          iv2.plot({               ""Title""      :""Example FITS Image'"",               ""ColorTable"" :""16"",               ""RangeValues"":firefly.serializeRangeValues(""Sigma"",-2,8,""Linear""),               ""URL""        :""http://localhost/demo/E000_RAFT_R01.fits""});  }    Follow up:    The bug was traced to java.awt.image.BufferedImage objects not being evicted from VIS_SHARED_MEM cache.    Further search showed that java.awt.image.BufferedImage (along with java.io.BufferedInputStream) is in src/firefly/java/edu/caltech/ipac/firefly/server/cache/resources/ignore_sizeof.txt, which lists the classes that have to be ignored when calculating the size of cache.    Testing on single node server (VIS_SHARED_MEM cache is not replicated), using [host:port]/fftools/admin/status page:    BEFORE (java.awt.image.BufferedImage was commented out in ignore_sizeof.txt)    After 14 reloads:  Memory    - Used                      :      3.7G    - Max                       :     3.55G    - Max Free                  :    488.0M    - Free Active               :    488.0M    - Total Active              :     3.55G     Caches:   	VIS_SHARED_MEM @327294449  	Statistics     : [  Size:15  Expired:0  Evicted:0  Hits:246  Hit-Ratio:NaN  Heap-Size:1120MB  ]  OUT OF MEMORY on next reload    AFTER THE CHANGE (Commented java.awt.image.BufferedImage in ignore_sizeof.txt)    After 36 reloads:  Memory    - Used                      :   1672.9M    - Max                       :     3.55G    - Max Free                  :   1968.0M    - Free Active               :   1468.0M    - Total Active              :      3.6G    Caches:   	VIS_SHARED_MEM @201164543  	Statistics     : [  Size:3  Expired:0  Evicted:34  Hits:659  Hit-Ratio:NaN  Heap-Size:1398MB  ]    ",2
"Local LSST Sec Meeting
Local cyber security meeting at NCSA with DM group.",2
"Image viewer: choosing pixel interpolation algorithm for scaled images
Pixel values are defined at integer coordinate locations. This means that when an image is rendered in a scaled, rotates, or otherwise transformed coord. system, an interpolation algorithm should be used to provide a pixel value at any continuous coordinate.    Currently, Firefly is using     RenderingHints.KEY_INTERPOLATION = RenderingHints.VALUE_INTERPOLATION_NEAREST_NEIGHBOR,    which means that when an image is rendered in a transformed coord. system, the pixel value of the nearest neighboring integer coordinate sample in the image is used.     ""As the image is scaled up, it will look correspondingly blocky. As the image is scaled down, the colors for source pixels will be either used unmodified, or skipped entirely in the output representation.""    Jon Thaler's team would like to be able to choose a different interpolation algorithm, depending on the situation.      As an example see various resize algorithms in [thttp://stackoverflow.com/questions/4756268/how-to-resize-the-buffered-image-n-graphics-2d-in-java].",4
"LSST PO Security Meeting
Bi-weekly meetings with PO to discuss cyber security issues in LSST.",1
"Please include obs_subaru in CI
{{obs_subaru}} should be included in the CI system.",1
"Create GitLFS Technical Note
Create a SQuaRE Technical Note describing the architecture of the GitLFS service implementation.",2
"Literature search for DCR -- Reiss
Go through the literature to find relevant seminal papers on DCR.    The outcome will be a bibliography and executive summary.  This should be posted on Discourse.",15
"Implement simple reference index files
We would a very simple way to make small reference catalogs for astrometry and photometry. The use case is testing and small projects, where the overhead of making a full up a.net index file (or whatever we replace that with) is excessive.",5
"Determine scope and requirements for CalibrateTask redesign
There are two stories in DM-3579 that define two requirements for the redesigned CalibrateTask.  Those along with the requirement that CalibrateTask be less brittle are a starting point for what the new task needs to do.  All other requirements should be gathered using the RFC process.    Using the input from the RFC and other requirements, the scope for this particular redesign will be defined and stated in a discourse thread.",10
"Sphinx support of sqr-001 technical note
Support the distribution of a technical note SQR-001    - Remove oxford comma in author list (documenteer)  - Solve issue where title is repeated if the title is included in the restructured text document  - Solve issue where name of the HTML document is README.html",1
"Investigate current dipole measurement examples and tests
There are tests in ip_diffim/tests/dipole.py for the dipole fitting.  There is also an example in the examples directory of ip_diffim.  This story will investigate these tests and examples to see to what precision the tests go as well as how complete the tests are.    An outcome of this will be an understanding of how precise the tests need to be to show that dipole measurement is behaving as we expect.",4
"Create a set of tests (or update the current ones) to facilitate refactoring of dipole measurement
This will create a test (not necessarily a unit test) that will simulate dipoles and measure them so that the measurement can be compared to truth values.  This may be simply refactoring the current tests.    This task should also include generating more generalizable utilities needed to create the dipoles and incorporating these and other test data into the stack so that they can be used in other studies.",5
"F17 Integrate L1 Database with Alert Production pipeline
Integrate the prototype of the L1 database built through DM-2036 with the Alert Production pipelines.  The design of the Alert Production L1 database is covered [here|http://ldm-135.readthedocs.org/en/master/#alert-production-and-up-to-date-catalog].",79
"Integrate qserv docs into the new doc system
See Frossie 11/04/2015 email to qserv-l list:    {{5- Docs. So you guys have a sphinx site. Fab, that’ll make it super easy to drop it into the new doc system that hosts sphinx on readthedocs - you can see a small example here http://sqr-001.lsst.codes/en/master/ - the idea is to continuously deploy the docs so the release step should be very lightweight and doable via API calls. (My holy grail is to do a release with no local checkout involved).  }}",2
"SQuaRE design meeting 1
Hold an in-person design discussion with members of the SQuaRE team.  ",4
"SQuaRE supertask design meeting 2
Hold a teleconference design discussion with members of the SQuaRE team.  ",1
"Create SuperTask design proposal
Create a design document covering the initial SuperTask concept and a scheme for how it would lead to solutions to longer-term pipeline construction problems, support provenance recording, etc.",16
"Should only read fringe data after checking the filter
The fringe subtraction is not necessarily performed if {{doFringe}} is True. It is only if the filter of the raw exposure is listed in config fringe.filters.      Fringe data should not be read unless the filter is indicated. There are likely no such filter data and it would cause runtime errors.      Seems related to changes from RFC-26 and DM-1299. ",8
"Adapt an existing task to be usable as a SuperTask
Either by wrapping, or by converting, make an existing task usable as a SuperTask subclass so that it can be run under an Activator.",8
"FY20 Define Procedures and Build Tools for Schema Evolution
As described in [LDM-135|http://ldm-135.readthedocs.org/en/master/#data-production-related-requirements], The database system must allow for occasional schema changes for the Level 1 data, and occasional changes that do not alter query results for the Level 2 data after the data has been released. This epic involves preparing for dealing with schema changes in production syste.",79
"Week end 10/03/15
Support for lsst-dev cluster, OpenStack, and accounts  for week ending October 3, 2015.",1
"Week end 10/10/15
Support for lsst-dev cluster, OpenStack, and accounts  for week ending October 10, 2015.",4
"Week end 10/17/15
Support for lsst-dev cluster, OpenStack, and accounts  for week ending October 17, 2015.",3
"Week end 10/24/15
Support for lsst-dev cluster, OpenStack, and accounts  for week ending October 24, 2015.",7
"Week end 10/30/15
Support for lsst-dev cluster, OpenStack, and accounts  for week ending October 30, 2015.",3
"Planning for new equipment setup (week end 10/03/15)
* Planning for hardware upgrades in racks  * Evaluate the setup of the new storage server installs lsst-store101, lsst-store141,lsst-store143,lsst-store144  * Set up and tested ESXi server on new Mac Pro. Worked on getting Mac OS X installed inside of ESXi on the new Mac Pro hardware. Found solution, need to find a better one if we are expecting to bring up and tear down instances on demand.",5
"New equipment setup (week end 10/10/15)
* Planning on how to start setting up for new equipment    * Started to coordinate with Josh Hobblitt on what has been ordered and delivered   * Received 15 Dell R730 servers (plan to set up in temporary rack before Thursday’s maintenance)  * Installed spare rack at the east end of NCSA 3003-Row A. Added three 125V 30A drops to spare rack  * Resolved problems with Puppet on lsst-stor101  * Updated OS on lsst-stor101, lsst-stor142-144  * Installed ZFS on lsst-stor101, lsst-stor142-144  * Checking config of stor142-144 and stor101 for Thursday outage  * More ESXi testing with Mac Pro",6
"New equipment setup and regular maintenance (week end 10/17/15)  
* Unpack and mount 6 Dell R730’s  * Update the bios and firmware on 6 Dell R730’s  * Install ESXi 6.0 on 6 Dell R730’s  * Rearranged hardware in NCSA 3003, move systems around in racks, reconnect power and networking, troubleshoot startup issues   * Applied kernel and other software updates to infrastructure as it was moved  * Setup vSphere vCenter server and compute nodes  * Discovered need to order drive caddies for lsst10 and updated iDRAC Enterprise license for 15 new R730 servers",12
"New equipment setup and configuration (week end 10/24/15)  
* Unbox and mount six UPS in racks, mount two new power panel PDU's for 30 Amp service, connect power cabling to UPS and to each of the supported systems. TODO: Complete the setup on each of the servers.  * Unbox and mount in the rack two R730 servers, connect network and power  * Itemized list of new Dell servers received  * Debugging networking issues on new lsst-esxi1 server - confirmed it's not an issue with switch ports or cables  * Setup vSphere virtual networking and moved 4 test VMs to new setup  * Setup vSphere Data Protection (Vmware backups via snapshots)",6
"New equipment setup and configuration (week end 10/30/15)
* Moved spare rack to Row C  * Documented setup of new LSST vSphere setup (https://wiki.ncsa.illinois.edu/display/LSST/LSST+vSphere)  * Debugging networking issues on new lsst-esxi1 server - testing with new/alternate hardware  * Installed 6 new drives for historical log storage on lsst10 MySQL server",5
"bi-weekly IaM meeting
Meeting Oct 22nd  Notes on LSST confluence",1
"Bootcamp meeting
IAM group attended DM bootcamp",4
"write meeting agenda, for Oct 19 meeting 
nan",1
"Conduct, document, initial follow through on Oct JCC meeting 
nan",1
"Visit to Argonne - facility coordination
nan",2
"Specify L1 system - design and WBS plan for construction
Spent 3 days in focused planning to knock out version 1 of the L1 system plan.  SPs cover me, Don.",12
"Processing of DECAM COSMOS field - Part I
This story covers work on the verification plan done in October.     The DECAM Cosmos field was selected, however DECAM ISR is not available so the starting point for now is Community Pipeline reduced data. Have put the data through processCcdDecam and makeCoaddTempExp but had a number of failures that we have so far not been able to pin down. A list of user experience issues is being collated and [~frossie] will generate Summer 16 cycle stories to address those that fall within SQuaRE's defined scope of activities.     Story closed to fit within month, but work is ongoing. ",20
"Software Stack Introduction
Installation of LSST stack for initial steps towards further understanding of the software components of the DM architecture. ",1
"L1 Function and Design Mtgs
2 days of design discussions and refining functional diagrams of the Alert Productions and Image Ingest system.",4
"Vendor Discussions re: specification documents
Specification document sent. Discussions with vendors covering document and schedule.",1
"SC15 Scheduling and Prep
Vendor appt scheduling and conference workshop scheduling, logisitics, etc for SuperComputing 15 in Austin",1
"LSST IaM Project
nan",2
"investigate distributing automated lsst_apps builds via docker containers
nan",15
"Run and document multinode integration tests on Openstack+Docker
Boot openstack machines using vagrant, then deploys docker images and finally launch multinodes tests.    FYI, lack of DNS on OpenStack Cloud cause problems, but a vagrant plugin seems to solve this.",8
"Write up some introductory guides for Nebula usage
We write up some introductory guides for Nebula usage to enable new users to get started.  These are located on Confluence under :    https://confluence.lsstcorp.org/display/LDMDG/NCSA+Nebula+OpenStack+User+Guide",4
"want equiv of m1.xlarge flavor with smaller disk
I'd like to be able to build images with vcpus & ram from the {{m1.xlarge}} flavor that can be run on a {{m1.medium}} with it's smaller disk image, this would require a new flavor with 16GiB ram/8vcpus but only 40GiB of disk.  Something along the lines of:    {{openstack flavor create --ram 16384 --disk 40 --vcpus 8 ...}}    Is that possible?",2
"Vagrant for Nebula OpenStack
Create and document a Vagrant configuration to use [~jhoblitt] lsstsw machine images on NCSA's Nebula OpenStack cloud.",4
"Convert banner and menu to react/flux
Add flux data model to capture menu and banner information.  Convert banner and menu UI from gwt to react.  As part of this task, bring in Fetch API to simplify client/server interactions.",8
"update obs_lsstSim
obs_lsstSim has seen some bitrot.  In particular, the ingest task and the addition of the getExposureId methods on processImageTask have not been propagated to obs_lsstSim.  This ticket will deal with those issues.",4
"re-deploy lsstsw on Jenkins
Pandas was added to the bin/deploy script in lsstsw to support  sims development.  This has already been merged to master in 4b1d1a0fa.  The ticket is to ask that lsstsw be redeployed so the sims team can build branches that use pandas.",3
"Add unit testing into gradle build for Firefly's server-side code
Add a test task to Firefly's common build script.  This can be used by any sub-project to run unit test.  Added unit testing to Jenkins continuous integration job to ensure new code does not break unit testing.",1
"FY17 Implement Image Caching
When a file is requested, a cache maintained by the service is checked. If the file exists in the cache, it is returned. If the file does not exist, configurable rules are consulted to remove one or more files to make room for it in the cache, if necessary. (If no room is currently available because all cached files are being used, the request is blocked.) The file is then regenerated by invoking application pipeline code based on provenance and metadata information stored in the repository. The regenerated file is placed in the cache.    This is documented in [LDM-152 §4.1|http://ldm-152.readthedocs.org/en/master/#image-file-services-baseline].",60
"Please add HSC tests to CI
In DM-3663 we (= [~price]) provided an integration test for processing HSC data through the stack with the intention that it should be integrated with the CI system.    Having this test available and regularly run would be enormously helpful with the HSC port -- we've already run into problems which it could have helped us avoid (DM-4305).",1
"Try out nebula for stack developing and data processing
Follow instructions on:    https://confluence.lsstcorp.org/display/LDMDG/NCSA+Nebula+OpenStack+User+Guide  https://community.lsst.org/t/creating-a-nebula-instance-a-recipe/353    and create a nebula instance with the stack, update and install more packages in the stack, test by constructing a data repository and processing ISR with DECam raw images.    ",2
"Missing Doxygen documentation
As of [2015-11-10 02:53.26|https://lsst-web.ncsa.illinois.edu/doxygen/xlink_master_2015_11_10_02.53.27/] there were 19 ""mainpages in subpackages"" available through Doxygen.    In the next build, [2015-11-10 21:16.19|https://lsst-web.ncsa.illinois.edu/doxygen/xlink_master_2015_11_10_21.16.19/], most of them have vanished and we only provide links for {{ndarray}} and {{lsst::skymap}}.    As of filing this issue, they were still missing from the [latest build|https://lsst-web.ncsa.illinois.edu/doxygen/x_masterDoxyDoc/].    Please bring them back!",1
"Oct. on-going support to Camera team in UIUC
Attend UIUC weekly meeting and give support as needed. ",2
"Nov. on-going support to Camera team in UIUC
Attend UIUC weekly meeting and give support as needed. ",2
"Configure sshd with pam_krb5 and document
Configure sshd in our test IAM VM to use pam_krb5 so the user gets a Kerberos ticket when logging in as discussed in our design doc. Document the configuration steps.",4
"Configure DAX web app with Kerberos authentication in IAM test VM
Install the DAX server (https://github.com/lsst?query=dax) in our test IAM VM at NCSA and configure it to use Kerberos authentication using standard Python flask Kerberos support. Document the process.",17
"Set up test IAM MariaDB instance with Kerberos
Install MariaDB in our test IAM VM at NCSA and configure it for Kerberos password authentication. I understand that Kerberos ticket authentication is not yet supported by MariaDB. Document the process and include references to the ongoing work to add Kerberos ticket support.    Create example tables that demonstrate access control for different logged in users.",2
"Configure sssd with NCSA LDAP for accounts in test IAM VM
Configure [sssd|https://fedorahosted.org/sssd/] with NCSA LDAP for accounts in test IAM VM using instructions from Doug Fein.",5
"Configure httpd with SSL and mod_krb5 in IAM test VM
Get SSL certificate and configure httpd for mod_auth_krb5 authentication in IAM test VM. Document the setup.",4
"Replace fitsthumb in obs_subaru (port HSC-1196)
{{fitsthumb}} is now obsolete; all the functionality we need is available in {{afw}}. Further, we want to drop it as a dependency to make the job of integrating {{obs_subaru}} with CI easier.",1
"Bootstrap for new Sphinx/reST/RTD technical reports
Create a template repository for LSST (DM/SQuaRE) technical reports that are written in reStructuredText, built with Sphinx and published with RTD",2
"investigate distributing automated lsst_apps builds
(aggregation ticket for [previous existing]] tightly related tasks in progress related to binary distribution)",38
"Coadd_utils tests should run and skip if afwdata is missing
Currently, the {{coadd_utils}} tests are completely skipped at the scons layer if afwdata can not be located. This is bad for two reasons:  1. Are there any tests that can be run even if afwdata is missing?.  2. When we switch to a proper test harness (e.g. DM-3901) an important metric is the number of tests executed compared with the number of tests skipped.  Each test file (or even each test) should determine itself whether it should be skipped based on afwdata availability. This should not be a global switch.",1
"ActiveMQ Broker upgrade
Version 5.8.0 of the ActiveMQ broker, which is not part of the regular distribution, is quite out of date.  [~tjenness] is working upgrading the ActiveMQCPP library in DM-4330 to the current version, and this would be a good time to upgrade the broker to the latest release so we try and stay in sync.",1
"Sizing model fixes
* Improve API with LDM-144: need to separate spec of czar nodes from spec of worker nodes  * Revisit model for qserv czar nodes  * Fix chunk count for DiaObject (dbL2 F139)  * Change low volume query result size, 0.5 GB --> 1 MB (sciReq G271).  * Revisit model for shared scans and multiple releases  * Add model for recovery from node failure for Query Access cluster  * Consider modeling intermediate results (concluded that intermediate results are relatively small, and given they are distributed across the cluster, it is not worth modelling them)  * Fix issue with transfer time of nightly products from Archive to Base  ---  * Add Dia* to scans  * Consider modeling nodes for data loading  * LDM-139 and LDM-141 don't document where MOPS Scratch comes from or what it's supposed to hold.  * revisit bulk data transfer from HQ to Base (shipping disks --> over the network)",10
"The astrometry task should print a warning or throw an exception when there is no reference star in the field
When there is no reference star in the field the exception raised by anetAstrometry is identical to the one issued when the fit has not converged.    astrometry (matchOptimisticB) is throwing an exception with the message lsst::pex::exceptions::InvalidParameterError: 'posRefBegInd too big'.  I suggest a test to detect that there is not enough stars to fit the astrometry and to throw an exception accordingly.",1
"Global Metadata
Revisit design of the global metadata for LSST image and file archive. This includes understanding interactions between butler, local per repository metadata, global metadata, and metaserv. Sort out the dividing line between global metadata and dax (e.g., the schema https://github.com/lsst/dax_metaserv/tree/master/sql should live in cat/sql, not in dax)    Deliverable: document describing the architecture of the global metadata, including how pipelines, users and SUI will interact with it. It should cover both writing/ingest and reading aspects.",60
"S17 Implement Image and File Archive
Implement fully working system capable of managing images and files. It will be used by the Alert Production system that we will be putting together in FY17",90
"F17 Integrate Image and File Archive with Alert Production
nan",60
"FY18 Integrate Image and File Archive with DRP
nan",79
"S17 Butler
nan",100
"Disentangle log messages from different processes
It's difficult to disentangle interleaved log messages when running with multiprocessing.  Two possible ways to do this would be:  1. Have each process write to a different log file.  Maybe we could allow the user to specify log filenames including {{%(pid)s}} and {{%(hostname)s}}.  More useful would be to allow the full range of keys from a {{dataId}}, but that might require some changes in how processing runs.  2. Prepend each line of the log with context information ({{dataId}}, pid, hostname), allowing the user to use {{grep}} to do the disentangling.  To avoid overwhelming the log with the context information, a hash of the context information could be used instead, with the lookups published in the log before the processing starts.        h5. Brief summary on the changes:  - Chain logs of sub-tasks to their parent task logs  - Add {{PrependedFormatter}} as the new default formatter for log files (specified through {{--logdest}} command line argument for {{cmdLineTask}}). The standard output to the terminal remains the same as before.  - Any string can be used to label a pex_logging Log, and with {{PrependedFormatter}} this label prepends each log message. For {{cmdLineTask}}, the dataId is prepended.  - The HSC's commit on prepending the timestamps is also ported to {{PrependedFormatter}}.  - An example log message:  {code:java}  2016-03-08T22:29:56.889933: [{'filter': 'r', 'tract': 0, 'patch': '2,3'}]: mergeCoaddDetections: Culled 1731 of 7751 peaks  {code}  ",12
"Add args to s3s3bucket CLI
Add {{source-bucket}} and {{dest-bucket}} arguments to {{s3s3bucket}} the command line script. This is to allow for one off duplication of buckets.    Increment version to 0.1.10.",2
"tested upgraded activemqcpp package
The activemqcpp package was upgraded as part of DM-4330, and I tested it to be sure the upgrade was backwards compatible with the code that exercises it in ctrl_events.   It is.",1
"dax_imgserv 2015_10.0 build error
{{2015_10.0}} has a build error under a current {{lsstsw/bin/deploy}} environment.  Current speculation is that this is related to the conda version of numpy being upgraded to {{1.10.1}}.",1
"Replace XML-RPC with in-process communication
With all recent development in CSS sector we should be able now to get rid of Python in czar entirely. This is also a good opportunity to move from XML-RPC between proxy and czar with direct in-process ""communication"". Daniel said it's a good idea :)  ",10
"Fix publishing script async issue and add additional release notes.
Async command execution causes unpredictable and unreliable results.  Switches to synchronous where possible.  Also add additional description to the release notes.",2
"Write technote on the new technical note platform
Write a technote about the platform that github.com/lsst-sqre/lsst-technote-bootstrap lets DM members publish in. Discuss current status and outline future plans.",6
"Design Mtg, Review and discussions of L1 processing
Design/planning meeting for L1 system. Materials read and discourse discussions on EFD, MOPs, T&S docs and calibration production",3
"Reviewing quotes, power requirements, rack layouts
Review of multiple quotes from multiple vendors across full year of purchases. Derived power requirements and rack layouts for placement, networking, electrical work discussions to begin.",3
"SC15 Scheduling, Processor Futures Refresh
Scheduling of several more meeting opportunities for next week. Also, time spent reviewing NDA materials on processor futures. ",1
"S19 Run Large Scale Tests
nan",26
"FY19 Implement Qserv Software Upgrading Tools
nan",53
"FY19 Finalize Internal DRP Database
Final changes to make DRP database commissioning ready.",79
"obs_subaru fails to compile after DM-3200
Due to atypical calls in {{obs_subaru}}'s {{hsc/SConscript}} to run scripts in the {{bin}} directory, {{obs_subaru}} fails to compile after the changes made in DM-3200.",1
"SuperTask phase 1 implementation
This story represents the implementation of the first part of the SuperTask framework design,",8
"Implement configuration for activator parsing
Need to add configuration to the CmdLineActivator in new Workflow tasks",2
"First implementation demo
First stage demo of Super Task and WorkFlowTask Framework",4
"write documentation for registry free repo
nan",1
"Improve overscan correction for DECam raw data
Currently, the default overscan correction from IsrTask is used for processing DECam raw data. Overscan subtraction is done one amplifier at a time.     However, a bias jump occurs due to the simultaneous readout of the smaller ancillary CCDs on DECam, some images show discontinuity in the y direction across one amplifier, as in the example screen shot.     This ticket is to improve overscan correction for DECam data so to mitigate this discontinuity in the ISR processing.    Arrangement of CCDs on DECam: http://www.ctio.noao.edu/noao/sites/default/files/DECam/DECamPixelOrientation.png      h3. More details:  There are 6 backplanes in the readout system, shown by the colors in DECamPixelOrientation.png. In raw data files, the CCD's backplane is noted in the header keyword ""FPA"".  Examination of some images suggests that science CCDs on orange and yellow backplanes show bias jump at 2098 pixels from the y readout. That is the y size of the focus CCDs.     h3. Actions:  For CCDs on the affected backplanes, divide the array into two pieces at the jump location, and do overscan correction on the upper and lower pieces separately.    ",5
"Fix bug and add unit tests for PsfShapeletApprox 
We discovered during this Sprint that this plugin was giving us faulty values for all the models except for SingleGaussian.  I will fix that bug on this issue.    Obviously, a better unit test would have caught this.  I am adding a DoubleGaussian unit test, plus a test that the default models provide different results.  Also a timing test for all the models, as we do not really have enough information about the performance of the shapelet approximation.  ",3
"Duration for various ShapeletPsfApprox Models
This is just a report of the amount of time it takes to run ShapeletPsfApprox and CModel over 10000 galaxies from GalSim",2
"Migrate lsst/ci_hsc repo to git-lfs.lsst.codes
The github lfs backed repo https://github.com/lsst/ci_hsc needs to be migrated to git-lfs.lsst.codes.  A sanity check for any other ""live"" lfs repos under the lsst github org might also be a good idea.",4
"Migrate testdata for DECam from disk to git-lfs
There is a package full with test data for the obs_decam.  It is an eups package currently, but not a git repository.  I would like to migrate that into our hosted git-lfs so the obs_decam package can be built by Jenkins.  [~jmatt] I'm hoping you would be willing to handle this for me.  If not I can find somebody else.  Thanks!",1
"HSC backport: Add tract conveniences
This is a port of [HSC-715: Add tract conveniences|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-715].  Here is the original description:    {panel}  In regular HSC survey processing, we'll run with a ""rings"" skymap to cover the entire survey area. meas_mosaic does not currently efficiently or conveniently iterate over tracts. For example:  {code}  mosaic.py /tigress/HSC/HSC --rerun price/cosmos --id field=SSP_UDEEP_COSMOS filter=HSC-R  {code}  Note the lack of a tract in the --id specifier — we want to iterate over all tracts. This is not currently possible. Instead, if we do not know the tract of interest (which the user should not be required to know), we have to iterate over all the tracts (e.g., tract=0..12345), but the user should not be required to know the number of tracts, and this is slow (and possibly memory-hungry: currently consuming 11GB on tiger3 just for 12 exposures).  We need an efficient mechanism to iterate over all tracts by not specifying any tract on the command-line.  {panel}    As this functionality was added specifically for {{meas_mosaic}}, it was going to be ported as part of DM-2674.  Due to a recent desire to use this functionality, this ticket will be ported here.",1
"A slimmer testdata_decam
Before this ticket, the files in {{testdata_decam}} are as they are downloaded from the archive.  Some are MEF, and the total is a bit big (1.2G).    I made a trimmed down version of {{testdata_decam}}, 109M and available here:  [https://github.com/hchiang2/testdata_decam.git]  I trimmed it down by only saving the primary HDU and one data HDU.  However the unit tests in {{getRaw.py}} become less meaningful and I am not sure if we really want to do this, because some complexities of DECam files are about the MEF. {{getRaw.py}} tests Butler retrieval of multiple dataset types, in particular tests if the correct HDU is retrieved.     Nonetheless, {{getRaw.py}} can pass  (with branch u/hfc/DM-4375 of obs_decam)    Note: the old testdata_decam still live on lsst-dev:/lsst8/testdata_decam/",2
"Learn and setup nebula as a development machine
Personally establish Nebula as a stack development platform.",2
"Review VAO/IVOA protocols for use in LSST IAM
Following the good principals of re-using prior work, review VAO/IVOA protocols for use in LSST IAM, in particular the [IVOA Credential Delegation Protocol|http://www.ivoa.net/documents/latest/CredentialDelegation.html] and include a summary in our LSST IAM Design Doc.",1
"X16 Revisit Public Interfaces / ADQL
We need to revisit the interfaces we will be exposing to public, wiht particular focus on ADQL vs SQL92 (mysql flavor?).    Deliverable: recommendation which public interfaces should be exposed to users from the Data Access Services, with particular focus on ADQL vs mysql-flavor.",11
"""SHUTOFF"" nebula instances consume core/ramIt  quota
It appears that halted/shutoff instances have no effect on resource quota usage.  Eg:    {code:java}  $ openstack server list  +--------------------------------------+-----------------------+-------------------+----------------------------------------+  | ID                                   | Name                  | Status            | Networks                               |  +--------------------------------------+-----------------------+-------------------+----------------------------------------+  ...  | 1956c6d0-8aec-4f42-a781-8a68fd10179d | el7-jhoblitt          | SHUTOFF           | LSST-net=172.16.1.171, 141.142.208.150 |  ...  $ nova absolute-limits  +--------------------+--------+--------+  | Name               | Used   | Max    |  +--------------------+--------+--------+  | Cores              | 141    | 150    |  | FloatingIps        | 0      | 10     |  | ImageMeta          | -      | 128    |  | Instances          | 44     | 100    |  | Keypairs           | -      | 100    |  | Personality        | -      | 5      |  | Personality Size   | -      | 10240  |  | RAM                | 342016 | 400000 |  | SecurityGroupRules | -      | 20     |  | SecurityGroups     | 1      | 10     |  | Server Meta        | -      | 128    |  | ServerGroupMembers | -      | 10     |  | ServerGroups       | 0      | 10     |  +--------------------+--------+--------+  $ openstack server delete 1956c6d0-8aec-4f42-a781-8a68fd10179d  $ nova absolute-limits  +--------------------+--------+--------+  | Name               | Used   | Max    |  +--------------------+--------+--------+  | Cores              | 133    | 150    |  | FloatingIps        | 0      | 10     |  | ImageMeta          | -      | 128    |  | Instances          | 43     | 100    |  | Keypairs           | -      | 100    |  | Personality        | -      | 5      |  | Personality Size   | -      | 10240  |  | RAM                | 325632 | 400000 |  | SecurityGroupRules | -      | 20     |  | SecurityGroups     | 1      | 10     |  | Server Meta        | -      | 128    |  | ServerGroupMembers | -      | 10     |  | ServerGroups       | 0      | 10     |  +--------------------+--------+--------+    {code}  ",2
"Port registryInfo.py from obs_subaru into Butler
in butler (probably butlerUtils), users would like the ability to dump info from a repository's sqlite registry to text (console output). This is already implemented in {{obs_subaru/.../registryInfo.py}}, and basically just needs to be ported to butler in a sensible place. There are a few cases that assume certain columns are present and we need either to make the script more generic, or [~rhl] suggests that maybe we need to standardize the registries.",8
"Avoid restarting czar when empty chunk list changes
Currently czar caches empty chunk list after it reads the list from file. This complicates things when we need to update the list, integration test for example has to restart czar process after it loads new data to make sure that czar updates its cached list. Would be nice to have simpler mechanism to resetting cached list in czar without restarting it completely. It could be done via special query (abusing FLUSH for example) or via sending signal (problematic if czar runs remotely).    This can be potentially useful even after we replace empty chunk list file with some other mechanism as I expect that cache will stay around even for that.",2
"Clean up ProcessCcdDecam
ProcessCcdDecam needs some cleanup:  * {{run}} method simply delegates to the base class  * {{propagateCalibFlags}} is a no-op (deliberately in {{cab69086}}, need to explore if the original problem still exists)  * The config overrides (in config/processCcdDecam.py):  ** Uses the catalog star selector, which isn't wise given the current heterogeneity of reference catalogs.  ** Sets the background {{undersampleStype}} to {{REDUCE_INTERP_ORDER}}, which is the default.",1
"Skymap fails tests on testFindTractPatchList
When skymap is built and healpy is loaded, {{testFindTractPatchList}} fails with:    {quote}======================================================================  FAIL: Test findTractPatchList  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""/Users/ctslater/lsstsw/build/skymap/tests/SkyMapTestCase.py"", line 245, in testFindTractPatchList      self.assertClosestTractPatchList(skyMap, [tractInfo.getCtrCoord()], tractId)    File ""/Users/ctslater/lsstsw/build/skymap/tests/SkyMapTestCase.py"", line 284, in assertClosestTractPatchList      tractPatchList = skyMap.findClosestTractPatchList(coordList)    File ""/Users/ctslater/lsstsw/build/skymap/python/lsst/skymap/baseSkyMap.py"", line 146, in findClosestTractPatchList      tractInfo = self.findTract(coord)    File ""/Users/ctslater/lsstsw/build/skymap/python/lsst/skymap/healpixSkyMap.py"", line 97, in findTract      index = healpy.ang2pix(self._nside, theta, phi, nest=self.config.nest)    File ""/Users/ctslater/lsstsw/stack/DarwinX86/healpy/1.8.1+12/lib/python/healpy-1.8.1-py2.7-macosx-10.5-x86_64.egg/healpy/pixelfunc.py"", line 367, in ang2pix      check_theta_valid(theta)    File ""/Users/ctslater/lsstsw/stack/DarwinX86/healpy/1.8.1+12/lib/python/healpy-1.8.1-py2.7-macosx-10.5-x86_64.egg/healpy/pixelfunc.py"", line 110, in check_theta_valid      assert (np.asarray(theta) >= 0).all() & (np.asarray(theta) <= np.pi + 1e-5).all(), ""Theta is defined between 0 and pi""  AssertionError: Theta is defined between 0 and pi{quote}    This was missed during regular CI testing since healpy is not normally setup. ",1
"Update testCoadds.py to accommodate changes in DM-2915
As of DM-2915, the config setting:  {code}self.measurement.plugins['base_PixelFlags'].masksFpAnywhere = ['CLIPPED']{code}  is set as a default for {{MeasureMergedCoaddSourcesTask}}.  However, this *CLIPPED* mask plane only exists if a given coadd was created using the newly implemented {{SafeClipAssembleCoaddTask}}.  If a coadd was built using {{AssembleCoaddTask}}, the *CLIPPED* mask plane is not present, so the above default must be overridden to exclude it when using {{MeasureMergedCoaddSourcesTask}}.  This is the case for the mock coadd that is assembled in the unittest code in {{testCoadds.py}}, so the config needs to be set for the test to run properly.    Note that the associated tests for {{SafeClipAssembleCoaddTask}} will be added as part of DM-4209.",1
"Get analysis script working for HSC/LSST stack comparisons
A script for performing pipeline output QA is under development for HSC.  The script provides many useful tools for plotting and analyzing pipeline outputs on single visits and coadds.  This is of general use for LSST and, in particular, will be adapted/expanded to include tools for the direct comparisons of identical data sets processed with both the HSC and LSST pipelines (i.e. DM-2984).  Appropriate adaptations for this script to run with the LSST stack will be made here (with the understanding that development is still ongoing on HSC and further adaptations will be accommodated as necessary/desired).    See [HSC-1320|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1320] and [HSC-1359|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1359] for details and examples of the output of this script.",6
"Update cmsd configuration for multi-node tests
A particular cmsd configuration parameter prefixes a hardcoded path for QueryResource, which needs to be removed. This seems to appear only during multi-node tests.",1
"ctrl_execute test fails to find test binary
There's a test in ctrl_execute that exercises the bin/dagIdInfo.py test program.   Since the rewrite_shebang rewrites happen after the tests are executed, the test that looks for the bin/dagIdInfo.py binary fails, since it's not there before the tests execute.",1
"Scale CommandLineTask multiprocessing timeout with workload
The default timeout value for aborting a multiprocessing run in CommandLineTask is too short. Currently if no time length is supplied by the user, the default value gets set to 9999s. However if a processing task is quite large it is possible for the processing pool to take much longer to arrive at the result. Currently if the processing pool does not complete it's run within that time limit, python multiprocessing will throw a timeout error. The timeout value should be scaled such that the supplied value is assumed to be the timeout length for one processing task, and should be scaled by the number of tasks divided by the number of cpus available. The command line task documentation should be updated to reflect this change.",2
"Fix regexp for gcc48
DM-2622 inttoduced some regexes which raise exceptions when built with gcc48 (e.g. on centos7). gcc48 support for regexes is generally broken, so it's better to replace that with boost regexes.",1
"ctrl_execute test fails under El Capitan
The test/testDagIdInfo.py because it runs a script from bin.src, rather than bin.   This test needs to be rewritten.",1
"SuperTask demo on other older tasks
The exampleCmdLine task worked fine, need to show demo for other tasks from pipe_tasks",4
"W16 Qserv Refactoring #2
Additional refactoring of Qserv as found necessary in W16",70
"Experiment with light-weight SQL databases for secondary index
Evaluate the use of light-weight SQL, such as InnoDB, TokuDB (now Kyoto Cabinet), or RocksDB to create and manage the secondary index.",8
"Review of [DM-2983] part 2
Second part and final of reviewing DM-2983",2
"Debug Qserv on ccqserv125..ccqserv149
It seems that some chunkQuery doesn't return on long queries like ""Select count(*) From Object""  The query stall and print in czar log:  {code}  2015-11-24T15:07:55.324Z [0x7f06b37fe700] INFO  root (core/modules/qdisp/Executive.cc:395) - Still 9 in flight.  {code}  If we look in the logs with next commands:  {code:bash}  cat  /qserv/run/var/log/qserv-czar.log | egrep  ""Executive::add\(job\(id="" | grep LSST | cut -d'=' -f2 | cut -d' ' -f1 | sort > JOBID_ADD.txt  cat  /qserv/run/var/log/qserv-czar.log | egrep  ""markCompleted""  | cut -d'(' -f 3 | cut -d',' -f1 | sort > JOBID_DONE.txt  {code}  And then  {code}  qserv@ccqserv125:/qserv$ diff JOBID_ADD.txt JOBID_DONE.txt   1826d1825  < 2639  2217d2215  < 2991  2904d2901  < 3609  3088d3084  < 3775  3152d3147  < 3832  3433d3427  < 4085  4227d4220  < 480  5478d5470  < 5926  6937d6928  < 7239  {code}  9 jobs aren't completed on czar.  If we look the the chunk_id of one of this job:  {code}  /opt/shmux/bin/shmux -c ""locate 5299"" ccqserv{125..149}.in2p3.fr  ...  ccqserv148.in2p3.fr: /qserv/data/mysql/LSST/Object_5299.MYD  ccqserv148.in2p3.fr: /qserv/data/mysql/LSST/Object_5299.MYI  ccqserv148.in2p3.fr: /qserv/data/mysql/LSST/Object_5299.frm  ...  {code}  Data is on worker node and onccqserv148 xrootd log:  {code}  [2015-11-24T15:07:42.990Z] [0x7ffbc8244700] INFO  root (core/modules/xrdsvc/SsiSession.cc:125) - Enqueued TaskMsg for Resource(/chk/LSST/5299) in 0.000465  {code}  But a thread seems to be locked:  {code}  #gdb on ccqserv148 xrootd process:  (gdb) thread 7  [Switching to thread 7 (Thread 0x7ffbad7fa700 (LWP 192))]  #0  pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185  185     ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S: No such file or directory.  (gdb) bt  #0  pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185  #1  0x00007ffbd4c45c7c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6  #2  0x00007ffbccbe8db9 in std::condition_variable::wait<lsst::qserv::wsched::BlendScheduler::getCmd(bool)::<lambda()> >(std::unique_lock<std::mutex> &, lsst::qserv::wsched::BlendScheduler::<lambda()>) (      this=0xfba620, __lock=..., __p=...) at /usr/include/c++/4.9/condition_variable:98  #3  0x00007ffbccbe88f9 in lsst::qserv::wsched::BlendScheduler::getCmd (this=0xfba5c0, wait=true) at core/modules/wsched/BlendScheduler.cc:156  #4  0x00007ffbccba3b07 in lsst::qserv::util::EventThread::handleCmds (this=0xfbe890) at core/modules/util/EventThread.cc:45  #5  0x00007ffbccbab137 in std::_Mem_fn<void (lsst::qserv::util::EventThread::*)()>::operator()<, void>(lsst::qserv::util::EventThread*) const (this=0xfbe900, __object=0xfbe890)      at /usr/include/c++/4.9/functional:569  #6  0x00007ffbccbaafff in std::_Bind_simple<std::_Mem_fn<void (lsst::qserv::util::EventThread::*)()> (lsst::qserv::util::EventThread*)>::_M_invoke<0ul>(std::_Index_tuple<0ul>) (this=0xfbe8f8)      at /usr/include/c++/4.9/functional:1700  #7  0x00007ffbccbaae45 in std::_Bind_simple<std::_Mem_fn<void (lsst::qserv::util::EventThread::*)()> (lsst::qserv::util::EventThread*)>::operator()() (this=0xfbe8f8) at /usr/include/c++/4.9/functional:1688  #8  0x00007ffbccbaacfa in std::thread::_Impl<std::_Bind_simple<std::_Mem_fn<void (lsst::qserv::util::EventThread::*)()> (lsst::qserv::util::EventThread*)> >::_M_run() (this=0xfbe8e0)      at /usr/include/c++/4.9/thread:115  #9  0x00007ffbd4c49970 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6  #10 0x00007ffbd50ae0a4 in start_thread (arg=0x7ffbad7fa700) at pthread_create.c:309  #11 0x00007ffbd43b904d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:111  {code}    Something seems to prevent return of chunk query result...          ",5
"HSC backport: fix memory leak in afw:geom:polygon
This is a backport of a bug fix that got included as part of [HSC-1311|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1311].  It is not related to that issue in particular, so is being ported here as an isolated bug fix.    {panel}  Original commit message:  pprice@tiger-sumire:/tigress/pprice/hsc-1311/afw (tickets/HSC-1311=) $ git sub  commit 55ad42d37fd1346f8ebc11e4077366dff4eaa87b  Author: Paul Price <price@astro.princeton.edu>  Date:   Wed Oct 21 10:59:56 2015 -0400         imageLib: import polygonLib to prevent memory leak             When doing ""exposure.getInfo().getValidPolygon()"", was getting:             swig/python detected a memory leak of type 'boost::shared_ptr< lsst::afw::geom::polygon::Polygon > *', no destructor found.             This was due to the polygonLib not being imported in imageLib.      Using polygonLib in imageLib then requires adding polygon.h to all      the swig interface files that use imageLib.i.      examples/testSpatialCellLib.i              | 1 +   python/lsst/afw/cameraGeom/cameraGeomLib.i | 1 +   python/lsst/afw/detection/detectionLib.i   | 1 +   python/lsst/afw/display/displayLib.i       | 1 +   python/lsst/afw/geom/polygon/Polygon.i     | 1 +   python/lsst/afw/image/imageLib.i           | 2 ++   python/lsst/afw/math/detail/detailLib.i    | 1 +   python/lsst/afw/math/mathLib.i             | 1 +   8 files changed, 9 insertions(+)  {panel}",1
"Port detection task footprint growth changes from HSC
In hsc the default behavior for the detection task is to updated footprints with a footprint which has been grown by the psf. This behavior needs to be ported to LSST, as some source records have footprints which are too small. When making this change, the new default needs to be overridden for the calibrateTask, as it needs the original size.    The port includes 8e9fb159a3227f848e0db1ecacf7819599f1c03b from meas_algorithms and 8bf0f4a44c924259d9eefbd109aadec7d839e0f2 from pipe_tasks",5
"Write a DM Collaborative Workflow document
Document our procedures for collaborative development with JIRA, Git and GitHub for the new docs.",11
"Add git-lfs to packer-newinstall
git-lfs is not available in our deliverables. Artifacts (binaries) such as VM images and docker data containers should provide a stable version of git-lfs.",2
"Meetings - Nov 2015
Verification dataset meetings, Illinois DES meeting, single-frame processing discussions, supertask meeting, OpenStack User meeting",2
"Other LOE - Nov 2015
Local LSST group meetings, NCSA postdoc meetings, NCSA Physics & Astronomy theme meeting, or other local meetings, events, or tasks to comply with NCSA policies",5
"Crash course on using git-lfs
Learn to install and use git-lfs; help testing with migrating {{testdata_decam}} to git-lfs; verify tests pass with the new repository (DM-4370).",1
"Learn about the task design in ISR processing
Learn the concept behind the previous API changes (RFC-26) in the tasks of ISR processing, and data storage/retrieval involved. ",3
"Explore basic middleware and orchestration tools
Use {{runOrca}} to launch jobs through {{lsst-dev}} and do some single frame processing with it. Also learn a little about process execution. ",3
"Avoid bash usage in batch submission
{{ctrl_pool}} currently creates bash submission scripts with an explicit {{/bin/bash}} bang line.  [~rhl] [argues| https://github.com/lsst/ctrl_pool/commit/047f0de5a682ad9e9a6f65ccc7cc296e0a0d4ee7#commitcomment-14573759] on the review of DM-2983 that we should using posix shell constructions instead.",1
"faulty assumption about order dependency in ctrl_event unit tests
A recent change to daf_base uncovered a couple of faulty tests in ctrl_events that incorrectly assumes the order in which assumed the order in which data in a PropertySet would be received.   We can't assume which order these values will be put into the property set, and therefore into the list retrieved from the Event object.",1
"Image preparation time at server side measurement 
Setup the mechanism to measure the time needed to prepare the image (generate the image in PNG or other suitable format) for client display. ",20
"Image preparation time at server side measurement
Measure the time needed to prepare the image on server side for client display.   Reach the goal of less than one second.",20
"Image preparation time at server side measurement
Measure the time needed to prepare the image on server side for client display.   Reach the goal of less than half second.",20
"setup the measurement for Image rendering time to display at web UI
Setup the mechanism to measure the time needed to render the image after the data received by the client for display.  ",30
"measure Image rendering time to display at web UI
Measure the image rendering time, reach the goal of 1 second.",30
"measure Image rendering time to display at web UI
Measure the image rendering time, reach the goal of 0.5 second.  ",30
"Remove Task.display()
As of DM-927 (included in release 9.2, end of S14), {{lsst.pipe.base.task.Task.display()}} was marked as deprecated. It should now be removed.",3
"Revisit mysql connections from czar
Need to revisit connections we maintain from czar to mysql. This include revisiting whether we need both sql/SqlConnection and mysql/MySqlConnection classes. (In some cases, like in InFileMerger we have instances of both, which gets very confusing.)",12
"setup mechanism to measure the query response time 
Setup the method to measure the response time after query has been submitted from client.   1. query sent to the data provider from client  2. result returns from data provider  3. result displayed in the client    We can measure   1. the time from query submission to been sent to data provider  2. the time from data returned from provider to been displayed in the client",30
"measure the query response time 
Measure the time from query returned from data provider to been displayed in the client.",30
" measure the query response time 
Measure the time from query returned from data provider to been displayed in the client.",30
"Setup the load test for Web UI portal 
Setup the load test system to measure the performance of web UI portal",50
"load test of Web UI portal: support 100 concurrent users
Run the load test system with 100 concurrent users. Measure the performance against other KPM epics in the same cycle. ",50
"load test of Web UI portal: support 100 concurrent users
Run the load test system with 100 concurrent users. Measure the performance against other KPM epics in the same cycle.  ",50
"Improve docker storage backend on RedHat-like distributions
Solve startup log message RedHat-like distro: ""level=warning msg=""Usage of loopback devices is strongly discouraged for production""?    This is due to use of DeviceMapper (default package option on RedHAt-like distros) without a dedicated hard-disk, use of ""overlay"" backed storage seems better.  ",4
"Replace sed with stronger template engine in docker scripts
Dockerfile are generated using templates and sed, this should be strengthened.",2
"Remove useless xrootd client parameters
This extract of etc/sysconfig/docker:    {code:bash}  # used by qserv-czar  export QSW_RESULTDIR=${QSERV_RUN_DIR}/qserv-run/tmp  # Disabling buffering in python in order to enable ""real-time"" logging.  export PYTHONUNBUFFERED=1    export XRD_LOGLEVEL=Debug  export XRDSSIDEBUG=1    # Increase timeouts, without that, long-running queries fail.  # The proper fix is coming, see DM-3433  export XRD_REQUESTTIMEOUT=64000  export XRD_STREAMTIMEOUT=64000  export XRD_DATASERVERTTL=64000  export XRD_TIMEOUTRESOLUTION=64000  {code}    Has to be moved to:    {code:bash}   # used by qserv-czar  export QSW_RESULTDIR=${QSERV_RUN_DIR}/qserv-run/tmp  export XRD_LOGLEVEL=Debug  export XRDSSIDEBUG=1  # Disabling buffering in python in order to enable ""real-time"" logging.  export PYTHONUNBUFFERED=1  {code}    And then tested in multi-node, and on in2p3 cluster.",1
"Remove QSW_RESULTPATH and XROOTD_RUN_DIR if useless
These parameters may be useless (see DM-4395). If yes they can be removed to simplify configuration procedure.",2
"IAM process for granting data access rights
Document a process for granting of data access rights to LSST users according to the Data Access White Paper ([Doc-53733|http://ls.st/Document-5373]).    On the wiki: https://confluence.lsstcorp.org/display/LAAIM/Granting+Data+Access+Rights",1
"Please document the --rerun option
DM-3371 adds the {{--rerun}} option to command line tasks. The help for this option reads:  {quote}  rerun name: sets OUTPUT to ROOT/rerun/OUTPUT; optionally sets ROOT to ROOT/rerun/INPUT  {quote}  While essentially correct, that's not particularly helpful in understanding what's actually going on here. A motivation and description of this functionality is available in RFC-95: please ensure that, or some variation of it, is included in the stack documentation.",1
"ISR and calibration of a tiny set of DECam raw data
Learn more about the beginning steps of single frame processing by processing a small subset of DECam Stripe 82 raw data (~10 visits) and performing instrument signature removal with features currently implemented.",6
"IaM work in November
Work done in support of LSST's IaM efforts.",1
"Management for November
Centered around DMLT meeting, design process and hiring, in addition to general steering of activities at NCSA ",6
"November TOWG/opeartions design  work
Towg attendance/ note + participating in Beth's group.       Detailed  WBS for DM,  Condensed  WBS  to show high level,  Effort estimates, point out problematic thinking in the estimates. ",8
"JCC
Two day JCC activity at NCSA, including extended JCC meeting with HEP centers likely to   host people exploiting LSST Data.      writeup of extended meeting is here: https://confluence.lsstcorp.org/display/JCC/Extended+JCC+meeting+--+2015-11-23   in the JCC section.     organize, coordinate, and write up meeting notes. ",5
"Design refinement for the L1 system
Further specification of L1 design,     Long list of Questions for group but handled by GDF, began procession replies.    Understood Chilean Buffer in L1 system  could be eliminated,  posed question about systems engineering process needed to support this.     Genera work casing further developing the design into WBS -- which is not 30+ lines of detail  Began L1 con ops ,to guide group    Learned of some (possible undocumented) ""fifth device"".",11
"Data Distrib proto (dec)
nan",15
"S17 Implement Async Queries in Qserv
* Design and implement *basic* system for determining whether particular query is synchronous or asynchronous. The complete version will come through DM-1490. Note that this work is related to shared scans (e.g., we need to know what scans we have running)  * Design SQL API for starting and interacting with async queries.  * Modify Qserv to support async queries (starting, getting status, retrieving results)    Note, async queries are indirectly related to authentication (users should not see each other' async queries).    Deliverable: Qserv that accepts and executes queries asynchronously, and allows users to retrieve results.",50
"HTML5 Sphinx theme for technotes
Build a Sphinx theme for the Technote platform. Treat this work as exploratory, proof of concept work for customizing the HTML, CSS and JS of the software docs.    Objectives are    1. Create a Sphinx theme repo  2. Show how gulp can be used to help develop web assets for the theme  3. Establish a pattern for table contents columns that scroll independently of the main article  4. Show how we can implement a HTML5 rst builder in documenteer to fix permalink issues and build true HTML5 output.",2
"Finish documentation and comments on SuperTask 
Need to finish documentation, implementation and comments on the code",4
"Fix multiple patch catalog sorting for forcedPhotCcd.py
{{forcedPhotCcd.py}} is currently broken due to the requirement of the {{lsst.afw.table.getChildren()}} function that the *SourceCatalog* is sorted by the parent key (i.e. {{lsst.afw.table.SourceTable.getParentKey()}}).  This occurs naturally in the case of *SourceCatalogs* produced by the detection and deblending tasks, but it may not be true when concatenating multiple such catalogs.  This is indeed the case for {{forcedPhotCcd.py}} as a given CCD can be overlapped by multiple patches, thus requiring a concatenation of the reference catalogs of all overlapping patches.    There two places in the running of {{forcedPhotCcd.py}} where calls to {{getChildren()}} can cause a failure: one in the {{subset()}} function in {{references.py}}, and the other in the {{run}} function of *SingleFrameMeasurementTask* in {{sfm.py}}.",2
"Understand how the proposed interfaces fit with Qserv code
Understand how the interfaces proposed by [~abh] in DM-3755 fit with the existing qserv code.",3
"Re-locate LSST PS server and configure to reside on new layer2 circuits
nan",2
"Investigate MemSQL
Take a look at the MemSQL distributed database.",8
"Week end 11/07/15
Support for lsst-dev cluster, OpenStack, and accounts  for week ending November 7, 2015.",2
"Week end 11/14/15
Support for lsst-dev cluster, OpenStack, and accounts  for week ending November 14, 2015.",4
"Week end 11/21/15
Support for lsst-dev cluster, OpenStack, and accounts  for week ending November 21, 2015.",2
"New equipment setup and configuration (week end 11/07/15)
* Three Dell R730's:  ** Mount in A row racks  ** Complete Bios updates  * Moved ~25 VMs over to new lsst-vsphere infrastructure  * Setup lsst-condor\[1-6\] VMs  * Setup lsst-esxmac1 with networking  * Fixed networking issues on new lsst-esx1 server (an undocumented host was squatting on the IP address)",5
"New equipment setup and configuration (week end 11/14/15)
* Received Dell iDrac license upgrades for new Dell R730 servers  * Received and configured VMware vSphere licenses from CDW-G & AURA  * Converted three physical systems to VM's:  ** lsst-nagios  *** Problems with software mirror raids.  **** VMware converter does not see software raided drives.  **** Split the raid 1 into discrete drives.  **** Chose sda to modify - failed as sad was faulty.  *** Built new Centos 6.6 VM  **** Used CrashPlan to rebuild.  ** lsst7 - converted after learning how to convert system to a fixed IP subnet  ** lsst8 - converted  *** Debugged lsst8 system migration to vmware.  Partition table was invalid and was preventing move  * Finished moving all VMs to new lsst-vsphere infrastructure",6
"New equipment setup, configuration, and regular monthly maintenance (week end 11/21/15)
* Virtualized physical system lsst-xfer  * Worked with bmather or dell Dirac licensing issue  * Cleaned up old and new hosts in NCSA DNS, Nagios monitoring, and Qualys vulnerability scanner  * Completed connections for six UPS",2
"New equipment setup and configuration (week end 11/28/15)
* Obtained Dell iDRAC Enterprise licenses & upgraded 4 of the 13 servers  * Installed base VM templates for OS X 10.8-10.11",1
"Decommissioning old equipment (week end 11/14/15)
* Shutdown 17 (all) old ESXi servers  * Shutdown 3 old condor servers",1
"Decommissioning old equipment (week end 11/21/15)
* Shutdown last 3 old condor servers  * Shutdown lsst-netem, lsst-ps, & lsst-ps-base servers  * Surplussed equipment:  ** NCSA servers ( 5 Dell 1950's, 2 Dell 2950) repurposed from A22 to C17  ** Moved blade chassis to C20  * Move lsst-test systems in A23",2
"Write DM Git LFS Guide
Refactor words from DM-4412 into a top-level Doc page for using Git LFS. This will leave DM-4412 more as a Collaboration Workflow document.",1
"Consulting in November
Review of design documents, correspondence with the design team regarding Data Center details and floor space, and conferences via web links.",4
"Shared scan implementation
Fine tune the API proposed in DM-3755 and implement it.",10
"Improvements to logging in xrootd
Improve logging in xrootd to make it more compatible with qserv logging.",6
"Fit Visualizer porting: Begin
Display fits image, server round trip, organize initial data structures, initial render",16
"FITS Visualizer porting: group, group scrolling
nan",6
"FITS Visualizer porting: zooming: group zooming, zoom fit, zoom fill, active plot selection
nan",6
"Upgrade to react 0.14.3
nan",2
"Fit Visualizer porting: create toolbar
nan",6
"FITS Visualizer porting: Add canvas drawing infrastructure
nan",10
"Fit Visualizer porting: Distance Tools
nan",4
"Fit Visualizer porting: Drawing Target Center 
nan",3
"FITS Visualizer porting: Marker tool
improve the menu organization from the current one when doing the migration.   The implementation includes:  * DrawObj for marker * Marker dropdown list under the marker icon to show the marker and footprint items (will be added later). * Marker drawing layer rendering and operation including action creator and action dispatch functions. * Marker UI component shown in Drawing Layers popup. The implementation set up some work which can be similarly expanded to footprints in the future. * Title of marker layer on the layer control is like ""show: <title>"".   The title change as the text you enter for the label. * the mouse changes to a pointer once the cursor moves onto the marker, and the mouse becomes a resize sign when the cursor moves on the handler of the marker. * When the cursor becomes a pointer, the marker can be relocated by dragging the mouse, and when the cursor becomes a resize one, the marker can be resized by dragging the mouse. * the marker size changes as the image is zoomed.     Maker drawing layer operation:  * select 'Add Marker' from dropdown list to add a new drawing layer * click anywhere to locate the newly added marker * click and drag the mouse to relocate or resize the marker: for relocation: click and drag inside the circle, then drag the mouse. for resize: click inside the circle, then click and drag the handler to resize * label and its location are set from Marker tool UI  ",10
"FITS Visualizer porting: Grid drawing
nan",10
"FITS Visualizer porting: Catalog drawing
nan",6
"FITS Visualizer porting: Region Drawing
region drawing for the following regions,   circle, ellipse, box, polygon, point (circle, box, diamond, cross, x, arrow, boxcircle), line, text, annulus, box annulus, ellipse annulus. (annulus is made for the case with at least two repeated regions of the same type).    add the following functions to file ShapeDataObj.js    makeRectangleByCenter, makeEllipse  drawEllipse,  makeTextLocationRectangle, makeTextLocationEllipse  update drawRectangle by adding the case which is given the rotating angle and rectangle center.",10
"FITS Visualizer porting: Mouse Readout: part 1: projection
show the fits XY readout so that it update the position in the users selected coordinate system. The readout should also show the plot title.    Write the dialog to change the coordinate system readout.    Also show the pixel size and write the dialog that will change it between pixel size and screen pixel size.    [1/28/16]  Move this ticket to the next sprint.  I cannot get it done in this sprint because  # Spent time to work on other two tickets  # Take off from work  # It takes more time than estimates since I am not familiar with reducer and store etc.  More study is needed.      ",15
"FITS Save Dialog
nan",10
"FITS Visualizer porting: Rotate
nan",2
"FITS Visualizer porting: North/East Arrow
Add the north and east arrow like the gwt system has.",10
"FITS Visualizer porting: Layer Control Popup
nan",6
"FITS Visualizer porting: Stretch Pulldown
nan",1
"FITS Visualizer porting: Color bar pulldown
nan",1
"FITS Visualizer porting: Restore to defaults & re-center
nan",1
"FITS Visualizer porting: Show FITS Header
Task involves several steps:    * Server side: VisServerCommands.Header needs to change to check to the JSON_DEEP parameter. In this case the return from  VisServerOps.getFitsHeaderInfo should be converted into a format that the new javascript tables should understand (see loi how to get this format). Look at VisServerCommands.AreaStat for an example.  * Client side: a call to the server: need to add getFitsHeaderInfo into PlotServiceJson.js. For reference, look at other calls in  PlotServiceJson.js and the java version of the getFitsHeaderInfo PlotServiceJson.java  * Client side: when header toolbar button pushed then make the call to the server.  * Client side: when server call promise is resolved then show a dialog with the table data. Remember 3 color images should have a tab per color.",6
"FITS Visualizer porting: Flip
nan",1
"FITS Visualizer porting: Expanded View
nan",4
"FITS Visualizer porting: Expanded Single
nan",4
"FITS Visualizer porting: Expanded View : WCS Match
nan",6
"FITS Visualizer porting: Expanded View: Grid
nan",6
"FITS Visualizer porting: Crop
nan",4
"Fit Visualizer porting: Select Area
nan",4
"FITS Visualizer porting: Statistics - part 1
dialog only",4
"FITS Visualizer porting: selecting points of catalog from image view, showing selected points
able to draw a rectangle on the image, and select the catalog entries overlaid on the image",5
"FITS Visualizer porting: Image Select Panel/Dialog
Converting the image select dialog/panel is a very big job and should be to be broken up into several tickets: Each ticket should reference this ticket as the base.    Panel includes the following:  * issa, 2mass, wise, dss, sdss tabs  * file upload tab, upload widget might have to be written  * url tab  * blank image tab  * target info reusable widget  * 3 color support - any panel should show for 3 times, for read, green, and blue in 3 color mode  * must be able to appear in a panel or dialog  * must add or modify a plot  * Allow to create version with most or less than the standard tabs. example - see existing wise 3 color or finder chart 3 color  * A plot might need to be tied to specific type of image select dialog, we need a way to tie a plotId to and non-standard image select panel.",1
"Fit Visualizer porting: Thumbnail
nan",3
"Fit Visualizer porting: Magnifier
nan",2
"Experimentation and testing of new SuperTask infrastructure
WBS deliberately left unset as this is tracking LOE work.",6
"Migrate prototype SuperTask code to upstream repository
The prototype SuperTask code is now ready to be moved from the experimental repo to the upstream {{pipe_base}} repo. This requires the commits to be squashed and tidied.",6
"makeDocs uses old style python
{{makeDocs}} is written in python 2.4 style. This ticket is for updating it to python 2.7.",1
"Improve reStructuredText documentation
Enhance docs by covering    - Images as links  - Table spans  - Abbreviations  - :file: semantics, etc.",2
"Quoting of paths in doxygen configuration files breaks makeDocs
In DM-3200 I modified {{sconsUtils}} such that all the paths used in {{doxygen.conf}} files were quoted so that spaces could be used. This change broke documentation building (DM-4310) because {{makeDocs}} did not expect double quotes to be relevant. This ticket is to fix {{makeDocs}} and to re-enable quoting of paths in config files.",4
"Assess DECam ISR up to currently implemented
Not all known ISR corrections are applied or implemented to DECam data yet. For example, no cross-talk, edge-bleed, non-linearity, sky pattern removal, satellite trail masking, brighter-fatter, or illumination correction.    But we have most of the basic ISR already. With what we already have, identify issues that would severely affect the quality of post-ISR processing.",3
